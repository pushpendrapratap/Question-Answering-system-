<?xml version="1.0" encoding="utf-8"?>
<comments>
  <row Id="5" PostId="5" Score="9" Text="this is a super theoretical AI question. An interesting discussion! but out of place..." CreationDate="2014-05-14T00:23:15.437" UserId="34" />
  <row Id="6" PostId="7" Score="4" Text="List questions are usually not suited for Stack Exchange websites since there isn't an &quot;objective&quot; answer or a way to measure the usefulness of an answer. Having said that, one of my recommendations would be MacKay's &quot;Information Theory, Inference, and Learning Algorithms.&quot;" CreationDate="2014-05-14T00:38:19.510" UserId="51" />
  <row Id="9" PostId="7" Score="3" Text="This question appears to be off-topic because it is asks for a favorite resource.  On other SE sites, this would immediately be closed.  Since this is a new site, we still have to decide if this is a valid question here" CreationDate="2014-05-14T01:16:12.623" UserId="66" />
  <row Id="12" PostId="15" Score="3" Text="This question is far too broad. It may be salvaged by restricting the question to a particular use case." CreationDate="2014-05-14T02:00:22.797" UserId="51" />
  <row Id="13" PostId="10" Score="2" Text="Nice one, @Nicholas... Another book from Hastie and Tibshirani is [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/), which is a bit gentler of an entry compared to ESL." CreationDate="2014-05-14T02:16:20.503" UserId="24" />
  <row Id="14" PostId="7" Score="0" Text="Fair enough regarding what constitutes a &quot;valid&quot; question, although on other SE sites this question would **not** be immediately closed as you've stated: e.g., [2495 votes](http://stackoverflow.com/questions/194812/list-of-freely-available-programming-books), [1440 votes](http://stackoverflow.com/questions/1711/what-is-the-single-most-influential-book-every-programmer-should-read), [168 votes](http://tex.stackexchange.com/questions/11/what-is-the-best-book-to-start-learning-latex), and so on. There's great interest for these kinds of questions, even if this isn't deemed the right place." CreationDate="2014-05-14T02:35:50.090" UserId="36" />
  <row Id="17" PostId="22" Score="3" Text="Yes, using 1-of-n encoding is valid too." CreationDate="2014-05-14T06:47:00.223" UserId="21" />
  <row Id="19" PostId="19" Score="6" Text="A nice article about when your data starts to be too big for normal usage chrisstucchio.com/blog/2013/hadoop_hatred.html" CreationDate="2014-05-14T07:48:10.370" UserId="115" />
  <row Id="20" PostId="14" Score="0" Text="As to the second part of your question, I have proposed a discussion in meta: http://meta.datascience.stackexchange.com/questions/5/should-we-adopt-a-self-study-tag-like-stats-se-in-scope-and-approach How that gets received could shape whether your proficiency concern is answerable or within scope." CreationDate="2014-05-14T08:00:35.420" UserId="53" />
  <row Id="21" PostId="31" Score="1" Text="What do you mean under &quot;interesting groups&quot;? Do you have some predefined important feature list?" CreationDate="2014-05-14T09:08:56.007" UserId="120" />
  <row Id="22" PostId="31" Score="0" Text="Interesting groups are any groups of size greater than some threshold an that are much bigger than other possible clusters." CreationDate="2014-05-14T09:11:09.143" UserId="118" />
  <row Id="24" PostId="31" Score="1" Text="It isn't clear how you will perform preparement steps of your data. But you should look at algorithms described at http://en.wikipedia.org/wiki/Anomaly_detection . If I were you, I've checked SVM method first" CreationDate="2014-05-14T09:31:01.977" UserId="120" />
  <row Id="26" PostId="37" Score="3" Text="+1 I pretty much appreciate the stress out on big data being not about *what is the size*, and rather about *what is the content (characteristics of)*." CreationDate="2014-05-14T10:49:41.527" UserId="84" />
  <row Id="29" PostId="51" Score="0" Text="Excellent! Anything similar on Windows? I suppose if I want to condition it on something like the size of the database I can have my R script check that at the beginning and either stop or continue generating the report. What if I add this to the beginning of the R script and then I want to run it every 15 minutes? Do I need 24x4 `crontab` commands? Or can I iterate it somehow?" CreationDate="2014-05-14T14:58:48.107" UserId="151" />
  <row Id="30" PostId="51" Score="1" Text="@rnorberg You can set a cron job to repeat in any periodic time pattern as long as the time period is constant, and greater than 1 minute. Read the man page for more details, or such google &quot;crontab&quot;. Lots of stuff available." CreationDate="2014-05-14T15:14:00.130" UserId="62" />
  <row Id="31" PostId="37" Score="2" Text="That is a very refreshing perspective. I have never heard this before, but it is very true. This suggests that SQL and NoSQL technologies are not competetive, but complementary." CreationDate="2014-05-14T15:16:37.650" UserId="157" />
  <row Id="32" PostId="46" Score="0" Text="Thanks for your answer. It looks like different from SVM. I'll survey it. :)" CreationDate="2014-05-14T15:32:16.760" UserId="63" />
  <row Id="35" PostId="31" Score="0" Text="I've read about SVM and I think its more about classification of newly created data after manual training on existing dataset - not about clustering existing data and finding abnormally big clusters. Am I right? If I am then this method isn't what I want." CreationDate="2014-05-14T16:15:01.610" UserId="118" />
  <row Id="37" PostId="46" Score="4" Text="Just a reminder that we don't encourage linking off-site to an answer because its easy for links to break, causing an otherwise useful community resource to instead turn into a dead end. It's always best to put the answer directly into your post." CreationDate="2014-05-14T17:30:36.967" UserId="41" />
  <row Id="38" PostId="41" Score="4" Text="In addition to the answers below a good thing to remember is the fact that most of the things you need from R regarding Big Data can be done with summary data sets that are very small in comparison to raw logs.  Sampling from the raw log also provides a seamless way to use R for analysis without the headache of parsing lines and lines of a raw log.  For example, for a common modelling task at work I routinely use map reduce to summarize 32 gbs of raw logs to 28mbs of user data for modelling." CreationDate="2014-05-14T17:45:41.430" UserId="92" />
  <row Id="40" PostId="24" Score="6" Text="If your scale your numeric features to the same range as the binarized categorical features then cosine similarity tends to yield very similar results to the Hamming approach above.  I don't have a robust way to validate that this works in all cases so when I have mixed cat and num data I always check the clustering on a sample with the simple cosine method I mentioned and the more complicated mix with Hamming.  If the difference is insignificant I prefer the simpler method." CreationDate="2014-05-14T17:53:54.897" UserId="92" />
  <row Id="41" PostId="57" Score="0" Text="hm - get data, format data(awk sed grep stuff), remove noise as first step, then go deeper. so is't *hard preprocessing* comes at first, if use your therms" CreationDate="2014-05-14T18:09:54.360" UserId="146" />
  <row Id="42" PostId="52" Score="1" Text="probably it's good idea to clear a bit, what you mean under *cleaning data* , looks a bit confusing for my opinion" CreationDate="2014-05-14T18:11:45.363" UserId="146" />
  <row Id="43" PostId="57" Score="0" Text="@MolbOrg Yes, that's what I meant. I called *hard preprocessing* the *scripting side*, and *soft preprocessing* the use of data mining algorithms that generally reduce the *size* of the problem (cleans up the database). I also noted that the *second part, hard preprocessing, actually comes prior to any other process*. If it's not very clear with such terms, I'd gladly consider any other suggestions to improve the answer." CreationDate="2014-05-14T18:20:28.510" UserId="84" />
  <row Id="44" PostId="57" Score="1" Text="ah yes, did not paid enough attention, *raw data preprocessing*. Tested atm - yes perl oneliner is 3times slower then grep ) for 3.5kk strings in 300MB, for perl it took 1.1 sec, for grep 0.31 sec . I saw article where points that perl regexp is slow, much slower then it may be in practice, (i suspect that is also for grep too) [http://swtch.com/~rsc/regexp/regexp1.html](http://swtch.com/~rsc/regexp/regexp1.html)" CreationDate="2014-05-14T18:34:15.603" UserId="146" />
  <row Id="46" PostId="57" Score="0" Text="@MolbOrg Nice reference! AFAIK, `grep` uses POSIX basic regex by default, and allows for extended POSIX regex when run as `grep -E`, and for PCRE when run as `grep -P`." CreationDate="2014-05-14T18:44:15.457" UserId="84" />
  <row Id="47" PostId="57" Score="0" Text="yes, I used both variants, default was 6 times faster(just two attempts because lost a window with results, and second pattern was not suitable for default grep), and in the link above states that grep uses different(rigth?) approach, did't mean it's new for me, but new angle on that stuff for sure, have to think about ))" CreationDate="2014-05-14T18:49:49.287" UserId="146" />
  <row Id="48" PostId="24" Score="1" Text="That sounds like a sensible approach, @cwharland.  On further consideration I also note that one of the advantages Huang gives for the k-modes approach over Ralambondrainy's -- that you don't have to introduce a separate feature for each value of your categorical variable -- really doesn't matter in the OP's case where he only has a single categorical variable with three values.  Better to go with the simplest approach that works." CreationDate="2014-05-14T19:54:45.137" UserId="14" />
  <row Id="49" PostId="69" Score="0" Text="Can you give an example of a workflow which is reproducible without being a replication?" CreationDate="2014-05-14T21:00:16.777" UserId="157" />
  <row Id="50" PostId="46" Score="1" Text="Agree with that. At this point, it barely exists as more than that link anyhow. I will add a link to the underlying project." CreationDate="2014-05-14T21:02:21.930" UserId="21" />
  <row Id="51" PostId="69" Score="0" Text="@JayGodse: Good question, first I'd suggest reading the update I posted to the question, which might make my use of the terms a bit clearer. To answer your question, **no reproduction of a workflow is a replication of the original workflow**; which is to say, I don't have an answer to your question, since it is unclear to me how to you execute a data science workflow that allows for reproducibility without simply being a replication (that is, an exact copy) of what has already been done?" CreationDate="2014-05-14T21:46:39.713" UserId="158" />
  <row Id="52" PostId="70" Score="0" Text="Right, I get that, so maybe my question is unclear. The intent of my question is how to build a workflow that allows for reproducibility. Meaning if replication requires a step-by-step guide for taking a given input and reaching a given output, how do you execute a data science workflow that allows for reproducibility without simply being a replication of what has already been done?  Meaning you run the build, and it executes the code pulls the input creates the output AND generates an abstraction in text that would allow the build to be reproducible." CreationDate="2014-05-14T22:14:32.813" UserId="158" />
  <row Id="53" PostId="70" Score="0" Text="Think of it as a [reproducibility documentation generator](http://en.wikipedia.org/wiki/Comparison_of_documentation_generators)." CreationDate="2014-05-14T22:16:44.943" UserId="158" />
  <row Id="54" PostId="70" Score="0" Text="+1 So, I've thought about it, and unable to think of a way to make the question more clear without make your answer invalid, nor would it be fair for me to ask you delete your answer so I'm able to delete my question. So, just going to leave things be, and sorry my question was unclear. Cheers!" CreationDate="2014-05-14T22:32:23.857" UserId="158" />
  <row Id="55" PostId="71" Score="2" Text="Snarky answer: almost always. There's a huge incentive to create Type 1 errors (i.e., &quot;false alarms&quot;) when analysts examine data, so almost all p-values you'll encounter are &quot;too&quot; small." CreationDate="2014-05-14T23:07:08.427" UserId="36" />
  <row Id="56" PostId="71" Score="7" Text="Just throwing this out there, but wouldn't this sort of question best be posed on [Cross Validated](http://stats.stackexchange.com/)?" CreationDate="2014-05-14T23:47:53.803" UserId="24" />
  <row Id="57" PostId="58" Score="1" Text="Open source and some wiki. It looks good. Thanks for your suggestion. :)" CreationDate="2014-05-15T01:05:14.973" UserId="63" />
  <row Id="58" PostId="70" Score="0" Text="@blunders: please change the question.  I would rather have my answer be invalid and your question answered :)." CreationDate="2014-05-15T01:37:11.243" UserId="178" />
  <row Id="59" PostId="70" Score="0" Text="Done, thank you!" CreationDate="2014-05-15T02:02:21.317" UserId="158" />
  <row Id="60" PostId="76" Score="1" Text="About how much tweets a &quot;run&quot; are we talking?" CreationDate="2014-05-15T07:02:00.873" UserId="115" />
  <row Id="61" PostId="71" Score="1" Text="@buruzaemon: Maybe.  I did a search, this is the closest match: http://stats.stackexchange.com/questions/67320/hypothesis-testing-with-big-data  There don't seem to be more than a handful of questions that touch on this." CreationDate="2014-05-15T08:55:18.250" UserId="26" />
  <row Id="64" PostId="78" Score="0" Text="thank you for posting a link to that paper" CreationDate="2014-05-15T11:01:32.950" UserId="59" />
  <row Id="65" PostId="76" Score="0" Text="It's hard to know without a range of tweets, 1000, 100,000 full datahose etc." CreationDate="2014-05-15T13:01:58.240" UserId="59" />
  <row Id="66" PostId="76" Score="1" Text="@Johnny000: [500 million Tweets a day](https://blog.twitter.com/2013/new-tweets-per-second-record-and-how); my understanding is that Twitter limits streams to vendors based on trust/need, but to insure the solution covers the current daily averages, the solution should account what is the max, or you're able to reference as the max via a reliable source other than yourself." CreationDate="2014-05-15T13:24:51.407" UserId="158" />
  <row Id="68" PostId="85" Score="0" Text="I would add that FDR and FER are used when you have many hypotheses tested simultaneously." CreationDate="2014-05-15T14:19:14.513" UserId="178" />
  <row Id="69" PostId="93" Score="0" Text="The context would be info trackable within a single site with a 3rd party cookie, via an iframe. The site would be ecommerce. I find google analytics mostly looks at IP, sometimes at useragent, and I am able to get very similar numbers from looking only at IP in a timeframe. But google analytics is known to over-report by 30% ish, depending on context" CreationDate="2014-05-15T14:57:50.803" UserId="116" />
  <row Id="70" PostId="93" Score="0" Text="Looking at visited product pages doesn't help much either, as the structure of the shop is such that it leads users down predetermined paths, leading to very similar behaviour" CreationDate="2014-05-15T14:59:59.857" UserId="116" />
  <row Id="71" PostId="93" Score="1" Text="Also, I am aware that ML does not fit in the context of this question. Rather, hard coded algorithms are used by most tracking solutions that offer sensible results. The last few degrees of accuracy, that would be achievable with ML are of less relevance, since this info is rather used for observing trends." CreationDate="2014-05-15T15:03:21.477" UserId="116" />
  <row Id="78" PostId="76" Score="1" Text="Here's more information, appear the &quot;firehose&quot; data feed is pricy, so guessing it would be more relevant to limit input to the volume produce via the [Twitter Streaming APIs](https://dev.twitter.com/docs/api/streaming) via the Public Stream; [guide to processing the data is here](https://dev.twitter.com/docs/streaming-apis/processing#Scaling)." CreationDate="2014-05-15T20:48:23.770" UserId="158" />
  <row Id="79" PostId="76" Score="0" Text="Public Stream is a random sample of the &quot;firehose&quot; and appears to 1% of its total volume; meaning I estimate the feed to be 5 million tweets per day, and spikes might reach 1432 tweets per second; appears the spike must be account for, otherwise the feed gets discounted." CreationDate="2014-05-15T20:49:10.247" UserId="158" />
  <row Id="80" PostId="7" Score="1" Text="@statsRus: Try posting a question like that to SO, and it'll be closed; these questions exists because they have historical significance, but they are not considered good, on-topic questions for Stack Exchange sites, so **please do not use them as evidence that you can ask similar questions here.**" CreationDate="2014-05-15T21:08:13.933" UserId="158" />
  <row Id="82" PostId="91" Score="0" Text="I edited the question to add another query." CreationDate="2014-05-16T02:48:03.413" UserId="189" />
  <row Id="83" PostId="92" Score="0" Text="First, I edited the question to add another query. Also: I imagine even with a significant minority pre-computed, the rest of the query should still take long time to complete. Besides, when the process is delegated from one machine to 100 machines, isn't the latency actually increased (network latency between machines, and total latency is maximum of the latencies of all machines)?" CreationDate="2014-05-16T02:52:12.253" UserId="189" />
  <row Id="84" PostId="91" Score="0" Text="@namehere I tried to address your edit; hope it helps answering the question." CreationDate="2014-05-16T04:29:25.523" UserId="84" />
  <row Id="85" PostId="101" Score="0" Text="A very good answer! For those reading, I'd like to add that in the case of 3rd party cookies, many safari mobile versions will not take those by default, and other browsers have the same in their pipelines. Keep those in mind and treat them separately." CreationDate="2014-05-16T05:10:18.347" UserId="116" />
  <row Id="86" PostId="52" Score="2" Text="Explaining further what cleaning data means would be helpful. In the context where I work, cleaning has nothing to do with formatting - I'd just call that parsing/importing - But rather it would mean talking noisy user data and verifying it for coherence. The techniques use are dataset specific, from simple statistical rules, to fuzzy algorithms, especially when the data is sparse." CreationDate="2014-05-16T05:17:27.677" UserId="116" />
  <row Id="87" PostId="102" Score="1" Text="Alot of browsergames use a documentdatabase like Mongo DB or Couch DB" CreationDate="2014-05-16T06:15:20.240" UserId="115" />
  <row Id="88" PostId="102" Score="1" Text="As @Johnny000 mentioned, there are ones like MongoDB and CouchDB, that are widely used for that purpose.  I would add that you should consider developer time as well when making the decision." CreationDate="2014-05-16T12:35:21.483" UserId="178" />
  <row Id="89" PostId="92" Score="0" Text="I mean that answering the query &quot;spaghetti diamond&quot;, which is a weird rare query, might be sped up by precomputed results for &quot;spaghetti&quot; and &quot;diamond&quot; individually. Intra-DC connections are very fast and low latency. An extra hop or two inside is nothing compared to the ~20 hops between your computer and the DC. The dominating problem in distributing work is the straggler problem; you have to drop results from some subset if they don't respond in time. These are all gross generalizations but point in the right direction." CreationDate="2014-05-16T13:07:54.197" UserId="21" />
  <row Id="90" PostId="61" Score="1" Text="Is your question actually whether there is a case where it's impossible to overfit?" CreationDate="2014-05-16T13:09:01.880" UserId="21" />
  <row Id="91" PostId="61" Score="0" Text="@SeanOwen: No, how would it be impossible to overfit?" CreationDate="2014-05-16T13:13:46.987" UserId="158" />
  <row Id="92" PostId="61" Score="0" Text="Agree, just checking as you asked if overfitting caused models to become worse regardless of the data" CreationDate="2014-05-16T13:14:24.237" UserId="21" />
  <row Id="93" PostId="61" Score="0" Text="Just to be clear, to me, you asking if it's &quot;impossible to overfit&quot; and &quot;overfitting caused models to become worse regardless of the data&quot; are completely different in my opinion; that said, as far as I know, both are impossible." CreationDate="2014-05-16T13:49:13.207" UserId="158" />
  <row Id="94" PostId="101" Score="1" Text="Cookie churn is quite the problem for services that don't require log in. Many users simply don't understand cookies though so you are likely to have some cohort that you can follow for an appreciable amount of time." CreationDate="2014-05-16T14:44:37.800" UserId="92" />
  <row Id="95" PostId="103" Score="2" Text="I wonder why you emphasized that you do **not** have a distance. I'm not an expert here, but wonder whether it should not be possible to convert such a similarity into a distance, if required, basically by considering its inverse. Regardless of that, I doubt that there are clustering algorithms that are completely free of parameters, so some tuning will most likely be necessary in all cases. When you considered k-Means, can one assume that you have real-valued properties (particularly, that you *can* take the &quot;mean&quot; of several elements)?" CreationDate="2014-05-16T16:28:18.287" UserId="156" />
  <row Id="96" PostId="103" Score="4" Text="You don't need to know k to perform k means. You can cluster with varying k and check cluster variance to find the optimal.  Alternatively you might think about going for Gaussian mixture models or other restaraunt process like things to help you cluster." CreationDate="2014-05-17T00:12:00.940" UserId="92" />
  <row Id="97" PostId="102" Score="1" Text="You may want to consider a graph database that will help you manage relationships much faster. My favorite is OrientDB, which we use to manage a social network." CreationDate="2014-05-17T03:51:36.990" UserId="70" />
  <row Id="98" PostId="20" Score="3" Text="For anything social networking, I would **highly** recommend a graph database like [Neo4j](http://neo4j.org) or [OrientDB](http://orientechnologies.com)" CreationDate="2014-05-17T04:21:42.693" UserId="70" />
  <row Id="99" PostId="112" Score="0" Text="Thanks for the suggestion. I've looked at neo4j earlier but never at orientdb. Currently I can't envision a lot of benefit in modelling leadeboard data as graph but I will still look at streaming options in orientdb" CreationDate="2014-05-17T08:15:16.490" UserId="200" />
  <row Id="100" PostId="113" Score="1" Text="This page http://www.mongodb.com/nosql-explained provides some details about it" CreationDate="2014-05-17T08:29:53.633" UserId="211" />
  <row Id="102" PostId="115" Score="2" Text="I doubt there's any general API for this. You can try crawling various services likes Academia.edu, publishers' sites and so on. Nevertheless, it would be easier to build a local database of documents first, and then experiment with extracting the abstracts." CreationDate="2014-05-17T08:55:28.927" UserId="173" />
  <row Id="104" PostId="103" Score="0" Text="@Marco13 good point. Sometimes it works like that, but not in all cases, making sure that you keep the properties (e.g. triangle inequality). Regarding the mean of several elements, I can't think how I can do that, or if it is possible. Good point, too!" CreationDate="2014-05-17T09:01:14.093" UserId="113" />
  <row Id="105" PostId="103" Score="0" Text="@cwharland Veru useful comment. Doesn't the initial choice of k, though influence the result? I will look at the Gaussian mixture models. Thanks!" CreationDate="2014-05-17T09:02:59.783" UserId="113" />
  <row Id="106" PostId="115" Score="0" Text="Thanks for your answer! I have already built a local database for this. The problem of crawling from various services is that I have to make parse rules for each website." CreationDate="2014-05-17T09:05:02.373" UserId="212" />
  <row Id="107" PostId="115" Score="0" Text="So, how about converting PDFs to TXTs and then extracting the abstracts with regular expressions?" CreationDate="2014-05-17T09:35:04.733" UserId="173" />
  <row Id="108" PostId="115" Score="0" Text="thx! However, the contract states that massive download of papers are not allowed. This creates some headache." CreationDate="2014-05-17T11:39:36.970" UserId="212" />
  <row Id="109" PostId="115" Score="2" Text="I think this stack-overflow answer [link](http://stackoverflow.com/questions/14530019/avoiding-google-scholar-block-for-crawling) gives the best answer I can get. Maybe people who encounter this problem could also have a look at this page." CreationDate="2014-05-17T11:55:48.903" UserId="212" />
  <row Id="111" PostId="103" Score="2" Text="I asked the questions for a specific reason: **If** you could apply k-Means, but the only problem was finding the initial &quot;k&quot;, then you could consider a http://en.wikipedia.org/wiki/Self-organizing_map as an alternative. It has some nice properties, and basically behaves &quot;similar&quot; to k-Means, but does not require the initial &quot;k&quot; to be set. It's probably not an out-of-the-box solution, because it has additional tuning parameters (and the training may be computationally expensive), but worth a look nevertheless." CreationDate="2014-05-17T16:07:09.590" UserId="156" />
  <row Id="112" PostId="103" Score="2" Text="The initial choice of k does influence the clustering results but you can define a loss function or more likely an accuracy function that tells you for each value of k that you use to cluster, the relative similarity of all the subjects in that cluster.  You choose the k that minimizes variance in that similarity.  GMM and other dirichlet processes take care of the not-knowing-k problem quite well.  One of the best resources I've ever seen on this is [Edwin Chen's tutorial](http://blog.echen.me/2012/03/20/infinite-mixture-models-with-nonparametric-bayes-and-the-dirichlet-process/)." CreationDate="2014-05-17T16:30:06.077" UserId="92" />
  <row Id="113" PostId="116" Score="1" Text="When you say &quot;many more features than samples&quot; I assume you mean the unique number of liked sites is &gt;&gt; num users.  Is that also the case for the root domain of the sites?  i.e. are they a number of youtube.com or cnn.com urls in the sites or are they already stemmed to domain?  I'm leaning towards dimensionality reduction by collapsing URLs to domain roots rather than specific pages if it's possible." CreationDate="2014-05-17T18:12:20.767" UserId="92" />
  <row Id="114" PostId="116" Score="0" Text="Thanks for answer. The number of features (unique liked sites) is 32k, while the number of samples (users) is 12k. The features are Facebook Pages, so there's no need to stem the URLs. A user may either like facebook.com/cnn or not. I like the idea of trying to estimate users' age based on the links they share, though :)" CreationDate="2014-05-17T18:29:41.743" UserId="173" />
  <row Id="115" PostId="116" Score="0" Text="Ahhh, I misread the liked sites description.  Thanks for the clarification." CreationDate="2014-05-17T18:47:06.270" UserId="92" />
  <row Id="116" PostId="103" Score="4" Text="Just a thought: If your similarity score is normalized to _1_, than `1-sim(ei, ej) = Distance`. With distance metric you may apply for example hierarchical clustering. Going down from the root you will see at what level of granularity clusters would make sense for your particular problem." CreationDate="2014-05-18T02:16:46.537" UserId="31" />
  <row Id="118" PostId="76" Score="1" Text="I'd suggest Apache Kafka as message store and any stream processing solution of your choice like Apache Camel or Twitter Storm" CreationDate="2014-05-18T11:31:19.250" UserId="118" />
  <row Id="119" PostId="121" Score="0" Text="Hi, your answer is quite similar to my actual strategy. I used `sklearn.neighbors.KNeighborsRegressor` with cosine metric on SVD-reduced space (after applying SVD the average estimation error went down from ~6 years to ~4).&#xA;&#xA;Users in my database are aged 18-65 (older users were filtered out), so there are 48 possible classes. I wonder whether that's not too many classes for kNN, and whether I should treat it as regression or a classification problem (I think both are applicable)." CreationDate="2014-05-18T12:09:17.057" UserId="173" />
  <row Id="120" PostId="76" Score="0" Text="+1 @KonstantinV.Salikhov: Please post your comment as an answer, thanks!" CreationDate="2014-05-18T13:09:02.180" UserId="158" />
  <row Id="122" PostId="113" Score="0" Text="It depends on what kind of performance you are interested in: being able to handle a very large number of concurrent requests, being able to retrieve a specific record among a very large amount of records, being able to compute complex summary values from the data, etc?" CreationDate="2014-05-18T13:43:52.580" UserId="172" />
  <row Id="123" PostId="134" Score="2" Text="http://blog.mongodb.org/post/57611443904/mongodb-connector-for-hadoop" CreationDate="2014-05-18T14:16:42.150" UserId="118" />
  <row Id="124" PostId="126" Score="0" Text="Sorry for duplicate question, I search with a phrase and don't see other question, excuse me. Thanks for you answer. And, do you have a use case, personal experience, etc?" CreationDate="2014-05-18T16:07:01.423" UserId="109" />
  <row Id="125" PostId="138" Score="1" Text="Can you clarify the exact question? Maybe some possible answers you have in mind can also help." CreationDate="2014-05-18T19:10:48.500" UserId="227" />
  <row Id="127" PostId="159" Score="0" Text="Although this is a very nice and interesting question, I guess it will raise rather primarily opinion-based answers." CreationDate="2014-05-18T19:51:54.947" UserId="84" />
  <row Id="128" PostId="159" Score="0" Text="Please define long-term, computer science itself really is not that old." CreationDate="2014-05-18T20:00:18.810" UserId="158" />
  <row Id="129" PostId="155" Score="10" Text="This question might be more appropriate on the dedicated [opendata.SE](http://opendata.stackexchange.com/). That said, I cross my fingers for [dat](http://usodi.org/2014/04/02/dat), which aspires to become a &quot;Git for data&quot;." CreationDate="2014-05-18T21:23:52.687" UserId="216" />
  <row Id="130" PostId="135" Score="3" Text="R is a pleasure to work with for data manipulation (`reshape2`, `plyr`, and now `dplyr`) and I don't think you can do better than `ggplot2`/`ggvis` for visualization" CreationDate="2014-05-18T21:52:42.273" UserId="236" />
  <row Id="131" PostId="161" Score="0" Text="Sorry for the misunderstanding. My intention was to bring up answers concerning the importance of having control over an application, and how this control is *loosened* by libraries. Of course you can assume things about them (people don't normally rewrite pthreads), but if the data changes (load, throughput, ...), you may need to access the lib source to grant performance. And yes, it is not necessarily C/C++ -- although they're usually the languages chosen for hpc. May I delete my question, or would you like to change it into something more specific? I accept any suggestions to improve it." CreationDate="2014-05-18T21:56:12.430" UserId="84" />
  <row Id="132" PostId="138" Score="0" Text="@AmirAliAkbari SeanOwen posted an answer and I noticed the lack of specificity in my question. I've added a comment to his post. Please, feel free to suggest any improvements on the post -- I'm planing to delete it, otherwise." CreationDate="2014-05-18T21:59:09.827" UserId="84" />
  <row Id="133" PostId="159" Score="0" Text="While the &quot;Data&quot; subject still alive in digital world, It's scientific improvements / researchs stay alive ;)" CreationDate="2014-05-18T23:38:07.320" UserId="229" />
  <row Id="134" PostId="131" Score="0" Text="thanks for your answer." CreationDate="2014-05-19T07:18:05.963" UserId="212" />
  <row Id="135" PostId="120" Score="0" Text="thanks a lot. However arXiv does provide the papers I need." CreationDate="2014-05-19T07:18:44.240" UserId="212" />
  <row Id="136" PostId="140" Score="0" Text="I've read some basics about Apache Storm, it looks like it's concerned about issues related to scalability/reliability of stream processing, leaving you to handle the actual algorithms. Esper on the other handle process data for you based on your queries" CreationDate="2014-05-19T07:36:33.670" UserId="200" />
  <row Id="137" PostId="135" Score="0" Text="@pearpies As said in the beginning of my answer, I admit the good libraries available for R, but as a whole, when considering all areas needed for big data (which I as said a few of them in the answer), R is no match for the mature and huge libraries available for Python." CreationDate="2014-05-19T08:08:53.207" UserId="227" />
  <row Id="138" PostId="155" Score="2" Text="@ojdo Thanks, I never heard of opendata.SE before, I also found [this](http://opendata.stackexchange.com/q/266/2872) interesting (and very similar) question there." CreationDate="2014-05-19T08:28:56.713" UserId="227" />
  <row Id="139" PostId="165" Score="0" Text="Thank you very much Rapaio :) The points you gave me are very useful and gets something clearer..Since I'm a .NET developer and curious one on plain C (i start to learn) and new, fast, reliable, scalable ancd of course fully controllable -in a short term : very excited- techniques..So i need to learn very much..To learn, i try to read so many documents but as you can guess i'm at the start-line..&#xA;I didn't know that BTree has advantages on disk (In .Net world, so many writers explain it like : A hierarchical data structure like Linked-List..No More!) Thank you very much again" CreationDate="2014-05-19T11:41:09.810" UserId="229" />
  <row Id="140" PostId="165" Score="0" Text="And if you permit me, until there is a higher quality explanation / answer than yours, i want to accept this as answer.. And BTW, Lucene.NET is a .NET implementation of Java's Lucene" CreationDate="2014-05-19T11:45:21.380" UserId="229" />
  <row Id="141" PostId="84" Score="1" Text="While Bonferroni is definitely old-school it is still pretty popular. Related to it is a method called Šidák correction ( https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction ). I am calling it out, because in a large scale targeting advertising system I worked on we were able to implement this approach as a UDF in Hive. However this only works better when you have independence between tests. If not you have to fall back to Bonferroni or another method." CreationDate="2014-05-19T23:38:38.263" UserId="249" />
  <row Id="142" PostId="174" Score="1" Text="With LR you would have to make multiple models for age bins i think. How would compare two models for different age bins that predict the same prob on inclusion for a user?" CreationDate="2014-05-20T15:18:13.717" UserId="92" />
  <row Id="143" PostId="174" Score="1" Text="Note that LR fails when there are more variables than observations and performs poorly if the assumptions of the model is not met.  To use it, dimensionality reduction must be a first step." CreationDate="2014-05-20T17:06:04.223" UserId="178" />
  <row Id="144" PostId="59" Score="0" Text="Also see [Is the R language suitable for Big Data](http://datascience.stackexchange.com/q/41/227)." CreationDate="2014-05-20T17:42:19.287" UserId="227" />
  <row Id="145" PostId="135" Score="1" Text="[Peter](http://continuum.io/our-team) from Continuum Analytics (one of the companies on the [DARPA project referenced above](http://www.computerworld.com/s/article/9236558/Python_gets_a_big_data_boost_from_DARPA)) is working on some very impressive [opensource code](http://continuum.io/developer-resources) for [data visualization that simply do things that other sets of code are not able to do](http://bokeh.pydata.org/)." CreationDate="2014-05-20T18:46:45.713" UserId="158" />
  <row Id="146" PostId="174" Score="1" Text="@cwharland you should not consider the response variable to be categorical as it is continuous by nature, and discretized by the problem definition. Considering it categorical would mean telling the algorithm that predicting age 16 when it actually is 17 is as a serious error as predicting 30 when it actually is 17. Considering it continuous ensures that small errors (16 vs 17) are considered small and large errors (30 vs 17) are considered large. The logistic regression is used in this case to predict the continuous value and not estimate posterior probabilities." CreationDate="2014-05-20T19:28:01.027" UserId="172" />
  <row Id="147" PostId="174" Score="0" Text="@ChristopherLouden You are right that the vanilla version of logistic regression is not suitable for the 'large p small n' case, I should have mentioned that regularization is important in the present case. I update my answer. But  L1-regularized LR is a sort of feature selection so I consider no need for a preliminary FS step." CreationDate="2014-05-20T19:33:50.253" UserId="172" />
  <row Id="148" PostId="174" Score="0" Text="@damienfrancois: Agreed, thank you." CreationDate="2014-05-20T19:48:42.710" UserId="178" />
  <row Id="149" PostId="175" Score="1" Text="It may be easier to help if you show some two or three entries of your input file." CreationDate="2014-05-20T22:38:19.070" UserId="84" />
  <row Id="151" PostId="179" Score="1" Text="Hi, thanks for this tip, but I would prefer something that's easily publicable on the Web in a dynamic form. Also, I prefer free solutions, while Tableau - correct me if I am wrong - is only available as a trial version." CreationDate="2014-05-21T12:08:21.993" UserId="173" />
  <row Id="152" PostId="183" Score="0" Text="Thanks, I like this Python/networkx/matplotlib solution since it's my default working environment, and it's easy to make a gif out of this code. Still, something that looks nicer on the Web would beat this solution :)" CreationDate="2014-05-21T12:10:22.683" UserId="173" />
  <row Id="153" PostId="179" Score="0" Text="It also has &quot;Public&quot; edition, which means you have to store/share your results in the web, and can't save it locally." CreationDate="2014-05-21T12:10:59.243" UserId="97" />
  <row Id="154" PostId="187" Score="1" Text="Thanks, I misunderstood it. Now it is clear." CreationDate="2014-05-21T17:39:11.487" UserId="133" />
  <row Id="155" PostId="174" Score="0" Text="@damienfrancois: I definitely agree.  I'm just a little concerned that in this case LR will penalize intermediate values too harshly.  There's seem to be no motivation to map to a sigmoidal like curve given that you are not particularly interested in extreme age values.  Perhaps I'm misinterpreting the use though." CreationDate="2014-05-22T20:32:52.293" UserId="92" />
  <row Id="156" PostId="121" Score="0" Text="I can say, anecdotally, that I have use per class Random Forests to fit a number of classes individually then combined the results of each of those models in various ways.  In this case you might even think about assigning prior probabilities to each user's age with the kNN, then run through each class based model, use those scores to update the prior probabilities for each class and choose the most probable class from those posteriors.  It sounds like over complicating a bit but at worst you would have the kNN accuracy." CreationDate="2014-05-22T20:36:20.637" UserId="92" />
  <row Id="158" PostId="204" Score="2" Text="One interesting aspect of using the top 5000 sites is the fact that they may not be good at segmenting users on age.  The top sites, by construction, are ones that everyone visits.  They therefore are not very good at segmenting your users since all possible classifications (ages) have engaged with those sites.  This is a similar notion to the idf part of tf-idf.  idf helps filter out the &quot;everyone has this feature&quot; noise.  How do the most visited sites rank as features in your variable importance plots with your RF?" CreationDate="2014-05-24T04:59:04.420" UserId="92" />
  <row Id="159" PostId="209" Score="0" Text="There's no problem that -1 doesn't mean dislike. It's simply a way to differentiate that someone saw the item. In that sense it carries more info than a missing value. It may actually increase the accuracy of your recommendation. Depending on your distance metric in recommending you may consider changing it from a -1 to a slight metrics value so it doesn't influence the distance as much." CreationDate="2014-05-25T15:10:34.063" UserId="92" />
  <row Id="160" PostId="155" Score="2" Text="See http://www.quora.com/Where-can-I-find-large-datasets-open-to-the-public." CreationDate="2014-05-26T17:08:57.360" UserId="289" />
  <row Id="161" PostId="209" Score="0" Text="The canonical paper for implicit feedback is [Hu, Koren, and Volinsky](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.167.5120). Lots of good recommendations in there, including estimating your confidence in which -1 indicates a dislike or merely a &quot;didn't see.&quot;" CreationDate="2014-05-27T00:00:55.560" UserId="159" />
  <row Id="162" PostId="213" Score="0" Text="I see. I did study the README file, however I still can't figure out how the data it self can be read. For example, the train/X_train.txt'file represents training set (the sample data that I showed in post comes from this file)." CreationDate="2014-05-27T12:30:31.233" UserId="295" />
  <row Id="163" PostId="204" Score="1" Text="Good point. An easy fix for this would be to stratify the training dataset into J age bins (e.g., 13-16, 17-20, 21-24, etc.) and take the top (K/J) pages for each group. That would ensure you have significant representation for each group. There will certainly be some overlap across groups, so if you were really picky you might want to take the top (K/J) unique pages for each group, but I think that might be overkill." CreationDate="2014-05-27T13:37:09.423" UserId="250" />
  <row Id="164" PostId="213" Score="0" Text="It seems to me that the data set is rather wide, there are 561 variables per row which are listed inside of the features.txt file. I believe that is what you are referring to." CreationDate="2014-05-27T13:38:22.867" UserId="59" />
  <row Id="165" PostId="197" Score="0" Text="I had experimented with C45/J48 in a previous project. I did not realize there were rules I could retrieve from it. I'll also check out RIPPER. Thanks!" CreationDate="2014-05-27T13:53:22.987" UserId="275" />
  <row Id="166" PostId="213" Score="0" Text="So each of the variable from 'features' file corresponds to each column in e.g 'x-test.txt' file, or Am I wrong?" CreationDate="2014-05-27T14:40:54.283" UserId="295" />
  <row Id="167" PostId="213" Score="0" Text="That is how I am taking it. From what I could gather from the README is that is what the file contains." CreationDate="2014-05-27T14:53:03.520" UserId="59" />
  <row Id="168" PostId="213" Score="0" Text="@Jakubee Yes. There are 561 rows/variable names in the `features.txt` file, and 561 columns in the `X_train.txt` file, one for each variable." CreationDate="2014-05-27T14:53:30.540" UserId="156" />
  <row Id="169" PostId="213" Score="0" Text="Ok. I do get it now! Thanks!! But what about the data itself. What kind of analytics I can process on this? (That might be stupid question) You think I can apply this for future analysis with python, using e.g hadoop?" CreationDate="2014-05-27T15:08:46.697" UserId="295" />
  <row Id="170" PostId="213" Score="0" Text="You can definitely use Python, I don't know if Hadoop will be necessary for files of this size, you could also use R and install the package Rattle with the following command `install.packages(&quot;rattle&quot;)` it is a good data mining package for R. In the future if you are going to be using massive data sets then for sure Hadoop will be good to know and use" CreationDate="2014-05-27T15:12:05.937" UserId="59" />
  <row Id="171" PostId="212" Score="2" Text="I think imputing slight negative values for items that have been seen many times but never chosen is reasonable. The OP doesn't indicate they have access to data that qualifies these negative imputations but I wouldn't rule that tactic out entirely. The optimal magnitude of the negative value can be determined from the data.  I've had small gains from doing this in recsys scenarios. In any case...would you suggest other ways of differentiating between item seen once and not chosen vs seen N times and never chosen besides negative imputing?" CreationDate="2014-05-28T02:37:14.990" UserId="92" />
  <row Id="172" PostId="216" Score="1" Text="+1 automation doesn't necessarily make things better, only more consistently and often faster!" CreationDate="2014-05-29T13:52:18.137" UserId="297" />
  <row Id="173" PostId="161" Score="1" Text="No it's a fine question, you can reflect your comments here in edits to the question if you like." CreationDate="2014-05-29T13:57:43.450" UserId="21" />
  <row Id="174" PostId="161" Score="0" Text="Please, check if the question makes sense now. I've added a small case in order to make it more straightforward. In case you want to add some consideration in the question, please, feel free to edit it." CreationDate="2014-05-29T15:04:26.473" UserId="84" />
  <row Id="175" PostId="175" Score="0" Text="For next time, use [ASCII Delimited Text](https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/) if you are unsure of which delimiter to select." CreationDate="2014-05-30T06:16:20.733" UserId="227" />
  <row Id="176" PostId="179" Score="2" Text="@WojciechWalczak Maybe [gephi](http://gephi.org/) can be used instead of Tableau." CreationDate="2014-05-30T06:31:29.873" UserId="227" />
  <row Id="177" PostId="179" Score="0" Text="@Amir Ali Akbari, looks great thanks" CreationDate="2014-05-30T06:49:36.470" UserId="97" />
  <row Id="178" PostId="220" Score="0" Text="Thanks for your answer mate. unfortunately, if I transpose the matrix it will make the false calculation." CreationDate="2014-05-30T11:46:02.917" UserId="273" />
  <row Id="179" PostId="223" Score="0" Text="One of SGML's major purposes (the same holds for its offspring, XML) was to provide the means for tagging text documents (POS **and** semantic tags)." CreationDate="2014-05-30T20:47:32.157" UserId="318" />
  <row Id="180" PostId="175" Score="0" Text="@Rubens yes I will mock up some entries and edit this weekend." CreationDate="2014-05-31T06:56:25.183" UserId="249" />
  <row Id="181" PostId="175" Score="0" Text="@AmirAliAkbari unfortunately I cannot get the source system to change the format otherwise I wouldn't have the issue, and SASB on AIX has limited options, but thank you for the link." CreationDate="2014-05-31T07:00:48.387" UserId="249" />
  <row Id="182" PostId="182" Score="0" Text="Besides the metadata used to declare the hive table there is also information on the original informat from the SAS environment. Even if the case is more obscure than int, string, int I am hoping that that additional information could be used to get there if the table metadata is insufficient due to potential ambiguity (e.g. string, string, string)" CreationDate="2014-05-31T07:03:29.753" UserId="249" />
  <row Id="184" PostId="229" Score="0" Text="I referred to the processing via MapReduce in Hadoop Echo system as simply Hadoop because that's the term commonly used (Though technically wrong and I have changed the question accordingly)." CreationDate="2014-06-01T21:50:16.317" UserId="339" />
  <row Id="185" PostId="229" Score="0" Text="May be I am wrong but I think there is more to that than to just have near-real-time processing. If there were no trade-offs between them, everyone would have like to do things in near-real-time. A hybrid approach allows for getting the best of both worlds (to some extent). That's why Summingbird was created." CreationDate="2014-06-01T21:55:28.310" UserId="339" />
  <row Id="186" PostId="223" Score="0" Text="Could be more specific/restrictive about what kind of metadata you want to add? With your two examples, I doubt that there is a less verbose way that has the same generic expressiveness as XML tags." CreationDate="2014-06-01T22:28:33.520" UserId="216" />
  <row Id="188" PostId="229" Score="1" Text="A major difference is that a stream processing system can just touch data once, and by itself has no long-term state. Some problems can't be solved this way. For problems for which this is OK, it's faster to use a system that does not require first persisting data into (re-readable) storage. MapReduce is not inherently slower than Storm; both are containers. They are different paradigms for different problems." CreationDate="2014-06-01T22:53:58.930" UserId="21" />
  <row Id="189" PostId="229" Score="0" Text="By not having long-term persistent state does it mean that such near-real-time systems can not accumulate input updates over a long duration? Can you refer me to any resources that discuss further on this?" CreationDate="2014-06-01T23:07:22.743" UserId="339" />
  <row Id="190" PostId="229" Score="0" Text="This is kind of the definition of a streaming system. If you imagine a system that can access long-term state at will, it's not really streaming." CreationDate="2014-06-02T00:39:08.850" UserId="21" />
  <row Id="191" PostId="223" Score="0" Text="@ojdo The most of the meta-data is either for disambiguation (like the relative times), or for specifying special entities (i.e. FKs)." CreationDate="2014-06-02T16:59:48.337" UserId="227" />
  <row Id="192" PostId="220" Score="1" Text="Can you explain why you have to calculate such a giant covariance matrix? Would be easier to find a workaround if you could shed light on the motivation." CreationDate="2014-06-03T15:01:16.907" UserId="250" />
  <row Id="193" PostId="197" Score="0" Text="Also check out the C50 package in R." CreationDate="2014-06-06T14:56:59.903" UserId="375" />
  <row Id="194" PostId="232" Score="1" Text="When you say, when you sum results you can not interpret totals, you mean that each classification can have a different weight and its contribute can be over/under estimated in the total? If I suppose to run, e.g., 4 independent tests, may I assume that each classification has the same weight and interpret (painlessly) the totals? Hope it is clear.." CreationDate="2014-06-08T16:56:17.433" UserId="133" />
  <row Id="1194" PostId="224" Score="2" Text="Seems to be just a text reformatting problem, have you written any code yet?" CreationDate="2014-06-09T10:43:11.727" UserId="227" />
  <row Id="1196" PostId="223" Score="1" Text="I have used http://brat.nlplab.org/ in the past.  There is a nice interface for many different types of annotations.  The annotations are stored in a separate .annot file which is a list of the words that are annotoated and their position in the document." CreationDate="2014-06-09T13:09:03.373" UserId="387" />
  <row Id="1198" PostId="236" Score="0" Text="I would check out research by faculty and number of citations. Generally that is a good way to rank programs." CreationDate="2014-06-09T20:58:14.230" UserId="141" />
  <row Id="1199" PostId="208" Score="1" Text="I think you should note that for processes on a single machine you can memory map variables with joblib/Numpy. You lose that ability for processes on different machines." CreationDate="2014-06-09T21:55:20.937" UserId="403" />
  <row Id="1200" PostId="44" Score="0" Text="But does using R with Hadoop overcome this limitation (having to do computations in memory)?" CreationDate="2014-06-09T23:07:41.553" UserId="413" />
  <row Id="1201" PostId="188" Score="1" Text="That would still have mongo doing the processing, which I believe from the question is to be avoided in the final solution.  Giving you an upvote anyways for bringing up an important piece of knowledge." CreationDate="2014-06-10T04:15:12.013" UserId="434" />
  <row Id="1203" PostId="232" Score="1" Text="What I meant to convey is that we lose track of what the actual numbers mean. E.g., if I have 4 in a specific entry in run 1 and get 5 in that same entry on run 2, it's hard to say exactly what 4+5=9 means. I'd rather look at a distribution (%'s) or averages of where individuals fall across the matrix. It seems much more intuitive." CreationDate="2014-06-10T06:11:12.453" UserId="375" />
  <row Id="1204" PostId="251" Score="1" Text="+1 That may be my fault for being not very clear in my post, so others had not got it before. This is surely the kind of answer I was looking for. Thanks." CreationDate="2014-06-10T07:06:17.757" UserId="84" />
  <row Id="1205" PostId="218" Score="0" Text="Isn't this a programming question that should be on Stack OVerflow? Getting an error when computing a covariance is not data science." CreationDate="2014-06-10T08:15:35.330" UserId="471" />
  <row Id="1206" PostId="256" Score="1" Text="Thank you for the detailed answer. That is relaxing!" CreationDate="2014-06-10T08:29:43.067" UserId="456" />
  <row Id="1207" PostId="191" Score="0" Text="http://en.wikipedia.org/wiki/Winner-take-all" CreationDate="2014-06-10T08:45:00.260" UserId="11" />
  <row Id="1208" PostId="271" Score="0" Text="Interesting.  The reason I asked the question is that I wondered if something similar to the &quot;gambler's fallacy&quot; (or even gf itself).  I thought there might be a chance it had already been proven to be a fruitless venture.  Still - these other answers are intriguing." CreationDate="2014-06-10T11:56:17.943" UserId="434" />
  <row Id="1210" PostId="274" Score="5" Text="From personal experience I'd add that built-in documentation/label is huge. Now all my datasets can can be stored with explicit records of where they came from, sampling frequency, anomalies, etc. etc." CreationDate="2014-06-10T13:04:36.180" UserId="403" />
  <row Id="1212" PostId="272" Score="5" Text="+1 For Andrew Ng's course.  It is very well done." CreationDate="2014-06-10T15:12:40.480" UserId="533" />
  <row Id="1216" PostId="245" Score="0" Text="You are right, the data science PhD is not yet listed.  Machine learning has a lot math behind it, we can't just use an algorithm without proving the theory part why it converges, optimizes, etc..." CreationDate="2014-06-10T16:38:53.567" UserId="386" />
  <row Id="1217" PostId="241" Score="0" Text="As Andrew Gelman [mentions HERE](http://andrewgelman.com/2014/05/25/decided-physicist/), it's pointless to be a math major if you are the best.  I am not even that great at math, actually.  That's what led me to think of data science." CreationDate="2014-06-10T16:41:45.913" UserId="386" />
  <row Id="1218" PostId="291" Score="0" Text="Thanks, see my edit. I have a lot of coding experience and have taken MOOCs. I have a masters in Statistics and a minor in applied mathematics, I would consider math my biggest strength. I am really looking for things to put on a PhD application." CreationDate="2014-06-10T19:23:53.090" UserId="560" />
  <row Id="1219" PostId="248" Score="0" Text="You have very good points.  But, imagine you are Data Science PhD grad, which journal will you publish in?  Stat, CS, journal of machine learning?  Each one of those fields has its own experts." CreationDate="2014-06-10T19:31:58.813" UserId="386" />
  <row Id="1220" PostId="135" Score="5" Text="This answer seems to be wholly anecdotal and hardly shows anywhere where R is weak relative to Python." CreationDate="2014-06-10T20:31:38.787" UserId="598" />
  <row Id="1221" PostId="291" Score="2" Text="Then write some papers and get them published in a good conference: that's the best signal that you are fit for research--and a PhD program. Maybe you can use your economics background to write a paper on [multi-agent learning](http://mitpress.mit.edu/books/theory-learning-games). You don't have to stick to the same subject once you get accepted; it's just to demonstrate your ability." CreationDate="2014-06-10T20:59:51.017" UserId="381" />
  <row Id="1222" PostId="295" Score="0" Text="Thanks for the answer. I have a minor in applied mathematics and a masters in statistics. I have been taking graduate math courses for the last two years, as I did my masters in statistics. Are there any specific classes I should take? I have taken my calc sequence, linear algebra, differential equations, fourier analysis, stochastic processes, advanced probability, statstical inference, bayesian analysis, time series and a few others. Any others in particular" CreationDate="2014-06-10T21:42:57.950" UserId="560" />
  <row Id="1226" PostId="289" Score="2" Text="He said it specifically about research job though." CreationDate="2014-06-10T23:58:32.280" UserId="615" />
  <row Id="1228" PostId="295" Score="0" Text="Statistics MS/MA is offered everywhere these days, they don't help you get into a stat PhD. Stat PhD is looking for solid math undergrads: real analysis, optimization, numerical analysis. CS PhD is looking for cs and math undergrad. Why don't you continue on economics?" CreationDate="2014-06-11T00:12:25.993" UserId="386" />
  <row Id="1230" PostId="253" Score="1" Text="This class of questions is being discussed on meta. You can voice your opinion on this [meta post.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:19:34.887" UserId="62" />
  <row Id="1232" PostId="234" Score="0" Text="This class of questions is being discussed on meta. You may voice your opinion [here.](http://meta.datascience.stackexchange.com/q/41/62)" CreationDate="2014-06-11T02:22:39.393" UserId="62" />
  <row Id="1234" PostId="44" Score="0" Text="RHadoop does overcome this limitation.  The tutorial here: https://github.com/RevolutionAnalytics/rmr2/blob/master/docs/tutorial.md spells it out clearly.  You need to shift into a mapreduce mindset, but it does provide the power of R to the hadoop environment." CreationDate="2014-06-11T06:34:50.310" UserId="434" />
  <row Id="1235" PostId="308" Score="1" Text="Thank you. This is very helpful. I know its a bug space and there is no one right answer. I am very interested to know how one selects big data tools and technology to suit their needs. I am not marking this as the right answer for now but it certainly deserve lot of UP votes. Cheers :)" CreationDate="2014-06-11T07:40:08.317" UserId="496" />
  <row Id="1236" PostId="306" Score="3" Text="I assume that the OP's reference to TBs means &quot;for data on the small end of what you might use Hadoop for.&quot;  If you have multiple petabytes or more, Redshift clearly isn't suitable. (I believe it's limited to a hundred 16TB nodes.)" CreationDate="2014-06-11T07:47:01.247" UserId="14" />
  <row Id="1237" PostId="309" Score="2" Text="`easier to develop because of Redshift's maturity` contradicts with `Redshift isn't that mature yet` so what is your verdict?" CreationDate="2014-06-11T08:41:41.310" UserId="646" />
  <row Id="1238" PostId="309" Score="0" Text="@M.Mimpen: Edited answer to be more specific" CreationDate="2014-06-11T08:56:18.213" UserId="638" />
  <row Id="1239" PostId="287" Score="0" Text="Thanks for your answer, I will read it carefully." CreationDate="2014-06-11T09:09:30.187" UserId="133" />
  <row Id="1240" PostId="287" Score="0" Text="What do you mean for misclassification ratio in the fourth paragraph?" CreationDate="2014-06-11T09:14:36.593" UserId="133" />
  <row Id="1241" PostId="287" Score="0" Text="misclassification ratio = (number of instances correctly classified)/(total number of instances); in that paragraph we have 0.33 = proportion of each class (let's name labels as c1, c2, c3); we have 0.33*1.0 (c1 are all correctly classified), + 0.33*0.5 (c2 are random classified as c2 or c3) + 0.33*0.5 (c3 are random classified as c2 or c3) = 0.33 + 0.166 + 0.166 = 0.66 (instances classified correctly/total number of instances)" CreationDate="2014-06-11T09:36:33.673" UserId="108" />
  <row Id="1242" PostId="19" Score="13" Text="&quot;Anything too big to load into Excel&quot; is the running joke." CreationDate="2014-06-11T12:07:51.477" UserId="471" />
  <row Id="1244" PostId="313" Score="0" Text="Asking about &quot;good&quot; books will attract opinion-based answers and so this is off-topic. Flagged." CreationDate="2014-06-11T14:32:37.843" UserId="471" />
  <row Id="1245" PostId="313" Score="3" Text="I've changed it so I am just looking for books. Nothing opinion-based." CreationDate="2014-06-11T14:34:15.243" UserId="663" />
  <row Id="1246" PostId="313" Score="0" Text="It's spelled S-t-a-t-i-s-t-i-c-s :)   Stick with something pragmatic that focuses on prediction rather than inference. Both _Elements of Statistical Learning_ and _An Introduction to Statistical Learning_ are on most people's lists." CreationDate="2014-06-11T15:17:28.750" UserId="515" />
  <row Id="1248" PostId="291" Score="0" Text="Thank you, that is the best advice I have received." CreationDate="2014-06-11T16:07:35.017" UserId="560" />
  <row Id="1249" PostId="295" Score="0" Text="When I left undergrad I was 12 credit hours short of a math major. After I finished my MS in stats I could have pursued a PhD where I got my MS(top 30 school), however I am more interested in ML. I really don't think my math background will be a problem, as I feel it is very strong. I left economics and went to pure statistics in graduate school because economics no longer interested me, so that is definitely out. So do you think I should try to finish a math undergrad? It would take less than two semesters" CreationDate="2014-06-11T16:15:30.020" UserId="560" />
  <row Id="1250" PostId="316" Score="1" Text="+1 Thank you for taking the time to improve upon the existing answer, and given in my opinion your answer is now the better answer, I've selected your answer as the answer. Cheers!" CreationDate="2014-06-11T17:35:10.527" UserId="158" />
  <row Id="1251" PostId="316" Score="0" Text="Glad to answer!" CreationDate="2014-06-11T19:01:09.080" UserId="514" />
  <row Id="1252" PostId="305" Score="0" Text="Do you mean Hadoop or do you mean a specific counterpart to Redshift, like Impala?" CreationDate="2014-06-11T19:17:18.213" UserId="21" />
  <row Id="1253" PostId="311" Score="1" Text="I've already implemented feature extraction, and a toolkit for it (publication awaits some bugchecking)." CreationDate="2014-06-11T19:18:26.090" UserId="555" />
  <row Id="1254" PostId="272" Score="1" Text="John Hopkins also has a data science certificate track (9 classes) that started last week at Coursera.  https://www.coursera.org/specialization/jhudatascience/1?utm_medium=listingPage - it's not all machine learning, but worth sharing.  Coursera is full of awesomeness (and Andrew Ng is a great lecturer)." CreationDate="2014-06-11T20:49:17.350" UserId="434" />
  <row Id="1257" PostId="324" Score="1" Text="RODBC is for connecting to relational databases - often ones that are on your premises. You question alludes to  connecting to public API - that is something else." CreationDate="2014-06-12T03:33:12.170" UserId="366" />
  <row Id="1258" PostId="303" Score="0" Text="actually the ppt and the SO site you've linked to have nothing to do with the phrase alignments. It only achieves the word alignments that I have showed in the original post. =(" CreationDate="2014-06-12T04:38:42.697" UserId="122" />
  <row Id="1259" PostId="324" Score="2" Text="You're asking two questions. Please post them separately. You'd better flesh out the part that comes before sentiment analysis too." CreationDate="2014-06-12T05:12:19.767" UserId="381" />
  <row Id="1265" PostId="339" Score="4" Text="R hardly beats python in visualization. I think it's rather the reverse; not only does python have [ggplot](https://github.com/yhat/ggplot/) (which I don't use myself, since there are more pythonic options, like [seaborn](http://stanford.edu/~mwaskom/software/seaborn/)), it can even do interactive visualization in the browser with packages like [bokeh](http://bokeh.pydata.org)." CreationDate="2014-06-12T15:57:49.973" UserId="381" />
  <row Id="1266" PostId="343" Score="1" Text="Deep learning is about increasing the _number_ of hidden layers. Otherwise it would be called fat learning :)" CreationDate="2014-06-12T16:00:20.453" UserId="381" />
  <row Id="1267" PostId="24" Score="2" Text="Good answer. Potentially helpful: I have implemented Huang's k-modes and k-prototypes (and some variations) in Python: https://github.com/nicodv/kmodes" CreationDate="2014-06-12T16:08:03.140" UserId="554" />
  <row Id="1271" PostId="203" Score="1" Text="Thank you for your post, but can you include some of the essential information to answer the question here? The material you linked is a great way to support the information in your answer, but this site was created to compile a great collection of questions with answers. Posts that send users elsewhere to find that information are not really considered &quot;answers&quot; here. Thanks." CreationDate="2014-06-12T16:56:07.723" UserId="50" />
  <row Id="1272" PostId="320" Score="0" Text="Good answer. Realize that 2, 3, 4 can interact in complex ways, though. Debugging could be done by checking the activation values of the ANN, the magnitude of the weights of the ANN, keeping an eye on the in-sample and out-of-sample error and convergence of the optimizer, etc." CreationDate="2014-06-12T16:59:43.233" UserId="554" />
  <row Id="1274" PostId="339" Score="0" Text="ggplot was built in 2005 and it continues to be the favorite of many researchers due to its intuitive nature and well defined grammar of visualizations. Seaborn is built on top of matplotlib and it could be adopted by many in the coming years but ggplot still leads the charts in statistical data analysis visualizations. ggplot for python is so unpythonic in nature. The API is directly taken from the R implementation. Coming to interactive visualisations part, both python and R have d3js and d3py to take leverage of interactive visualisations." CreationDate="2014-06-12T17:12:15.527" UserId="514" />
  <row Id="1275" PostId="343" Score="0" Text="@Emre definitely meant that. Curse my punctuation!" CreationDate="2014-06-12T18:08:53.677" UserId="754" />
  <row Id="1276" PostId="341" Score="0" Text="Apparently most of the people are using Hadoop for analytics. What I am thinking is do I need something like that or knowledge about database, ML, statistics is enough?" CreationDate="2014-06-12T18:26:34.837" UserId="456" />
  <row Id="1278" PostId="295" Score="0" Text="No, you should not back dig for that math major, but take courses you need like real analysis, and optimization.  I know these courses sound irrelevant, but PhD programs want to see it, please them.  They want to know do you have the theories down.  They don't worry if you don't understand neural network well.  As Prof. LeCun said, take as many math courses as you can." CreationDate="2014-06-12T20:46:56.670" UserId="386" />
  <row Id="1280" PostId="339" Score="5" Text="Also R has the ability to interactive viz with Shiny." CreationDate="2014-06-12T22:04:23.300" UserId="598" />
  <row Id="1281" PostId="350" Score="0" Text="This is an example of a career question. Please join in on the [discussion here](http://meta.datascience.stackexchange.com/q/41/62) and voice your opinion if you feel this should be on-topic." CreationDate="2014-06-13T01:41:26.040" UserId="62" />
  <row Id="1282" PostId="334" Score="3" Text="This is too broad and primarily opinion based. Please take a look at http://datascience.stackexchange.com/help/dont-ask" CreationDate="2014-06-13T01:44:05.283" UserId="62" />
  <row Id="1283" PostId="334" Score="3" Text="@AsheeshR - We're averaging 2 questions a day and 2 answers per question.  At this point the focus needs to be on encouraging participation and increasing interest." CreationDate="2014-06-13T04:29:41.007" UserId="434" />
  <row Id="1284" PostId="334" Score="10" Text="Engagement at the expense of site quality is not the solution. Engagement is transient. Quality is much harder to alter later on." CreationDate="2014-06-13T05:05:00.603" UserId="62" />
  <row Id="1285" PostId="334" Score="0" Text="@AsheeshR I'll agree with you if you can point me to a single instance of a QA site that is considered authoritative on less than 10 questions a day." CreationDate="2014-06-13T05:11:38.660" UserId="434" />
  <row Id="1286" PostId="334" Score="4" Text="[bicycles.se], [workplace.se], [money.se], [skeptics.se], [gamedev.se] all launched with less than 10 questions per day. [Bicycles](http://area51.stackexchange.com/proposals/2305/bicycles) was launched with 4 per day because it was considered to be a high quality site." CreationDate="2014-06-13T05:18:36.150" UserId="62" />
  <row Id="1287" PostId="334" Score="3" Text="Well... I guess I have to declare you the winner at this point.  :)" CreationDate="2014-06-13T05:37:04.533" UserId="434" />
  <row Id="1288" PostId="339" Score="9" Text="Librariers - I do not agree at all with that. R is by far the richest tool set, and more than that it provides the information in a proper way, partly by inheriting S, partly by one of the largest community of reputed experts." CreationDate="2014-06-13T06:11:10.367" UserId="108" />
  <row Id="1289" PostId="349" Score="2" Text="Sometimes experience is hard to gain if your current job is not focused on data science but on some related field (in my case statistics). I use the courses to gain some knowledge and stay on topic, which I cannot do in my daytime job." CreationDate="2014-06-13T07:16:19.973" UserId="791" />
  <row Id="1290" PostId="235" Score="6" Text="Voted down as question does not show any research effort. Its not that hard." CreationDate="2014-06-13T07:26:14.520" UserId="471" />
  <row Id="1291" PostId="38" Score="3" Text="Voted down for lack of research effort. Hadoop and noSQL are well-defined elsewhere." CreationDate="2014-06-13T07:29:24.280" UserId="471" />
  <row Id="1293" PostId="354" Score="5" Text="Career-question, off-topic: http://meta.datascience.stackexchange.com/questions/41/is-a-question-about-future-career-paths-appropriate" CreationDate="2014-06-13T07:33:53.157" UserId="471" />
  <row Id="1294" PostId="354" Score="0" Text="Thank you, I was wondering if this was an feasible question, but did not see the meta thread. maybe adding an description to this site?" CreationDate="2014-06-13T07:34:51.107" UserId="791" />
  <row Id="1297" PostId="351" Score="2" Text="And what about stats.SE, datascience.SE profiles. Do you think they can say much about relevant level of knowledge?" CreationDate="2014-06-13T08:26:14.040" UserId="97" />
  <row Id="1299" PostId="327" Score="2" Text="But what about the libraries? There are advanced R packages (think Ranfom Forest or Caret) that would be utterly impractical to reimplement in a general purpose language such us C or Java" CreationDate="2014-06-13T09:35:46.040" UserId="457" />
  <row Id="1300" PostId="352" Score="0" Text="This is a good question.  The answer has been laid out in papers relating to various graphing libraries, but it will always depend on the assumptions and restrictions that you put in place.  Are the circles area appropriate? Will they always have at least X points of intersection?  Are you limited in size or quantity of categories?  More importantly is the question of whether or not a Euler diagram will be useful.  More than a dozen intersecting circles is hard to interpret, but if there's a mostly hierarchical relationship that you are depicting a much larger quantity can work." CreationDate="2014-06-13T09:48:54.630" UserId="434" />
  <row Id="1301" PostId="352" Score="0" Text="I could be wrong and there's an easier test, but with my experience with visualizations, the question has always been a practical one first, and honestly if I'm looking at intersecting categories I usually have a small collection of them." CreationDate="2014-06-13T09:53:37.330" UserId="434" />
  <row Id="1302" PostId="345" Score="2" Text="+1 for the bonus paper. Great read" CreationDate="2014-06-13T10:00:56.180" UserId="457" />
  <row Id="1303" PostId="350" Score="1" Text="There is no easy answer to this question, IMO.  Workloads and project goals vary significantly.  Think of immunology, for example.  An immunologist's tasks will vary throughout his career and at the senior level the focus will be very specialized.  Data science is like that, but even more broad because the technologies they work with and the data they work with can vary so much.  A data scientist at Facebook is going to have a very different workload and type of data to analyze than one at Baxter." CreationDate="2014-06-13T10:16:04.927" UserId="434" />
  <row Id="1304" PostId="327" Score="0" Text="mahout i.e. supports random forest for java" CreationDate="2014-06-13T10:17:04.727" UserId="115" />
  <row Id="1305" PostId="327" Score="0" Text="ok, maybe RF wasn't a good example, but you get my meaning: there are hundreds of statistical packages in R not implemented in other platforms" CreationDate="2014-06-13T10:21:46.037" UserId="457" />
  <row Id="1306" PostId="327" Score="0" Text="Yeah maybe, but R doesn't bring the performance at all that you need for proccessing big sets of data and most of the time you have really big datasets in industrial use." CreationDate="2014-06-13T10:41:21.297" UserId="115" />
  <row Id="1307" PostId="351" Score="0" Text="What do dropouts have to do with it? Presumably, certification is contingent upon completing the course, not merely registering…" CreationDate="2014-06-13T10:49:41.657" UserId="762" />
  <row Id="1309" PostId="355" Score="1" Text="I thinkt that's a good answer, thank you for the hints. &#xA;I especially like the idea to build up from a better fitting job - I had the impression that I had to know all, which is not the case.+" CreationDate="2014-06-13T11:15:12.490" UserId="791" />
  <row Id="1310" PostId="358" Score="1" Text="That's pretty close to my initial thought. (Which was Export, hadoop m/r the merge, import to temp, atomically replace table).  Export/Import/Indexing will still be very slow, but it will scale beyond 100M rows in 24hrs on everything but DB2.  DB2s indexing speed is a bear.  I'm wondering if there might be a high speed algorithm for finding a changed column.  Right now I'm thinking of hashing the row and hashing some row segments to at least narrow down changes without having to iterate each column for comparison.  I'm also wondering about in-memory caching of the production dataset." CreationDate="2014-06-13T12:46:46.617" UserId="434" />
  <row Id="1311" PostId="358" Score="0" Text="future questions for when I get moving on it, I suppose :)." CreationDate="2014-06-13T12:47:26.107" UserId="434" />
  <row Id="1312" PostId="358" Score="1" Text="Yeah. If you go down this route then pre-process step after exporting is probably where you wrangle it to a good state for some kind of comparison algorithm. Sounds like a really fun project tbh :)" CreationDate="2014-06-13T13:20:10.287" UserId="587" />
  <row Id="1313" PostId="349" Score="0" Text="I agree fully, the courses are very valuable for giving you a starting point, and some structure to gain that experience.  To get the most out of the Mooc I suggest taking a very specific example, lets say logistic regression, and really working through it with a different data set, double bonus if you do it in a language other than the one the course is taught in." CreationDate="2014-06-13T13:36:51.167" UserId="780" />
  <row Id="1314" PostId="349" Score="0" Text="That's a good idea. &#xA;What#s missing for statistics in general is a training website. E.g. a set of databases, along with goals and possible results at the end.&#xA;Something like khancademy, but more powerful ;)" CreationDate="2014-06-13T14:07:52.810" UserId="791" />
  <row Id="1315" PostId="339" Score="0" Text="I am not sure if the number of libraries for R is a purely good thing. There are too many ways to accomplish a goal, which confuses beginners and advanced students alike (ok, how can I summarize my data? summary() or describe() or...)&#xA;Also, often packages have theor own styles, which leads to headaches when you are working with multiple ones. Third, the naming is atrochious, a software engineer would be flogged for methods like read.csv2 oder cut2..." CreationDate="2014-06-13T14:10:40.397" UserId="791" />
  <row Id="1316" PostId="361" Score="0" Text="I guess you meant, &quot;*Logic often states that by (over)underfitting a model, it's capacity to generalize is increased.*&quot;" CreationDate="2014-06-13T16:51:41.793" UserId="84" />
  <row Id="1317" PostId="361" Score="0" Text="@Rubens: Correct, thanks, updated the text!" CreationDate="2014-06-13T16:56:24.207" UserId="158" />
  <row Id="1318" PostId="360" Score="0" Text="+1 Thanks, as a result of your answer, I've posted a followup to the question above, &quot;[When is a Model Underfitted?](http://datascience.stackexchange.com/questions/361/when-is-a-model-underfitted)&quot;" CreationDate="2014-06-13T16:59:09.830" UserId="158" />
  <row Id="1319" PostId="351" Score="0" Text="There are many people who mention that they are undergoing certification by doing a course on these MOOCs. You need to be careful with that." CreationDate="2014-06-13T17:02:53.243" UserId="735" />
  <row Id="1320" PostId="352" Score="1" Text="I am not familiar with this topic, but I spent in the past a lot of time studying graphs. I think that the property that an Euler diagram could be drawn is related with the planarity of the graph where the sets are the nodes. This paper seems to shade some light on this relation: [Ensuring the Drawability of Extended Euler Diagrams for up to 8 Sets](http://www.google.ie/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=3&amp;ved=0CDMQFjAC&amp;url=http://www-rocq.inria.fr/~verroust/diagrams04.pdf&amp;ei=eJKaU9nPAYXH7Ab7g4HQBQ&amp;usg=AFQjCNF_H6ptg8ApWGmrWed4TN6FIH5mAA&amp;sig2=5ZDb8tEOrWG1xv-O7ckn7w)." CreationDate="2014-06-13T06:04:40.787" UserId="108" />
  <row Id="1321" PostId="362" Score="0" Text="This works well in Oracle, but not on SQL Server (at least some older versions).  For some reason I see bad hash comparisons (false positives, unflagged mismatches).  Pulling the data into an external environment should alleviate that." CreationDate="2014-06-13T18:32:29.797" UserId="434" />
  <row Id="1322" PostId="365" Score="0" Text="Part of the question is meant to gauge whether or not the community placed value on certification.  In some areas, certification is an absolute necessity.  In others, certification doesn't matter at all.  In still others, certifications by a particular company are held in high regard and competitive certifications are not.  The other part was meant to understand the difference in topical focus of the certifications that are out there.  Data Science is a broad term.  Certifications are normally more focused.  This is a bad question for QA format - it's more of a discussion, subject to opinion." CreationDate="2014-06-14T03:26:47.133" UserId="434" />
  <row Id="1323" PostId="365" Score="0" Text="My purpose in noting that I chose the answer by votes was to make it plain that all of the answers deserved reading.  Everybody makes good points, including you way down here at the bottom.  Somebody who is wondering about these things shouldn't limit themselves to the top one or two answers." CreationDate="2014-06-14T03:29:39.203" UserId="434" />
  <row Id="1324" PostId="365" Score="0" Text="Voting to find the right answer is a horrible idea.  It is the wrong way to approach math.  You clearly missed my point." CreationDate="2014-06-14T04:04:26.623" UserId="386" />
  <row Id="1325" PostId="223" Score="0" Text="@user1893354 Very helpful! Specially the &quot;[brat standoff format](http://brat.nlplab.org/standoff.html)&quot; used by it seems very suitable to my needs. I suggest posting an answer if you like." CreationDate="2014-06-14T08:09:09.150" UserId="227" />
  <row Id="1326" PostId="50" Score="2" Text="This is off-topic and has probably already been answered a hundred times on more general computing forums. Flagged." CreationDate="2014-06-14T09:31:19.403" UserId="471" />
  <row Id="1327" PostId="379" Score="1" Text="Thank you !!! This is helpful.." CreationDate="2014-06-15T05:30:48.820" UserId="496" />
  <row Id="1330" PostId="388" Score="1" Text="+1 Thanks for the references. Do you have any small example, or could show just an intuition behind one of the approaches? I'm not acquainted to neural networks, but I can check it out if the example requires such knowledge base." CreationDate="2014-06-15T15:25:05.307" UserId="84" />
  <row Id="1331" PostId="388" Score="0" Text="Could you be more specific about your use case?  Strategy can vary wildly depending on how you tend to implement the solution." CreationDate="2014-06-15T20:26:30.203" UserId="780" />
  <row Id="1332" PostId="388" Score="0" Text="I mean, I just would like to see what is the *idea* behind a different approach. For example, if I was to tell you what is done by using blacklists (which we know is not good), I could describe the *algorithm* as: scan the dataset looking for entries containing &quot;viagra&quot;; add such entries to the blacklist. I just would like to see a high-level description of a *methodology/algorithm*. Do they gather spam network usage information and put on a neural network classifier, or what do they do?" CreationDate="2014-06-15T20:35:17.443" UserId="84" />
  <row Id="1333" PostId="305" Score="0" Text="@SeanOwen in my question, I was referring to Apache Hadoop. Although it would be interesting to make the Impala comparison as well." CreationDate="2014-06-16T03:46:01.843" UserId="534" />
  <row Id="1334" PostId="395" Score="0" Text="good, thanks! Though not sure yet if I can use R package &quot;randomForest&quot; (http://cran.r-project.org/web/packages/randomForest/randomForest.pdf) to generate ERF. Probably not." CreationDate="2014-06-16T08:05:55.243" UserId="97" />
  <row Id="1335" PostId="399" Score="0" Text="I agree that I'm still only scratching the surface. That's mainly the result of being in the early stages of my project-design. (I had initially ruled out R since I was more interested in using a general purpose language)" CreationDate="2014-06-16T09:57:46.783" UserId="872" />
  <row Id="1336" PostId="61" Score="1" Text="Overfitting is bad by definition. If it weren't it wouldn't be *over*-fitting." CreationDate="2014-06-16T10:51:52.677" UserId="762" />
  <row Id="1337" PostId="351" Score="0" Text="@Kunal It makes sense but your answer jumps from the “certification” to “dropouts” (who presumably *don't have a certification*). The key here is *undergoing*. It's a bit like being registered as a student or having a Kaggle account. None of this tells us whether you should value someone who did actually get a degree, complete a course or participate in a competition to the end." CreationDate="2014-06-16T11:02:19.743" UserId="762" />
  <row Id="1338" PostId="400" Score="1" Text="Your point 2 is very important. Juggling with different tools means importing data. And data importing is an enormously error prone step and feels extremely unproductive - so avoid it, whereever you can." CreationDate="2014-06-16T13:25:03.337" UserId="791" />
  <row Id="1339" PostId="61" Score="0" Text="-1 for not being stated clearly. I would propose a fix, but the only ones I can think of would fundamentally change the meaning of the question. I think blunders might be conflating &quot;overfitting&quot; with &quot;adding model complexity&quot;." CreationDate="2014-06-16T14:06:19.127" UserId="675" />
  <row Id="1340" PostId="398" Score="9" Text="All these questions have been beaten to death on StackOverflow.  What value is there in rehashing it here?" CreationDate="2014-06-16T14:21:06.007" UserId="515" />
  <row Id="1341" PostId="404" Score="0" Text="Being a web developer, JSON seems completely reasonable to me, but, can you elaborate on the exact format of mapping words to entities?" CreationDate="2014-06-16T15:49:04.183" UserId="227" />
  <row Id="1342" PostId="61" Score="0" Text="@NathanGould: Thanks for commenting, though appears you're both inferring a meaning that is simply not present in the question, and quoting text that is also not present; meaning no where in the text are the words &quot;adding model complexity.&quot;" CreationDate="2014-06-16T15:57:44.980" UserId="158" />
  <row Id="1343" PostId="404" Score="0" Text="@AmirAliAkbari Updated answer to include more details." CreationDate="2014-06-16T17:35:52.767" UserId="548" />
  <row Id="1344" PostId="408" Score="1" Text="Another very useful package for forecasting and time series analysis is [forecast](http://cran.r-project.org/web/packages/forecast/index.html) by Prof. Rob J. Hyndman." CreationDate="2014-06-16T18:05:32.453" UserId="554" />
  <row Id="1345" PostId="409" Score="0" Text="Yes, visualization is an essential first step in any analysis." CreationDate="2014-06-16T18:42:16.387" UserId="178" />
  <row Id="1346" PostId="398" Score="0" Text="@DirkEddelbuettel Because they are off-topic on SO, hence often closed?" CreationDate="2014-06-16T18:56:18.513" UserId="843" />
  <row Id="1348" PostId="411" Score="11" Text="There is no question here.  If you need to do basic research on programming language, you are better off reading Wikipedia than to wait for someone to pop up here to push his hobby-horse." CreationDate="2014-06-16T20:05:09.750" UserId="515" />
  <row Id="1350" PostId="411" Score="0" Text="@DirkEddelbuettel Very good point. Thought it was better to try producing content than refining it at this point in the Beta, but I don't know a huge amount about SE betas. Was that a good move on my part or not?" CreationDate="2014-06-16T20:11:36.353" UserId="548" />
  <row Id="1351" PostId="61" Score="1" Text="I didn't mean to quote you on &quot;adding model complexity&quot; -- I was just highlighting the phrase. Anyhow I guess my issue is basically the same as @GaLa, which is that overfitting means fitting too much. So it seems you are asking us to confirm a tautology. So, I would tend to think that you actually meant to ask a different question. E.g., does increasing model complexity cause models to become worse? Or, how does complexity of the data relate to the tendency of a model to overfit?" CreationDate="2014-06-16T20:19:32.450" UserId="675" />
  <row Id="1352" PostId="411" Score="1" Text="Look at [these](http://economics.sas.upenn.edu/~jesusfv/comparison_languages.pdf) numbers." CreationDate="2014-06-16T20:20:29.817" UserId="381" />
  <row Id="1353" PostId="132" Score="1" Text="Both of these fall into the category of feature engineering as they involve manually creating or selecting features. Dimensionality reduction typically involves a change of basis or some other mathematical re-representation of the data" CreationDate="2014-06-16T21:05:47.767" UserId="890" />
  <row Id="1354" PostId="411" Score="0" Text="@DirkEddelbuettel you're not wrong, but my hope was to foster a discussion about the useful characteristics and tools associated with various languages. The language you use is an important tool in data science, so my thinking was that people could discuss the tools they preferred and there objective benefits here, as a resource for those looking to attempt similar work." CreationDate="2014-06-16T21:08:55.900" UserId="890" />
  <row Id="1355" PostId="411" Score="0" Text="This area is subject to such rapid change that it's probably best to strictly limit the scope of this question to &quot;what qualities make for the best tool&quot; rather than &quot;which tools exist and what are their qualities&quot; - even as a community wiki, the latter would require an intimidating amount of continued maintenance in order to be net useful" CreationDate="2014-06-16T21:54:22.960" UserId="322" />
  <row Id="1357" PostId="408" Score="0" Text="Do you know if this is already implemented in any other language?  I'm not exactly a pro with R.  I will definitely read the paper at least." CreationDate="2014-06-17T01:18:21.087" UserId="886" />
  <row Id="1358" PostId="409" Score="0" Text="I did add the month, day of month, day of week, and year as features.  I even tried a linearly decreasing &quot;Recentness&quot; value.  I don't think I have tried OLS.  I'm observing a time frame that could range anywhere from a couple weeks to multiple years.  As far as visualizing it goes, I did try to do that.  The problem is, we want the software to be able to predict automatically, without human intervention, for different customers." CreationDate="2014-06-17T01:24:29.210" UserId="886" />
  <row Id="1359" PostId="426" Score="0" Text="I am not 100% convinced by the topics on that course. For example, are SVMs actually of practical use these days? You never see a winning Kaggle entry that used SVMs as the main part." CreationDate="2014-06-17T07:45:32.770" UserId="910" />
  <row Id="1360" PostId="411" Score="0" Text="You need to learn either R  or Python properly and also be able to understand the other.  R has an unbeatable number of stats packages and nothing will ever be able to compare to that." CreationDate="2014-06-17T07:48:23.087" UserId="910" />
  <row Id="1361" PostId="405" Score="2" Text="R is unbeatable in terms of number of stats packages. So if you go for python you still need to learn enough R to be able to use rpy2 to call the missing packages." CreationDate="2014-06-17T08:07:24.300" UserId="910" />
  <row Id="1362" PostId="426" Score="2" Text="I think OP's question is specifically about *online* techniques - i.e. where system is expected to learn at least partially &quot;on the job&quot;. Not *online tutorials*" CreationDate="2014-06-17T08:40:15.450" UserId="836" />
  <row Id="1363" PostId="421" Score="2" Text="Could you clarify key aspects of &quot;online&quot; that you are interested in? Do you have a specific form for the data, or any options to pre-train your algorithm before the online part?" CreationDate="2014-06-17T08:41:49.577" UserId="836" />
  <row Id="1364" PostId="426" Score="0" Text="I agree with @NeilSlater since the OP mentioned &quot;compared to normal machine learning methods&quot;." CreationDate="2014-06-17T09:45:23.663" UserId="343" />
  <row Id="1365" PostId="426" Score="4" Text="lol, &quot;online&quot; is ambiguous" CreationDate="2014-06-17T10:19:41.480" UserId="122" />
  <row Id="1366" PostId="430" Score="0" Text="Did you make any research about this? There are many youtube videos and slideshare presentations describing different architectures" CreationDate="2014-06-17T10:53:46.840" UserId="478" />
  <row Id="1367" PostId="430" Score="1" Text="Hey Stanpol, thanks for your response - I did some initial searches and didn't really find anything besides AWS and cloudera stuff - maybe if you can give me some search terms that a promising, I'll be happy to take it from there." CreationDate="2014-06-17T11:28:41.010" UserId="913" />
  <row Id="1368" PostId="421" Score="0" Text="do you mean to analyze datastreams?" CreationDate="2014-06-17T11:42:23.613" UserId="115" />
  <row Id="1369" PostId="424" Score="0" Text="My guess is that this is based on learning topics in a large corpus." CreationDate="2014-06-17T12:49:06.043" UserId="241" />
  <row Id="1370" PostId="408" Score="0" Text="I am not familiar with one.  If you would like to use python, you can use the [rpy2](http://rpy.sourceforge.net/) package to call the glarma function while doing most of the rest of the programming in python.  Most other languages have such a connector as well." CreationDate="2014-06-17T12:59:46.167" UserId="178" />
  <row Id="1371" PostId="433" Score="0" Text="That's pretty awesome, exactly what I was looking for! Thanks a lot :)" CreationDate="2014-06-17T13:35:17.570" UserId="913" />
  <row Id="1372" PostId="433" Score="0" Text="@chrshmmmr You're welcome. Don't forget to upvote/mark as accepted if this helped!" CreationDate="2014-06-17T13:37:02.053" UserId="241" />
  <row Id="1373" PostId="433" Score="3" Text="These links seem very useful indeed, but then again, they are links, and I guess we should strive to maintain the answers independent of the stability of outer sources. Thus, it'd be nice if you could take some two or three minutes to add, for example, the diagram from [this link](http://blog.twitch.tv/2014/04/twitch-data-analysis-part-1-the-twitch-statistics-pipeline/), posting it along with a quick description. Something in the lines of: &quot;For example, this is the workflow of a ... system. &lt;img&gt;. Further info may be found in &lt;link&gt;.&quot;" CreationDate="2014-06-17T13:40:48.977" UserId="84" />
  <row Id="1374" PostId="427" Score="3" Text="There is no question here." CreationDate="2014-06-17T13:41:37.477" UserId="515" />
  <row Id="1375" PostId="433" Score="1" Text="@Rubens I will propose an edit in a bit.&#xA;fgnu: Will do so, just need a bit more reputation to actually upvote answers, but I certainly will honor your contribution :)" CreationDate="2014-06-17T13:42:47.237" UserId="913" />
  <row Id="1376" PostId="433" Score="0" Text="@Rubens That would be no more than reproducing the information at the link. I would if there were something I felt would add to the explanation already given there." CreationDate="2014-06-17T13:53:34.867" UserId="241" />
  <row Id="1377" PostId="433" Score="0" Text="The point is that the information would be here, instead. Of course, I'm not asking you to deliberately copy the whole of the references, but it'd be nice to have a small example in the answer itself, and not just in a link. I see this [*rule of thumb*](http://meta.stackexchange.com/questions/8231/are-answers-that-just-contain-links-elsewhere-really-good-answers) to be very praised in [stackoverflow](http://meta.stackoverflow.com/questions/251006/flagging-link-only-answers), but then again, we're not stackoverflow... I'll add a discussion about this on meta, anyway." CreationDate="2014-06-17T14:01:49.917" UserId="84" />
  <row Id="1378" PostId="411" Score="0" Text="@Lembik I appreciate the evangelism, but there isn't a single piece of statistical functionality that `R` has that `Python` doesn't. There are on the other hand, a large number of scientific and machine learning functions that `R` lacks, which are preset in `python`. Good `opencv` bindings are a great example." CreationDate="2014-06-17T14:45:46.360" UserId="548" />
  <row Id="1379" PostId="433" Score="0" Text="I've opened a discussion [here](http://meta.datascience.stackexchange.com/questions/63/flagging-link-only-answers). You're very welcome to participate. Thanks for raising this topic." CreationDate="2014-06-17T14:45:59.170" UserId="84" />
  <row Id="1380" PostId="411" Score="0" Text="@indico I am not sure if  I understand your point. There are literally thousands of packages in CRAN which don't exist for python. See http://cran.r-project.org/web/packages/available_packages_by_name.html ." CreationDate="2014-06-17T14:54:54.170" UserId="910" />
  <row Id="1381" PostId="411" Score="0" Text="@Lembik packages yes, but I am referring to functionality. The vast majority of the statistical functionality present in those `R` packages is contained simply within `scipy`" CreationDate="2014-06-17T14:56:48.123" UserId="548" />
  <row Id="1382" PostId="411" Score="0" Text="@indico This simply isn't true I am afraid.  People who do PhDs in stats, for example, still routinely write an R package as part of that. These very very rarely get implemented in scipy. How many new stats functions does scipy add every year? Try just picking a few of the R packages at random to see what I mean." CreationDate="2014-06-17T14:58:16.727" UserId="910" />
  <row Id="1383" PostId="405" Score="1" Text="@Lembik I would disagree with both of those points actually. As someone who has very frequently used both `R` and `python`, I've never had the need to use `rpy2`, and even though `R` wins in pure number of packages, `python` has decidedly more functionality across data science." CreationDate="2014-06-17T14:58:30.103" UserId="548" />
  <row Id="1384" PostId="411" Score="0" Text="@Lembik I don't really want to argue on this one. Could you please provide an example? `scipy` adds a huge amount of statistical functions every year, not really keeping track I would guess a few hundred each year." CreationDate="2014-06-17T15:00:20.027" UserId="548" />
  <row Id="1385" PostId="411" Score="0" Text="@indico Try http://cran.r-project.org/web/packages/overlap/index.html which is just the first one I happened to pick at random.  But really, I have personally known many statisticians who have written R packages. Not one of them has yet written a python one.  To broaden the conversation a little, http://www.kdnuggets.com/2013/08/languages-for-analytics-data-mining-data-science.html is interesting." CreationDate="2014-06-17T15:08:22.267" UserId="910" />
  <row Id="1386" PostId="411" Score="0" Text="@Lembik and my rebuttal in the form of a blog post: http://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/" CreationDate="2014-06-17T15:11:36.837" UserId="548" />
  <row Id="1387" PostId="431" Score="0" Text="I thank you very much for posting this reference, but I was expecting the answers here to point a publicly available dataset/API for social network, *andalso* describe what is provided by such source (either the download rate of posts, or what kind of information about users). As your answer is, I guess it would be very welcome to the list of [publicly available datasets](http://datascience.stackexchange.com/questions/155/publicly-available-datasets) we have." CreationDate="2014-06-17T15:12:51.027" UserId="84" />
  <row Id="1388" PostId="411" Score="0" Text="@indico Oh I read that before. If you are the author of that blog I greatly admire it. However it is in no way a rebuttal to the fact that there are thousands of R packages written by statisticians which have no python equivalent :)" CreationDate="2014-06-17T15:13:00.223" UserId="910" />
  <row Id="1389" PostId="411" Score="0" Text="@Lembik I'm not trying to argue about a direct 1-to-1 correlation here. I'm trying to argue that in terms of raw functionality, here referring to kernel density estimation, there is nothing fundamentally missing from `python`, if we're going down the missing functionality line, there are an entire order of magnitude more python packages, and bindings to amazing academic projects such as opencv, which do not exist in R" CreationDate="2014-06-17T15:15:23.390" UserId="548" />
  <row Id="1390" PostId="411" Score="0" Text="@indico  You are right in terms of non stats packages. My comment was only meant to refer to statistics.  You may also be right about kernel density estimation although it looked like the overlap package had more in it than your blog post referred to. It would perhaps be interesting for someone to go through the thousands of R packages and see what is missing from python but I still claim that the fact is that PhD students from the best stats departments write R packages for their work and put them on CRAN. So if you have an interest in cutting edge stats you need to use those packages." CreationDate="2014-06-17T15:21:57.660" UserId="910" />
  <row Id="1391" PostId="411" Score="0" Text="@indico Just picking some more at random which have the word &quot;kernel&quot; in them. See http://cran.r-project.org/web/packages/bark/index.html , http://cran.r-project.org/web/packages/bbefkr/index.html , http://cran.r-project.org/web/packages/bpkde/index.html , http://cran.r-project.org/web/packages/DBKGrad/DBKGrad.pdf" CreationDate="2014-06-17T15:25:56.273" UserId="910" />
  <row Id="1392" PostId="405" Score="0" Text="I think that may be right for data science in general although I would argue that for cutting edge stats you still need to use R packages." CreationDate="2014-06-17T15:27:51.723" UserId="910" />
  <row Id="1393" PostId="405" Score="3" Text="@indico then you must be using the definition of &quot;Data Science&quot; that excludes statistics." CreationDate="2014-06-17T16:15:13.170" UserId="539" />
  <row Id="1395" PostId="405" Score="0" Text="I've accepted this answer since it gives the most direct answer to my question; not because it is the one true answer (read: check the other answers as well)" CreationDate="2014-06-17T17:30:31.490" UserId="872" />
  <row Id="1399" PostId="439" Score="0" Text="What interests me about the paper I linked is that it claims to show a method for performing this sort of computation *without knowledge of both input strings*. In the paper, each actor has knowledge of *one* string, which isn't useful for my purposes; I would need one actor to be able to perform the calculation without knowledge of *either* string. Calculating them beforehand is only feasible for very small datasets or very limited products; a full cross product of integer distances on my dataset would take ~10 PB of storage." CreationDate="2014-06-17T19:44:59.190" UserId="322" />
  <row Id="1400" PostId="439" Score="0" Text="That's why I brought up the idea of a substitution cipher (ROT13) since it preserves the distance between strings; but it's not secure, and I suspect it may be impossible to securely encrypt the strings while preserving the edit distance. (Would love to be wrong!)" CreationDate="2014-06-17T19:48:14.150" UserId="322" />
  <row Id="1401" PostId="416" Score="0" Text="Appreciate the k-anonymity reference, and the bin suggestion - that gives me some new things to think about." CreationDate="2014-06-17T19:57:32.907" UserId="322" />
  <row Id="1402" PostId="439" Score="0" Text="Right, I would just filter the matrix to only include Levenshteins below a certain cutoff, so you are only populating where there is high likelihood of overlap.  Additionally, when it comes to PII I am of the mindset that if you include enough information to determine a relationship among disparate entities in your datasets, its very unlikely you are preserving the customers anonymity.  The point of anonymizing the data is to avoid potential PII related regulatory headaches down that line, (standards can always be tightened), so personally I wouldn't take the risk." CreationDate="2014-06-17T20:27:56.733" UserId="780" />
  <row Id="1403" PostId="303" Score="0" Text="Maybe so.  As I said, this isn't my area.  Did you try going through all the steps in the Tutorials and also the &quot;Training&quot; section?  It appears that they take you through all the steps, starting with source texts.  Best wishes." CreationDate="2014-06-17T22:14:55.200" UserId="609" />
  <row Id="1404" PostId="303" Score="0" Text="it's the steps to get word alignments not phrases..." CreationDate="2014-06-17T22:30:15.820" UserId="122" />
  <row Id="1405" PostId="303" Score="0" Text="My impression was that the Tutorials and Training sections take you through all these steps, including phrase alignment.  I suggest going through all of them and you might discover which stage and tool accomplishes this task." CreationDate="2014-06-17T22:42:58.523" UserId="609" />
  <row Id="1406" PostId="449" Score="0" Text="thanks for the clarification. Keeping data in memory sounds like it has some interesting implications -I'll read up on Spark's Resilient Distributed Dataset concept a bit more." CreationDate="2014-06-18T10:30:55.527" UserId="426" />
  <row Id="1407" PostId="445" Score="0" Text="This is a pretty good idea. The Masters is in Statistics." CreationDate="2014-06-18T12:30:41.317" UserId="839" />
  <row Id="1408" PostId="453" Score="1" Text="Can you link to/reference where you read that the unary method is not universal? Context may help." CreationDate="2014-06-18T16:06:03.520" UserId="322" />
  <row Id="1409" PostId="453" Score="2" Text="I... am not sure how this relates to data science. It seems off-topic for this stack exchange. Could you possibly relate this back to data science?" CreationDate="2014-06-18T16:14:01.900" UserId="869" />
  <row Id="1410" PostId="453" Score="0" Text="@SlaterTyranus I... am not sure too (and that made me think about some two other questions I posted). My idea was to add this question since compression methods are largely used in information retrieval (mainly during indexing). In general, I find this related to efficiency, and it may be put in the *hacking skills* area of this [Venn diagram](http://en.wikipedia.org/wiki/Data_science). Anyway, I guess it'd be nice to discuss whether this kind of question is on topic." CreationDate="2014-06-18T16:33:25.077" UserId="84" />
  <row Id="1411" PostId="455" Score="1" Text="I think more details about your problem are necessary before anyone could recommend a dataset." CreationDate="2014-06-18T16:40:58.183" UserId="836" />
  <row Id="1412" PostId="451" Score="1" Text="how about some compression on the messages?" CreationDate="2014-06-18T17:54:09.643" UserId="743" />
  <row Id="1413" PostId="451" Score="0" Text="@user798196 That's one of the answers I was expecting. I'm just not sure about why it came as just a comment :D" CreationDate="2014-06-18T17:55:37.270" UserId="84" />
  <row Id="1414" PostId="453" Score="0" Text="@Rubens That seems like a reasonable discussion, in my mind efficiency talk fits much more into something like theoretical CS than explicit *hacking skills*. In my mind, hacking skills are much more related to things like databases, deployment, and knowledge of tools." CreationDate="2014-06-18T18:08:48.673" UserId="869" />
  <row Id="1415" PostId="451" Score="0" Text="I would totally suggest using zmq, what are you using for your message passing layer right now?" CreationDate="2014-06-18T18:10:56.937" UserId="869" />
  <row Id="1416" PostId="451" Score="0" Text="@SlaterTyranus I'm using a platform built on OpenMPI, but it does not perform message compression. Does zeromq do so? In fact, my intention here was to bring up such concepts (caching, compression, aggregation), and I was expecting them to come as answers. I'm also adding this question to a meta post, to discuss whether it is on topic." CreationDate="2014-06-18T18:22:29.890" UserId="84" />
  <row Id="1417" PostId="451" Score="1" Text="@Rubens They considered adding it, but decided it didn't make sense for a couple reasons. Discussion here: https://github.com/JustinTulloss/zeromq.node/issues/285" CreationDate="2014-06-18T18:26:11.227" UserId="869" />
  <row Id="1418" PostId="451" Score="0" Text="Also, @indico totally beat me to it. OpenMPI is very good for parallelism, but generally not optimal for actual distributed systems. My impression is that message compression adds more latency than it prevents." CreationDate="2014-06-18T18:27:44.977" UserId="869" />
  <row Id="1419" PostId="455" Score="3" Text="For what purpose? Spam filtering? Sentiment analysis? Without a clear purpose it is *very* difficult to suggest a dataset." CreationDate="2014-06-18T18:37:12.887" UserId="553" />
  <row Id="1420" PostId="453" Score="0" Text="@SlaterTyranus Hope you join the discussion [here](http://meta.datascience.stackexchange.com/questions/65/are-hacking-skills-questions-off-topic). I've tried to point what I consider to be *hacking skills*. Although I understand theoretical CS to be &quot;the place for algorithms&quot;, there are some *pratical* concepts that may would be nice to be added to this site." CreationDate="2014-06-18T18:41:55.673" UserId="84" />
  <row Id="1421" PostId="457" Score="0" Text="That's a very nice suggestion, thanks. I'm actually using a framework implemented upon OpenMPI, and therefore I won't be able to make such changes -- although they seem to be a very promissing improvement. Btw, I very much enjoyed the citation :D" CreationDate="2014-06-18T18:50:20.903" UserId="84" />
  <row Id="1422" PostId="452" Score="0" Text="Could you elaborate on how you would use this feature matrix?  Are you trying to learn based on change of traffic across days?" CreationDate="2014-06-18T19:19:17.557" UserId="886" />
  <row Id="1423" PostId="352" Score="2" Text="I'm assuming that by &quot;simple&quot; you mean the diagram uses only circles and that by &quot;proportional&quot; you mean that the area of each section of the diagram is proportional to the population it represents from the total set. It might help to make these definitions explicit in the question." CreationDate="2014-06-18T23:04:18.013" UserId="322" />
  <row Id="1424" PostId="466" Score="2" Text="What do you mean, bad results? Describe your process, your results, and how they differ from what you expected, instead of only linking to the git repository. Otherwise this discussion will be of no use to anyone." CreationDate="2014-06-18T23:08:41.183" UserId="322" />
  <row Id="1425" PostId="466" Score="0" Text="It's also true this :D. I added the description in the page &quot;The results has a 14.14 RMSE, so it can't predict so well the gas consumptions, consecutevely I can't run a good outlier detection mechanism. I see that in some papers that even if they predict daily or hourly consumption in the electric power, they have errors like MSE = 0.01.&quot;" CreationDate="2014-06-18T23:25:19.710" UserId="989" />
  <row Id="1426" PostId="466" Score="1" Text="@marcodena This is a QA site, and others need to know what you're trying to solve, so that they'll understand the answers, and will hopefully be able to use them in their own problems. That's what AirThomas meant, and is also why it'd be nice if you could describe what you're doing and what exactly you think is wrong. If the link to your git-hub page changes, the link here will be invalid, and others won't be able to understand what the problem is. Please, take a minute to make your question self-contained. Thanks." CreationDate="2014-06-19T00:35:40.470" UserId="84" />
  <row Id="1427" PostId="466" Score="0" Text="@Rubens you are right, but as you see in the github link, the description is more than 6 A4 pages long, so here it would be very difficult to explain everything. I'll try." CreationDate="2014-06-19T08:50:22.207" UserId="989" />
  <row Id="1429" PostId="477" Score="4" Text="This question appears to cross posted across multiple sites.  While it may be on topic here, it is also on topic (and has gained an answer) on Data Science.SE.  Questions should exist in one place only unless there are extenuating circumstances." CreationDate="2014-06-19T02:43:14.847" UserDisplayName="MichaelT" />
  <row Id="1430" PostId="477" Score="0" Text="As a computing problem it's hard to even make a start without some idea of scale and the success criteria. Is a cross-product possible? Should the answer be optimal, or just good enough? More info please." CreationDate="2014-06-19T06:30:51.003" UserDisplayName="david.pfx" />
  <row Id="1432" PostId="477" Score="0" Text="You need to define the question better, e.g., what is &quot;better suit their needs&quot;" CreationDate="2014-06-19T11:01:24.493" UserId="743" />
  <row Id="1434" PostId="61" Score="0" Text="@NathanGould: I think you're over thinking the question, or somehow expecting that I would ask a question that on a topic that I completely understood. Every single reference by me to complexity within the question related to the data being modeled, not the complexity of the model." CreationDate="2014-06-19T11:50:57.280" UserId="158" />
  <row Id="1437" PostId="468" Score="0" Text="I suggest you add the R tag as well." CreationDate="2014-06-19T13:28:55.910" UserId="178" />
  <row Id="1438" PostId="452" Score="0" Text="I edited the response to hopefully give more clarity." CreationDate="2014-06-19T14:32:56.907" UserId="403" />
  <row Id="1440" PostId="485" Score="0" Text="Your answer is very nice, and nice also would that be if you added a little example, so as to emphasize your point on data mining being more related to *detecting something new* rather than trying to *solve and reach results*." CreationDate="2014-06-19T16:54:42.093" UserId="84" />
  <row Id="1441" PostId="474" Score="1" Text="In what context did you see these terms? It seems that `p` and `k` simply show the size of the subgraph, not a special category of them." CreationDate="2014-06-19T17:07:13.287" UserId="227" />
  <row Id="1442" PostId="382" Score="0" Text="Great question! I think this is an important and non-trivial problem." CreationDate="2014-06-19T17:24:09.717" UserId="1011" />
  <row Id="1443" PostId="455" Score="0" Text="@lsdr Looking at the answers, it seems that the question does not necessarily need more details." CreationDate="2014-06-19T17:25:47.420" UserId="227" />
  <row Id="1445" PostId="455" Score="0" Text="@AmirAliAkbari I think they came after an edit. I retracted my close-vote, anyway." CreationDate="2014-06-19T17:30:36.410" UserId="84" />
  <row Id="1447" PostId="403" Score="0" Text="I don't know much about SAS, but your math is clearly right. Perhaps it's a bug in how you called the model predicted output?" CreationDate="2014-06-19T17:49:57.240" UserId="1011" />
  <row Id="1448" PostId="172" Score="0" Text="How large are your list of values ? Have you tried to pass it as a set ? For parallelism, you may be interested in Joblib. It is easy to use and can speed up computations. Use it with large chunks of data." CreationDate="2014-06-19T18:22:28.557" UserId="1023" />
  <row Id="1449" PostId="489" Score="0" Text="Many thanks for your answer." CreationDate="2014-06-19T18:22:56.117" UserId="1021" />
  <row Id="1451" PostId="61" Score="0" Text="@blunders Maybe the question should simply be “What is overfitting?” or “How can a model fitting the data more precisely be worse?”" CreationDate="2014-06-19T20:27:23.540" UserId="762" />
  <row Id="1452" PostId="492" Score="0" Text="Have you tried training the Stanford NER on your corpus? There's a tutorial [here](http://www.linguisticsweb.org/doku.php?id=linguisticsweb:tutorials:linguistics_tutorials:automaticannotation:stanford_ner)." CreationDate="2014-06-19T20:58:53.030" UserId="381" />
  <row Id="1454" PostId="488" Score="1" Text="I think machine learning is usually an application of statistical modelling, so I'd say it is both." CreationDate="2014-06-19T21:08:37.097" UserId="922" />
  <row Id="1455" PostId="492" Score="0" Text="I have not -- should give that a go to see how it fares." CreationDate="2014-06-19T21:30:42.020" UserId="684" />
  <row Id="1456" PostId="475" Score="2" Text="Logistic regression can be used as a binary classifier, but it isn't inherently one.  You could be using it to estimate odds or determine the relationship of a predictor variable to the outcome." CreationDate="2014-06-19T21:32:33.223" UserId="953" />
  <row Id="1457" PostId="492" Score="0" Text="I'd like to use word2vec features or similar, though, since I have access to a relatively small labeled dataset and need to make the most of unlabeled data I have on hand." CreationDate="2014-06-19T21:37:43.327" UserId="684" />
  <row Id="1459" PostId="61" Score="0" Text="@GaLa: Thanks for the feedback, though I wanted to know why overfitting is bad, not what overfitting is, or how fitting the model to the data more precisely is bad. Beyond that, at this point, given there are 3 answers, and 40+ votes, I do not feel that it would either be fair or a good idea to change the meaning and/or request made by the question. If you believe that the question might be better expressed, my suggestion would be just to post a question yourself; please feel free to link to it in the comments here. Again, thanks!" CreationDate="2014-06-19T21:56:03.637" UserId="158" />
  <row Id="1460" PostId="466" Score="1" Text="When you find that your problem takes a very long time to explain, that is when it's *most important* to spend the time to explain your question to others, explicitly and with plenty of details and discussion of your research/attempts. Often during that process you will find some or all of the answers yourself. Not only is that a great feeling, if what you find is useful to others, you can still post that question you spend so much time on, *and* the answer(s) you came up with." CreationDate="2014-06-19T23:53:47.010" UserId="322" />
  <row Id="1461" PostId="474" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:27:44.880" UserId="957" />
  <row Id="1462" PostId="481" Score="0" Text="please see the edit" CreationDate="2014-06-20T02:29:46.337" UserId="957" />
  <row Id="1463" PostId="495" Score="0" Text="Thanks for the input. I have taken a very similar approach. So, its good that I am proceeding in the right direction. I am going to keep the question open for a few more days to see if anyone comes up with other ideas or more complex analysis techniques." CreationDate="2014-06-20T02:56:48.260" UserId="1028" />
  <row Id="1464" PostId="495" Score="2" Text="That's very reasonable and I will be curious to read other peoples' approaches to this problem. I would just add that the goal of good data science should be to use data to answer the question at hand, not to concoct the most complex analysis technique." CreationDate="2014-06-20T03:56:30.570" UserId="1011" />
  <row Id="1465" PostId="202" Score="0" Text="We home/hope the same about tex support! :D" CreationDate="2014-06-20T06:15:57.220" UserId="84" />
  <row Id="1466" PostId="191" Score="0" Text="Please, add information/reference link on that *MNIST data*, so as to make your post self-contained. Thanks." CreationDate="2014-06-20T06:19:47.490" UserId="84" />
  <row Id="1467" PostId="500" Score="0" Text="Using that logic neural networks are statistical models since the architecture is decided in advance. I don't think attempts to define a clear cut between statistics and machine learning are possible nor necessary." CreationDate="2014-06-20T06:54:45.653" UserId="119" />
  <row Id="1468" PostId="415" Score="0" Text="To add to this answer: in terms of scalability, `Scala` and `Go` are worth mentioning." CreationDate="2014-06-20T07:17:43.250" UserId="119" />
  <row Id="1469" PostId="61" Score="0" Text="@blunders We are back to my original comment but it seems to me that if you know what overfitting is, you know why it's bad. Since you seemed to suggest earlier you didn't fully understand the topic (which is fine), I would think the first thing to ask is simply what overfitting is." CreationDate="2014-06-20T07:37:44.590" UserId="762" />
  <row Id="1470" PostId="61" Score="0" Text="Also, the answers seem quite messy, perhaps because the question was so confusing to begin with but some of them do seem to address it in the way I suggest." CreationDate="2014-06-20T07:39:37.753" UserId="762" />
  <row Id="1471" PostId="449" Score="2" Text="+1 for a really clear and useful answer for a lot of people who had this question, like me." CreationDate="2014-06-20T09:20:30.793" UserId="113" />
  <row Id="1473" PostId="500" Score="0" Text="This is exactly the reason why I have mentioned the word 'rarely' in machine learning paragraph. I haven't said that you absolutely don't! Well, to the people who start exploring these things, it's good to know the nuances between statistical learning and machine learning" CreationDate="2014-06-20T10:17:59.630" UserId="514" />
  <row Id="1474" PostId="61" Score="0" Text="@GaLa: If you want to ask your own question, as stated above, please do. If you have any issues with the answers, please address them within the comments to the answer. The top answer, which I selected when it had I believe one or two votes, now has 18+ votes, and the question received a number of valid answers; meaning nothing to see, moving on." CreationDate="2014-06-20T11:49:26.910" UserId="158" />
  <row Id="1475" PostId="481" Score="0" Text="an n-clique is something else altogether from a k-clique or a p-clique. The n-clique refers to a maximal distance between two subgroups, where a k-clique refers to a clique of size k, where k is some constant chosen, and a p-clique is simply a clique composed of p nodes again where p is a chosen constant" CreationDate="2014-06-20T11:50:47.167" UserId="59" />
  <row Id="1476" PostId="463" Score="0" Text="Wright. The main issue here is the number of dimensions, i.e the level of detail I need to use. Could you clarify to me  “IR approach”." CreationDate="2014-06-20T14:49:06.473" UserId="986" />
  <row Id="1478" PostId="502" Score="0" Text="Thank you very much for the answer and references." CreationDate="2014-06-20T15:02:47.023" UserId="1021" />
  <row Id="1479" PostId="500" Score="0" Text="Thanks for the further clarifications!" CreationDate="2014-06-20T15:03:33.823" UserId="1021" />
  <row Id="1481" PostId="442" Score="0" Text="Another vote for dimensionality reduction. Just some additions: `Principal Component Analysis` or `Non-Negative Matrix Factorization` will reduce number of variables, enrich sparse data, and transform all variables to quantitative. Moreover, evaluating quality of dimensionality reduction model, question author can estimate usefulness of textual variables." CreationDate="2014-06-20T16:13:35.127" UserId="941" />
  <row Id="1482" PostId="463" Score="1" Text="By IR, I meant Information Retrieval. You might think that the documents (sellers) in your collection and the query (a buyer) are all vectors embedded in a term (attribute) space. As I said, such an approach needs a preset number of dimensions to work with." CreationDate="2014-06-20T16:38:49.233" UserId="984" />
  <row Id="1483" PostId="510" Score="1" Text="Sheer awesomeness! I was actually expecting something like this dissolved into many answers, and you came carrying the whole :D Thanks for the answer. Nice job! :)" CreationDate="2014-06-20T18:08:24.743" UserId="84" />
  <row Id="1485" PostId="504" Score="0" Text="Can you clarify you question by defining the goals of this analysis?  What is the nature of the 10 to 15 categories?  Are these categories you define a priori or are they clusters suggested by the data itself?  I appears that your question is centered on the choosing a good data encoding/transformation process rather than on data analysis methods (e.g. discriminant analysis, classification)." CreationDate="2014-06-20T18:25:53.873" UserId="609" />
  <row Id="1486" PostId="511" Score="1" Text="I guess a prior question is: doesn't this selection depend on the algorithm (mainly its behavior) being evaluated?" CreationDate="2014-06-20T18:28:44.200" UserId="84" />
  <row Id="1487" PostId="504" Score="1" Text="If your documents range from single words to full page of text, and you aim to have any combination of document lengths/types in any category, then you'll need to use a very simple encoding method such as Bag of Words.  Anything more complicated (e.g. grammar style) won't scale across that range." CreationDate="2014-06-20T18:29:26.120" UserId="609" />
  <row Id="1488" PostId="511" Score="1" Text="@Rubens, I've updated the question: I'm intersted in RF and logistic regression" CreationDate="2014-06-20T18:34:39.000" UserId="97" />
  <row Id="1489" PostId="510" Score="1" Text="I left behind the scope LinkedIn, YouTube, Secret. Maybe other regional networks (QQ?). And would be glad to get any info about them." CreationDate="2014-06-20T18:36:03.220" UserId="941" />
  <row Id="1491" PostId="403" Score="0" Text="Ben, thanks for the reply. I called the Scoring node directly from SAS Enterprise Miner, not much options we can change there. I compared the scoring output with what R predicted, and they perfectly matched. Then used the estimates manually , R's estimates theoretically matched with the output. It's just the problem with what SAS is giving as output, I am unable to understand that." CreationDate="2014-06-20T18:45:56.357" UserId="880" />
  <row Id="1492" PostId="509" Score="0" Text="No, it has nothing to do with spatial data in particular. It's applicable to temporal, spatio-temporal, and other sorts of data too." CreationDate="2014-06-21T06:10:06.757" UserId="381" />
  <row Id="1493" PostId="516" Score="3" Text="What is this algorithm *fmincg*? Please, add some references to such algorithm in your question." CreationDate="2014-06-21T06:51:15.523" UserId="84" />
  <row Id="1494" PostId="61" Score="0" Text="@blunders I just think this question is a bit of a mess and since the comments seemed to confuse you, I tried to explain them a bit." CreationDate="2014-06-21T09:17:29.030" UserId="762" />
  <row Id="1495" PostId="464" Score="0" Text="Thanks :), I already visited it before but I found it's classifications are weak not abstract enough or it may be not related to my content" CreationDate="2014-06-21T10:25:19.050" UserId="960" />
  <row Id="1496" PostId="466" Score="1" Text="Just a clarification, when you mention that &quot;in some papers  they have errors like MSE = 0.01&quot;, do you refer to the same dataset you are using? Or is it a different dataset altogether?" CreationDate="2014-06-21T17:29:42.433" UserId="1085" />
  <row Id="1497" PostId="507" Score="0" Text="@1 for crazy marketing guys. Working in market research and the twisting done to poor statistics makes me sad..." CreationDate="2014-06-22T05:18:22.043" UserId="791" />
  <row Id="1498" PostId="389" Score="0" Text="Wouldn't that also mean that my *bug* is stable across subsamples?" CreationDate="2014-06-22T06:13:51.587" UserId="846" />
  <row Id="1499" PostId="516" Score="0" Text="What is your alternate cost function? The optimal optimization algorithm depends on it :)" CreationDate="2014-06-22T12:03:45.373" UserId="1061" />
  <row Id="1500" PostId="403" Score="0" Text="Well I'm glad you were able to get a reasonable result with R! Hopefully someone who knows why this issue exists in SAS can answer the question!" CreationDate="2014-06-22T13:04:26.187" UserId="1011" />
  <row Id="1501" PostId="518" Score="0" Text="Your question is very weak, since you are neither presenting any problem, nor trying to understand something specific. Asking for comparisons between [online/batch k-means](http://datascience.stackexchange.com/questions/458/k-means-vs-online-k-means), or for how to efficiently implement some part of the algorithm is ok; asking for simple descriptions is too broad -- a google search may be a better fit." CreationDate="2014-06-22T16:56:52.783" UserId="84" />
  <row Id="1502" PostId="518" Score="0" Text="@Rubens I'm asking for a recommendation so what's wrong about that ?, I already googled it but I couldn't find something useful." CreationDate="2014-06-22T17:27:57.603" UserId="960" />
  <row Id="1503" PostId="389" Score="0" Text="That is a possible outcome, but you'll only knew once you try. And if so, you could at least debug on smaller data sets." CreationDate="2014-06-22T18:15:10.680" UserId="515" />
  <row Id="1505" PostId="527" Score="2" Text="It might be easier to answer your question if you gave an example of a matrix that you would consider similar to the first, and explained what qualities you are looking for in terms of similarity. Or if there is a general goal here, what is the task you mean to accomplish?" CreationDate="2014-06-23T02:25:12.900" UserId="322" />
  <row Id="1506" PostId="466" Score="0" Text="@AirThomas that's why I created a repository with a full explanation, with graphs etc." CreationDate="2014-06-23T09:38:09.920" UserId="989" />
  <row Id="1507" PostId="466" Score="0" Text="@insys no.. different :)" CreationDate="2014-06-23T09:38:25.643" UserId="989" />
  <row Id="1508" PostId="415" Score="0" Text="I would add *clarity and brevity* (related to syntax and language architecture, but not only). Being able to write fast and read without pain makes a huge difference (as programmers time is more expensive than machine time)." CreationDate="2014-06-23T11:14:14.553" UserId="289" />
  <row Id="1509" PostId="536" Score="1" Text="The question you are asking is vague. Please correct me if I'm wrong but the question essentially sums up to &quot;what to do if I have a small amount of data entries that need processing?&quot;. There are about a gazillion different answers to that depending on an equally large number of factors. You might at least present an example or make your question more specific. Things to consider: - What kind of data/fixes/errors are we talking about? -What prevents you from using a script exactly? -What are the &quot;unintended consequences&quot; you mention?" CreationDate="2014-06-23T11:15:13.220" UserId="1085" />
  <row Id="1510" PostId="536" Score="0" Text="@insys I gave examples (with city names) and modified question to address your doubts." CreationDate="2014-06-23T11:21:30.430" UserId="289" />
  <row Id="1511" PostId="536" Score="1" Text="About your phrase &quot;is not 100% clean&quot;: get used to it - it will always be like that :) Other than that, you have multiple questions in one. For example, named entities (cities) disambiguation reserves to be a question on its own. Overall there is no best practice, there's a whole field called data cleaning..." CreationDate="2014-06-23T11:29:16.800" UserId="418" />
  <row Id="1513" PostId="536" Score="0" Text="@iliasfl It's not my first day (but where data is far from being clear manual fixes usually make no sense). Point of this question is not city disambiguation (which is a fascinating topic on its own, and depends on a lot of things). It is how to incorporate manual changes (of, say, a few entries) into the workflow." CreationDate="2014-06-23T12:36:51.537" UserId="289" />
  <row Id="1514" PostId="483" Score="0" Text="I was looking into Neural Networks as an option, but didn't know what kind of parameters I would use.  I'll have to give those a shot." CreationDate="2014-06-23T12:54:17.227" UserId="886" />
  <row Id="1516" PostId="408" Score="0" Text="I will look into this option. Thanks." CreationDate="2014-06-23T12:59:31.563" UserId="886" />
  <row Id="1520" PostId="533" Score="1" Text="Adjusting the threshold for logistic regression did the trick. Thanks for the list of sources." CreationDate="2014-06-23T15:13:12.467" UserId="793" />
  <row Id="1521" PostId="538" Score="3" Text="off topic - discussion, opinion, and fist-fights will result." CreationDate="2014-06-23T15:15:53.153" UserId="471" />
  <row Id="1522" PostId="538" Score="2" Text="['To prevent your question from being flagged and possibly removed, avoid asking subjective questions where every answer is equally valid: “What’s your favorite ______?”'](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-06-23T15:29:06.037" UserId="322" />
  <row Id="1524" PostId="540" Score="4" Text="This is too broad - please read [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask) in the help center." CreationDate="2014-06-23T16:58:38.603" UserId="322" />
  <row Id="1525" PostId="541" Score="0" Text="Thanks, but you answered a different question. Mine is on workflow, not - science. I mean, in this particular case my goal is not making numerical estimations (then removing 'exceptions' by hand, or any other form of censoring, needs serious justification; and even with it is is easy to delude oneself or others), but rather (say) modifying data for visualization so end-user don't see misspelled cities (or worse: data split between copies of the same city, one with misspelled name). In any case you are right that explicit documentation of changes is crucial." CreationDate="2014-06-23T17:15:46.740" UserId="289" />
  <row Id="1526" PostId="541" Score="0" Text="My point is that the same principle applies to *how* you change the data, even if only how it is displayed. You need to make clear to the end user that some data has been modified. If what you are asking is exactly how one should modify their data, there is no good answer that will apply to every circumstance; it is entirely dependent on your use case, your software, your data structure, etc. etc. etc." CreationDate="2014-06-23T17:25:05.013" UserId="322" />
  <row Id="1527" PostId="540" Score="0" Text="Depends what type of dataset it is and more importantly what do you want to do with it? Ask yourself these questions... i) what is my objective; ii) if i'm going to train a model, will it be supervised or unsupervised?; iii) if supervised, do i have a train-test split? iv) do i have the reference class labels? etc." CreationDate="2014-06-23T17:28:50.353" UserId="984" />
  <row Id="1530" PostId="497" Score="0" Text="There is a whole class of statistical analytics devoted to modeling longitudinal data. If you had repeated measures on the same subjects then mixed models are used often as state of the art in social sciences to determine if there is impact of an intervention. If you have a time series only something like an Arima can be used." CreationDate="2014-06-23T18:14:57.430" UserId="1138" />
  <row Id="1532" PostId="527" Score="0" Text="Yeah, I'd like to see an example of what a 1 would look like and what a 0 would look like." CreationDate="2014-06-23T18:58:14.737" UserId="1011" />
  <row Id="1533" PostId="546" Score="3" Text="And you can formalize this and make it &quot;prettier&quot; by constructing in your script a set of known cities and an `N:1` mapping of known misspellings." CreationDate="2014-06-23T19:11:23.767" UserId="322" />
  <row Id="1534" PostId="497" Score="0" Text="A RDD approach might also be useful for you: http://austinclemens.com/blog/2014/06/08/436/" CreationDate="2014-06-23T19:30:58.427" UserId="1138" />
  <row Id="1535" PostId="549" Score="0" Text="My concern with this answer is that PCA doesn't recognize the clear dependency between the series t and t+1." CreationDate="2014-06-24T01:50:57.443" UserId="1138" />
  <row Id="1541" PostId="559" Score="1" Text="Perhaps this question is better suited to the cross validation SE site, now that I think of it. The distinction is somewhat unclear to me..." CreationDate="2014-06-24T12:36:56.940" UserId="1147" />
  <row Id="1542" PostId="559" Score="4" Text="I think the question is very much fitting to this site, since it discusses a practical application of machine learning. btw, silly question, why so few photos of cats? Do they only come around for just five seconds?" CreationDate="2014-06-24T12:45:46.473" UserId="1085" />
  <row Id="1545" PostId="552" Score="0" Text="Thanks for the link. I had also considered using DTW and hierachical clustering. I have experimented with the R package for DWT. http://www.jstatsoft.org/v31/i07/paper" CreationDate="2014-06-24T13:58:49.467" UserId="1138" />
  <row Id="1546" PostId="552" Score="1" Text="I considered specifically creating n clusters and using the clustering membership as a feature." CreationDate="2014-06-24T13:59:40.057" UserId="1138" />
  <row Id="1547" PostId="550" Score="0" Text="Nice suggestions! Can you flesh out the use of derivatives more?" CreationDate="2014-06-24T14:04:44.013" UserId="1138" />
  <row Id="1548" PostId="550" Score="0" Text="I agree completely with your first statement. I would LOVE to see a box written which collected case studies on feature engineering / extraction. The adage is that feature creation is much more important than the latest greatest algorithm in predictive model performance." CreationDate="2014-06-24T14:07:06.180" UserId="1138" />
  <row Id="1550" PostId="523" Score="0" Text="I tried your FFT method but I really don't get how to set the frequency threshold and amplitude with my data. I'll keep looking, but if u could help me..." CreationDate="2014-06-24T15:28:00.807" UserId="989" />
  <row Id="1552" PostId="559" Score="0" Text="@insys, rumors about my vigilance with the soaker appears to have spread in the feline community. They tend not to linger like they used to. I guess that's a good thing w/r/t the actual objective of ridding my garden of cats, even though it complicates my preferred, more sophisticated solution." CreationDate="2014-06-24T19:54:37.957" UserId="1147" />
  <row Id="1553" PostId="561" Score="0" Text="This is a nice idea for a controlled environment but I'm not sure of it's applicability in this case, since we are dealing with the natural environment where there is continuous change, i.e. change in weather, position of sun, plants and trees because of wind, seasons etc. I believe the region of change as you describe would grow close to the size of the whole image in any case." CreationDate="2014-06-24T20:05:18.477" UserId="1085" />
  <row Id="1555" PostId="567" Score="0" Text="thanks but I'm aiming this at non technical business users, ive updated question. I think a colormap would go over their heads" CreationDate="2014-06-24T20:34:33.083" UserId="237" />
  <row Id="1556" PostId="567" Score="0" Text="@blue-sky Updated my answer. May not be what you're looking for, but that's all I've got =)" CreationDate="2014-06-24T20:37:47.817" UserId="1163" />
  <row Id="1557" PostId="561" Score="0" Text="@insys - I see your point but I disagree - I believe it makes the detector more resilient to change. The time difference between relative frames should be small (~ seconds to a minute) so sun, season, weather should be negligible. I agree that wind will cause plants to move but the classification step can avoid those since their size/shape/color are different than a cat. Plus, using two frames at similar times enables normalizing pixel intensities to better handle varying illumination conditions (e.g., a cat on a sunny vs. cloudy day)." CreationDate="2014-06-24T20:55:20.650" UserId="964" />
  <row Id="1558" PostId="559" Score="2" Text="Seems like the obvious next step (after you get the cat detection working) is a raspberry pi controlled super soaker :-)" CreationDate="2014-06-24T20:56:38.683" UserId="1167" />
  <row Id="1559" PostId="561" Score="0" Text="Actually, I am more confused about your answer now that I read through your comment :) Perhaps I misunderstood, but if you actually use the &quot;extracted change regions&quot; to form your positive samples, as mentioned in your question, how do you even make sure they are cats? They could be anything. As such, your classification step would fail to detect anything but what is taught to detect – that is, changes of any kind. So it is actually repeating the job of the &quot;change&quot; detector." CreationDate="2014-06-24T21:21:19.717" UserId="1085" />
  <row Id="1560" PostId="561" Score="0" Text="Furthermore, illumination conditions are definitely of concern, but, if I get your point right, it is unclear what two similar images, taken with a difference of 1 minute would offer towards normalising pixel intensities?" CreationDate="2014-06-24T21:22:00.297" UserId="1085" />
  <row Id="1561" PostId="561" Score="0" Text="The extracted regions can represent either positive or negative examples - they are what you would use to train the cat classifier. With regard to intensities, Suppose the classifier is trained from regions extracted primarily from sunny images. The classifier might then easily find cats with bright white fur but that won't work well later on a cloudy day (when the white fur isn't nearly as bright) or near dusk. Performing a normalization of the two images helps mitigate that problem (i.e., a pair of bright images and a pair of dim images would appear similar to the classifier)." CreationDate="2014-06-24T22:00:18.433" UserId="964" />
  <row Id="1562" PostId="446" Score="0" Text="Marking this as the accepted answer because out of the approaches suggested, it's the most promising for my particular use case." CreationDate="2014-06-24T22:00:36.210" UserId="322" />
  <row Id="1564" PostId="512" Score="0" Text="I don't think this suggestion addresses the problem as it's presented in the question. Where is the flexibility post-encryption? How do I refine your analysis without access to the original data?" CreationDate="2014-06-24T22:17:32.963" UserId="322" />
  <row Id="1565" PostId="568" Score="0" Text="would this work on a manchester encoded signal where the value is encoded in the state transitions?" CreationDate="2014-06-24T22:21:32.167" UserId="890" />
  <row Id="1566" PostId="559" Score="1" Text="@Kryten related: YouTube [Militarizing Your Backyard with Python: Computer Vision and the Squirrel Hordes](https://www.youtube.com/watch?v=QPgqfnKG_T4)" CreationDate="2014-06-24T22:26:40.920" UserDisplayName="user1170" />
  <row Id="1570" PostId="512" Score="0" Text="@AirThomas I'm sorry but I don't understand your two questions.  What do you mean by &quot;flexibility post-encryption&quot;? I didn't see anything in your question/description like that.  What do you mean &quot;refine your analysis without access to the original data&quot;? I didn't see anything about &quot;refining&quot;." CreationDate="2014-06-25T02:53:14.440" UserId="609" />
  <row Id="1571" PostId="566" Score="0" Text="So can I say that your question is about &quot;Is combining two consecutive NNP Trees a good approach?&quot;" CreationDate="2014-06-25T03:43:55.783" UserId="1177" />
  <row Id="1572" PostId="512" Score="0" Text="I tried to identify the problem in the second paragraph of the **Motivation** section. Imagine, for example, that you want to release your data set to various researchers who want to do some modeling. There are any number of clever and effective methodologies that could be applied, and each researcher works a little differently. You can't disclose the names of private individuals in your data set. If you perform that portion of the analysis before releasing the data, it forces your choice of methodology on everyone." CreationDate="2014-06-25T03:49:58.180" UserId="322" />
  <row Id="1573" PostId="512" Score="0" Text="If you additionally provide hashes of the names, the benefit is that third parties can distinguish exact identity, but no more. So the question is, how might you provide more information about the data that you can't release? For example, is there a method that preserves in the hashing/encryption output the edit distance between arbitrary inputs? I have found at least one method that at least approximates that functionality (for more information, see my own answer). I hope that makes things more clear." CreationDate="2014-06-25T03:57:55.367" UserId="322" />
  <row Id="1574" PostId="47" Score="1" Text="Another package is distributedR which allows you to work with distributed files in RAM." CreationDate="2014-06-25T07:03:56.810" UserId="1155" />
  <row Id="1575" PostId="560" Score="0" Text="And what would happen if your block splits the cut into two or more slices? The blocking strategy is a very common approach, but when having a camera completely fixed to a certain position, motion detection is a better and less time-consuming approach, from my point of view." CreationDate="2014-06-25T07:08:37.443" UserId="1155" />
  <row Id="1576" PostId="568" Score="0" Text="It depends on the Manchester codification but I would say so. Nonetheless, previous to a HMM training, I'd suggest to use a zero-crossing algorithm to detect flanks of the signal. With this, you could detect the minimum time a change occurs which can give you a hint on the clock speed." CreationDate="2014-06-25T07:11:45.257" UserId="1155" />
  <row Id="1578" PostId="565" Score="1" Text="Any chance of a reproducible example with code and data?" CreationDate="2014-06-25T07:29:35.100" UserId="471" />
  <row Id="1579" PostId="560" Score="0" Text="@adesantos – What you say may well be true, and for prediction differentiating between moving and non-moving parts has it's advantages. But for training, the way it is described by bogatron, it is unclear what benefits it brings to the table. Overall, my opinion is it adds complexity, which lengthens the debugging time significantly. The advantage of moving window is in it's simplicity." CreationDate="2014-06-25T07:36:28.230" UserId="1085" />
  <row Id="1580" PostId="560" Score="0" Text="Btw, regarding the split that you mention, an obvious strategy is to let your windows overlap, so that split position does not affect your classifier." CreationDate="2014-06-25T07:36:50.183" UserId="1085" />
  <row Id="1581" PostId="570" Score="0" Text="please see question update, so for my use case instead of a movie display a user (that is similar) , the stars indicate the degree of similarity ?" CreationDate="2014-06-25T08:01:22.837" UserId="237" />
  <row Id="1584" PostId="560" Score="0" Text="I would add to my proposal (motion detection) the use of SIFT algorithm with a cat texture. The SIFT method can also be used with that strategy of blocks, but in that case you will compare more blocks than require.&#xA;&#xA;Notice that a cat moves, but a tree or a bush not that much." CreationDate="2014-06-25T09:06:43.937" UserId="1155" />
  <row Id="1585" PostId="570" Score="0" Text="Exactly - instead of a movie poster, you can display a user profile picture.  So several rows - Users similar to A, Users similar to B, etc. and across each row, user profile pictures.  Below each user profile picture a star rating.  You indicate that the similarity is stronger the closer to 0, so .2-.0 = 5 stars, .4-.2 = 4 stars, etc.  If there are no profile pictures available, a user name would do, but people will respond to photos better." CreationDate="2014-06-25T09:08:57.833" UserId="434" />
  <row Id="1586" PostId="221" Score="0" Text="I do not agree trees are of help here. It is a matter of filtering rules, and that can be achieved with the _arules_ package in R." CreationDate="2014-06-25T10:13:45.667" UserId="1155" />
  <row Id="1587" PostId="566" Score="0" Text="True that... good approach for Named Entity Recognition (NER) ?" CreationDate="2014-06-25T14:12:52.660" UserId="1165" />
  <row Id="1588" PostId="568" Score="0" Text="Why would I need clock speed? Manchester encoding is self clocking. Timing should be unimportant." CreationDate="2014-06-25T14:21:50.763" UserId="890" />
  <row Id="1590" PostId="435" Score="2" Text="What about rules mining? It is not clear to me what is your aim." CreationDate="2014-06-25T14:32:26.520" UserId="1155" />
  <row Id="1591" PostId="24" Score="1" Text="I do not recommend converting categorical attributes to numerical values. Imagine you have two city names: NY and LA. If you apply NY number 3 and LA number 8, the distance is 5, but that 5 has nothing to see with the difference among NY and LA." CreationDate="2014-06-25T14:38:17.337" UserId="1155" />
  <row Id="1592" PostId="568" Score="0" Text="I though it could be helpful to know the clock speed in order to know how fast are the transitions between low/high values." CreationDate="2014-06-25T14:39:49.943" UserId="1155" />
  <row Id="1594" PostId="559" Score="0" Text="@MichaelT Yeah, I've been looking at that to solve a pigeon problem at my house" CreationDate="2014-06-25T15:39:56.180" UserId="1167" />
  <row Id="1596" PostId="559" Score="0" Text="There is open-source software for this, known as &quot;motion&quot;: http://www.lavrsen.dk/foswiki/bin/view/Motion/WebHome" CreationDate="2014-06-25T15:54:51.600" UserId="924" />
  <row Id="1597" PostId="565" Score="2" Text="which playground competition is it?" CreationDate="2014-06-25T16:01:01.027" UserId="153" />
  <row Id="1598" PostId="264" Score="1" Text="Can you be more specific? EM refers to an optimization algorithm that can be used for clustering. There are many ways to do this and it is not obvious what you mean." CreationDate="2014-06-25T19:18:45.110" UserId="1193" />
  <row Id="1600" PostId="559" Score="0" Text="@Anony-Mousse I tried the motion software before posting the question here, but it failed miserably in detecting any kind of animals, instead reacting only to leaves/trees moving, sunlight intensity changes and such – _except_ in the sample image above where it actually detected a bee flying close to the camera. Might work better in other environments." CreationDate="2014-06-25T20:43:05.533" UserId="1147" />
  <row Id="1601" PostId="518" Score="0" Text="This question appears to be off-topic because Data Science Stack Exchange is not a link recommendation service. Please use Google for basic information and tutorials." CreationDate="2014-06-26T01:41:16.660" UserId="62" />
  <row Id="1603" PostId="586" Score="2" Text="This online book will be useful http://www.nltk.org/book/" CreationDate="2014-06-26T00:40:36.690" UserId="1196" />
  <row Id="1604" PostId="200" Score="0" Text="Thank you for your post, but can you include here an overall description of the solution linked? The link may have the answer to the question, but link-only answers are discouraged." CreationDate="2014-06-26T04:41:26.317" UserId="84" />
  <row Id="1607" PostId="555" Score="1" Text="+1 This is one of the best answers I've ever read." CreationDate="2014-06-26T09:14:03.430" UserId="1155" />
  <row Id="1609" PostId="595" Score="1" Text="What do you mean by 'restoring parameters'? Do you mean populating those users who didn't indicate gender/age and so forth?" CreationDate="2014-06-26T13:00:25.503" UserId="1155" />
  <row Id="1611" PostId="595" Score="0" Text="@adesantos Yes. Is it possible?" CreationDate="2014-06-26T13:09:04.630" UserId="1207" />
  <row Id="1612" PostId="598" Score="0" Text="Which classification algorithm do you recommend in my case? Will be Naive Bayes classifier good choice?" CreationDate="2014-06-26T14:23:49.747" UserId="1207" />
  <row Id="1613" PostId="598" Score="0" Text="I suggest, as I said, not to populate and keep a variable called unknown." CreationDate="2014-06-26T14:25:01.953" UserId="1155" />
  <row Id="1614" PostId="598" Score="0" Text="I have no choice. It's must be done. But haven't expirience enough." CreationDate="2014-06-26T14:32:36.413" UserId="1207" />
  <row Id="1615" PostId="595" Score="0" Text="You may wish to look up the term data imputation as well" CreationDate="2014-06-26T14:46:13.673" UserId="1085" />
  <row Id="1616" PostId="598" Score="0" Text="Then use the second approach I told you: Create a distribution. But if you want to keep it simple, populate with the most common value. That's the easiest option." CreationDate="2014-06-26T14:53:21.123" UserId="1155" />
  <row Id="1617" PostId="600" Score="3" Text="[Cross posting](http://stats.stackexchange.com/q/104868) seems in poor form. Why is this tagged SQL?" CreationDate="2014-06-26T15:34:29.440" UserId="322" />
  <row Id="1618" PostId="600" Score="0" Text="Solve the (approximate) travelling salesman problem (TSP)..." CreationDate="2014-06-26T16:08:01.983" UserId="984" />
  <row Id="1620" PostId="597" Score="0" Text="Well, *someone* knows for sure what Google uses, and there are plenty of Google employees with SE accounts. Perhaps one of them will come along and provide us all with some interesting (non-proprietary) information." CreationDate="2014-06-26T17:28:18.160" UserId="322" />
  <row Id="1621" PostId="596" Score="1" Text="I think you could improve this question by defining &quot;gold standard dataset&quot; in a more objective fashion. What makes it &quot;must-know&quot;? Should it be referenced in a number of textbooks? Used in a number of published models? Etc. Otherwise the answers will be subjective AND they will change as time passes. A bad combination here." CreationDate="2014-06-26T17:33:37.880" UserId="322" />
  <row Id="1623" PostId="605" Score="0" Text="Normalize which matrix? These similarity measures do not require arguments in [0,1], by the way." CreationDate="2014-06-26T22:33:11.973" UserId="21" />
  <row Id="1624" PostId="599" Score="0" Text="Thanks, but it's not exactly what I'm looking for. See update for more details." CreationDate="2014-06-27T05:23:06.967" UserId="941" />
  <row Id="1625" PostId="612" Score="0" Text="Yes I still intend to cross-validate to check for over-fitting, my question is more basic than that - I cannot find any references to using regularisation with a per-item weight adjustment in MLP at all, and am concerned there is a good reason for that - e.g. it does not work in that learning mode, or needs adjustment. The crossvalidated question *is* very similar though and gives me more confidence, thank you. The SGD algorithm page seems to have a different, stochastic method for introducing regularisation, which might be a bit advanced for me, but is exactly what I am looking for." CreationDate="2014-06-27T07:30:40.650" UserId="836" />
  <row Id="1626" PostId="612" Score="0" Text="Regularization is relevant in per-item learning as well. I would still suggest to start with a basic validation approach for finding out lambda. This is the easiest and safest approach. Try manually with a number of different values. e.g. 0.001. 0.003, 0.01, 0.03, 0.1 etc. and see how your validation set behaves. Later on you may automate this process by introducing a linear or local search method." CreationDate="2014-06-27T09:38:04.807" UserId="1085" />
  <row Id="1627" PostId="612" Score="0" Text="If your comment above was edited in and replaced the first sentence/question in your answer, then I think I could accept it." CreationDate="2014-06-27T09:40:46.120" UserId="836" />
  <row Id="1628" PostId="612" Score="0" Text="Thanks for pointing out, I agree. Edited it in. Hope it is more clear." CreationDate="2014-06-27T09:55:29.610" UserId="1085" />
  <row Id="1629" PostId="468" Score="1" Text="Given that this is a question about the statistical model, you may want to go to [CrossValidated](http://stats.stackexchange.com) website, but keep in mind that it is a terrible practice to cross-post the questions: you would either want to formulate it to highlight the methodological issues you are facing, or migrate the whole question." CreationDate="2014-06-27T13:23:38.907" UserId="1237" />
  <row Id="1633" PostId="468" Score="0" Text="Without really explaining why, [ISL](http://www-bcf.usc.edu/~gareth/ISL) notes (on p 137) that discriminant analysis (like LDA, QDA) is more often used than multiple class extensions of logistic regression. Packages like [penalizedLDA](http://cran.r-project.org/web/packages/penalizedLDA/index.html) may therefore be worth examining." CreationDate="2014-06-27T21:03:57.007" UserId="953" />
  <row Id="1636" PostId="607" Score="1" Text="First question you should ask here is do you need a virtual machine as an instrument for your data science practice? (Maybe even rephrase the original question.) And there are more than the two mentioned VM to choose from. Here is a link to a [post comparing four VMs for data science.](http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html)" CreationDate="2014-06-28T09:51:37.420" UserId="454" />
  <row Id="1637" PostId="619" Score="0" Text="Thank u for ur kind answer..." CreationDate="2014-06-28T16:20:27.867" UserId="1235" />
  <row Id="1638" PostId="619" Score="0" Text="Can we do Regresssion,clustering,classifications using Rmr...... If it is possible then we can do using R directly.. only because of Parallelism we are using Rmr.. If i am correct.. Is there any main Difference between Hadoop Mapreduce and R mapreduce (apart from Parallelism)..." CreationDate="2014-06-28T16:21:39.303" UserId="1235" />
  <row Id="1639" PostId="619" Score="0" Text="rmr is a framework for running R functions in MapReduce. That is the thing it lets you do that you could not do before. It is not a library of new statistical functions of course." CreationDate="2014-06-28T17:32:36.687" UserId="21" />
  <row Id="1640" PostId="607" Score="0" Text="This would even be sufficient as a reply. Fantastic. Even though the poster is the creator of the data science tool box so it is not an unbiased report, but it's the best thing I've seen - so thanks." CreationDate="2014-06-28T20:54:44.647" UserId="1223" />
  <row Id="1641" PostId="561" Score="0" Text="Have a look at background subtraction in opencv...it is doing anomaly detection on pixels, by basically calculating historical mean, variance (in fact gaussian mixture model)." CreationDate="2014-06-28T23:06:53.930" UserId="1256" />
  <row Id="1642" PostId="623" Score="0" Text="If you have different classifiers trained on different datasets, how can you compare them in a meaningful way? Apples and oranges, chalk and cheese come to mind. Also, if you have multiclass classifiers, how do you calculate precision and recall?  Even knowing N=1 is not necessarily helpful - if there is only one egg in the world, your egg classifier is fine." CreationDate="2014-06-29T06:58:18.617" UserId="1230" />
  <row Id="1643" PostId="623" Score="0" Text="They're different classifiers trained on the same datasets, e.g. we know we have a document that's about apples and oranges, so we run an apple classifier on it to determine  type of apple it's talking about, and an orange classifier to determine type of orange it talks about. If our documents are 99% about apples, 1% about oranges, and both classifiers have the same prec/rec (summing rows/cols over confusion matrix), is there any info we can present that takes into account the differences in quantities of each? (it might be that no, there isn't, which is an answer I'd be happy with)" CreationDate="2014-06-29T09:06:28.753" UserId="474" />
  <row Id="1644" PostId="619" Score="0" Text="Then any other  specific feature where hadoop mapreduce can't handle?....." CreationDate="2014-06-29T11:32:23.643" UserId="1235" />
  <row Id="1645" PostId="414" Score="0" Text="In practice, you will use a learning rate with adadelta. On some problems it does not work without." CreationDate="2014-06-29T18:18:55.930" UserId="1193" />
  <row Id="1646" PostId="535" Score="0" Text="This seems to work pretty well except the scaling of the numbers between 0 and 1. Not sure whether the python version is as intended?" CreationDate="2014-06-29T22:08:50.460" UserId="1107" />
  <row Id="1647" PostId="534" Score="0" Text="True, I'd forgotten about norms, I'll look into this thanks." CreationDate="2014-06-29T22:09:23.323" UserId="1107" />
  <row Id="1648" PostId="603" Score="0" Text="I agree with Steve K that the key to tackling this is to aim for approximately optimal or just good route strategies. Many times the difference between &quot;best&quot; and &quot;good enough&quot; isn't much." CreationDate="2014-06-30T03:55:22.573" UserId="609" />
  <row Id="1651" PostId="535" Score="0" Text="I simplified your python version. And what's wrong with scaling? Assuming, that all values in original matrix are between 0 and 1, result should also be of the same scale." CreationDate="2014-06-30T05:38:40.697" UserId="941" />
  <row Id="1652" PostId="586" Score="0" Text="@RajaPasupuleti Ah yes this is very useful. Thanks!" CreationDate="2014-06-30T07:38:22.640" UserId="1192" />
  <row Id="1653" PostId="203" Score="0" Text="Sorry, I take your point.  I thought &quot;Symphony / COIN-OR seem to be the dominant suggestions&quot; answered the question &quot;is there an open source [...] alternative [...]?&quot; - the links were for backup as to why. I did caveat that the OP's scenario is on a bigger scale than I'm used to - I cannot give any extra information from personal experience but as this is a new stackexchange site and no-one had jumped in, I thought I'd try to assist. Will delete if you think it's not helpful / an answer." CreationDate="2014-06-30T10:45:17.277" UserId="265" />
  <row Id="1654" PostId="535" Score="0" Text="Nothing is wrong with the scaling now... I must of had a bug in my code. Thanks for the help this works great on my dataset" CreationDate="2014-06-30T11:38:42.077" UserId="1107" />
  <row Id="1655" PostId="636" Score="0" Text="That is a lot of open ended questions to ask all at once. One of the goals of this site is that questions and their answers should be useful to future visitors. Can you be more specific? What looks bad in your linear model? When you tried the logistic model, how did it come out? Were there specific problems?" CreationDate="2014-06-30T15:52:28.900" UserId="322" />
  <row Id="1656" PostId="30" Score="1" Text="If the total amount of data in the world doubles every 40 months, then surely it *can* get bigger than that. ;p" CreationDate="2014-06-30T16:00:06.990" UserId="322" />
  <row Id="1660" PostId="636" Score="0" Text="Sorry,an eg:&#xA;&#xA;On trying logit in R with these values:&#xA; &#xA;day-28, hour-11,day of the week-7, state-New Mexico, OS-iOS, OS Version-7.0, browser family-Mobile Safari,  browser version-7.0, device manufacturer-apple, IP carrier- Comcast, user age-20, gender-male, click happened-yes, predicted probability from GLM in R-0.000000001.&#xA;&#xA;In another instance,day-28, hour-12th, day of the week-7, state-Connecticut, OS-iOS, OS version-7.0, browser-mobile safari, browser version-7.0, device manufacturer-apple, IP carrier-Comcast, user age-22, user gender-female,click happened-no, predicted probability-.046" CreationDate="2014-06-30T17:02:06.487" UserId="1273" />
  <row Id="1662" PostId="636" Score="0" Text="Thanks. There's an &quot;edit&quot; link at the bottom of your question so that you can update it with this additional information." CreationDate="2014-06-30T17:13:58.197" UserId="322" />
  <row Id="1663" PostId="636" Score="0" Text="That is what I meant by how the model does not look good. I think this is due to the sparsity of instances of clicks happening and random nature of event of clicks happening.&#xA;&#xA;So how can I go about sampling the data? Also how should I add weights to the parameters for improving the prediction? Sorry for adding more open-ended questions but encountering issues as they come, I also want to ask how to work with large datasets in R?" CreationDate="2014-06-30T17:44:17.230" UserId="1273" />
  <row Id="1664" PostId="641" Score="1" Text="Recommend asking on http://opendata.stackexchange.com/" CreationDate="2014-06-30T21:28:41.943" UserId="322" />
  <row Id="1665" PostId="520" Score="0" Text="I added some graphs, after having improved the model A LOT. In github there are the new steps. May I ask you how I can apply linear regression in a time series problem? :(" CreationDate="2014-06-30T22:59:25.760" UserId="989" />
  <row Id="1666" PostId="523" Score="0" Text="I added the sources also" CreationDate="2014-06-30T23:00:24.603" UserId="989" />
  <row Id="1667" PostId="519" Score="0" Text="I added some graphs and you can check also about the parameters now :)" CreationDate="2014-06-30T23:01:05.820" UserId="989" />
  <row Id="1668" PostId="466" Score="0" Text="Project updated!" CreationDate="2014-06-30T23:01:21.583" UserId="989" />
  <row Id="1669" PostId="603" Score="0" Text="Of course the optimum can be found, it might just take longer than the age of the universe to iterate over all the possibilities. Your answer fails to mention this." CreationDate="2014-07-01T13:23:16.427" UserId="471" />
  <row Id="1670" PostId="600" Score="0" Text="Beyond lat-long, what's the geography like? A gridded city? An almost tree-shaped suburb with smaller roads into cul-de-sacs? Has a MASSIVE influence." CreationDate="2014-07-01T13:24:47.337" UserId="471" />
  <row Id="1671" PostId="24" Score="0" Text="@adesantos Yes, that's a problem with representing multiple categories with a single numeric feature and using a Euclidean distance.  Using the Hamming distance is one approach; in that case the distance is 1 for each feature that differs (rather than the difference between the numeric values assigned to the categories).  Making each category its own feature is another approach (e.g., 0 or 1 for &quot;is it NY&quot;, and 0 or 1 for &quot;is it LA&quot;)." CreationDate="2014-07-01T14:36:16.047" UserId="14" />
  <row Id="1672" PostId="642" Score="0" Text="Is this the Gelman paper referenced: http://www.stat.columbia.edu/~gelman/research/published/rsquared.pdf?" CreationDate="2014-07-01T17:51:00.910" UserId="684" />
  <row Id="1673" PostId="641" Score="1" Text="Thanks for the recommendation!" CreationDate="2014-07-01T17:51:26.133" UserId="684" />
  <row Id="1674" PostId="644" Score="0" Text="I didn't quite get your question. Do you intend to compute pairwise cosine similarities between every pair of row vectors (observations)?" CreationDate="2014-07-01T18:37:05.693" UserId="984" />
  <row Id="1675" PostId="644" Score="0" Text="Moreover, I didn't quite get why are you taking the average?" CreationDate="2014-07-01T18:37:53.627" UserId="984" />
  <row Id="1677" PostId="642" Score="0" Text="That works, but I highly recommend his [book on multilevel modeling](http://www.stat.columbia.edu/~gelman/arm/)." CreationDate="2014-07-01T19:01:15.310" UserId="159" />
  <row Id="1678" PostId="642" Score="0" Text="Ah, awesome, thanks!" CreationDate="2014-07-01T19:14:50.183" UserId="684" />
  <row Id="1679" PostId="644" Score="0" Text="If your observations are just 0s and 1s, then I don't think cosine similarity will work. Perhaps you should consider using Tanimoto similarity, which considers similarity across bitmaps." CreationDate="2014-07-02T01:30:01.480" UserId="24" />
  <row Id="1680" PostId="644" Score="0" Text="@Debasis yes, I do plan to compute similarities between each row. And I'm only taking the average as an example strategy to show what my computations might involve." CreationDate="2014-07-02T03:30:08.680" UserId="754" />
  <row Id="1681" PostId="644" Score="0" Text="@buruzaemon my observations aren't actually 0s and 1s, though I do like the simplicity of those kind of similarities. I actually asked this question because I'm interested in what the right answer for cosine similarities is. But yeah, I do appreciate the advice." CreationDate="2014-07-02T03:35:10.417" UserId="754" />
  <row Id="1682" PostId="649" Score="1" Text="Thanks! I would upvote this, but I don't have the reputation necessary. Maybe someone else can do that for me?" CreationDate="2014-07-02T03:43:26.020" UserId="754" />
  <row Id="1683" PostId="555" Score="1" Text="+1, and you should always put the original data into some kind of source control. Very sound advice, @Steve" CreationDate="2014-07-02T04:48:19.950" UserId="24" />
  <row Id="1684" PostId="628" Score="0" Text="Did you mean to say &quot;increase reduce the model complexity&quot; on the last bullet point? I think just &quot;increase the model complexity&quot; . . . BTW good timing I am enrolled in that course and had only just watched the video you are referring to." CreationDate="2014-07-02T10:24:36.813" UserId="836" />
  <row Id="1685" PostId="654" Score="1" Text="So, kNN is not training but lazy learning, because it does not abstract and generalize but, in case of classification, for each new observation it learns from the nearest k-labeled observations. While NTP is kNN-1 between an observation and the signature (that can be considered a pseudo-observation). Am I right?" CreationDate="2014-07-02T14:57:33.457" UserId="133" />
  <row Id="1686" PostId="616" Score="0" Text="Note to readers: [This question is also under discussion on Stack Overflow, where a high-quality solution has been offered.](http://stackoverflow.com/q/24455620/2359271)" CreationDate="2014-07-02T15:23:50.970" UserId="322" />
  <row Id="1687" PostId="654" Score="2" Text="While the kNN classifier is referred to as a &quot;lazy learner&quot;, that term is somewhat misleading and it is more accurately described as a &quot;lazy classifier&quot;. With regard to your comment, it doesn't actually *learn* when you give it a new observation to classify - it simply calculates the appropriate class. If you give it that same observation later, it would perform the same computation the second time because it doesn't actually learn from the first observation. In short, kNN doesn't produce/learn a model - it simply stores training data, then uses those data when it is time to classify." CreationDate="2014-07-02T16:48:34.320" UserId="964" />
  <row Id="1688" PostId="628" Score="0" Text="@NeilSlater Thanks, good catch, there was indeed a typo :)" CreationDate="2014-07-02T16:56:18.073" UserId="843" />
  <row Id="1690" PostId="655" Score="0" Text="I'm not sure if you'll do better than predicting 1/f noise when using past values as indicators for future ones. http://www.scholarpedia.org/article/1/f_noise#Stock_markets_and_the_GNP - your results so far seem consistent with that. Probably you should look at other possible features that have some reason to correlate with future exchange rates. If this were easy, there would be more rich data scientists." CreationDate="2014-07-02T23:32:03.263" UserId="836" />
  <row Id="1691" PostId="636" Score="0" Text="The class imbalance is such that you don't really want to predict click vs no click.  Since almost no one clicks your best result is to simply predict they don't click. Instead, think about predicting for all the users that click...which ad will they click on. This is a much more useful problem." CreationDate="2014-07-03T03:49:44.793" UserId="92" />
  <row Id="1692" PostId="660" Score="1" Text="You need to find the log output from your R script, which would indicate the error. &quot;hadoop streaming failed with error code 1&quot; just means &quot;the script failed for some reason&quot;" CreationDate="2014-07-03T11:19:25.687" UserId="21" />
  <row Id="1694" PostId="660" Score="1" Text="Sometimes the folder where you write must be deleted before writing (if it exists). Check that out." CreationDate="2014-07-03T11:41:17.003" UserId="1155" />
  <row Id="1695" PostId="646" Score="1" Text="Shouldn't any old spatial index that supports containment queries work? i.e. an r-tree?" CreationDate="2014-07-03T12:40:20.623" UserId="1283" />
  <row Id="1696" PostId="660" Score="0" Text="thank u for ur answer ... but i have check all the possibilites what u have mentioned... i doubt that there is some problem with code itself...can someone please rectify..." CreationDate="2014-07-03T15:19:02.067" UserId="1235" />
  <row Id="1697" PostId="661" Score="2" Text="try `sudo hadoop fs -ls -l /` and see what comes back.  Usually a permissions error comes back explicitly, but lets have a gander at what the root path looks like.  I wonder if the file system was never formatted." CreationDate="2014-07-03T17:39:19.290" UserId="434" />
  <row Id="1698" PostId="661" Score="2" Text="This might be better at serverfault.com" CreationDate="2014-07-03T21:36:59.580" UserId="21" />
  <row Id="1699" PostId="667" Score="0" Text="Algorithm-wise: what would you recommend ?" CreationDate="2014-07-03T21:59:30.367" UserId="1315" />
  <row Id="1700" PostId="663" Score="0" Text="I would love to see your example." CreationDate="2014-07-03T22:03:13.677" UserId="1315" />
  <row Id="1701" PostId="663" Score="0" Text="Updated with quick example." CreationDate="2014-07-03T22:52:43.353" UserId="375" />
  <row Id="1702" PostId="653" Score="3" Text="Other keywords that might be useful for what you are looking are *clustering* and the more general *unsupervised learning*." CreationDate="2014-07-04T07:24:06.803" UserId="113" />
  <row Id="1703" PostId="667" Score="0" Text="you mean algorithm for computing the most similar resume vectors given a query job vector? you can use any standard algorithm such as BM25 or Language Model..." CreationDate="2014-07-04T11:35:38.297" UserId="984" />
  <row Id="1704" PostId="667" Score="0" Text="I have never heard of these algorithms at all. Are these NLP algorithms or ML algo ?" CreationDate="2014-07-04T13:32:43.553" UserId="1315" />
  <row Id="1705" PostId="667" Score="0" Text="these are standard retrieval models... a retrieval model defines how to compute the similarity between a document (resume in your case) and a query (job in your case)." CreationDate="2014-07-04T15:23:49.700" UserId="984" />
  <row Id="1706" PostId="667" Score="0" Text="I have no knowledge about information retrieval, do you think machine learning algorithms like clustering / nearest neighbour will also work in my case ?" CreationDate="2014-07-04T16:37:18.267" UserId="1315" />
  <row Id="1707" PostId="629" Score="0" Text="This. Is. Fantastic! Thanks @AsheeshR" CreationDate="2014-07-05T08:07:48.490" UserId="1223" />
  <row Id="1708" PostId="669" Score="0" Text="great thanks. I will evaluate robust regression and see how it goes." CreationDate="2014-07-05T11:04:54.390" UserId="1300" />
  <row Id="1709" PostId="655" Score="0" Text="yes, Maybe other variables are contributing to the next value more than the time series values it self.. I will experiment with that too. Thank you for the pointers." CreationDate="2014-07-05T11:08:20.760" UserId="1300" />
  <row Id="1710" PostId="667" Score="0" Text="yes, nearest neighbour is in fact very similar in objective to that of information retrieval... that is that of finding the most similar k points given a query point... the only problem is that when N (the number of data points) is too large, IR is more efficient" CreationDate="2014-07-05T12:00:14.867" UserId="984" />
  <row Id="1712" PostId="675" Score="0" Text="Say I am analyzing linkedin data, do you think it would be a good idea for me to merge the previous work experience, educations recommendations and skills of one profile into one text file and extract keywords from it ?" CreationDate="2014-07-05T14:46:08.110" UserId="1315" />
  <row Id="1713" PostId="675" Score="0" Text="LinkedIn now has skill tags that people assign themselves and other users can endorse, so basically there's no need to extract keywords manually. But in case of less structured data - yes, it may be helpful to merge everything and then retrieve keywords. However, remember main rule: **try it out**. Theory is good, but only practical experiments with different approaches will reveal best one." CreationDate="2014-07-05T20:33:05.633" UserId="1279" />
  <row Id="1714" PostId="675" Score="0" Text="true said. Thanks a lot" CreationDate="2014-07-05T23:33:49.547" UserId="1315" />
  <row Id="1715" PostId="679" Score="1" Text="Search queries are not noisier (there are very few words in a query not actually related to the search), but may contain misspellings, ambiguity, slang and other stuff that you have to deal with separately. Beyond these issues, queries and documents may be processed pretty much the same way." CreationDate="2014-07-06T00:13:02.033" UserId="1279" />
  <row Id="1716" PostId="677" Score="1" Text="Your second import is not correctly indented.  I would correct the code myself if the edit was long enough." CreationDate="2014-07-07T10:08:28.323" UserId="1367" />
  <row Id="1717" PostId="684" Score="0" Text="Thanks. I wouldn't have known what had caused it. I only know most of the time it's my work that's at fault :)" CreationDate="2014-07-07T12:56:31.427" UserId="974" />
  <row Id="1718" PostId="679" Score="0" Text="maybe you can extract keyword vectors from queries, and then compute the distance between those vectors, and how the similarity is defined, i think this is still an open question:)" CreationDate="2014-07-06T06:15:21.697" UserId="1006" />
  <row Id="1719" PostId="684" Score="0" Text="@elksie5000 : I have added how to debug the call. I hope the last call is what you would expect from a successful call to the function (?). Otherwise, it is always good to know how to step into the code with `pdb` :)" CreationDate="2014-07-07T13:44:45.997" UserId="1367" />
  <row Id="1720" PostId="684" Score="0" Text="I must admit pdb was something I was looking at again after working through the Python for Data Analysis book by Wes McKinney. I already work in IPython, but had been reasonably happy with print statements. Thank you again." CreationDate="2014-07-07T15:03:33.867" UserId="974" />
  <row Id="1721" PostId="684" Score="0" Text="As a side note, the debugger prompt says &quot;ipdb&quot; because it is the ipython debugger - this is an extra install in my setup. Under normal circumstances, it would be the regular pdb that is called. Just noticed this difference." CreationDate="2014-07-07T15:27:48.683" UserId="1367" />
  <row Id="1722" PostId="680" Score="1" Text="Note that when doing LSA, typically you use the cosine distance on the LSA projections of the original dataset. Just to clarify." CreationDate="2014-07-07T18:09:18.477" UserId="1301" />
  <row Id="1723" PostId="691" Score="0" Text="How exactly have you used LSA? It's worth noting that LDA is actually a pretty thin wrapper around LSA (it's pLSA with a dirichlet prior) that has been empirically shown to greatly increase generalization. You would almost certainly see better accuracies with LSA, but that's generally a result of overfitting, which is a very notable problem with LSA. Also, what exactly do you mean by scaling here? doc2vec does not actually require a new model for each document, and for computation there's no notable difference between LSA and LDA, both being very scalable." CreationDate="2014-07-07T19:36:26.987" UserId="869" />
  <row Id="1725" PostId="697" Score="2" Text="This is quite a broad question. You may be memory-bound (dataset doesn't fit into memory, and thus swap is used extensively, making it deadly slow) or CPU-bound (memory is ok, but operations just take too long). In latter case you can try to use vectorization more widely or write extensions in C directly. You can also try to compile your functions with `cmpfun()` or parallelize code to use several cores. Finally, you can rent Amazon web server to run your experiments, which will cost much less than buying hardware yourself. Anyway, try to determine your bottleneck first." CreationDate="2014-07-08T06:18:59.317" UserId="1279" />
  <row Id="1726" PostId="701" Score="0" Text="I just added an answer assuming that you want to cluster the samples `id_1`...`id_n` *based on their distances*. If you do want to cluster *the distances themselves*, you just need to use them as a 1-dimensional array." CreationDate="2014-07-08T09:28:49.813" UserId="1367" />
  <row Id="1729" PostId="679" Score="1" Text="Both of your questions are broad, subjective and will require significant maintenance to avoid becoming obsolete. Since the community appreciates that sort of question, keeping one of them might be reasonable - but certainly not both, when this discussion is a proper subset of the other. Please review [What types of questions should I avoid asking?](http://datascience.stackexchange.com/help/dont-ask)" CreationDate="2014-07-08T15:13:07.897" UserId="322" />
  <row Id="1731" PostId="693" Score="0" Text="Do you still use SVM when you have 3 or more classes ? And what features do you want to extract using a natural language parser? For what purpose ?" CreationDate="2014-07-08T15:40:48.113" UserId="1315" />
  <row Id="1733" PostId="694" Score="1" Text="See also http://stackoverflow.com/q/2276933/2359271" CreationDate="2014-07-08T20:14:34.267" UserId="322" />
  <row Id="1734" PostId="691" Score="0" Text="I have not observed over fitting with LSA, and like I said, I have met multiple other people who have seen better performance over LDA. Also, I have seen LSA used in many winning entries in semeval competitions, I have never seen LDA used in a winning entry. That is the academic conference for comparing semantic similarity between documents, so I assume they know what they are doing. Doc2vec, if you are referring to Mikolov's paragraph vector implementation, does SGD on each document separately. So it's very slow." CreationDate="2014-07-08T20:48:55.170" UserId="1301" />
  <row Id="1736" PostId="686" Score="0" Text="Note my comments below about paragraph vector scalability. This technique looks very promising, but is hard to implement, and does not scale well at all, as you are doing a separate SGD for each document, which is very costly, if I remember the paper correctly" CreationDate="2014-07-08T20:52:43.710" UserId="1301" />
  <row Id="1737" PostId="700" Score="0" Text="I can't fully answer your question, but I know of one way I have selected the k-value before.  You can look at minimizing a function of (Sum of squares within clusters)/(Sum of Squares between clusters), or maximizing the inverse." CreationDate="2014-07-08T23:15:07.113" UserId="375" />
  <row Id="1740" PostId="687" Score="0" Text="Thanks for your answer. Do I understand right: your starting point is to merge the 2 datasets ?" CreationDate="2014-07-09T14:31:34.480" UserId="906" />
  <row Id="1741" PostId="690" Score="0" Text="Thanks for your answer. It's a great example of what generative models can do that discriminative models cannot." CreationDate="2014-07-09T14:34:15.320" UserId="906" />
  <row Id="1743" PostId="712" Score="2" Text="I don't mean to be overly critical, but SVMs are NOT efficient. They have a cubic complexity in most cases, which is why there is a lot of phasing out happening." CreationDate="2014-07-09T19:05:12.953" UserId="548" />
  <row Id="1744" PostId="713" Score="1" Text="This is probably better for serverfault.com" CreationDate="2014-07-09T20:01:26.083" UserId="21" />
  <row Id="1745" PostId="712" Score="1" Text="yes, standard convergence methods takes O(n^3)... but i think i've seen somewhere (may be from the home page of T. Joachims) that it's been reduced to O(n^2)" CreationDate="2014-07-09T20:21:05.123" UserId="984" />
  <row Id="1746" PostId="713" Score="1" Text="@SeanOwen Yes, or [cloudera support](http://www.cloudera.com/content/cloudera/en/about/contact-form.html), even" CreationDate="2014-07-09T21:13:50.973" UserId="322" />
  <row Id="1747" PostId="705" Score="0" Text="okay, can you elaborate what does interaction mean? I added classes like wifi enabled, gps enabled. now lasso performed slightly better than ridge, I also added more days for computation. However, the AUC is now in range of .51 to .55 only. Is there anything else i can do? Will try adding quadratic features, and what else?" CreationDate="2014-07-10T05:34:55.647" UserId="1273" />
  <row Id="1748" PostId="705" Score="1" Text="For every feature, you can create quadratic features (x_i ^ 2) and interaction features (x_i * x_j). Also be aware that there are hyper-parameters for both methods of regularization that should be tuned rather than left at their defaults" CreationDate="2014-07-10T08:19:26.393" UserId="1399" />
  <row Id="1749" PostId="636" Score="0" Text="for selecting what users will click on, we will have to search our system for keywords etc as relevant. so if there is a way to reject a request without delaying time in keyword lookup, it will save a lot of time in processing for us" CreationDate="2014-07-10T11:03:43.300" UserId="1273" />
  <row Id="1750" PostId="716" Score="1" Text="A non-random correlation might be an indicator that the feature *is* useful. But I'm not so sure about pre-training tests that could rule ideas out. The paper you link makes it clear that non-linear correlations are not well detected by the available tests, but a neural net has a chance of finding and using them." CreationDate="2014-07-10T11:28:15.503" UserId="836" />
  <row Id="1751" PostId="711" Score="1" Text="See also http://stats.stackexchange.com/questions/tagged/svm" CreationDate="2014-07-10T11:55:01.303" UserId="1237" />
  <row Id="1752" PostId="705" Score="0" Text="i took alpha in between 0 and 1 as well for trying regression, is that what you mean?" CreationDate="2014-07-10T14:47:05.807" UserId="1273" />
  <row Id="1753" PostId="700" Score="0" Text="Look up the Chinese Restaurant Process to help deal with dynamic numbers of clusters." CreationDate="2014-07-10T15:49:54.777" UserId="684" />
  <row Id="1754" PostId="718" Score="0" Text="I always thought to compare the value to predict with the features, you are talking about correlation between features. Is your answer applicable also to my case? in theory I should add only new features that are correlated to the value to predict, right?" CreationDate="2014-07-10T19:06:57.880" UserId="989" />
  <row Id="1755" PostId="718" Score="0" Text="That's also a valuable metric -- just updated my answer  to address that as well." CreationDate="2014-07-10T19:18:34.210" UserId="684" />
  <row Id="1756" PostId="718" Score="0" Text="In short, strong correlations with the value to predict is a great sign, but weak correlation with the value to predict is not necessarily a bad sign." CreationDate="2014-07-10T19:19:17.200" UserId="684" />
  <row Id="1757" PostId="718" Score="0" Text="Thanks. I'm writing a report and I wanted to show the linear/non-linear correlations in order to justify the features (even before the results). Does it make any sense?&#xA;From your answer I could make a matrix of correlations but maybe it's nosense" CreationDate="2014-07-10T19:27:33.073" UserId="989" />
  <row Id="1758" PostId="718" Score="0" Text="A matrix of correlations would make more sense if you were performing linear regression, but it could also be a useful metric for neural nets.  Give it a go and see what you get!" CreationDate="2014-07-10T21:29:06.873" UserId="684" />
  <row Id="1759" PostId="718" Score="1" Text="I would use non-linear correlations, but ok thanks" CreationDate="2014-07-10T23:17:47.063" UserId="989" />
  <row Id="1760" PostId="687" Score="0" Text="@cafe876 That is certainly one way to start, and then trying to basically recreate a clustering that closely approximates the original." CreationDate="2014-07-11T00:36:54.230" UserId="548" />
  <row Id="1761" PostId="720" Score="0" Text="Thank you for your answer. I already know these tools. Unfortunatly none of them are neither able to summarize a collection of documents nor perform query-based summarization" CreationDate="2014-07-11T09:23:52.577" UserId="979" />
  <row Id="1765" PostId="713" Score="0" Text="Could you provide the list of misconfigurations (that are just below the yellow bar)?" CreationDate="2014-07-12T00:35:10.563" UserId="2460" />
  <row Id="1766" PostId="730" Score="0" Text="This post was researched and presented well. It makes a poor question for an SE network site but it would be a great topic to start on a discussion forum." CreationDate="2014-07-12T19:38:44.743" UserId="322" />
  <row Id="1767" PostId="730" Score="0" Text="@AirThomas Thanks for the warning. I tried to save the post by making a proper question out of it." CreationDate="2014-07-13T03:06:30.877" UserId="84" />
  <row Id="1768" PostId="705" Score="1" Text="I suspect by alpha you mean the step size? If so, this is not the parameter I'm referring to---there's another parameter that regulates how much the regularization term is multiplied by in the objective, which allows you to balance model fit (likelihood) with sparseness as measured by the regularizer. This needs tuning." CreationDate="2014-07-14T10:04:29.207" UserId="1399" />
  <row Id="1769" PostId="704" Score="1" Text="Ah, the joys of deployment ... enhanced by the joys of distributed systems :)" CreationDate="2014-07-14T10:44:38.130" UserId="1367" />
  <row Id="1770" PostId="658" Score="0" Text="Please clarify: you say you want to use Column 2 as a feature, but then you say you want to predict/classify Column 2. Also, you call this feature 'non-atomic' ... do you mean it is not categorical?" CreationDate="2014-07-14T13:12:22.047" UserId="1367" />
  <row Id="1771" PostId="729" Score="0" Text="I think your answer is useful to the problem.  Just some suggestions: I would move the data generation code to the bottom, or even to an external Gist, since it is not really part of the proposed solution.  And I would elaborate a bit more on the fact that you are using 4 standard deviations to detect resets: right now, it is just a comment lost in the code, and it is the core of your solution." CreationDate="2014-07-14T13:26:23.830" UserId="1367" />
  <row Id="1772" PostId="729" Score="0" Text="Good ideas.  Will do." CreationDate="2014-07-14T15:18:33.427" UserId="375" />
  <row Id="1773" PostId="737" Score="0" Text="Thanks for the reply! I have actually tried numerous other algorithms/kernels and still have the same type of problem. So I am looking for more of an approach like undersampling or some way to even out the classes." CreationDate="2014-07-14T16:40:26.047" UserId="802" />
  <row Id="1774" PostId="737" Score="0" Text="Ok, you might also want to try replicating rows for classes containing sparse data, although its useful only if the features of the sparse data are really good." CreationDate="2014-07-14T17:25:06.797" UserId="2485" />
  <row Id="1775" PostId="740" Score="0" Text="thanks for the advice, do you know if libsvm automatically does this or do I need to manually pass in the class weights?" CreationDate="2014-07-14T19:24:14.940" UserId="802" />
  <row Id="1776" PostId="740" Score="0" Text="You have to manually pass in the class weights. The way to do that is different based on the interface you are using (python, java, matlab, c). It is well documented in the read me files if you download the tool from http://www.csie.ntu.edu.tw/~cjlin/libsvm/.&#xA;Also your data size seems to be large and the default multi-class implementation of libsvm will use one-vs-one classification which may take too long to run. You can try training 50 one-vs-all binary classifiers specifying the weights appropriately." CreationDate="2014-07-14T19:57:27.733" UserId="1350" />
  <row Id="1777" PostId="734" Score="0" Text="Thanks very much! Your comments are very helpful and provide a basis on which I may be able to tease more out of the article. John" CreationDate="2014-07-14T20:24:37.367" UserId="2458" />
  <row Id="1778" PostId="301" Score="1" Text="As a side comment: I think spotting errenous data caused by some problem further up the pipeline is a gold skill. Many a time I have wondered why my analysis produced weird results and when I looked at the pipeline i found some kind of error . E.g: I wondered why all my data where heavily skewed towards high prices - WAY out of my mental model. When I asked around, I found out that some subcontractor misunderstood the briefing and delivered data for high income groups, while we whanted mixed data..." CreationDate="2014-07-15T09:33:38.703" UserId="791" />
  <row Id="1779" PostId="742" Score="1" Text="+1 You do need a lot of engineering experience to be an effective data science, but you don't get that at school. Use school for the theory and use jobs for engineering skill." CreationDate="2014-07-15T11:07:06.440" UserId="21" />
  <row Id="1780" PostId="742" Score="1" Text="+1 for the role definition and the overall good advice" CreationDate="2014-07-15T14:03:13.123" UserId="1367" />
  <row Id="1781" PostId="506" Score="0" Text="Thank you very much for such a thorough explanation! I really appreciate it. Between yourself and neone4373 I was able to solve the problem! This community rocks!&#xA;&#xA;Thanks!" CreationDate="2014-07-15T15:06:43.223" UserId="1047" />
  <row Id="1782" PostId="301" Score="0" Text="Yes! Data errors are frequently signs of process problems. Knowing where in the process the errors were introduced and also the mechanism, will greatly help with the cleaning process.  But better still is to fix the process problems so that they produce clean (or cleaner) data." CreationDate="2014-07-15T19:12:57.530" UserId="609" />
  <row Id="1783" PostId="679" Score="0" Text="Thanks, AirThomas!  ffriend's post certainly seems to indicate that this is clearly a duplicate.  I'll see what I can do about this." CreationDate="2014-07-16T00:14:25.850" UserId="1097" />
  <row Id="1784" PostId="746" Score="1" Text="My vote is also for R. As far as I know, there is no time series decomposition functions in `statsmodel` (Python). Though in this case decomposition could be crucial to improving prediction. I see notable seasonal peaks." CreationDate="2014-07-16T08:45:53.197" UserId="941" />
  <row Id="1786" PostId="746" Score="0" Text="@sobach: Thank you for R solidarity. In regard to the rest of your comment, similarly to now famous tweet, I can neither confirm, nor deny that :-). [Since it's beyond my current level of knowledge on the subject.]" CreationDate="2014-07-16T10:38:45.793" UserId="2452" />
  <row Id="1787" PostId="755" Score="0" Text="I belive you are referring to OpenCV nactive_vars parameter (not max_depth), which I set to default sqrt(N) value, that is nactive_vars=sqrt(16) for first dataset and sqrt(200) for other two. max_depth determines whether trees grow to full depth (25 is its maximum value) and balances between underfitting and overfitting, more about it here: &#xA;http://stats.stackexchange.com/questions/66209/opencv-parameters-of-random-trees&#xA;Not sure about min_sample_count but I tried various values and setting it to 1 worked best." CreationDate="2014-07-17T07:05:44.933" UserId="1387" />
  <row Id="1788" PostId="755" Score="0" Text="OpenCV documentation gives brief explanation of parameters:&#xA;http://docs.opencv.org/modules/ml/doc/random_trees.html#cvrtparams-cvrtparams&#xA;For now I would like to make random trees work reasonably well and keep things simple because I want to focus on working with a multiple classifier system." CreationDate="2014-07-17T07:06:32.730" UserId="1387" />
  <row Id="1789" PostId="755" Score="0" Text="About kNN - these are all really good suggestions, but what I meant to say is that kNN performed better than random trees classifier and I think there is lots of room for improvement with random trees." CreationDate="2014-07-17T07:15:51.477" UserId="1387" />
  <row Id="1790" PostId="732" Score="0" Text="Thank you! Your answer really helped me!" CreationDate="2014-07-17T08:43:42.207" UserId="2471" />
  <row Id="1791" PostId="755" Score="0" Text="yes, i'm not sure why random forest is not performing as well (or better) than the simplistic k-NN approach... it just might be the case that a kernel based approach where you directly try to estimate P(y|D) (output given data) such as in k-NN without estimating P(theta|D) (latent model given data) such as in the parametric models." CreationDate="2014-07-17T09:28:49.887" UserId="984" />
  <row Id="1792" PostId="760" Score="1" Text="Transform your data to be a list of number of month between events (in Matlab this would be `diff(find(V))` where `V` is your current time series vector. Then try to fit an exponential distribution to this by estimating the *rate* parameter. *rate*, would be a decent metric of the frequency of job changes. The exponential distribution should show how the probability will increase with time since the last event. You also might want to [test for a goodness of fit](http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution) after estimating *rate*:" CreationDate="2014-07-17T09:41:18.727" UserId="2532" />
  <row Id="1793" PostId="760" Score="0" Text="what is the equivalent of `diff(find(V))` in R ?" CreationDate="2014-07-17T10:22:02.283" UserId="1315" />
  <row Id="1794" PostId="760" Score="1" Text="I don't know much `R` but I'll explain the Matlab code: `find` return the element number of the `1`s, `diff` returns the difference between each consecutive number. Hence that line just returns a vector of the number of months between each job change." CreationDate="2014-07-17T10:46:10.830" UserId="2532" />
  <row Id="1797" PostId="760" Score="0" Text="I'll point back to this link again: http://stats.stackexchange.com/questions/76994/how-do-i-check-if-my-data-fits-an-exponential-distribution looks like r has a `fitdistr` function" CreationDate="2014-07-17T16:00:12.900" UserId="2532" />
  <row Id="1798" PostId="753" Score="0" Text="Answers that rely on external resources to be useful are strongly discouraged on Stack Exchange. If a link stops working, or a reference book is not available, these answers become useless. Please *do* explain with words, either by paraphrasing or quoting your sources directly, and upload images (with attribution) as necessary and appropriate, using external links and references only for citation and &quot;further reading.&quot;" CreationDate="2014-07-17T16:01:42.867" UserId="322" />
  <row Id="1799" PostId="766" Score="0" Text="Essentially what I could do is to set self.error = error at the end of _tsne(), in order to retrieve it from the TSNE instance afterwards. Yes, but that would mean changing sklearn.manifold code, and I was wondering if the developers thought of some other ways to get the information or if not why they didn't (i.e.: is 'error' considered useless by them?). Furthermore, if I changed that code I would need all the people running my code to have the same hack on their sklearn installations. Is that what you suggest, or did I get it wrong?" CreationDate="2014-07-17T16:07:30.343" UserId="131" />
  <row Id="1800" PostId="760" Score="0" Text="actually, i just manually computed the MLE of $\lambda$ of exponential distribution." CreationDate="2014-07-17T16:08:29.123" UserId="1315" />
  <row Id="1801" PostId="769" Score="0" Text="Yes, indeed error rates on training data set are around 0. Changing parameters to reduce overfitting didn't result in higher accuracy on test dataset in my case. I will look into techniques you mention as soon as possible and comment, thank you." CreationDate="2014-07-17T16:16:51.433" UserId="1387" />
  <row Id="1802" PostId="769" Score="0" Text="What are the relative proportions of training and test dataset btw? Something line 70:30, 60:40, or 50:50?" CreationDate="2014-07-17T16:38:07.737" UserId="2556" />
  <row Id="1803" PostId="766" Score="0" Text="Yes, that is what I suggested as a possible solution. Since scikit-learn is open source, you could also submit your solution as a pull request and see if the authors would include that in future releases. I can't speak to why they did or didn't include various things." CreationDate="2014-07-17T17:10:10.463" UserId="159" />
  <row Id="1804" PostId="768" Score="1" Text="I like the watermark magic. For those who are unaware, GitHub now offers up to 5 free private repositories for users associated with academic institutions." CreationDate="2014-07-17T17:22:06.340" UserId="964" />
  <row Id="1805" PostId="773" Score="1" Text="&quot;distance metric&quot; is commonly used as an opposite of &quot;similarity&quot; in literature: the larger distance, the smaller similarity, but basically they represent same idea." CreationDate="2014-07-17T20:52:20.197" UserId="1279" />
  <row Id="1806" PostId="757" Score="0" Text="Do you have any advice related to your first sentence?" CreationDate="2014-07-17T23:42:37.857" UserId="989" />
  <row Id="1807" PostId="769" Score="0" Text="First dataset - UCI letter recognition is set to 50:50 (10000:10000), Digits is about 51:49 (1893:1796) and MNIST is about 86:14 (60000:10000)." CreationDate="2014-07-18T01:35:18.207" UserId="1387" />
  <row Id="1808" PostId="751" Score="0" Text="Why the dot product alone (equivalent to not normalizing) *not* account for features' data and frequency? I don't know that this is the difference." CreationDate="2014-07-18T11:27:57.540" UserId="21" />
  <row Id="1809" PostId="744" Score="0" Text="Note that neither of these are proper distance metrics, even if you transform them to be a value that is small when points are &quot;similar&quot;. It may or may not matter for your use case." CreationDate="2014-07-18T11:34:09.417" UserId="21" />
  <row Id="1810" PostId="751" Score="0" Text="Perhaps, I wasn't clear. I was talking about data diversity. E.g., we have two pairs of documents. Within each pair docs are identical, but pair-1 documents are shorter, than pair-2 ones. And we computing similarity within each pair. Dot product would produce different numbers, though in both cases maximum similarity estimate is expected." CreationDate="2014-07-18T12:36:29.073" UserId="941" />
  <row Id="1811" PostId="757" Score="0" Text="Create a dataset for each day of the week and fit a model to all seven of them." CreationDate="2014-07-18T12:58:53.813" UserId="325" />
  <row Id="1812" PostId="765" Score="0" Text="What do people generally due with predicted negative values if they know that the true target function cannot output negative values?" CreationDate="2014-07-18T14:05:20.120" UserId="1162" />
  <row Id="1813" PostId="758" Score="2" Text="Where is the *question* in this question? Please take a moment to review [the Help Center guidelines](http://datascience.stackexchange.com/help/dont-ask), specifically: &quot;If your motivation for asking the question is 'I would like to participate in a discussion about ______', then you should not be asking here.&quot;" CreationDate="2014-07-18T15:05:56.783" UserId="322" />
  <row Id="1814" PostId="766" Score="2" Text="Thanks. If anybody else is interested in this, https://github.com/scikit-learn/scikit-learn/pull/3422." CreationDate="2014-07-18T15:44:03.873" UserId="131" />
  <row Id="1815" PostId="764" Score="2" Text="How are points close to each other on the wrap-around point handled?" CreationDate="2014-07-18T17:00:53.687" UserId="2587" />
  <row Id="1816" PostId="765" Score="1" Text="I don't think it's applicable to what we're talking about, but if you have a strictly positive target, for example, you'd probably want to model the log of the target.  Then you'd exponentiate the predictions." CreationDate="2014-07-18T19:15:21.783" UserId="2543" />
  <row Id="1817" PostId="753" Score="0" Text="@keisZn, the pdf was a good explanation but there is no algorithm to explain how he parse through the alignment matrix to get consistent phrases..." CreationDate="2014-07-18T21:22:46.960" UserId="122" />
  <row Id="1818" PostId="785" Score="2" Text="It's over 4 GB of data. I should plot it by reading from stdin or something similar. It's a bad idea to load everything to RAM. I'll take a look at what you said in a couple of days - and hopefully, any other ideas that may arise - and I'll let you know how it went, thanks!" CreationDate="2014-07-20T01:39:58.163" UserId="2604" />
  <row Id="1819" PostId="787" Score="0" Text="Are you using &quot;poor&quot; Apache Hadoop or some other distribution like [HDP](http://hortonworks.com/hdp/) or [CDH](http://www.cloudera.com/content/cloudera/en/products-and-services/cdh.html). I would heavily recommend using automated tools like these two instead of messing up with native settings. In addition to easy installation, they provide tools for monitoring and managing your cluster later." CreationDate="2014-07-20T11:28:51.477" UserId="1279" />
  <row Id="1821" PostId="787" Score="0" Text="This is better at serverfault.com" CreationDate="2014-07-20T17:32:17.743" UserId="21" />
  <row Id="1822" PostId="784" Score="0" Text="https://www.otexts.org/fpp/6/5" CreationDate="2014-07-20T18:02:37.283" UserId="989" />
  <row Id="1823" PostId="784" Score="0" Text="I think your question would be better stated as, What is the importance of seasonality for forecasting. As it is, it seems someone told you to use &quot;STL&quot;, but you don't say who told you so, neither why (which is probably what you're trying to find out)." CreationDate="2014-07-20T19:37:34.577" UserId="84" />
  <row Id="1825" PostId="769" Score="0" Text="I experimented with PCA, still didn't get good results with random forrest, but boost and Bayes now give results similar to other classifiers. I found a discussion about random forrest here: &#xA;http://stats.stackexchange.com/questions/66543/random-forest-is-overfitting&#xA;It is possible I am actually not overfitting but couldn't find the out-of-bag (OOB) prediction error mentioned there. Running experiment now with a large number of trees to see if accuracy will improve." CreationDate="2014-07-21T16:04:50.647" UserId="1387" />
  <row Id="1826" PostId="782" Score="1" Text="Great answer, Aleksandr, very informative!" CreationDate="2014-07-21T16:18:43.513" UserId="2599" />
  <row Id="1827" PostId="769" Score="0" Text="Okay, sounds you are making a little bit of progress :) A trivial question, but have you standardized your features (z-score) so that they are centered around the mean with standard deviation=1?" CreationDate="2014-07-21T16:19:55.410" UserId="2556" />
  <row Id="1828" PostId="769" Score="0" Text="Actually no, I usually would scale features to range 0-1 but now I see I didn't even do that correctly before PCA. So that would not be the right thing to do anyway? After PCA mean = 0, std = 0.5754." CreationDate="2014-07-21T16:41:29.853" UserId="1387" />
  <row Id="1829" PostId="782" Score="0" Text="@DaveKay: Thank you for kind words, Dave! Glad to help." CreationDate="2014-07-21T16:46:43.350" UserId="2452" />
  <row Id="1830" PostId="795" Score="0" Text="I'm aware of edit distance like Levenshtein distance, but I'm looking for something like semantic similarity." CreationDate="2014-07-21T16:55:04.707" UserId="921" />
  <row Id="1831" PostId="795" Score="0" Text="That's significantly harder.  The only way I know to do something like this is to be able to access a dictionary.  Then you could look into text mining definitions of the words.  Try looking into accessing 'wordnet', maybe that could help. http://wordnet.princeton.edu/wordnet/" CreationDate="2014-07-21T17:16:23.593" UserId="375" />
  <row Id="1832" PostId="787" Score="0" Text="@ffriend I am using &quot;poor&quot; Hadoop. I actually didn't know HDP or CDH existed. Is HDP an add-on or would I have to reinstall Hadoop entirely?" CreationDate="2014-07-21T17:27:07.070" UserId="2614" />
  <row Id="1833" PostId="795" Score="1" Text="-0 for suggesting Levenshtein distance." CreationDate="2014-07-21T19:08:36.797" UserId="869" />
  <row Id="1834" PostId="787" Score="0" Text="@BigDataDude: You will have to reinstall it entirely, but unlike manual installation, automated way will take only 10-15 minutes even for large clusters (at least, this is true for CDH - I haven't used Hortonworks' manager). So unless you have already pushed too much unique data to existing HDFS, migrating to maintained cluster should be pretty easy and painless." CreationDate="2014-07-21T20:38:09.437" UserId="1279" />
  <row Id="1835" PostId="794" Score="1" Text="I'm was also a bit thrown by the YAML files at first, but have since come to love the clean separation between configuration and code.  You can choose to use Pylearn2 without YAML files, although this option is not well documented." CreationDate="2014-07-21T20:52:39.473" UserId="684" />
  <row Id="1836" PostId="794" Score="0" Text="In short, however, I wouldn't discard the library because of this simple design decision." CreationDate="2014-07-21T20:53:05.997" UserId="684" />
  <row Id="1837" PostId="758" Score="0" Text="&quot;You should only ask practical, answerable questions based on actual problems that you face.&quot;" CreationDate="2014-07-21T21:04:02.473" UserId="895" />
  <row Id="1838" PostId="758" Score="0" Text="This is practical, answerable and based on an actual problem in much the same way that &quot;Tell me how to perform data science&quot; is practical, answerable and based on an actual problem." CreationDate="2014-07-21T21:44:19.357" UserId="322" />
  <row Id="1839" PostId="799" Score="0" Text="This looks quite like what I am looking for. I am studying for finals now and am unable to take time to think about this again, but as soon as I can I'll let you know.&#xA;&quot;A 256 byte periodic pattern would have manifested as vertical lines.&quot; -- exactly what I was thinking of. I can also show an image where I put all 256 bytes in the same line, and that is already obvious in text. I'm quite curious about what will come out of it :)" CreationDate="2014-07-22T00:08:30.030" UserId="2604" />
  <row Id="1840" PostId="807" Score="0" Text="Could you add how AUC compares to an F1-score?" CreationDate="2014-07-22T07:00:40.660" UserId="2532" />
  <row Id="1841" PostId="798" Score="0" Text="But the problem is that 'senior' and 'primary' don't occur in one title. How can I even compare this two words using list of job titles ?" CreationDate="2014-07-22T08:57:25.677" UserId="921" />
  <row Id="1842" PostId="798" Score="0" Text="Yes, this might help you learn that &quot;senior&quot; and &quot;developer&quot; go together, but not that &quot;senior&quot; and &quot;lead&quot; have similar semantic content." CreationDate="2014-07-22T09:35:35.613" UserId="21" />
  <row Id="1843" PostId="791" Score="0" Text="I think you will need some extra information to learn this. For example, do you have salary info, industry, and number of direct reports? This defines when two roles should be considered similar. Then you can ask what terms seem to be synonymous among similar roles. But if you don't know anything about what makes things similar I'm not sure what you can do." CreationDate="2014-07-22T09:36:47.343" UserId="21" />
  <row Id="1844" PostId="793" Score="2" Text="The major use is storing data and retrieving data. In fact, that's about the only use for a NOSQL database, or any database. Want to make your question better?" CreationDate="2014-07-22T15:09:41.543" UserId="471" />
  <row Id="1845" PostId="811" Score="1" Text="_If you're trying to build a representative model -- one that describes the data rather than necessarily predicts_ ... who builds a model which doesn't predcit?? Didn't get you there..." CreationDate="2014-07-22T15:23:14.600" UserId="2661" />
  <row Id="1846" PostId="812" Score="0" Text="you can try Graphviz... not sure if it scales up to millions of vertices...." CreationDate="2014-07-22T15:29:54.013" UserId="984" />
  <row Id="1847" PostId="797" Score="0" Text="You are right. NOSQL databases are mainly used for storing unstructured or semi-structured data like json. Can you  explain some of the types of data analysis we can do with them. What are the tools built into mongodb that can used for data analysis?" CreationDate="2014-07-22T15:32:12.733" UserId="2643" />
  <row Id="1848" PostId="793" Score="0" Text="Yes, database is mainly used for storing and retrieveing data. How can they be used for data analysis? What are the tools  built into  NOSQL databases like mongodb  which makes data analysis easy and powerful?" CreationDate="2014-07-22T16:56:17.863" UserId="2643" />
  <row Id="1849" PostId="793" Score="1" Text="Improve your question by editing it, not adding to the comments." CreationDate="2014-07-22T16:57:58.290" UserId="471" />
  <row Id="1850" PostId="798" Score="0" Text="@Mher, They're not supposed to occur in the same title; the terms _following_ them are supposed to occur in both, e.g., senior *developer*, or primary *developer*." CreationDate="2014-07-22T17:12:30.653" UserId="381" />
  <row Id="1851" PostId="798" Score="0" Text="@SeanOwen, If the titles are semantically similar, you would expect their co-occurrence vectors to be similar too since they would be used interchangeably." CreationDate="2014-07-22T17:14:14.703" UserId="381" />
  <row Id="1852" PostId="797" Score="1" Text="@jithinjustin there aren't data analysis tools built into mongo, or really any database. Also, `json` is totally structured data. You can technically do any kind of data analysis on it, using a NOSQL database is actually not related. There are tools built *on top of* mongo, like analytica though." CreationDate="2014-07-22T17:39:42.677" UserId="548" />
  <row Id="1853" PostId="811" Score="4" Text="Unsupervised learning would be an example where you build a model that isn't necessarily geared to predict. In some instances you might want to explore or summarize your data." CreationDate="2014-07-22T18:07:02.207" UserId="2513" />
  <row Id="1854" PostId="801" Score="0" Text="Related methods like LDA may also be a good bet." CreationDate="2014-07-22T18:39:23.373" UserId="684" />
  <row Id="1855" PostId="807" Score="1" Text="@Dan- The biggest difference is that you don't have to set a decision threshold with AUC (it's essentially measuring the probability spam is ranked above non-spam). F1-score requires a decision threshold. Of course, you could always set the decision threshold as an operating parameter and plot F1-scores." CreationDate="2014-07-22T19:14:58.447" UserId="2513" />
  <row Id="1856" PostId="815" Score="1" Text="Just as a comment, it looks like you have a pretty strict schema for your data at this. Depending on how many new &quot;columns&quot; you expect to appear at a later time, SQL may actually the best solution for you. But again, I'm just speculating." CreationDate="2014-07-22T22:38:43.677" UserId="1163" />
  <row Id="1857" PostId="815" Score="2" Text="As someone who is a huge fan of NOSQL, SQL is probably the right choice for this project." CreationDate="2014-07-23T06:16:20.823" UserId="869" />
  <row Id="1858" PostId="819" Score="0" Text="thanks for this, it is really useful." CreationDate="2014-07-23T09:32:40.020" UserId="906" />
  <row Id="1859" PostId="798" Score="0" Text="@Emre yes but you also 'learn' that &quot;strategist&quot; and &quot;ballerina&quot; and &quot;chef&quot; are similar because they follow &quot;head&quot; or something. I am not sure if this is enough to learn on by itself?" CreationDate="2014-07-23T10:52:17.920" UserId="21" />
  <row Id="1860" PostId="802" Score="2" Text="I think the structure you want to implement is a &quot;trie&quot; - whether you can find a DB that efficiently works with that structure, or need to roll your own in RDBMS of your choice I cannot say." CreationDate="2014-07-23T13:43:04.077" UserId="836" />
  <row Id="1861" PostId="822" Score="0" Text="Thanks for the answer, it also makes me think about the ethics of when you do an experiment in order to see its influence such as the recent [fabeook](http://online.wsj.com/articles/furor-erupts-over-facebook-experiment-on-users-1404085840) issue, perhaps that itself would make a good question as to the moral implications." CreationDate="2014-07-23T13:55:09.943" UserId="95" />
  <row Id="1862" PostId="821" Score="2" Text="This was a generic question. I did not include code because I did not want to solve a specific problem. I never stated I'm a data scientist and... IMHO it is not necessary to be so &quot;rude&quot; :). Thank you anyway for the explanation." CreationDate="2014-07-23T13:56:06.313" UserId="989" />
  <row Id="1863" PostId="822" Score="0" Text="You're welcome.  Ethics is something we deal with a lot in Biostatistics due to the history of medical research.  As a statistician/data scientist, I would argue that it is ethical to give an accurate portrait of the data and to not torture it into confessing.  A good place to start for the ethics of trials, from the medical point of view is the [Nurenburg Code](http://en.wikipedia.org/wiki/Nuremberg_Code).  It certainly has application for what Facebook did." CreationDate="2014-07-23T13:59:51.797" UserId="178" />
  <row Id="1864" PostId="823" Score="0" Text="I don't have enough rep to add an ethics tag or perhaps social experimentation and facebook, if someone could oblige" CreationDate="2014-07-23T14:06:20.877" UserId="95" />
  <row Id="1865" PostId="798" Score="0" Text="@SeanOwen: perhaps some constraints/context should be specified in this case. For me &quot;head chief&quot; and &quot;head developer&quot; sound similar because they both lead their teammates. If you want to distinguish between professions only, you can use only keywords meaning profession titles. But question clearly refers to employees level as well as his profession." CreationDate="2014-07-23T15:27:10.410" UserId="1279" />
  <row Id="1866" PostId="798" Score="1" Text="Yeah it must be about level and role. Two &quot;head&quot;s are similar, but that's obvious because both have the word &quot;head&quot;. My point was that &quot;chef&quot; and &quot;ballerina&quot; aren't necessarily similar just because you see &quot;head chef&quot; and &quot;head ballerina&quot; which is how I understood the cooccurrence idea. How do you learn that &quot;lead developer&quot; and &quot;senior developer&quot; are similar but &quot;junior developer&quot; is not? I think some other data has to enter the picture to tell us that the first two are supposed to be similar, then we can figure out why terms explain it." CreationDate="2014-07-23T15:40:02.637" UserId="21" />
  <row Id="1867" PostId="769" Score="0" Text="It depends on your data whether you want to do a Min-max normalization to unit range (e.g., 0-1) or Z-score normalization/standardization to unit variance (variance=1, mean=0). Sorry, but I forgot that you are doing text classification. I think normalization after you stemmed the words and used a vectorizer function would not be necessary" CreationDate="2014-07-23T16:22:00.253" UserId="2556" />
  <row Id="1868" PostId="823" Score="0" Text="I'm a bit confused as to what you're actually asking here. The title of this question seems inconsistent with the body and how ethics &quot;should&quot; be applied is way beyond the scope of this site. Academic programs are pretty diverse, but a question about professional standards accountability seems like it would be both answerable and reasonably scoped. Can you clarify or refocus your question?" CreationDate="2014-07-23T19:40:09.743" UserId="322" />
  <row Id="1869" PostId="823" Score="1" Text="You may also find [this meta discussion](http://meta.datascience.stackexchange.com/q/7/322) interesting, if you haven't read it already." CreationDate="2014-07-23T19:40:25.260" UserId="322" />
  <row Id="1870" PostId="823" Score="0" Text="@AirThomas Thanks for that meta link, I wasn't sure if this would be n topic or not. I guess what I'd like to know is whether ethics are taught or imbued academically and in the professional workplace and whether a moral code of practice exists for data science, this is a naive and broad question I admit" CreationDate="2014-07-23T20:00:22.007" UserId="95" />
  <row Id="1871" PostId="808" Score="0" Text="possible duplicate of [Starting my career as Data Scientist, is Software Engineering experience required?](http://datascience.stackexchange.com/questions/739/starting-my-career-as-data-scientist-is-software-engineering-experience-require)" CreationDate="2014-07-23T21:03:31.483" UserId="553" />
  <row Id="1872" PostId="798" Score="0" Text="@SeanOwen: My answer below should clarify the idea, but in short it's all about context. If &quot;head&quot; is the only common word in chef and ballerina profiles, their semantic vectors will still be quite far from each other. Two chefs will have many common words about cooking, and two _head_ chefs will additionally refer to same ratings/certificates/management techniques, etc. Same thing with developers: lead and senior devs frequently use words &quot;client&quot;, &quot;strategy&quot;, etc., while juniors normally mention few most commonly known technologies." CreationDate="2014-07-23T21:49:55.393" UserId="1279" />
  <row Id="1873" PostId="826" Score="0" Text="I don't mean to be negative, but regular expressions are an extremely poor choice for this problem. Between state-by-state variation, vanity plates, and differing formats, regular expressions are a poor choice." CreationDate="2014-07-24T02:07:45.667" UserId="548" />
  <row Id="1874" PostId="829" Score="0" Text="What error are you getting? It might actually help us to know..." CreationDate="2014-07-24T06:46:00.500" UserId="471" />
  <row Id="1875" PostId="825" Score="2" Text="Any particular country? Different countries have different license plate formats. Its a big world out there." CreationDate="2014-07-24T11:48:14.770" UserId="471" />
  <row Id="1876" PostId="831" Score="1" Text="It looks like there are no prerequisites for membership, without which I see no reason why membership should carry any meaning. Membership is supposed to be a &quot;Valuable credential to signal to clients and employers&quot;, but this will only be true if there is some sort of qualification requirement/entrance exam and also some industry experience requirements. With a free for all membership, being a member doesn't distinguish you from a non-member." CreationDate="2014-07-24T11:56:29.887" UserId="2532" />
  <row Id="1877" PostId="826" Score="0" Text="How many different variations are there? I'm from a different country." CreationDate="2014-07-24T12:53:56.637" UserId="325" />
  <row Id="1878" PostId="826" Score="1" Text="A conservative estimate would be 51, probably close to a few hundred." CreationDate="2014-07-24T16:18:06.910" UserId="548" />
  <row Id="1879" PostId="826" Score="0" Text="Ah right I see the problem. I was thinking that I could find all the plates registered here with 3 regular expressions." CreationDate="2014-07-24T16:55:11.487" UserId="325" />
  <row Id="1880" PostId="825" Score="0" Text="@Spacedman Any country is fine :)" CreationDate="2014-07-24T17:38:07.027" UserId="1192" />
  <row Id="1881" PostId="826" Score="0" Text="@germcd Thanks for the attempt though! Glad to hear I wasn't the only person to think of regex too." CreationDate="2014-07-24T17:42:29.130" UserId="1192" />
  <row Id="1882" PostId="827" Score="0" Text="Never seen brat before; thanks for the tip!" CreationDate="2014-07-24T17:52:43.600" UserId="1192" />
  <row Id="1883" PostId="832" Score="0" Text="Do you want to be able to print the plot?" CreationDate="2014-07-24T19:54:46.143" UserId="2575" />
  <row Id="1884" PostId="799" Score="0" Text="I can't seem to run this on Debian Linux. I installed the packages `python-scitools` and `ipython`. The error message is `ValueError: invalid literal for int() with base 10: '#'`. I'll see if I can make it work anyway..." CreationDate="2014-07-25T02:41:49.133" UserId="2604" />
  <row Id="1885" PostId="797" Score="1" Text="I don't know about all that. MongoDB can perform better than MySQL. You'd have a better argument if you said PostgreSQL (which, by the way can accept JSON). Either way, I wouldn't consider some arbitrary &quot;performance&quot; (we don't know what the use case is) to be a reason not to use NoSQL. Also don't discount using multiple databases. Remember, MongoDB has amazing aggregation features that SQL does not have." CreationDate="2014-07-25T02:56:24.790" UserId="2711" />
  <row Id="1886" PostId="799" Score="0" Text="I succeeded (by running the code directly inside `ipython`, and changing `map(int, line)` to `map(ord, line)`, and updated the question with the new picture." CreationDate="2014-07-25T03:30:23.273" UserId="2604" />
  <row Id="1887" PostId="783" Score="0" Text="An example/excerpt of the data (maybe only a few MB) could be interesting." CreationDate="2014-07-25T09:43:55.557" UserId="156" />
  <row Id="1889" PostId="835" Score="1" Text="I don't see anything in that link or here that proves you can't do sentiment analysis in an SQL database. the mongoDB examples benefit from Javascript in the DB, so you could use any embedded language in an SQL database. For example Postgres + R." CreationDate="2014-07-25T12:54:27.213" UserId="471" />
  <row Id="1890" PostId="785" Score="0" Text="Don't load it in and treat it like a dataframe, its not a dataframe, its a stream of bytes." CreationDate="2014-07-25T12:56:20.203" UserId="471" />
  <row Id="1891" PostId="835" Score="0" Text="Would love to see where you could execute code and map/reduce in those databases. In all seriousness (especially Postgres). ...and even if you could, that still doesn't make the answer any less valid by the way. One simply just might want to use NoSQL. It does work." CreationDate="2014-07-25T14:07:06.143" UserId="2711" />
  <row Id="1892" PostId="835" Score="1" Text="Postgres + C, Python, Perl, R, feed your Postgres DB into the latest machine learning algorithms. Easy: http://www.postgresql.org/docs/9.0/static/xplang.html" CreationDate="2014-07-25T15:00:05.993" UserId="471" />
  <row Id="1893" PostId="835" Score="0" Text="Nice. I'll have to try that out sometime. How about MySQL?" CreationDate="2014-07-25T15:16:52.210" UserId="2711" />
  <row Id="1894" PostId="783" Score="0" Text="If you're interested in the periodic nature of the data taking a look at the DFT of the data could be revealing." CreationDate="2014-07-25T17:44:44.230" UserId="2724" />
  <row Id="1895" PostId="840" Score="0" Text="Your code please." CreationDate="2014-07-25T17:52:56.020" UserId="381" />
  <row Id="1896" PostId="832" Score="1" Text="@dsign, If you mean do I need to print it on a physical medium--no. But I would like a graphical representation of the data." CreationDate="2014-07-25T18:28:05.447" UserId="2702" />
  <row Id="1897" PostId="833" Score="0" Text="Unfortunately this type of plot doesn't work because it doesn't scale to show how many actions are taking place at the same time. I would estimate that at any given time there could be 100 actions taking place over the whole data set there are millions of actions." CreationDate="2014-07-25T18:32:48.153" UserId="2702" />
  <row Id="1898" PostId="833" Score="0" Text="you can change the thickness of the lines e.g. `geom_line(size = 2)`" CreationDate="2014-07-25T18:56:38.250" UserId="325" />
  <row Id="1899" PostId="797" Score="0" Text="@Tom on performance, you'll find that the only task that mongo actually outperforms mysql on is inserts (http://www.moredevs.ro/mysql-vs-mongodb-performance-benchmark/), which is a comparatively small part of data analysis. SQL's aggregation features are FAR more mature than Mongo's. As far as MYSQL versus Postgres, the numbers are very temporily skewed and both tend to offer similar performance. MYSQL is more common, which is why I mentioned that instead, but the two are quite similar." CreationDate="2014-07-25T21:59:38.130" UserId="548" />
  <row Id="1901" PostId="845" Score="0" Text="Thank you so much, that helped clear up a lot of things." CreationDate="2014-07-26T01:14:10.963" UserId="2726" />
  <row Id="1902" PostId="783" Score="0" Text="@mrmcgreg: I'll have to re-learn how the DFT works. I should've paid more attention to the signals &amp; systems classes :)" CreationDate="2014-07-26T03:10:46.017" UserId="2604" />
  <row Id="1903" PostId="797" Score="0" Text="I've always seen better performance on MongoDB when things fit into memory. I take benchmarks with a gain of salt because if you Google a bit you're gonna find a bunch of benchmarks showing MongoDB as faster. It truly depends on your needs. That said, to help answer the original question - I think there's plenty of uses for NoSQL in big data science and analytics." CreationDate="2014-07-26T03:29:46.807" UserId="2711" />
  <row Id="1904" PostId="797" Score="0" Text="@Tom If things fit into memory, one should take advantage of that and use something like `redis` or `memchached`. I'm not saying that benchmarks should be viewed as very authoritative, but I would certainly be interested in seeing benchmarks where mongo beats nosql in a read-heavy situation. I'm trying to be objective here, and I'm not saying that there's no place for NoSQL, I'm saying that SQL is an excellent, mature technology and there's no reason to move to NoSQL unless you have a specific use case that calls for a NoSQL database." CreationDate="2014-07-26T03:51:09.707" UserId="548" />
  <row Id="1906" PostId="797" Score="0" Text="ok, there's nosql. redis is key/value. i was just pointing out some actual use cases in my answer below and then all of a sudden i'm getting down voted by people who, i can only assume, have never used any other database other than MySQL and are afraid of doing so. a question like this wouldn't even be on normal stack exchange. it's just frustrating. though i do understand your point of view. don't get me wrong there." CreationDate="2014-07-26T04:22:40.623" UserId="2711" />
  <row Id="1907" PostId="797" Score="0" Text="http://stackoverflow.com/questions/9702643/mysql-vs-mongodb-1000-reads/9703513#9703513 is another great answer and thing to think about. it depends on how you're using these databases. if you're doing all these joins, then i just can't see how it's faster. mongodb of course can be faster than mysql and easily vica versa. and i always find mongodb useful when working with unpredictable and changing data." CreationDate="2014-07-26T04:40:30.683" UserId="2711" />
  <row Id="1908" PostId="797" Score="0" Text="Oh, another good one to look at is InfluxDB. It's newer. NoSQL technically, but has a SQL-like syntax. It's great for time series. I had a bit of fun with it." CreationDate="2014-07-26T05:29:29.660" UserId="2711" />
  <row Id="1909" PostId="841" Score="0" Text="Thanks, I was looking for something on these lines. It would be extremely helpful if you could give reference to a tutorial or ipython notebook discussing this." CreationDate="2014-07-26T06:41:43.930" UserId="1131" />
  <row Id="1910" PostId="844" Score="0" Text="See http://stackoverflow.com/questions/22592811/scala-spark-task-not-serializable-java-io-notserializableexceptionon-when/22594142#22594142" CreationDate="2014-07-26T09:58:00.053" UserId="2668" />
  <row Id="1911" PostId="753" Score="0" Text="does anyone have the algorithm to get the &quot;consistent phrases&quot;?" CreationDate="2014-07-26T10:48:48.253" UserId="122" />
  <row Id="1912" PostId="525" Score="0" Text="Thanks, this is awesome!" CreationDate="2014-07-27T07:08:12.487" UserId="913" />
  <row Id="1913" PostId="806" Score="1" Text="Consider a highly unbalanced problem. That is where ROC AUC is very popular, because the curve balances the class sizes. It's easy to achieve 99% accuracy on a data set where 99% of objects is in the same class." CreationDate="2014-07-27T10:26:44.380" UserId="924" />
  <row Id="1914" PostId="851" Score="4" Text="There are basically 2 possible ways to go. Simple one is to simply compare colour histograms. [This question](http://stackoverflow.com/questions/6499491/comparing-two-histograms) give pretty good description of several good measures/methods. But if you are going to use it in image search engine or something like that, it makes sense to also mimic human _perception_ of colours, which is much harder task. [This paper](http://ect.bell-labs.com/who/emina/papers/pfl.pdf) provides some cues for better human-aware comparison." CreationDate="2014-07-27T22:18:12.620" UserId="1279" />
  <row Id="1915" PostId="851" Score="1" Text="That's a perfectly adequate answer, @ffriend." CreationDate="2014-07-28T01:27:55.820" UserId="381" />
  <row Id="1916" PostId="851" Score="0" Text="This is a cross post from Cross Validated. Anyone know what to do?" CreationDate="2014-07-28T02:23:36.587" UserId="1241" />
  <row Id="1917" PostId="831" Score="0" Text="@AsheeshR, yes this question is a matter of opinion but I think that the community can really benefit from it. How do we proceed?" CreationDate="2014-07-28T04:53:52.953" UserId="366" />
  <row Id="1919" PostId="850" Score="0" Text="Yes this is what I was looking for. I would be interested to see if this reduces to something like a Welch t-test because it may illuminate some complications. Eq. 52 in the paper you gave is nearly exactly what I want except it is not just a multinomial but a multinomial called a number of times for each user. I think the problem boils down to a generalized Behrens–Fisher problem. I am however a little out of my depth. Do you think you could sketch out a solution for this particular case? I know there are others on **Cross Validated** interested in the solution." CreationDate="2014-07-28T11:12:26.290" UserId="2511" />
  <row Id="1921" PostId="764" Score="0" Text="You need to find an algorithm that takes a pre-computed distance matrix or allows you to supply a distance-function that it can call when it needs to compute distances. Otherwise it wont work." CreationDate="2014-07-28T13:45:35.470" UserId="471" />
  <row Id="1922" PostId="823" Score="0" Text="Please, re-edit your question. I feel you have more than one question in your mind. Can you appropriately list them (first paragraph for introducing what you have read, second paragraph for your opinion about it, and third paragraph for listing your questions)?" CreationDate="2014-07-28T17:12:41.130" UserDisplayName="user1361" />
  <row Id="1925" PostId="831" Score="0" Text="@power Please take a look at http://datascience.stackexchange.com/help/dont-ask. If you still disagree, please raise the issue on [meta]." CreationDate="2014-07-29T02:08:03.853" UserId="62" />
  <row Id="1926" PostId="831" Score="0" Text="@AsheeshR Okay, I will re-word the question shortly." CreationDate="2014-07-29T03:56:38.243" UserId="366" />
  <row Id="1931" PostId="823" Score="1" Text="I'd suggest removing the word &quot;moral&quot; from the title of the question. Ethics, by definition, implies moral aspect as foundational." CreationDate="2014-07-29T12:38:08.847" UserId="2452" />
  <row Id="1932" PostId="823" Score="1" Text="@AleksandrBlekh sure will do, thanks for feedback, I noted today that okcupid have just admitted experimenting on users." CreationDate="2014-07-29T12:39:48.540" UserId="95" />
  <row Id="1934" PostId="846" Score="0" Text="Getting the data is often one of the biggest challenge :)" CreationDate="2014-07-29T19:35:55.310" UserId="737" />
  <row Id="1935" PostId="865" Score="0" Text="This question is very difficult to understand. Can you please consider rewriting it with a better description of what you are looking for and what you have already tried." CreationDate="2014-07-30T12:22:03.380" UserId="802" />
  <row Id="1936" PostId="866" Score="0" Text="Can you provide some example data (in plain English, no codes)?" CreationDate="2014-07-30T13:49:20.293" UserId="1279" />
  <row Id="1937" PostId="866" Score="0" Text="I added some example data to my original post.  In this version, each condition is denoted by a three letter code." CreationDate="2014-07-30T13:59:47.707" UserId="2781" />
  <row Id="1941" PostId="866" Score="1" Text="R is cool, but not very human-readable. Could you please reformat sample of your data as a table (e.g. using CSV or TSV format; 5-6 columns is ok)? Also, some explanation of variables (what &quot;anx.any&quot;, &quot;flu.isbefore.ckd&quot;, etc. actually mean and what is to be predicted) will help a lot." CreationDate="2014-07-30T19:49:21.060" UserId="1279" />
  <row Id="1942" PostId="871" Score="1" Text="Let me know if you have further questions and I'll do my best to provide some more detail." CreationDate="2014-07-30T20:09:29.443" UserId="684" />
  <row Id="1943" PostId="870" Score="0" Text="Worth noting that this is not explicitly a problem in all of machine learning, but only a problem when it comes to generating feature vectors, which are not ubiquitous in machine learning." CreationDate="2014-07-30T20:12:37.283" UserId="869" />
  <row Id="1944" PostId="870" Score="0" Text="What kind of machine learning doesn't use features?" CreationDate="2014-07-30T20:22:26.113" UserId="381" />
  <row Id="1945" PostId="870" Score="0" Text="Random forest is a good example of something for which getting a feature vector of the sort you see in neural nets is not an issue. A lot of unsupervised methods also work on raw words rather than feature vectors. Note: I didn't say there are methods that don't use features, only that there are methods which do not rely on strictly structured vectors." CreationDate="2014-07-30T23:00:17.087" UserId="869" />
  <row Id="1946" PostId="870" Score="0" Text="I don't know what you mean by &quot;strictly structured&quot;." CreationDate="2014-07-30T23:05:35.013" UserId="381" />
  <row Id="1947" PostId="870" Score="0" Text="strictly structured is a 1d vector of unint8's as opposed to a list containing a dictionary, a weight matrix, and a series of strings" CreationDate="2014-07-30T23:48:32.817" UserId="869" />
  <row Id="1948" PostId="769" Score="0" Text="It took me a while to try everything out, I had an error earlier with PCA, now I see I just get much lower accuracy when using it. I reduce dimensions to 100, and that should be fine, but SVM gives me 4% error on MNIST (0.6% without PCA) and over 20% error on DIGITS (4% without PCA). Same for other classifiers. Earlier I somehow made the error of doing PCA on the whole dataset (train and test sets) which gave me too optimistic results." CreationDate="2014-07-31T05:29:44.877" UserId="1387" />
  <row Id="1949" PostId="875" Score="0" Text="Thanks, I'll try it" CreationDate="2014-07-31T08:13:05.463" UserId="988" />
  <row Id="1950" PostId="863" Score="0" Text="I'm not so sure about what you say on MapReduce, especially `So basically any problem that doesn't break data locality principle may be efficiently implemented using MapReduce`. From my understanding, you can only solve problems which can be expressed with the MapReduce pattern." CreationDate="2014-07-31T09:33:22.557" UserId="883" />
  <row Id="1951" PostId="863" Score="0" Text="@fxm: MapReduce framework may be used for pretty much different tasks. For example, Oozie - workflow scheduler for different Hadoop components - has so-called Java action, that simply creates one mapper and runs custom Java code in it. With this approach you can essentially run _any_ code. This, however, won't give you any MR advantages compared to simple Java app. The only way to gain these advantages is to run computations in parallel (using Map) on *nodes with data*, that is, locally. To summarize: you can run _any_ code with MR, but to obtain performance, you need to hold  data locality." CreationDate="2014-07-31T11:23:04.177" UserId="1279" />
  <row Id="1952" PostId="863" Score="0" Text="I agree, but what I meant is a developer should not worry about managing data locality (which is managed by hadoop), but rather about expressing the wanted algorithms with the MapReduce pattern." CreationDate="2014-07-31T12:00:27.257" UserId="883" />
  <row Id="1953" PostId="866" Score="0" Text="Understandable.  I was trying to provide a &quot;reproducible example.&quot;  I will update the question now.  Thanks." CreationDate="2014-07-31T12:11:59.803" UserId="2781" />
  <row Id="1954" PostId="863" Score="0" Text="@fxm: developer ensures locality by using mapper. Roughly speaking, &quot;map(f, data)&quot; means &quot;transfer f() to data nodes and run it locally&quot;.  If developer doesn't consider locality (e.g. puts all computations to reducer), he loses all advantages of MR. So expressing algo as (efficient) MR job implies utilization of data locality anyway. That said, I would still suggest using more flexible tools than pure MR such as Spark." CreationDate="2014-07-31T13:29:35.030" UserId="1279" />
  <row Id="1955" PostId="769" Score="0" Text="Which programming language are you using btw? If you are a Python guy, I'd have some examples here where I used PCA, maybe it helps: http://sebastianraschka.com/Articles/2014_about_feature_scaling.html http://sebastianraschka.com/Articles/2014_scikit_dataprocessing.html http://sebastianraschka.com/Articles/2014_pca_step_by_step.html Usually I prefer LDA since I am mostly working with supervised datasets (class labels), a separate article (like the step by step PCA) is in the works ;)" CreationDate="2014-07-31T13:47:17.910" UserId="2556" />
  <row Id="1956" PostId="871" Score="0" Text="Thanks, that gives me some great terms to continue exploring with!" CreationDate="2014-07-31T15:21:58.400" UserId="2790" />
  <row Id="1957" PostId="769" Score="0" Text="I am using C++ for classification and Matlab to prepare datasets. I will check out your links and try LDA too." CreationDate="2014-07-31T16:04:48.890" UserId="1387" />
  <row Id="1958" PostId="876" Score="0" Text="Thanks, Andy.  Could you elaborate a little? Is it because the variables don't capture enough detail?" CreationDate="2014-07-31T16:42:18.460" UserId="2781" />
  <row Id="1959" PostId="876" Score="0" Text="I have no idea. I guess it depends on how different models work." CreationDate="2014-07-31T16:54:46.747" UserId="1241" />
  <row Id="1960" PostId="876" Score="0" Text="Could you suggest some of the solutions you tried or considered?" CreationDate="2014-07-31T17:25:51.437" UserId="2781" />
  <row Id="1961" PostId="876" Score="0" Text="So far I haven't done either, so no help there. Sorry." CreationDate="2014-07-31T17:38:05.417" UserId="1241" />
  <row Id="1962" PostId="641" Score="0" Text="@Madison May. Did you find a data set? I'm looking for something similar. Thanks." CreationDate="2014-07-31T22:49:26.447" UserId="2507" />
  <row Id="1963" PostId="641" Score="0" Text="I had to make do with the twitter ner corpus from U. Washington (linked to in original post)." CreationDate="2014-07-31T23:01:20.507" UserId="684" />
  <row Id="1964" PostId="865" Score="0" Text="Not a data science question, its a programming question. Go ask on StackOverflow" CreationDate="2014-08-01T09:08:50.180" UserId="471" />
  <row Id="1965" PostId="840" Score="0" Text="Have you checked the contents of `df`?" CreationDate="2014-08-01T12:17:52.767" UserId="172" />
  <row Id="1966" PostId="881" Score="0" Text="Thanks for this! It confirms some of the steps I have already taken (exploratory analysis, hypothesis testing, etc.)." CreationDate="2014-08-01T15:15:02.910" UserId="2781" />
  <row Id="1967" PostId="883" Score="4" Text="What class is the Date column? It looks like it might be sorting by character (1 comes before 9) rather than date value." CreationDate="2014-08-01T20:13:47.650" UserId="2802" />
  <row Id="1968" PostId="883" Score="0" Text="Base R read.csv() converts strings to factor. I agree with @user1683454; you will find that your dates are in alphabetical order." CreationDate="2014-08-02T04:59:48.967" UserId="2666" />
  <row Id="1969" PostId="722" Score="2" Text="I would also add `dplyr`, which is an optimized rephrasing of certain `plyr` tools, and `data.table` which is a completely different approach to manipulating data. Both by Hadley Wickham." CreationDate="2014-08-02T05:22:07.200" UserId="1156" />
  <row Id="1970" PostId="722" Score="0" Text="@ssdecontrol: I agree - updated the answer. Hadley is the author of the `dplyr` package, but not of the `data.table` one." CreationDate="2014-08-02T08:22:18.307" UserId="2452" />
  <row Id="1971" PostId="722" Score="1" Text="Funny, I just kind of assumed he was. Thanks." CreationDate="2014-08-02T11:46:36.047" UserId="1156" />
  <row Id="1972" PostId="890" Score="0" Text="The computations I perform are both cpu and disk intense, which may occur concurrently. Using gpu's would surely speedup the cpu step, but disk access would still limit the performance." CreationDate="2014-08-02T16:53:34.197" UserId="84" />
  <row Id="1973" PostId="823" Score="1" Text="http://www.datakind.org/blog/meetup-recap-untangling-ethical-questions-in-data-science/" CreationDate="2014-08-02T19:58:25.010" UserId="381" />
  <row Id="1974" PostId="890" Score="1" Text="@Rubens you've to have good reason to use GPU for your computation. It expects fine-grained matrix type data, and well written parallel works, kernel implementations. For your disk problem, you'ld stream your data and feed them into RAM so that both (especially) GPU and CPU will benefits. Then, you can use your data in any, arithmetic, manner e.g. you can use map+reduce approach on your stream. To note, CUDA has built-in support for Map+Reduce+filter operations. However, testing performance on GPU based calculation is tricky (You can use manual calculation putting watcher around your code)." CreationDate="2014-08-02T20:51:25.250" UserDisplayName="user1361" />
  <row Id="1975" PostId="890" Score="0" Text="@Rubens Some patterns in [Patterns for Parallel Programming book](http://www.amazon.com/Patterns-Parallel-Programming-paperback-Software/dp/0321940784) will probably solve your disk problem." CreationDate="2014-08-02T20:53:10.027" UserDisplayName="user1361" />
  <row Id="1976" PostId="739" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:00.257" UserId="62" />
  <row Id="1977" PostId="808" Score="0" Text="This question appears to be off-topic because it is about career advice. Career advice has been proven to result in opinion-oriented, broad questions or sometimes extremely restricted questions, most of which result in no useful discourse. If you disagree with this opinion, please raise the issue on [meta]." CreationDate="2014-08-03T06:18:34.900" UserId="62" />
  <row Id="1978" PostId="769" Score="0" Text="I tried using LDA but can't get it working with my data. Matlab function classify should perform LDA but it works only up to 20 dimensions, at least on my data. Also I found that maximum dimensions given by LDA should be number_of_classes-1, which is too little." CreationDate="2014-08-03T09:20:08.827" UserId="1387" />
  <row Id="1981" PostId="893" Score="4" Text="I'd say [CV.SE](http://stats.stackexchange.com/) is a better place for questions about more theoretical statistics like this. If not, I'd say that the answer to your questions depend on context. Sometimes it makes sense to flatten multiple levels into dummy variables, other times it's worth to model your data according to multinomial distribution, etc." CreationDate="2014-08-03T14:00:11.460" UserId="1279" />
  <row Id="1985" PostId="876" Score="0" Text="I'm now on vacation for the next few weeks, but when I get back I'll look into it because it really has piqued my interest." CreationDate="2014-08-03T20:29:59.447" UserId="1241" />
  <row Id="1986" PostId="878" Score="0" Text="Thanks for the response, do you have some source explaining more thoroughly about what kind of data is collected and methods of user tracking?" CreationDate="2014-08-03T20:37:39.297" UserId="2798" />
  <row Id="1987" PostId="878" Score="0" Text="I believe tasks are too diverse to be described in a single source. For example, in Facebook you would be interested in what user likes, who he talks to most frequently, etc. In commercial website you'd like to know what actions led to conversion and what forced user to leave site. In entertainment software (like games or funny web pages) you would most likely want to optimize user experience and thus look for UI component usage, time spent on page, etc. Different tasks require different data and different methods. Just determine your use case and look for appropriate approach." CreationDate="2014-08-03T21:15:16.970" UserId="1279" />
  <row Id="1988" PostId="769" Score="0" Text="I just uploaded the LDA article, although I used Python for the step-wise implementation, the Intro might still be interesting and helpful: http://sebastianraschka.com/Articles/2014_python_lda.html" CreationDate="2014-08-03T21:40:36.477" UserId="2556" />
  <row Id="1989" PostId="883" Score="1" Text="You should probably ask this on stackoverflow, its a basic R programming question, not really data science." CreationDate="2014-08-03T22:15:35.913" UserId="471" />
  <row Id="1990" PostId="883" Score="0" Text="@Spacedman good point. I will put it there next time." CreationDate="2014-08-04T14:23:22.043" UserId="2614" />
  <row Id="1991" PostId="902" Score="3" Text="There's one way to get &quot;generalization&quot; that is good enough for any scenario - sample the entire population. In all other cases your best option is to select confidence level and take sample large enough to give reasonable confidence interval." CreationDate="2014-08-04T20:06:43.500" UserId="1279" />
  <row Id="1993" PostId="886" Score="1" Text="As an addendum, I'd recommend taking a look at the `lubridate` package, which can make certain date manipulation tasks simpler." CreationDate="2014-08-04T23:30:59.173" UserId="1156" />
  <row Id="1994" PostId="812" Score="0" Text="Hopefully an answer to this question can touch on how graphs like these were made: https://medium.com/i-data/israel-gaza-war-data-a54969aeb23e?_ga=1.106579909.790909978.1407183841" CreationDate="2014-08-04T23:39:21.430" UserId="1156" />
  <row Id="1995" PostId="821" Score="2" Text="I would downvote this answer if I could. Modeling a feature of the data for its own sake is absurd. Models exist to answer questions, and if seasonality does not affect your question or its answer, then it is not only allowable but desirable to ignore it. With that in mind, @marcodena, the only answer to your question is &quot;because sometimes ignoring it will bias your predictions.&quot; I think a better question would ask which times those are. I also disagree with implication here that data always precedes a model in statistics." CreationDate="2014-08-04T23:44:04.903" UserId="1156" />
  <row Id="1996" PostId="811" Score="1" Text="I'd say it's safer to balance your sample, but also collect sampling weights so you can later re-weight your data for representativeness if you need to. @pnp plenty of social scientists build non-predictive models, e.g. for confirming theories." CreationDate="2014-08-04T23:56:14.120" UserId="1156" />
  <row Id="1997" PostId="886" Score="0" Text="@ssdecontrol: Agree, thanks for the comment. Upvoted." CreationDate="2014-08-05T00:15:21.147" UserId="2452" />
  <row Id="1998" PostId="904" Score="0" Text="I suggest replacing `untagged` tag with `r`, `dashboards`, `reports` or similar." CreationDate="2014-08-05T07:22:27.870" UserId="2452" />
  <row Id="1999" PostId="769" Score="0" Text="Finally I found out what was going on... My function in Matlab that writes features to a file would add spaces sometimes and only on some datasets and then my reader function in C would apparently read wrong values... PCA actually helped, boost classifier is still bad but will try to play with parameters some more to make it work. Still didn't try LDA but will do that too." CreationDate="2014-08-05T08:49:26.577" UserId="1387" />
  <row Id="2000" PostId="907" Score="0" Text="I didn't know about sparkTable, looks like a great tool for the job." CreationDate="2014-08-05T12:16:28.987" UserId="1156" />
  <row Id="2003" PostId="57" Score="1" Text="This is a very good post. R is excellent for data *manipulation* but can be pretty cumbersome with data *cleaning* because of its verbose syntax for string manipulation and fairly rigid adherence to lists and matrices for data structures." CreationDate="2014-08-05T13:01:12.313" UserId="1156" />
  <row Id="2004" PostId="871" Score="0" Text="Incidentally, I can relate to &quot;feature hashing&quot; since that seems very similar to a [bloom filter](http://en.wikipedia.org/wiki/Bloom_filter), which I'm familiar with from working with cryptocurrency code. I wonder if it's more effective to have a hashing function relate an input feature to multiple index positions (bloom-filter-style) rather than need a second hash function to set the sign of an index..." CreationDate="2014-08-05T13:44:28.797" UserId="2790" />
  <row Id="2005" PostId="769" Score="0" Text="Nice! I am glad to here that it was &quot;just&quot; a technical problem :). For supervised training samples, LDA is often (but not always) a better choice than PCA. There is a research article where the authors discuss this point: http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=908974" CreationDate="2014-08-05T15:04:04.100" UserId="2556" />
  <row Id="2006" PostId="907" Score="0" Text="@ssdecontrol: Until recently, I didn't know about it, too. Looks like a great tool, for sure. I look forward to trying it in my project, if I will have a need and opportunity." CreationDate="2014-08-05T17:50:51.177" UserId="2452" />
  <row Id="2007" PostId="913" Score="0" Text="Nice answer! Any thoughts on my related questions? 1) http://stats.stackexchange.com/questions/101251/determining-characteristics-of-sampling-sets-for-efa-cfa-sem; 2) http://stats.stackexchange.com/questions/90386/optimal-sampling-strategy-for-efa-cfa-and-sem." CreationDate="2014-08-05T17:55:47.023" UserId="2452" />
  <row Id="2008" PostId="909" Score="0" Text="Hi, Alexei! It seems that you're proficient in R, so I'm wondering, if you have any advice on the problem I'm currently stuck with: http://stackoverflow.com/questions/25101444/errors-related-to-data-frame-columns-during-merging. Beyond that, I'd be glad to connect with you (see aleksandrblekh.com for my profiles on professional social networks), as it seems that we have some common interests (including the native language :-)." CreationDate="2014-08-05T19:53:25.950" UserId="2452" />
  <row Id="2009" PostId="891" Score="0" Text="I've always wondered about the difference between measures and metrics. According the government (NIST): &quot;...We use measure for more concrete or objective attributes and metric for more abstract, higher-level, or somewhat subjective attributes. ... Robustness, quality (as in &quot;high quality&quot;), and effectiveness are important attributes that we have some consistent feel for, but are hard to define objectively. Thus these are metrics.&quot; But the context is software engineering, not mathematics. What's your take?" CreationDate="2014-08-05T20:55:59.590" UserId="2507" />
  <row Id="2010" PostId="891" Score="1" Text="Wikipedia was more helpful. distance(x,y) is must be non-negative; d(x,y)=0 only if x=y; d(x,y) = d(y,x); and satisfy triangle inequality- d(x, z) ≤ d(x, y) + d(y, z)" CreationDate="2014-08-05T21:03:19.577" UserId="2507" />
  <row Id="2011" PostId="915" Score="0" Text="This is a good question. A lot of statistics books talk about the theoretical aspects of high-dimensional data and not the computational aspects." CreationDate="2014-08-06T00:23:33.773" UserId="1156" />
  <row Id="2012" PostId="896" Score="0" Text="Can you add a full citation for the paper? The link doesn't work." CreationDate="2014-08-06T00:47:34.150" UserId="1156" />
  <row Id="2013" PostId="917" Score="2" Text="Typo in the title: spare =&gt; sparse." CreationDate="2014-08-06T02:52:16.923" UserId="2452" />
  <row Id="2014" PostId="891" Score="0" Text="That's pretty much it: a metric has to meet certain axioms and a measure is less strictly defined." CreationDate="2014-08-06T03:34:29.120" UserId="2809" />
  <row Id="2015" PostId="893" Score="0" Text="Are your categorical variables ordered ? If yes, this can influence the type of correlation you want to look for." CreationDate="2014-08-06T06:58:25.637" UserId="906" />
  <row Id="2016" PostId="918" Score="3" Text="Thank you for the great answer! I am hesitant to classify the problem as sparse regression since I am not really trying to model and predict but rather solve for a set of coefficients. The reason I am using Genetic Algorithms is because I can also employ constraints on the equation. If no other answers come through I will gladly accept this though." CreationDate="2014-08-06T12:27:51.970" UserId="802" />
  <row Id="2017" PostId="919" Score="2" Text="With k=5 you will get 20k observations in training set and 5k in testing set. With k=25 you'll get 24k for training and 1k for testing. If you believe that additional 4k records will affect generalization a lot, use larger k. If you think that even, say, 10k records already give good generalization, use smaller k. If you are unsure, just use standard 10-fold cross validation, which is a good compromise in most cases." CreationDate="2014-08-06T13:02:48.630" UserId="1279" />
  <row Id="2018" PostId="924" Score="0" Text="Thanks a lot. As I see, it's all about learning Caret package.." CreationDate="2014-08-06T13:14:47.953" UserId="97" />
  <row Id="2019" PostId="927" Score="0" Text="I split my test/train sets this way:  http://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas  Maybe this can help." CreationDate="2014-08-06T17:09:21.220" UserId="375" />
  <row Id="2020" PostId="918" Score="1" Text="@mike1886: My pleasure! I have updated my answer, based on your comment. Hope it helps." CreationDate="2014-08-06T21:34:14.897" UserId="2452" />
  <row Id="2021" PostId="926" Score="1" Text="Stephan, I appreciate your kind words! Upvoted your nice answer. You might be interested in the update I made to my answer, based on comment by the question's author." CreationDate="2014-08-06T21:44:31.100" UserId="2452" />
  <row Id="2022" PostId="896" Score="0" Text="@ssdecontrol the paper is called &quot;Class-Based n-gram Models of Natural Language&quot; by Brown et al. I have actually managed to resolve this. I will post details as an answer." CreationDate="2014-08-07T03:02:32.407" UserId="2817" />
  <row Id="2023" PostId="685" Score="0" Text="Instead of &quot;saving the coefficients&quot; you could save the whole model to a file, later load it again and use the predict() function. Should make the process a bit easier and less error prone." CreationDate="2014-08-07T08:41:22.423" UserId="676" />
  <row Id="2024" PostId="685" Score="0" Text="How much data did you use for training? Did you tune glmnet's lambda parameter and how? Cross validation?" CreationDate="2014-08-07T08:43:01.317" UserId="676" />
  <row Id="2025" PostId="933" Score="1" Text="Also, his other publications are at publications at http://research.microsoft.com/en-us/people/cyl/publication.aspx" CreationDate="2014-08-07T09:34:05.100" UserId="2861" />
  <row Id="2026" PostId="934" Score="1" Text="I asked a somewhat related question : http://datascience.stackexchange.com/q/810/2661" CreationDate="2014-08-07T12:24:39.557" UserId="2661" />
  <row Id="2027" PostId="934" Score="1" Text="Also, did you try the `BayesNet` (Bayesian Networks) algorithm in Weka and tried tuning the `MaxNrOfParents` argument in K2 search algorithm? I found it of good help in class imbalance problems." CreationDate="2014-08-07T12:28:24.147" UserId="2661" />
  <row Id="2028" PostId="930" Score="0" Text="the first suggestion will work but it is kind of &quot;manual&quot;.I was looking for train_test_split way to get dataframe. The second explanation does what i normally do but it returns nparray to which it is hard to impute values since there are no column names." CreationDate="2014-08-07T13:57:43.967" UserId="2854" />
  <row Id="2029" PostId="929" Score="0" Text="I can do that but it does not preserve any column names from the original dataframe. Then I need to impute values by indexes which can be very painful and can be a source of errors." CreationDate="2014-08-07T14:00:36.443" UserId="2854" />
  <row Id="2030" PostId="929" Score="0" Text="You can simply get the columns from the first dataframe in &#xA;cols = list(df_orig.columns.values)&#xA;and then set the new column names with this&#xA;new_df = pf.DataFrame(a,columns=cols)" CreationDate="2014-08-07T14:06:47.757" UserId="802" />
  <row Id="2031" PostId="936" Score="1" Text="Have you tried running that code in pure R, not Knitr? It seems to be more of an R problem and rather than a rendering to graphics problem." CreationDate="2014-08-07T15:14:37.097" UserId="802" />
  <row Id="2032" PostId="929" Score="0" Text="That really worked! Thanks a lot!" CreationDate="2014-08-07T15:27:42.750" UserId="2854" />
  <row Id="2033" PostId="936" Score="0" Text="The code produces no error when run in R." CreationDate="2014-08-07T15:40:41.360" UserId="2792" />
  <row Id="2034" PostId="936" Score="1" Text="As it stands, this example is not reproducible and therefore nobody can help you. Can you provide a minimal working example (MWE) of your .Rmd file that replicates the error? Chances are, constructing the MWE will also help you figure out what the problem is. However, my guess is that you aren't loading your data anywhere in the code. Knitr searches a new environment and not the current global environment, so you will need to re-load all packages and objects you plan to use." CreationDate="2014-08-07T16:45:11.010" UserId="1156" />
  <row Id="2035" PostId="922" Score="0" Text="Hi @Ali, it would help if you added some more detail. What do you mean by &quot;similarity words&quot;? are you talking about some preprocessing step? SVMs don't use a similarity measure." CreationDate="2014-08-07T23:39:38.887" UserId="21" />
  <row Id="2036" PostId="777" Score="0" Text="I think this might be more suitable for StackOverflow if you're looking for how to use the Github API? Maybe ServerFault?" CreationDate="2014-08-07T23:40:33.203" UserId="21" />
  <row Id="2037" PostId="910" Score="0" Text="This answer is exactly what I was hoping to get here. Thank you very much." CreationDate="2014-08-08T07:18:47.767" UserId="1192" />
  <row Id="2038" PostId="866" Score="0" Text="Can you provide more information on the parameters used in the data set so that we can understand if there are any correlations. Some of the abbreviations mentioned by you are not clear to me. It would be great if you could share your email-id for us to collaborate offline. Thanks!" CreationDate="2014-08-08T06:04:45.150" UserId="1094" />
  <row Id="2039" PostId="748" Score="0" Text="Most of your question here is describing a problem with your question. The text does not contain your question. Please in-line it and improve the text if needed. I don't think you need to summarize the history on the other site." CreationDate="2014-08-08T12:20:17.923" UserId="21" />
  <row Id="2040" PostId="812" Score="0" Text="Hello @Cici, usually questions about recommended tools are discouraged on this and other SE sites, as they just invite a lot of opinion." CreationDate="2014-08-08T12:21:22.990" UserId="21" />
  <row Id="2041" PostId="821" Score="2" Text="I know but I'm a student, so I'm learning. The question is made for this reason :)" CreationDate="2014-08-08T13:27:54.967" UserId="989" />
  <row Id="2042" PostId="942" Score="0" Text="Thanks, that's very helpful! If you do a literature review for various predictive modelling techniques, I'm sure it would get referenced a lot. It would be very helpful for people who want to differentiate between which algorithms to use in large n or large p cases, or for medium values of those for more precise calculations. Do you happen to know how some of the more obscure techniques scale? (Like Cox proportional hazard regression or confirmatory factor analysis)" CreationDate="2014-08-08T13:31:01.120" UserId="2841" />
  <row Id="2044" PostId="934" Score="0" Text="http://cs229.stanford.edu/proj2005/AltendorfBrendeDanielLessard-FraudDetectionForOnlineRetailUsingRandomForests.pdf&#xA;A good read that involves a similar 'rare-event' problem.  The authors use a random forest and optimize based on the ratio of class occurrence in the training set. (I'm not affiliated, but was just reading this a few days ago for a problem I'm working on)." CreationDate="2014-08-08T16:12:01.837" UserId="375" />
  <row Id="2045" PostId="947" Score="0" Text="That's really helpful. In our case we have currently hundreds of patterns so we'd end up having vectors with hundreds of features. Do these algorithms behave well in this situation? Other thing, each time a totally new exception comes up we'd need to classify it manually, possibly add new features and launch the train process to get a new model. Is this an appropriate approach?" CreationDate="2014-08-09T09:35:12.643" UserId="2878" />
  <row Id="2046" PostId="947" Score="0" Text="NB and SVM should work fine, decision trees over hundreds of variables may be hard to visualize and interpret, but I don't expect degradation in accuracy (though **random forests** are often suggested in such settings). As for new exceptions, standard way is to keep special `&lt;UNKOWN&gt;` variable, and really add new variables only when percentage of unknowns exceeds, say, 5%." CreationDate="2014-08-09T12:09:35.123" UserId="1279" />
  <row Id="2047" PostId="947" Score="0" Text="Great. Thanks!!" CreationDate="2014-08-09T13:50:19.260" UserId="2878" />
  <row Id="2048" PostId="937" Score="0" Text="I created my own class for that but very surprised that sklearn doesn't have that." CreationDate="2014-08-09T14:36:09.090" UserId="2854" />
  <row Id="2049" PostId="713" Score="0" Text="FWIW, I just started the 5.1 QuickStart VM and there was no such warning." CreationDate="2014-08-10T10:08:52.010" UserId="21" />
  <row Id="2052" PostId="715" Score="0" Text="There's no configuration that is required, or certainly there shouldn't be. The 5.1 VM does not exhibit this." CreationDate="2014-08-11T14:18:47.123" UserId="21" />
  <row Id="2053" PostId="954" Score="1" Text="possible duplicate of [Coreference Resolution for German Texts](http://datascience.stackexchange.com/questions/955/coreference-resolution-for-german-texts)" CreationDate="2014-08-11T15:19:58.320" UserId="1279" />
  <row Id="2054" PostId="757" Score="0" Text="Have you tried a model using temperatures?" CreationDate="2014-08-11T19:03:57.133" UserId="325" />
  <row Id="2055" PostId="959" Score="0" Text="Thanks for the helpful suggestions. I will definitely check out that book! Though I have access to lab values, the data is unreliable and sporadic, so i am trying to stick to data I can get from claims. The variable abbreviations are actually AHRQ Clinical Classification Software groupings of diagnosis codes." CreationDate="2014-08-11T22:01:36.267" UserId="2781" />
  <row Id="2056" PostId="955" Score="2" Text="Your question is very concise, yet it'd be nice to hear from you what you've tried so far, or even see some example of what you're trying to achieve. Would you mind adding some further information to your post?" CreationDate="2014-08-12T01:55:23.210" UserId="84" />
  <row Id="2057" PostId="955" Score="0" Text="possible duplicate of [OpenNLP Coreference Resolution (German)](http://datascience.stackexchange.com/questions/954/opennlp-coreference-resolution-german)" CreationDate="2014-08-12T08:00:42.737" UserId="2920" />
  <row Id="2058" PostId="954" Score="0" Text="It is not a duplicate. It is a question specific to OpenNLP and the other question is about coreference resolution tools in general" CreationDate="2014-08-12T10:57:39.147" UserId="979" />
  <row Id="2059" PostId="955" Score="0" Text="I already added additional information to the post" CreationDate="2014-08-12T11:00:45.350" UserId="979" />
  <row Id="2060" PostId="954" Score="0" Text="I can't see the difference between these questions. Both ask for a coreference tool besides OpenNLP, because OpenNLP doesn't support it." CreationDate="2014-08-12T11:20:12.657" UserId="21" />
  <row Id="2061" PostId="965" Score="1" Text="I think you'll probably need to be more specific about what your data is like, what your tool requirements are, what you've tried, in order to get more specific replies." CreationDate="2014-08-12T11:22:30.637" UserId="21" />
  <row Id="2062" PostId="967" Score="1" Text="Actually I disagree. Since the author likes them being in trouble, it is a positive sentiment there. It's a negative comment on the company, but nevertheless a positive sentiment by the author. In this simpler scenario (I'm not saying this is the complete goal), **predicting which emojis a user would add to his post** sounds like a reasonable task to me. In fact you can construct many cases where the emoji will be essential.. Consider &quot;Got f_cked :-)&quot; as opposed to &quot;Got f_cked. :-(&quot;" CreationDate="2014-08-12T13:32:24.477" UserId="2920" />
  <row Id="2063" PostId="963" Score="0" Text="Don't believe that something like this exists currently, but would love it if you put something together for this!" CreationDate="2014-08-12T13:57:53.260" UserId="548" />
  <row Id="2064" PostId="967" Score="0" Text="In case you try to estimate person's emotion as opposed to person's attitude to a subject, then yes, this example doesn't work. But there are many others. Sarcasm is common case. Consider sentence &quot;oh yeah, you are real 'master' ;)&quot;. Human can catch negative context, but positive emoticon will point to positive emotion. But I haven't really got it: do you want to extract subjective information from tweets or just predict possible emojis? Even though they sound similar, second task is not really about sentiment analysis. Not directly, at least." CreationDate="2014-08-12T14:10:36.103" UserId="1279" />
  <row Id="2065" PostId="954" Score="0" Text="This question asks about training methods to add the coref resolution functionality to OpenNLP and the other one asks for available tools for German coreference resolution. This is a big difference!" CreationDate="2014-08-12T15:11:32.730" UserId="979" />
  <row Id="2066" PostId="967" Score="0" Text="The &quot;wink&quot; smiley is usually not considered to be &quot;positive&quot;, but &quot;ironic&quot;... which is why a good dictionary such as SentiWordNet makes sense. If you look up funny in SentiWordNet, is has more than one meaning, too! http://sentiwordnet.isti.cnr.it/search.php?q=funny&#xA;(So it *is* not trivial to annotate them manually, because it's not as simple as positive/negative; but you should do the usual interrater-agreement validation etc.)" CreationDate="2014-08-12T16:03:50.307" UserId="2920" />
  <row Id="2067" PostId="967" Score="0" Text="Now I see your idea. But I don't really think it will work, just because (most) emojis don't really sound like a good predictors to me, and you explicitly don't want to use other features. Anyway, this is just an opinion based on my experience, only data can give real answers. Good luck!" CreationDate="2014-08-12T21:39:06.460" UserId="1279" />
  <row Id="2068" PostId="967" Score="0" Text="Who said I don't want to use other features? But for these I have seen databases..." CreationDate="2014-08-13T10:13:26.760" UserId="2920" />
  <row Id="2069" PostId="954" Score="1" Text="OK. I suggest rewording these to make them more distinct. I think many readers will not get the nuance of what you are asking in each case." CreationDate="2014-08-13T12:17:52.483" UserId="21" />
  <row Id="2071" PostId="973" Score="0" Text="Can you clarify? It sounds like you are looking for feature importance, but the link you give talks about writing weights as a linear combination of input, which is not the same thing at all." CreationDate="2014-08-14T10:22:23.980" UserId="21" />
  <row Id="2072" PostId="973" Score="0" Text="@SeanOwen, actually, what I wanted to say was since you can compute the weight vector using the dual variables and the input examples (in a binary SVM), and the actual weights corresponding to features tell us the relative importance of the features [Gene Selection for Cancer Classification using&#xA;Support Vector Machines&#xA;](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.70.9598&amp;rep=rep1&amp;type=pdf)" CreationDate="2014-08-14T14:48:25.540" UserId="2949" />
  <row Id="2074" PostId="957" Score="0" Text="If this does not get answered here, consider migrating it to the statistics site (stats.stackexchange.com) where you will be more likely to find experts in [time series](http://stats.stackexchange.com/questions/tagged/time-series)." CreationDate="2014-08-14T17:08:25.320" UserId="1237" />
  <row Id="2075" PostId="976" Score="2" Text="Math formatting does not work on DataScience? Really? May be we should ask to get it." CreationDate="2014-08-14T17:29:51.060" UserId="1237" />
  <row Id="2077" PostId="976" Score="0" Text="Good point about numerical accuracy." CreationDate="2014-08-15T00:24:26.790" UserId="1156" />
  <row Id="2078" PostId="980" Score="1" Text="Thanks a lot for such detailed response. I will go through all your suggestions, a lot of work to try and test! Also I have found that carrot2 tool is really doing great job on unsupervised clustering of textual data. Posting link for future reference [http://project.carrot2.org/](http://project.carrot2.org/)" CreationDate="2014-08-15T19:20:33.777" UserId="2958" />
  <row Id="2079" PostId="980" Score="0" Text="@MaximGalushka: You're very welcome! I'm curious to learn about your findings and the progress that you will achieve eventually. Feel free to post here or connect directly with me." CreationDate="2014-08-16T01:25:37.757" UserId="2452" />
  <row Id="2080" PostId="977" Score="0" Text="Cross-post from stats http://stats.stackexchange.com/questions/111911/solutions-for-continuous-online-cluster-identification AND stackoverflow: http://stackoverflow.com/questions/24970702/algorithm-for-identifying-non-ambiguous-clusters" CreationDate="2014-08-16T10:33:48.610" UserId="924" />
  <row Id="2082" PostId="915" Score="0" Text="In many cases, the original literature will discuss complexity. But often theoretical complexity is useless. QuickSort has a worst-case of O(n^2), but often is the fastest - faster than HeapSort, which has worst case O(n log n).&#xA;If you do a little research, you will find out complexity results for many algorithms - if known. E.g. PCA being O(n d^3), k-means being O(n k i d) etc." CreationDate="2014-08-16T10:39:50.540" UserId="924" />
  <row Id="2083" PostId="984" Score="0" Text="I don't think it's possible for people to help you without the data set, and it sounds like that is intentionally not public. I am not sure it's a good thing to ask for it then, and questions asking for links or resources are not considered on-topic for StackExchange sites." CreationDate="2014-08-16T17:31:37.270" UserId="21" />
  <row Id="2084" PostId="985" Score="2" Text="Sure, you can. It's common to do dimensionality reduction as a preprocessing step." CreationDate="2014-08-16T17:39:49.320" UserId="381" />
  <row Id="2085" PostId="898" Score="1" Text="Thanks Alexey for the details. Based on more research i found about polyserial and polychloric correlation. How is your approach better than these? Please explain" CreationDate="2014-08-17T09:58:57.173" UserId="1151" />
  <row Id="2086" PostId="898" Score="0" Text="I'm not aware of these things, sorry." CreationDate="2014-08-17T14:04:27.123" UserId="816" />
  <row Id="2087" PostId="936" Score="0" Text="As @ssdecontrol suggests, you probably haven't created or loaded `sample` in the R in your document subsequent to this line." CreationDate="2014-08-17T14:21:23.310" UserId="2978" />
  <row Id="2088" PostId="961" Score="0" Text="Please share some info on the data you are looking at." CreationDate="2014-08-17T19:22:03.863" UserId="1193" />
  <row Id="2089" PostId="961" Score="0" Text="@bayer: please check out my update." CreationDate="2014-08-17T19:48:04.687" UserId="1279" />
  <row Id="2090" PostId="961" Score="0" Text="This looks like a poor training procedure. Can you add information on the number of CD steps, learning rate/momentum, batch size etc?" CreationDate="2014-08-18T11:34:15.643" UserId="1193" />
  <row Id="2091" PostId="961" Score="0" Text="@bayer: at the moment of these experiments I used CD-1, batch size of 10 images, learning rate of 0.01 (0.1 / batch_size) and no momentum at all. I also noticed that weight initialization has some impact: with weights initialized from N(0, 0.01) I have almost never seen described issue, but with weights from N(0, 0.001) I get the issue almost each time." CreationDate="2014-08-18T11:58:30.507" UserId="1279" />
  <row Id="2092" PostId="989" Score="0" Text="Could you provide the code? Also, does it training or testing takes so much time? How about smaller training/testing datasets?" CreationDate="2014-08-18T12:09:04.657" UserId="1279" />
  <row Id="2093" PostId="989" Score="0" Text="I am just reading data from a csv file into a pandas dataframe and passing it to the scikit learn function. That's all! Providing code wouldn't really help here" CreationDate="2014-08-18T12:49:25.843" UserId="793" />
  <row Id="2094" PostId="989" Score="3" Text="sklearn's SVM implementation implies at least 3 steps: 1) creating SVR object, 2) fitting a model, 3) predicting value. First step describes kernel in use, which helps to understand inner processes much better. Second and third steps are pretty different, and we need to know at least which of them takes that long. If it is training, then it may be ok, because learning is slow sometimes. If it is testing, then there's probably a bug, because testing in SVM is really fast. In addition, it may be CSV reading that takes that long and not SVM at all. So all these details may be important." CreationDate="2014-08-18T13:22:28.580" UserId="1279" />
  <row Id="2095" PostId="995" Score="0" Text="I just edited my question to add that the dependent variable is binary. Hence, a linear model isn't suitable." CreationDate="2014-08-18T17:36:56.743" UserId="1241" />
  <row Id="2096" PostId="995" Score="0" Text="&quot; you should not expect different models to perform nearly identically, unless they all provide the same predictive bias.&quot; I used MAE and the ratio of actual to predicted outcomes as validation measures and the ratios were very close." CreationDate="2014-08-18T17:40:01.437" UserId="1241" />
  <row Id="2097" PostId="995" Score="1" Text="Andy, I would include logistic regression (and linear SVM) as 'linear' model. They are all just separating the data by a weighted sum of the inputs." CreationDate="2014-08-18T17:59:17.967" UserId="1256" />
  <row Id="2098" PostId="995" Score="1" Text="@seanv507 Exactly - the decision boundary is still linear. The fact that binary classification is being performed doesn't change that." CreationDate="2014-08-18T18:07:36.887" UserId="964" />
  <row Id="2099" PostId="961" Score="0" Text="Do you randomly initialize your weights before training?" CreationDate="2014-08-18T18:18:08.873" UserId="403" />
  <row Id="2100" PostId="961" Score="0" Text="@gallamine: yes, I initialize them from `N(0, 0.01)` or `N(0, 0.001)` with later case producing smoother images but giving described issues more frequently." CreationDate="2014-08-18T19:07:31.317" UserId="1279" />
  <row Id="2101" PostId="961" Score="0" Text="I recommend playing around with the learning rate and adding momentum. Another thing you should check is if the features (i.e. $p(h|v)$) are saturating. That is the case if all the features are always close to 0 or 1--learning gets harder then." CreationDate="2014-08-18T19:43:41.783" UserId="1193" />
  <row Id="2102" PostId="997" Score="0" Text="Could you be more specific into what you're looking for? does it matter?" CreationDate="2014-08-19T13:48:19.933" UserId="2969" />
  <row Id="2103" PostId="997" Score="0" Text="I just want to have a look at the dataset such that I can perform data visualization , or even apply time series models to the dataset." CreationDate="2014-08-19T14:44:20.317" UserId="2972" />
  <row Id="2104" PostId="997" Score="0" Text="Would EEG data work for you? It's spatial (channel locations on the head) time series data." CreationDate="2014-08-19T14:47:49.047" UserId="2969" />
  <row Id="2105" PostId="997" Score="0" Text="Sure. Thanks a lot" CreationDate="2014-08-19T14:49:30.277" UserId="2972" />
  <row Id="2106" PostId="995" Score="0" Text="What about trees? They really don't seem linear to me." CreationDate="2014-08-19T14:52:54.113" UserId="1241" />
  <row Id="2107" PostId="995" Score="0" Text="Random forests can approximate a linear boundary pretty well. But there are no details in your question regarding characteristics of the data, model parameters, or model performance such that anyone can give a definitive answer as to why they all perform similarly." CreationDate="2014-08-19T15:10:19.713" UserId="964" />
  <row Id="2108" PostId="995" Score="0" Text="With regard to oberservations-to-variables ratio, consider a 2-variable problem where `x * y &gt; 0` is classified as `True` and `x * y &lt;= 0` is `False`. Even as sample size goes to infinity, a logistic regression model will, on average, have no better than accuracy of 0.5, whereas a 2x2x2 neural network will have accuracy approaching 1.0. One can easily construct such examples where sufficiently high ratio of sample size to number of variables will not guarantee similar performance of classifiers." CreationDate="2014-08-19T15:19:36.383" UserId="964" />
  <row Id="2109" PostId="942" Score="0" Text="Unfortunately no, but if I ever do that review I will try to be comprehensive. I'd hardly call Cox regression &quot;obscure,&quot; at least in my field." CreationDate="2014-08-19T16:31:00.380" UserId="1156" />
  <row Id="2110" PostId="995" Score="0" Text="bogatron- could you elaborate? I'm not quite sure what you're asking for." CreationDate="2014-08-19T18:37:12.850" UserId="1241" />
  <row Id="2111" PostId="999" Score="0" Text="I'll ask my boss if I can get the company to pay for it." CreationDate="2014-08-19T18:39:02.327" UserId="1241" />
  <row Id="2112" PostId="699" Score="0" Text="On a related note, I found [this relevant paper](http://research.microsoft.com/en-us/um/people/sdumais/ecir07-metzlerdumaismeek-final.pdf)." CreationDate="2014-08-19T19:11:38.010" UserId="1097" />
  <row Id="2113" PostId="995" Score="0" Text="It *may* be that for your data set, each of the models can accurately approximate the optimal decision boundary (see answer by @StasK), though I wouldn't say that for sure without looking at the model parameters and doing a comparative error analysis. My main point was simply that a high sample-to-variable ratio does not guarantee similar performance for a given set of classifiers because they each have different biases/limitations on the kind of solutions they can produce." CreationDate="2014-08-19T19:17:08.837" UserId="964" />
  <row Id="2114" PostId="991" Score="0" Text="Thank you for your answer! I have two follow-up questions :) 1) How are SVM (with a linear kernel) and Naive Bayes different in that they do not sum up their features and corresponding weights (i.e. what you call an &quot;additive model&quot;)? Both effectively create a separating hyperplane so isn't the result is always some kind of adding features multiplied by corresponding weights? 2) I'd like to try random forests, but unfortunately the feature space is too large to represent it in dense format (I'm using sklearn). Is there an implementation that can handle that?" CreationDate="2014-08-19T22:07:18.420" UserId="2979" />
  <row Id="2115" PostId="987" Score="1" Text="Some update: I was able to achieve acceptable results by l2-normalizing the additional dense vectors. I wrongfully assumed the sklearn StandardScaler would do that. I am still looking for more complex methods, though, that would allow me to model label dependencies or incorporate confidence of sub-classifiers." CreationDate="2014-08-19T22:11:42.240" UserId="2979" />
  <row Id="2116" PostId="961" Score="0" Text="@bayer: thanks, I'll try these parameters. I'm more interested, however, in _why_ this may happen. That is, I already can fix described effect by setting larger number of hidden units, but I'm curious why at all weights in random Markov field could tend to change synchronously." CreationDate="2014-08-19T23:37:53.633" UserId="1279" />
  <row Id="2117" PostId="993" Score="1" Text="Hm why is that? more observations seems to increase the chance that the decision boundary is more complex -- i.e. definitely not linear. And these models do different things in complex cases, and tend to do the same in simple ones." CreationDate="2014-08-20T09:53:57.613" UserId="21" />
  <row Id="2118" PostId="993" Score="0" Text="@SeanOwen: I think I'm not understanding your comment. What part of my answer does &quot;why is that&quot; refer to? The OP said nothing about using linear decision boundaries - after all, he might by transforming predictors in some way." CreationDate="2014-08-20T10:31:49.017" UserId="2853" />
  <row Id="2119" PostId="993" Score="0" Text="Why would more observations make different classifiers give more similar decisions? my intuition is the opposite. Yes, I'm not thinking of just linear decision boundaries. The more complex the optimal boundary the less likely they will all fit something similar to that boundary. And the boundary tends to be more complex with more observations." CreationDate="2014-08-20T10:51:28.617" UserId="21" />
  <row Id="2120" PostId="909" Score="0" Text="This is a great explanation, and is in fact the method that I ended up using. I like to think of this method as splitting the entire sample set into smaller sub-samples and using the means (average with CLT) of each sub-sample as the distribution of the data set. Thanks for the answer!" CreationDate="2014-08-20T11:00:40.643" UserId="2830" />
  <row Id="2121" PostId="999" Score="1" Text="ESL is 'free' as a pdf from their homepage...also worth downloading is ISL (by many of same authors) - more practical http://www-bcf.usc.edu/~gareth/ISL/" CreationDate="2014-08-20T12:02:47.173" UserId="1256" />
  <row Id="2122" PostId="961" Score="0" Text="If your learning rate is too high, the first sample (or the mean of the batch) will be what the RBM overfits to. If the &quot;neurons&quot; (i.e. p(h|v)) then saturate, learning stalls--the gradients of these neurons will be close to zero. This is one way of this happening." CreationDate="2014-08-20T15:11:44.423" UserId="1193" />
  <row Id="2123" PostId="991" Score="0" Text="1) In linear regression you are interested in points _on_ hyperplane, thus you add up weighted features to get predicted point. In SVM, on other hand, you are looking for points _on the sides_ of hyperplane. You do classification by simple checking on which side is your example, no summation involved during prediction. Naive Bayes may incorporate different kinds of models (e.g. binomial or multinomial), but basically you multiply probabilities, not add them." CreationDate="2014-08-20T15:33:08.117" UserId="1279" />
  <row Id="2124" PostId="991" Score="0" Text="2) I have seen some research in this topic, but never encountered implementation (probably googling will give some links here). However, you can always go another way - reduce dimensionality with, say, PCA and then run random forest based on reduced dataset." CreationDate="2014-08-20T15:35:33.363" UserId="1279" />
  <row Id="2125" PostId="1007" Score="3" Text="Why not point us to the page that you are trying to scrape? The &quot;best tool&quot; is hard to define otherwise." CreationDate="2014-08-20T15:55:06.153" UserId="471" />
  <row Id="2126" PostId="977" Score="0" Text="Is the problem that you are trying to maintain the identity of the clusters as much as possible at each time step? So that at N+1 you can say how a cluster has changed because there is some relation between clusters at N and those at N+1? And the tricky bit is what happens if clusters split and merge?" CreationDate="2014-08-20T16:01:32.980" UserId="471" />
  <row Id="2127" PostId="977" Score="0" Text="@Spacedman: BINGO :) http://www.joyofdata.de/blog/reasonable-inheritance-of-cluster-identities-in-repetitive-clustering/" CreationDate="2014-08-20T16:31:32.067" UserId="725" />
  <row Id="2128" PostId="1006" Score="1" Text="+1 for all the links too! I see my road is long!" CreationDate="2014-08-20T19:10:45.143" UserId="2861" />
  <row Id="2133" PostId="858" Score="0" Text="Hey, fist I want to thank you for your answer. I have one more question. Word vector that are returned from word2vec algorithm have float values, so words like big and bigger will have vectors that are close in vector space, but the values of vectors could be completely different. For example big = [0.1, 0.2, 0,3] and bigger = [0.11, 0.21, 0.31]. Isn't that a problem for CRF algorithm, because this algorithm would treat them as not simillar? Is there any addional processing that sould be done before using this word vectors in CRF? I hope my question is clear enough." CreationDate="2014-08-21T08:10:01.407" UserId="2750" />
  <row Id="2134" PostId="961" Score="0" Text="@bayer: now it makes sense to me, thanks!" CreationDate="2014-08-21T11:00:44.460" UserId="1279" />
  <row Id="2136" PostId="1017" Score="0" Text="This is a quite open-ended question. Can you be more specific about what problem you are considering, how RL applies, what you have done so far and what your specific confusion is?" CreationDate="2014-08-21T11:53:25.470" UserId="21" />
  <row Id="2138" PostId="1010" Score="0" Text="Some sites create their HTML from Javascript and then you need a Javascript-powered scraper (Selenium). Or you reverse engineer their JSON API and get the data directly..." CreationDate="2014-08-21T15:20:35.523" UserId="471" />
  <row Id="2139" PostId="1010" Score="0" Text="some sites do, but the majority do not. As OP didn't specify, one assumes it's a 'normal' site." CreationDate="2014-08-21T15:22:06.923" UserId="2861" />
  <row Id="2140" PostId="1007" Score="2" Text="On StackExchange sites, when you say &quot;doesn't work&quot; or &quot;not satisfied&quot;, you really *have* to say what that means. Was it too slow? expensive? feature missing? Otherwise, how do you expect a suggestion for something &quot;better&quot;?" CreationDate="2014-08-21T15:49:30.967" UserId="21" />
  <row Id="2145" PostId="1017" Score="0" Text="Please ask just one question per post, unless the questions are closely related.  Your questions are quite different." CreationDate="2014-08-22T15:25:42.997" UserId="609" />
  <row Id="2146" PostId="811" Score="0" Text="How would a balanced model compare to a representative model using weighted observations to mimic a balanced model?" CreationDate="2014-08-22T22:25:21.753" UserId="1241" />
  <row Id="2147" PostId="1022" Score="0" Text="This is more of a comment than answer." CreationDate="2014-08-23T09:22:35.743" UserId="21" />
  <row Id="2148" PostId="1021" Score="2" Text="PS I think the term you're looking for is &quot;canonical&quot;" CreationDate="2014-08-23T09:23:00.063" UserId="21" />
  <row Id="2149" PostId="1028" Score="2" Text="All algorithms will overfit to some degree. It's not about picking something that doesn't overfit, it's about carefully considering the amount of overfitting and the form of the problem you're solving to maximize more relevant metrics." CreationDate="2014-08-23T18:16:45.767" UserId="548" />
  <row Id="2150" PostId="1030" Score="0" Text="The reason I'm using regression is that I need the per year rate of change for reasons I'd rather not get into right now." CreationDate="2014-08-23T20:24:15.167" UserId="1241" />
  <row Id="2151" PostId="1030" Score="1" Text="@AndyBlankertz: I just updated my answer." CreationDate="2014-08-23T20:33:27.433" UserId="2452" />
  <row Id="2152" PostId="1030" Score="0" Text="Thanks. I'd love to delve into the resources, but I'm time limited- the report I'm working on is due on Friday. I also have some slack in statistical rigorousness, because the target audience is Management :) Hopefully next week." CreationDate="2014-08-23T20:44:59.527" UserId="1241" />
  <row Id="2153" PostId="1030" Score="1" Text="@AndyBlankertz: You're welcome. I understand, as I'm not a statistician myself :-). But I'm trying to learn wherever and whenever I can." CreationDate="2014-08-23T20:54:16.963" UserId="2452" />
  <row Id="2154" PostId="1029" Score="4" Text="This is probably not a great question for StackExchange. It's going to be mostly opinions and speculation." CreationDate="2014-08-24T09:24:29.747" UserId="21" />
  <row Id="2155" PostId="1022" Score="0" Text="Will move it to comments ASAP." CreationDate="2014-08-24T12:59:11.480" UserId="941" />
  <row Id="2156" PostId="1020" Score="0" Text="AFAIK you have to buy that data. The FB api is more for interacting with the platform not for retrieving personal info of users that use the service." CreationDate="2014-08-24T16:49:27.613" UserId="92" />
  <row Id="2157" PostId="1021" Score="0" Text="Is the &quot;most probable&quot;/&quot;most consensual&quot; string you are looking to idendify a regular expression?  Or one of the strings on the list?" CreationDate="2014-08-24T21:47:38.487" UserId="609" />
  <row Id="2158" PostId="1021" Score="0" Text="@MrMeritology I am not looking for a regular expression. I have shown a regular expression in my question just to illustrate how flexible I am in the kind of strings I would consider to be correct." CreationDate="2014-08-25T08:07:58.927" UserId="3047" />
  <row Id="2159" PostId="1037" Score="0" Text="Thanks!  I thought of #1, but maybe that there was a highly visited site that might be doing it.  And you're right about #2- heuristic optimization is not guaranteed to come up with a interpretable solution." CreationDate="2014-08-25T16:32:56.073" UserId="375" />
  <row Id="2160" PostId="1021" Score="0" Text="OK.  Then the answer I gave below should work for you." CreationDate="2014-08-25T18:53:19.470" UserId="609" />
  <row Id="2161" PostId="1030" Score="0" Text="This isn't time series analysis (unless you throw away loads of data). I think the data records are individual sales records with a year attached as a covariate. Time series analysis is used when the variable of interest (eg *total sales*) has a unique time point. You could compute total sales within years and do time series analysis, but that would mean losing all the other information from each sales record (eg item purchased, buyer age etc). Regression is the right thing here." CreationDate="2014-08-26T08:51:42.180" UserId="471" />
  <row Id="2162" PostId="1030" Score="0" Text="@Spacedman: The term I've emphasized in my answer is **time series regression**. Thus, in my view, it could be considered as a **special case** of either of the two approaches, depending on the **perspective**." CreationDate="2014-08-26T09:12:48.210" UserId="2452" />
  <row Id="2163" PostId="1030" Score="0" Text="All I'm saying is that individual sales records data are not time series data. So you can't treat them like time series data. So reading about fitting AR(1) models and time series regression approaches is a waste of the OP's time here when all they have to do is convert year to numeric and run the model again. My concern now is wondering exactly what the OP means by &quot;per-year rate of change&quot;, which may imply something more than a linear term in year is required (some kind of smoother or polynomial term perhaps)." CreationDate="2014-08-26T09:40:26.557" UserId="471" />
  <row Id="2164" PostId="1030" Score="0" Text="@Spacedman: I see. Thank you for the clarification. However, my initial impression was that for this particular task, the OP is only interested in future values of a **single aggregate** *outcome variable* (keeping the model's **full information** for *regression analysis*). That would be the case for *time series forecasting*, wouldn't it? Perhaps, I misunderstood the question." CreationDate="2014-08-26T10:01:11.867" UserId="2452" />
  <row Id="2165" PostId="1044" Score="5" Text="Asking for tool recommendations is usually considered off topic for StackExchange. I think it could be a better question if you could narrow down requirements and ask for how to accomplish this in the few tools you are considering using." CreationDate="2014-08-26T15:56:43.573" UserId="21" />
  <row Id="2166" PostId="1041" Score="0" Text="Hmm... it never occurred to be to make year a continuous variable. In retrospect it seems obvious." CreationDate="2014-08-26T18:51:31.157" UserId="1241" />
  <row Id="2167" PostId="729" Score="0" Text="Hi nfmcclure, I've applied your suggestion and updated the post. Please provide your comments." CreationDate="2014-08-27T07:16:43.810" UserId="870" />
  <row Id="2168" PostId="1045" Score="2" Text="Use SparkSQL, Shark is deprecated." CreationDate="2014-08-27T09:21:59.780" UserId="2668" />
  <row Id="2169" PostId="1044" Score="2" Text="The answer to your question is &quot;all of them&quot;. Depending on where the real detailed challenges to your problem are, one language might give you better support for what you want to do. But to answer that, the question needs a *lot* more detail." CreationDate="2014-08-27T10:29:04.177" UserId="836" />
  <row Id="2170" PostId="671" Score="1" Text="You have the right idea, except when plotting it you should start where the series starts every reset.  For estimating where it will hit, say 120, see my first edit in my answer." CreationDate="2014-08-27T16:13:41.443" UserId="375" />
  <row Id="2171" PostId="1028" Score="1" Text="ISTR that Breiman had a proof based on the Law of Large Numbers. Has someone discovered a flaw in that proof?" CreationDate="2014-08-28T01:18:43.393" UserId="1241" />
  <row Id="2172" PostId="1044" Score="0" Text="I think you need to decide how good is good enough." CreationDate="2014-08-28T16:36:20.693" UserId="1241" />
  <row Id="2173" PostId="1046" Score="1" Text="I've had no problem finding word2vec implementations, but I have been unable to find a working recursive net to use." CreationDate="2014-08-28T16:57:14.207" UserId="684" />
  <row Id="2174" PostId="1045" Score="0" Text="@samthebeast can you provide a reference?" CreationDate="2014-08-28T17:53:20.473" UserId="403" />
  <row Id="2175" PostId="1045" Score="1" Text="I tried Shark a few weeks ago. I was able to get it running using their documentation and a lot of sweat. I ultimately gave up on it as I was trying to pull/push data from s3 and I couldn't find a way to do that. We since switched to Impala." CreationDate="2014-08-28T17:54:23.883" UserId="403" />
  <row Id="2176" PostId="985" Score="1" Text="I don't get the point of the question. What would prevent you from doing such a thing? The data science police? (I don't mean to be disrespectful, I simply don't understand the reason for the question)." CreationDate="2014-08-28T18:20:24.147" UserId="1281" />
  <row Id="2177" PostId="1045" Score="0" Text="I got it! I'll try to make some tests with SparkSQL to see if I feel confortable with that." CreationDate="2014-08-28T20:30:35.243" UserId="3050" />
  <row Id="2179" PostId="1058" Score="1" Text="Although this is about science, and data, I'm not sure the 'data science' StackExchange is the best place for this. Can you elaborate why this is of interest?" CreationDate="2014-08-29T06:00:38.573" UserId="21" />
  <row Id="2180" PostId="1045" Score="1" Text="@gallamine http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html" CreationDate="2014-08-29T22:58:59.587" UserId="21" />
  <row Id="2181" PostId="1045" Score="0" Text="This doesn't make a great question. I'd rephrase to ask about specific issues you have in mind." CreationDate="2014-08-29T22:59:40.270" UserId="21" />
  <row Id="2182" PostId="1060" Score="0" Text="Thx a lot. I've seen [Breeze can read a CSV](http://www.scalanlp.org/api/breeze/#breeze.io.CSVReader$) and can calculate several statistics like [mean and variance](http://www.scalanlp.org/api/breeze/index.html#breeze.linalg.meanAndVariance$). The quartiles are missing, but this may be the best in the state of the art for Scala, a bit far from R." CreationDate="2014-08-30T02:14:50.483" UserId="1281" />
  <row Id="2183" PostId="1060" Score="0" Text="@Trylks R is a DSL for statistics, so out of box it will have a lot of helper functions for every day work for statiticians, but its totally inappropriate for building a product, doing big data or writting complicated machine laerning algorithms in a scalable way.  ScaLa is a Scalable Language and is designed to be expanded to whatever people want.  You can do just about anything in Scala, but it wont all be out-of-box. The payoff is massive though, once u climb a little way up the learning curve one realizes how much more powerful it is." CreationDate="2014-08-30T05:42:33.703" UserId="2668" />
  <row Id="2184" PostId="1061" Score="1" Text="I think recommendations, and for web-services, are mostly off-topic, even though this does concern hosting data." CreationDate="2014-08-30T16:13:10.113" UserId="21" />
  <row Id="2186" PostId="92" Score="0" Text="@Anony-Mousse That's a good topic for meta. I would like the site to be what it can be. I think I've contributed for my part as a user and moderator. I'm not sure what you suggest should be done better." CreationDate="2014-08-30T18:44:11.267" UserId="21" />
  <row Id="2187" PostId="1061" Score="0" Text="I was marginally aware of that though I find it perhaps a tad perverse. Where does one draw the line between the relevance of finding a suitable function, programming pattern, library, framework, or an app/web-service? Any of these may serve to solve data science issues, albeit at different resolutions." CreationDate="2014-08-30T20:12:14.117" UserId="3133" />
  <row Id="2188" PostId="1038" Score="1" Text="This isn't a great fit for the Data Science site. I'd suggest posting this in serverfault.com perhaps." CreationDate="2014-08-30T22:55:15.140" UserId="21" />
  <row Id="2189" PostId="1045" Score="0" Text="This is my issue! I nerd a technology to stage my resulting data from spark as I asked before. Thanks folks! I'm getting great results from it!" CreationDate="2014-08-30T23:54:34.620" UserId="3050" />
  <row Id="2190" PostId="1063" Score="0" Text="Looks cool, lots of stuff and I had no idea about that library. It seems a bit confusing, though, there is a textwriter but nothing named &quot;reader&quot;. In particular, I don't see anything about quartiles. In this case the problem is that mixing libraries is probably not a good idea, they will have different data formats and I will need to move the data from one to another back and forth, or something even worse." CreationDate="2014-08-31T01:58:32.983" UserId="1281" />
  <row Id="2191" PostId="1060" Score="0" Text="I know, that's why I'm asking for Scala libraries :) Making my own library would not be wise if there were good libraries out there, which is usually the case. I found some libraries like Breeze (and Breeze-Viz, and Breeze-Bokeh), Saddle, Spire, Spark, Scalding, etc. The problem is that I'm quite uncertain about which one to choose, to use or expand, the nicest ones in philosophy seem to be dead, the most efficient ones seem to be useless, and in general I found none that would suit this simple use case :/" CreationDate="2014-08-31T02:07:13.160" UserId="1281" />
  <row Id="2192" PostId="1053" Score="0" Text="I found [a thread](https://groups.google.com/forum/#!topic/cascading-user/YlCKDmjlkP0) discussing something similar for cascading, mahout, hadoop and some other technologies. I have to check it into detail, but I can't now, so I'll simply leave it here..." CreationDate="2014-08-31T02:13:36.947" UserId="1281" />
  <row Id="2194" PostId="1058" Score="0" Text="Try Fermi Estimation: http://en.wikipedia.org/wiki/Fermi_problem" CreationDate="2014-09-01T08:30:00.547" UserId="471" />
  <row Id="2195" PostId="1063" Score="0" Text="CSV data is usually read using `FileBasedDatabaseConnection` and `NumberVectorLabelParser`. Quantiles can be determined using `QuickSelect`.&#xA;If you don't want to mix libraries, then don't *ask* about libraries..." CreationDate="2014-09-01T08:58:01.877" UserId="924" />
  <row Id="2196" PostId="1061" Score="1" Text="The &quot;best&quot; depends very much on your application and your data. Without you expanding on that, this Q will just become a list of &quot;Howabout datafoo.io?&quot;. Even with that expansion, its still probably too much opinion." CreationDate="2014-09-01T12:14:02.260" UserId="471" />
  <row Id="2197" PostId="1058" Score="4" Text="As soon as you wrote a scientific article on this, you would be wrong." CreationDate="2014-09-01T12:21:10.427" UserId="471" />
  <row Id="2198" PostId="1063" Score="0" Text="1. I would have never guessed those names, thank you. 2. Spark (MLib) uses Breeze over Hadoop clusters, the combination is very natural for those libraries, AFAIK, because they are quite agnostic in the information representation. When moving to higher level (aka less raw) libraries decisions will be made and that means more compatibility problems, specially when that means decisions about information representation." CreationDate="2014-09-01T17:37:10.143" UserId="1281" />
  <row Id="2199" PostId="1066" Score="0" Text="Hi, thanks for your answer. &#xA;&#xA;I'm looking for an estimation of the amount of scientific knowledge. After that I want to study the distribution between different fields. Maybe if I use a wordcount and I don't take care about the pictures the problem is more easy to resolve." CreationDate="2014-09-02T00:23:28.073" UserId="3128" />
  <row Id="2200" PostId="1058" Score="0" Text="Why would be an error? &#xA;&#xA;I thing that to resolve this problem I need data science tools. After know the total amount I want to extract patters of activity in each field for every year and study the dynamics of science and how this would be under the influence of historical events" CreationDate="2014-09-02T00:26:47.553" UserId="3128" />
  <row Id="2201" PostId="1072" Score="0" Text="I am interested in analytics. I know HANA has a lot of transnational functionality but I am more interested what can be done in terms of machine learning, custom python reducers and hive interface. For example I know HANA has an library of machine learning algorithms built right in. Does Exasol?" CreationDate="2014-09-02T14:28:16.337" UserId="2511" />
  <row Id="2202" PostId="1072" Score="0" Text="EXASOL has a framework called &quot;EXAPowerlytics&quot; which uses protobuf and ZeroMQ to connect with Hadoop (for example). It also allows the ability to write User Defined Functions in Java, Lua, Python and R - for example importing any R machine-learning libraries you need without limit and running them in-memory and in parallel (where the algorithm is parallelisable). Definitely recommend signing-up at [link](www.exasol.com/portal) to get some more information." CreationDate="2014-09-02T14:57:12.140" UserId="3181" />
  <row Id="2203" PostId="1058" Score="0" Text="I'm not confident about the feasibility of an attempt to solve this problem. You can certainly come up with a very rough estimate (and I see such numbers from time to time), but the accuracy is pretty low, considering the diversity of the outlets human knowledge can found at, as well as frequency of appearing of new research studies and even research repositories." CreationDate="2014-09-02T15:22:54.523" UserId="2452" />
  <row Id="2204" PostId="1071" Score="1" Text="The variety of perspectives for this particular topic is not obvious for me. I'd say that: Data = symbols (of an alphabet). Information = data + syntax. Knowledge = information + semantics." CreationDate="2014-09-02T16:27:09.260" UserId="1281" />
  <row Id="2205" PostId="866" Score="0" Text="This is only a little bit related, but our most recent data science challenge concerned predicting claims from other claims. http://www.cloudera.com/content/cloudera/en/training/certification/ccp-ds/challenge/challenge2.html  When the solution is released it may contain a few interesting ideas." CreationDate="2014-09-03T14:20:04.223" UserId="21" />
  <row Id="2207" PostId="1075" Score="1" Text="How much memory one dataset takes? I.e. what is approximate size of one row?" CreationDate="2014-09-04T18:34:29.907" UserId="1279" />
  <row Id="2209" PostId="1076" Score="0" Text="Thanks for your comments. I am very aware and use scikit and caret regularly but I am looking for more of a state of the art online learning package, and neither scikit or caret are these. I am looking for people who are actually using something and can give their experience with a given package." CreationDate="2014-09-05T13:23:49.067" UserId="802" />
  <row Id="2210" PostId="1077" Score="2" Text="Few corrections. Firstly, Hadoop is a common name for a set of tools, file system is called HDFS. Secondly, data may be processed in parallel without distributing it on HDFS, e.g. sending data tuples to Storm or even RabbitMQ with a number of workers on a consumer side will do the trick as well. Thirdly, in this specific case you need to distribute not just 2 separate datasets, but instead all pairs from both (90 B tuples). Spark already has `.cartesian()` method which does exactly this, but custom lazy generator + RabbitMQ will work fine too." CreationDate="2014-09-05T23:44:41.593" UserId="1279" />
  <row Id="2211" PostId="1075" Score="0" Text="The size of rows differs, but on average about 30-50. Data sets are about 100Mb each." CreationDate="2014-09-06T12:44:19.140" UserId="3203" />
  <row Id="2212" PostId="1077" Score="0" Text="Splitting up using a queue and calculating in a distributed fashion is an option. just assume this will come with a price tag of handling a lot of complexity thats been solved in other distributed products. Spark seems like a good candidate on paper." CreationDate="2014-09-06T12:46:38.340" UserId="3203" />
  <row Id="2214" PostId="1084" Score="0" Text="Original Poster used the word &quot;classify&quot; but &quot;cluster&quot; is a more accurate description of his problem because he has no a priori definitions of categories. Therefore, this is not necessarily a supervised learning problem." CreationDate="2014-09-08T02:28:56.183" UserId="609" />
  <row Id="2215" PostId="1084" Score="0" Text="@MrMeritology: hmm, from context I would say that author is just not sure about concrete classes he's going to use, but still wants classification, not clustering. Anyway, he's the only person who knows the truth :)" CreationDate="2014-09-08T05:31:22.507" UserId="1279" />
  <row Id="2216" PostId="1084" Score="0" Text="Maybe i was not clear at the point. The categories are going to be selected in advice, so it is rather classification than clustering problem. The idea of creating a complex features vector seems to be quite reasonable - especially, that there are some particular tags, that are most likely probably to quickly classify some of samples. I'm not sure if SVM are going to fit the problem, as I predict high nonlinearities, but decision trees and Bayes seem to be applicable. I'm starting also to think about application of a hybrid algorithm (SVM based decision trees)." CreationDate="2014-09-08T05:46:48.257" UserId="3215" />
  <row Id="2217" PostId="1084" Score="0" Text="@GrzegorzE. -- If your categories are defined in advance, then please list these three categories in your question.  In my opinion, you are too focused on ML algorithms and not enough on the nature of your problem and the nature of your data. For example, you predict &quot;nonlinearies&quot; in features for web sites of unknown structure. Why? Also, you are mixing tags with web page text with who-knows-what-else, and they have different semantic significance." CreationDate="2014-09-08T06:07:25.307" UserId="609" />
  <row Id="2218" PostId="1084" Score="0" Text="@GrzegorzE. -- I strongly suggest that your classification method should be primarily driven by the nature of your a priori categories and the nature of the data. There are an infinite number of ways to categorize arbitrary web sites into 3 categories. Each way of categorizing will suggest salient features in the data or salient patterns. There's no substitute for manual analysis of individual data elements (web pages) and their context." CreationDate="2014-09-08T06:17:58.650" UserId="609" />
  <row Id="2219" PostId="1084" Score="0" Text="@MrMeritology -- I wan to classify if the particular page is a personal page (or social profile page). As I am a newbie in data science, my control engineering background drives my modelling and algorithmic focus in some particular areas. Similarly, the nonlinearities in feature vectors space and selection of appropriate algorithms have background in control..." CreationDate="2014-09-08T06:42:40.300" UserId="3215" />
  <row Id="2220" PostId="1086" Score="0" Text="From this description, I'm not at all clear what the ranking is supposed to be. There is no such thing as ordering by &quot;unique content&quot;. Clarify please what the ordering is supposed to be determined by?" CreationDate="2014-09-08T09:15:13.517" UserId="21" />
  <row Id="2221" PostId="1075" Score="0" Text="Meta-comment: N^2 algorithms are always bad news at scale. I would in any event investigate algorithms that do not require all-pairs computation. Cartesian joins destroy data locality optimization." CreationDate="2014-09-08T11:51:40.437" UserId="21" />
  <row Id="2222" PostId="1084" Score="0" Text="@GrzegorzE. - I'm familiar with Control Engineering as I have a EE background, so I can understand how you'd quickly focus on algorithms, nonlinearity, etc. I renew my suggestion to focus first on problem definition, understanding the data, and feature selection. Your problem is essentially a Social Science problem, not just a &quot;data classification problem.&quot; Insightful selection of features is worth more than 50% improvement in algorithm effectiveness." CreationDate="2014-09-08T18:17:17.203" UserId="609" />
  <row Id="2223" PostId="1084" Score="0" Text="@MrMeritology - great thanks for suggestions. Since yesterday I'm building a feature vector, focusing on text corpus, tags and tag arguments. I'll drop a line, with project status when get first results." CreationDate="2014-09-09T05:32:08.690" UserId="3215" />
  <row Id="2224" PostId="1075" Score="0" Text="You haven't specified the constraintd..eg response time, throughput etc" CreationDate="2014-09-09T06:32:03.607" UserId="1256" />
  <row Id="2225" PostId="1092" Score="0" Text="I would be interested to know where this sits, too, as currently I feel obliged to learn Python, R and Octave, just so I have access for tools for a hobby (whilst I know Ruby for professional reasons). I don't know enough about it to suggest an answer, but have known about http://sciruby.com/ for a while. My gut feel is that it is not ready yet" CreationDate="2014-09-09T09:29:27.130" UserId="836" />
  <row Id="2226" PostId="1092" Score="0" Text="Yeah, we took a look at sciruby, and while it looks nice, it seems limited to providing some data structures and linear algebra operations. If someone were to build a unified ML library for Ruby it would probably be a great basis for that." CreationDate="2014-09-09T16:59:37.457" UserId="2487" />
  <row Id="2227" PostId="1073" Score="0" Text="Dumb question, but do you mean `online` as in &quot;non-batch mode&quot;, or as in &quot;processed in the cloud&quot;?" CreationDate="2014-09-09T17:14:02.150" UserId="3248" />
  <row Id="2228" PostId="1081" Score="0" Text="I'm not sure data is a &quot;thing&quot;. If it is a thing, it should exist somewhere. Data doesn't exist anywhere. Take your own examples: (1) ethnicity: belonging to a social group - this quality does not &quot;exist&quot;, but is assigned to an individual and a group of individuals. What counts as an ethnicity is a social construction and may differ between individuals, societies, and eras; (2) housing prices do not have existence either, but are a manifestation of perceived property/material/social value. But anyway, the point is taken that data is a construction that is used to make meaningful inferences." CreationDate="2014-09-10T05:56:32.147" UserId="3178" />
  <row Id="2229" PostId="1086" Score="0" Text="@SeanOwen All documents come under the same theme for instance Tissue engineering for Bonemarrow related cancer. Within this theme we know that most research article will definetly speak about genes (ABC) so this becomes the predominant theme within those topics. Most of these articles dwell within these same set of genes. But very few articles start speak about latent themes which involves  genes (ADF) and (XYZ). The current thought process is we want to rank the articles based on the  content (XYZ),(FDA),(ABC). This is what I meant by unique content." CreationDate="2014-09-10T10:49:37.477" UserId="3232" />
  <row Id="2230" PostId="1089" Score="0" Text="Thanks! this is usefull" CreationDate="2014-09-10T10:56:00.230" UserId="3232" />
  <row Id="2231" PostId="1087" Score="0" Text="Thanks. Im looking at weighted TF-IDF in cases were the key-words are already known . any suggestions for that ?" CreationDate="2014-09-10T11:00:57.753" UserId="3232" />
  <row Id="2232" PostId="1096" Score="0" Text="I did look at GATE, i was not successful. I am not familiar with python. Any other tool.. I know spotfire but its way too expensive.." CreationDate="2014-09-10T16:33:45.287" UserId="3244" />
  <row Id="2233" PostId="1098" Score="0" Text="Thanks Harjeet, SKlearn is simple but it does not solve my problem. SKlearn is more of analytical algorithm approach which R also provides but I am looking for something like we do in Google, put in n-words and it gives all suitable hits.." CreationDate="2014-09-10T16:43:49.457" UserId="3244" />
  <row Id="2234" PostId="1101" Score="2" Text="Gini coefficient is not Gini impurity. See the links in the question" CreationDate="2014-09-10T19:15:20.863" UserId="21" />
  <row Id="2235" PostId="1073" Score="0" Text="Hi Mike, I mean non-batch mode. So soon after a prediction is made we learn the true label and then use that in the training. So think in terms of predicting a stock price, in a few minutes we would learn the true value, and then use that value in our training." CreationDate="2014-09-11T13:16:04.657" UserId="802" />
  <row Id="2236" PostId="1101" Score="1" Text="Wikipedia ist not always a reliable source of information :-)" CreationDate="2014-09-11T13:40:48.833" UserId="979" />
  <row Id="2237" PostId="1096" Score="0" Text="Sorry, I don't know of any easy-to-use free tools that do exactly what you want to do." CreationDate="2014-09-11T13:56:59.770" UserId="819" />
  <row Id="2238" PostId="1087" Score="0" Text="What are you hoping to achieve by boosting the TF-IDF scores of known keywords?" CreationDate="2014-09-11T13:58:46.060" UserId="819" />
  <row Id="2239" PostId="1101" Score="2" Text="Sure. Go look it up somewhere else: http://mathworld.wolfram.com/GiniCoefficient.html What makes you think Gini coefficient = Gini impurity?" CreationDate="2014-09-11T14:03:08.870" UserId="21" />
  <row Id="2240" PostId="1101" Score="0" Text="Look it up: http://books.google.de/books?id=DQXhYAgXRpEC&amp;pg=PA373&amp;dq=gini+coefficient+classification&amp;hl=de&amp;sa=X&amp;ei=Q7sRVOKDOcvAPLDugbAP&amp;ved=0CCkQ6AEwAQ#v=onepage&amp;q=gini%20coefficient%20classification&amp;f=false" CreationDate="2014-09-11T15:10:51.260" UserId="979" />
  <row Id="2241" PostId="1101" Score="0" Text="By the way: I have never seen a publication that cites mathworld.wolrfram.com !" CreationDate="2014-09-11T15:12:19.297" UserId="979" />
  <row Id="2242" PostId="1101" Score="0" Text="I'm sure you can find people that use coefficient and impurity interchangeably within the field of ML. Gini coefficient is not misclassification error, outside of ML. It has a meaning, whose interpretation when brought back into ML is something else. If the question is, are they the same thing, then I can't see how the answer is &quot;yes&quot;. Search for &quot;gini coefficient&quot; and tell me those are all about misclassification error?" CreationDate="2014-09-11T16:01:22.573" UserId="21" />
  <row Id="2243" PostId="1104" Score="0" Text="I'm sure this is tagged incorrectly and not enough info.  But i dont know what i dont know at this point" CreationDate="2014-09-12T04:23:19.613" UserId="3279" />
  <row Id="2244" PostId="1085" Score="0" Text="I do not understand the down vote.  This is a perfectly reasonable answer.  There are platforms other than Hadoop for computational grid computing." CreationDate="2014-09-12T12:57:55.210" UserId="961" />
  <row Id="2245" PostId="1106" Score="0" Text="Huge topic, have a look at [Into to IR](http://nlp.stanford.edu/IR-book/), this walks you from basic first principles how to build what you are asking about. Something to lookup is [tf-idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf) then realise this doesn't solve everything and look at bayesian probability" CreationDate="2014-09-12T13:00:09.070" UserId="95" />
  <row Id="2246" PostId="1105" Score="0" Text="This looks more like a programming question or maybe a stats question. Its not a data science question. Try StackOverflow?" CreationDate="2014-09-12T13:10:54.800" UserId="471" />
  <row Id="2247" PostId="1105" Score="1" Text="I was not that clear about it either. I think it'll be quite hard for someone who's not into the topic to see the problem, but I may give it a try" CreationDate="2014-09-12T13:40:05.153" UserId="3283" />
  <row Id="2249" PostId="1100" Score="0" Text="Why aren't other classification methods appropriate here? Sounds like you are concerned with multiple cases leading to a reduced set of outputs. But this is basically every case of representation learning in classification. Neural nets will help you sort out what interaction effects are predictive when base features are not too predictive. Otherwise you could use other methods." CreationDate="2014-09-13T19:03:16.963" UserId="92" />
  <row Id="2250" PostId="1113" Score="0" Text="How do text responses look like? And what are you trying to achieve with the groping?" CreationDate="2014-09-13T21:19:46.030" UserId="1279" />
  <row Id="2251" PostId="1054" Score="0" Text="magicharp, thank you for your answer." CreationDate="2014-09-13T21:26:27.317" UserId="3068" />
  <row Id="2252" PostId="1113" Score="0" Text="The text responses are sentences or phrases of 50-200 characters. The goal is just to group the numeric variable (that is, create cut points) based on the text responses." CreationDate="2014-09-13T23:02:06.283" UserId="36" />
  <row Id="2253" PostId="1113" Score="1" Text="Then you can group them by first letter of texts. It will satisfy your current description, but most probably you want something different. So what you actually want to achieve? Do you want to split people in age categories based on words they use? Or you are looking for specific signs in responses? Even assuming you want clustering, there are literally dozens of features that may extracted from text and same number of distance measures to use. But in current wording I can't even be sure we are talking about clustering. So please provide *example* and/or *context* of what you want to achieve." CreationDate="2014-09-14T00:18:08.677" UserId="1279" />
  <row Id="2256" PostId="1105" Score="1" Text="That's why I suggested stats and programming. Those would be the people into the topic. All you've got here are data miners and hadoop word-counters." CreationDate="2014-09-14T08:52:44.690" UserId="471" />
  <row Id="2257" PostId="1100" Score="0" Text="I welcome any solution. Neural net was just the one I was attempting to utilize." CreationDate="2014-09-14T20:08:15.810" UserId="3263" />
  <row Id="2260" PostId="1106" Score="0" Text="Why do you consider &quot;repairs&quot; as relevant?" CreationDate="2014-09-15T11:13:30.170" UserId="1279" />
  <row Id="2261" PostId="1114" Score="0" Text="My understanding is that gradient boosting suffers from the same limitations as RF when dealing with imbalanced data: http://sci2s.ugr.es/keel/pdf/algorithm/articulo/2010-IEEE%20TSMCpartA-RUSBoost%20A%20Hybrid%20Approach%20to%20Alleviating%20Class%20Imbalance.pdf" CreationDate="2014-09-15T13:02:40.797" UserId="3294" />
  <row Id="2262" PostId="1114" Score="0" Text="Boosting is an additional step you take in building the forest that directly addresses imbalance. The paper you link notes this in the intro stating boosting helps even in cases where there is no imbalance. And that paper concludes boosting significantly helps. So not sure where the equivalence between RF and boosting is shown there?" CreationDate="2014-09-15T13:09:40.260" UserId="92" />
  <row Id="2263" PostId="1119" Score="0" Text="Not sure I understand the question completely, but features based on RFM type calculations are almost always some of the most powerful in a predictive model in the database marketing domain." CreationDate="2014-09-15T16:42:22.670" UserId="1138" />
  <row Id="2264" PostId="1119" Score="1" Text="Your question is too broad.  Prediction for what purpose? RFM models have been around for decades. Pre-internet, every direct marketing organization used RFM for promotional spending -- i.e. who to send catalogs and flyers to." CreationDate="2014-09-15T17:08:02.633" UserId="609" />
  <row Id="2265" PostId="1108" Score="1" Text="you might want to rephrase. the definition for kappa is easily available, but i believe - but don't know - that it has some limitations that are relevant here." CreationDate="2014-09-15T19:40:48.483" UserId="3294" />
  <row Id="2266" PostId="1119" Score="0" Text="@MrMeritology, thanks, I have made edits in original questions. I'd like to predict customers tendency to continue buying, spending more or the end of customer life cycle." CreationDate="2014-09-15T20:06:36.593" UserId="97" />
  <row Id="2267" PostId="1113" Score="0" Text="What do you mean by &quot;linguistic similarity&quot;?  Grammar/syntax? reading comprehension level? semantics (meaning)?" CreationDate="2014-09-15T22:25:56.510" UserId="609" />
  <row Id="2268" PostId="1106" Score="0" Text="Was just an example. Didn't think too much about it :). I guess it is not relevant?" CreationDate="2014-09-16T08:06:47.127" UserId="3284" />
  <row Id="2269" PostId="1126" Score="0" Text="I have read something about ensemble learning (see scientific paper http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1105916&amp;tag=1). However, it is not clear to me how to apply them. `SVM-text` is returning a probability of belonging to class `C`, and the same holds for `SVM-image`. Is it sufficient to average them? You talk about weighted average: how to select weights?" CreationDate="2014-09-16T13:48:52.677" UserId="3321" />
  <row Id="2270" PostId="1103" Score="0" Text="I think this would be more helpful to recommend particular models or tools in answer to this question." CreationDate="2014-09-16T15:54:26.090" UserId="21" />
  <row Id="2271" PostId="1104" Score="1" Text="Can you give more detail on the data? it's not clear what pipeline means here, what type of data is in question, or what the budget concern is." CreationDate="2014-09-16T15:55:55.150" UserId="21" />
  <row Id="2272" PostId="1128" Score="1" Text="sounds more like something for stats or even maths stack exchange sites..." CreationDate="2014-09-16T16:35:52.827" UserId="471" />
  <row Id="2273" PostId="1126" Score="0" Text="There's huge number of ways to incorporate weights. In simplest case of binary classification you can use equal weights for both answers (from image- and text-based classifiers) and thus simply average probabilities. For multinomial classification you can treat probabilities themselves as weights and compute probability for each class separately. Or you can use adaptive weights like in AdaBoost. Or anything else. Just start reading about weighted ensembles and choose algorithm that in your opinion is the most appropriate for the task." CreationDate="2014-09-16T20:04:51.663" UserId="1279" />
  <row Id="2274" PostId="1128" Score="0" Text="Thanks for those suggestions. I am considering a similar question for the maths site... but hope to discover insights from data-science experts first. The question straddles practical data science and theoretical maths." CreationDate="2014-09-16T22:42:54.807" UserId="3328" />
  <row Id="2275" PostId="1113" Score="0" Text="@ffriend: It's a general methodological question, so I'll pick one at random: splitting the age categories based on the words respondents use." CreationDate="2014-09-17T00:58:38.523" UserId="36" />
  <row Id="2276" PostId="1108" Score="0" Text="http://stats.stackexchange.com/questions/19601/adjusting-kappa-inter-rater-agreement-for-prevalence" CreationDate="2014-09-17T02:16:14.377" UserId="3294" />
  <row Id="2277" PostId="1132" Score="3" Text="A general request for help and advice is not appropriate for StackExchange. You need to ask a more narrow and focused question that can be answered with specific responses." CreationDate="2014-09-17T06:43:45.697" UserId="609" />
  <row Id="2279" PostId="1106" Score="0" Text="@Hendrik: please, use @&lt;username&gt; to address user - SE didn't notify me about your comment. Counting relevancy is the key point in search engines (though normally you compute how relevant is web page to a search query, you need it, right?). Do I understand it right that you just want to know how to compute relevancy of document to a search query when there are similar, but not exact words (e.g. &quot;bike&quot; and, say, &quot;cyclist&quot;)?" CreationDate="2014-09-17T08:12:55.133" UserId="1279" />
  <row Id="2280" PostId="1131" Score="0" Text="Thank you for this interesting answer - it isn't what I anticipated.  Time/frequency domain (Fourier/Wavelet) transforms  operate on individual time-series, a very restrictive algebra.  &quot;Dynamic Systems&quot; - on the other hand - focus on establishing models for time series - as opposed to establishing algebras ranging over time series.  I hoped to discover sets of operators ranging over time-series that are 'somehow adequate'." CreationDate="2014-09-17T09:59:14.570" UserId="3328" />
  <row Id="2281" PostId="1132" Score="2" Text="Some people are rather vocal in their conviction that k-means is not appropriate for categorical data, e.g., [here](http://stats.stackexchange.com/questions/115573/what-are-clustering-techniques-for-this-case/) or [here](http://stats.stackexchange.com/questions/49887/appropriate-cluster-method) or [here](http://stats.stackexchange.com/questions/31308/distance-function-for-categories-in-k-means) or [here](http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust)." CreationDate="2014-09-17T12:27:02.470" UserId="2853" />
  <row Id="2282" PostId="1131" Score="0" Text="Then I'm not sure what you mean by &quot;algebra over time-domain time series&quot;. FWIW, you can operate algebraically in frequency domain to express relations over many time-series equivalents. A simple example: formulas for musical scales, incl. harmonics. Also, I don't know why you'd want to restrict yourself to time domain, given that frequency domain contains all the same information." CreationDate="2014-09-17T12:34:54.860" UserId="609" />
  <row Id="2283" PostId="1087" Score="0" Text="Exploring weighted TD-IDF  to introduce conscious bias of sorts." CreationDate="2014-09-17T13:30:28.420" UserId="3232" />
  <row Id="2284" PostId="1131" Score="0" Text="I didn't intend to 'knock' exploiting the frequency domain... an approach that often yields seemingly 'magical' results. This question is focused exclusively on functions transforming time-domain time-series to time-domain time-series. I've added some details about abstract algebras (and a &quot;Boolean Algebra&quot; analogy) which, I hope, makes my question clearer. I am not excluding the possibility that operations could be defined using the Fourier transform... for example... but I don't want frequency domain representations as &quot;outputs&quot; from the operations which make up the algebra." CreationDate="2014-09-17T13:32:40.867" UserId="3328" />
  <row Id="2285" PostId="1135" Score="1" Text="*Please* don't [cross-post](http://stats.stackexchange.com/questions/115775/statistical-commute-analysis-in-java). I'll flag your post on CrossValidated for closure, even though it already has an answer, since your question is really more appropriate here." CreationDate="2014-09-17T14:19:10.317" UserId="2853" />
  <row Id="2286" PostId="1103" Score="0" Text="I haven't seen any publicly-available models for Stanford NER, other than those distributed by the Stanford NLP Group itself. These include multiple versions for English (including case-insensitive models), as well as models for German, Spanish, and Chinese.&#xA;&#xA;http://nlp.stanford.edu/software/CRF-NER.shtml#Models&#xA;&#xA;However, these models were trained mostly on annotated news articles, which may not work well on other kinds of text data. Here's how you would train your own models for Stanford NER:&#xA;&#xA;http://nlp.stanford.edu/software/crf-faq.shtml#a" CreationDate="2014-09-18T13:31:50.873" UserId="819" />
  <row Id="2287" PostId="1103" Score="0" Text="Also, other NER tools offer languages for other models. For example, Apache OpenNLP offers a model trained on Dutch: http://opennlp.sourceforge.net/models-1.5/ But again, in the long run, the way to get the best performance on your own data is to train custom models on your own data. It takes a TON of work to acquire the training data and painstakingly annotate it by hand, but it *might* be worth it in the end. Once you have the annotated training data, most NER tools have methods for training new custom models on that data." CreationDate="2014-09-18T13:39:42.647" UserId="819" />
  <row Id="2288" PostId="1087" Score="0" Text="If you have a controlled vocabulary of terms you care about, you could boost the score of those specific items." CreationDate="2014-09-18T13:40:31.580" UserId="819" />
  <row Id="2289" PostId="1124" Score="0" Text="Why not test out both approaches on a small sample set and see if there's any difference?" CreationDate="2014-09-18T13:47:50.750" UserId="819" />
  <row Id="2290" PostId="794" Score="0" Text="As madison may, mentioned its all about separating configuration and code. It would be fine if you were running one network and knew all the parameters, but you don't. by splitting config and code, you can run multiple networks - different hidden neurons etc, etc. and source control is straight forward ( how do you keep track of which configuration you have tried if you keep it in the code)." CreationDate="2014-09-18T23:57:27.757" UserId="1256" />
  <row Id="2291" PostId="694" Score="0" Text="If you want to only use Restricted Boltzmann Machine, you can stick with scikit-learn as well." CreationDate="2014-09-17T11:42:42.543" UserId="3342" />
  <row Id="2292" PostId="1101" Score="1" Text="I think we are talking about decision trees. So we are in the field of machine learning! Please read the question more carefully" CreationDate="2014-09-19T12:38:02.420" UserId="979" />
  <row Id="2293" PostId="1144" Score="0" Text="Surely will read it! Really liked the last line...i think the urge and sometimes the pressure to get the results ASAP often leads to such Parodies.&#xA;&#xA;And its equally important to avoid the opposite of this wherein one goes so deep in the learning that it becomes useless for the real world issues.&#xA;&#xA;while growing/learning sometimes its more important to know what NOT to do, thanks a lot for the guidance hope to see more such insights that would enlighten me and others on a similar Journey." CreationDate="2014-09-19T12:15:34.950" UserDisplayName="Vinay Tiwari" />
  <row Id="2294" PostId="1144" Score="0" Text="&quot;what affects the convergence rate of the Fisher scoring algorithm in GLM&quot;- I guess you lost 99% of the Data Scientists here." CreationDate="2014-09-19T12:23:22.917" UserDisplayName="Momo" />
  <row Id="2295" PostId="1145" Score="0" Text="Would appreciate if you could suggest some Algorithms that one should not MISS ON, or better to say are the most useful for solving practical business issues. If possible please mention the best ways to learn them (particular books,self help articles or may be trial and error)" CreationDate="2014-09-19T13:18:33.680" UserDisplayName="Vinay Tiwari" />
  <row Id="2296" PostId="1145" Score="2" Text="I would say pretty much all the algos in ISL: linear regression, logistic regression, tree based methods, SVM; Clustering and dimension reduction eg PCA.  Go through the book and look at the corresponding online course ( http://online.stanford.edu/course/statistical-learning-winter-2014 - maybe on youtube?)." CreationDate="2014-09-19T14:26:34.747" UserId="1256" />
  <row Id="2297" PostId="1135" Score="0" Text="you have no knowledge of machine learning or stats methods, yet any answer to this question is going to involve lots of both. That makes this question off-topic as &quot;too broad&quot;, since its just going to have to tell you all about ML and/or stats. You can't just plug your numbers into a magic box. The one existent &quot;answer&quot; posted already is clearly not an answer, its just chatter." CreationDate="2014-09-19T14:33:47.160" UserId="471" />
  <row Id="2298" PostId="1144" Score="0" Text="@Momo: Well, &quot;data scientist&quot; is one of those ill-starred terms that has barely gained currency before starting to be devalued." CreationDate="2014-09-19T14:40:39.097" UserId="3361" />
  <row Id="2299" PostId="1145" Score="0" Text="Great Resource,good to have a book and videos on the same by the Authors themselves.Thanks a lot for the link,wasn't aware of this." CreationDate="2014-09-19T15:03:13.393" UserId="3360" />
  <row Id="2301" PostId="1135" Score="0" Text="@Spacedman: My response provides the theoretical information you need to solve this problem. To recap: once you have estimated the commute time function, you need but solve a bivariate constrained optimization problem. I made some simplifying assumptions in order to keep it simple. If you see a mistake feel free to leave a comment or edit my answer." CreationDate="2014-09-19T20:30:42.970" UserId="381" />
  <row Id="2302" PostId="1101" Score="0" Text="No need to guess. Did you click the links? Do you see that the gini coefficient in question is not the thing you keep talking about?" CreationDate="2014-09-20T13:21:32.087" UserId="21" />
  <row Id="2303" PostId="1141" Score="0" Text="A major point of clarification is whether you want to be in Academia or Industry. Both have radically different flavors about what is used, how it's used, and end goals." CreationDate="2014-09-22T00:14:16.310" UserId="3378" />
  <row Id="2304" PostId="1150" Score="0" Text="Thank you very much, I'll definitely have a look at it for educational purposes. I fortunately found a ready-made implementation for my system which works for me." CreationDate="2014-09-23T06:37:22.327" UserId="3283" />
  <row Id="2305" PostId="1152" Score="0" Text="Actually I would expect the size of the vocabulary to increase greatly when considering URLs, because the tweets vocabulary is quite limited. Regarding the second misunderstanding, one of the ideas was to simply append the features for URLs to the features for the tweet. But, if a tweet does not have URLs, I will have to put some &quot;impossible&quot; values in the URL features (like -1), because the lack of a certain feature (-1) must be different from the absence of a word (0 if considering term frequencies). This would not happen if considering your initial proposed approach to have one bag of words" CreationDate="2014-09-23T14:09:41.417" UserId="3054" />
  <row Id="2306" PostId="1152" Score="0" Text="I am not sure about your initial approach to have one big bag of words though, because, apart from the eventual different vocabulary between tweets and webpages, this would put on the same level the words from the tweet and the words from the URL, when actually they could have different importance in the classification" CreationDate="2014-09-23T14:15:03.557" UserId="3054" />
  <row Id="2307" PostId="1138" Score="1" Text="I was just trying to figure out if my reasoning was correct, so I will keep using Random Forests, thanks for your help!" CreationDate="2014-09-23T14:32:41.853" UserId="3054" />
  <row Id="2308" PostId="1103" Score="0" Text="Thanks @CharlieGreenbacker - what if I'm looking for models to use in the corporate environment? Meaning, if people talk about their work, teams, etc. Are there existing work around this domain?" CreationDate="2014-09-23T17:47:20.853" UserId="2785" />
  <row Id="2309" PostId="1151" Score="4" Text="I can suggest to try the subject in a simple way, with a little maths, and a little programming (both of which you will need to pursue the subject deeper), look into this Coursera course: https://www.coursera.org/course/ml - the next session also starts quite soon" CreationDate="2014-09-23T20:45:53.967" UserId="836" />
  <row Id="2310" PostId="1151" Score="0" Text="+1 to Neil Slater for the Coursera course. It was a great intro to Machine Learning for me, and would give anyone a good taste of what to study next." CreationDate="2014-09-24T14:50:10.907" UserId="3409" />
  <row Id="2312" PostId="1159" Score="0" Text="Hi, and welcome to DS! Perhaps you could elaborate a bit on your dataset. How many rows and columns do you have? This could have an impact on possible solutions." CreationDate="2014-09-25T11:30:31.503" UserId="2853" />
  <row Id="2313" PostId="1159" Score="0" Text="23711341 rows, and 8 columns. I could try to remove 1-2 columns. They does not seem to related to my problem." CreationDate="2014-09-25T13:16:53.580" UserId="3167" />
  <row Id="2315" PostId="1165" Score="1" Text="This is a very broad question. Please try to narrow it down. What are you interested in?" CreationDate="2014-09-25T21:08:43.703" UserId="21" />
  <row Id="2318" PostId="1159" Score="0" Text="You should sample rows before columns here. Is there a reason you cant randomly sample rows to reduce data size? I'm assuming rows here are related to users or something" CreationDate="2014-09-26T03:15:54.100" UserId="92" />
  <row Id="2319" PostId="1021" Score="0" Text="Would this come under NER (named entity recognition)?" CreationDate="2014-09-26T03:30:30.803" UserId="2474" />
  <row Id="2320" PostId="1163" Score="1" Text="+1 for *you do not want to apply svd to only 8 columns: you apply it when you have a lot of columns.*" CreationDate="2014-09-26T06:47:34.770" UserId="2853" />
  <row Id="2321" PostId="1166" Score="0" Text="Matlab is not the only good tool for maths, there are also Octave, R, Python/NumPy, SciLab, etc. Same applies to NLP - there are NLTK, Standford NLP, GATE and numerous others. Often it's a question of personal preference, so no single good answer can be given to your question. In general, try to avoid questions starting with &quot;what is the best ...&quot; - they are primary opinion-based and are forbidden on most SE sites." CreationDate="2014-09-26T07:34:34.757" UserId="1279" />
  <row Id="2322" PostId="1166" Score="0" Text="@ffriend Thank you I changed the title of question and detailed my question a bit" CreationDate="2014-09-26T07:49:44.047" UserId="3436" />
  <row Id="2323" PostId="1166" Score="2" Text="Then I would suggest trying out [NLTK](http://www.nltk.org/). It's pretty simple, and Python's REPL allows to do quick experiments and immediately see the result." CreationDate="2014-09-26T08:02:07.830" UserId="1279" />
  <row Id="2324" PostId="1101" Score="0" Text="It was not a guess ;-) I am pretty sure about it." CreationDate="2014-09-26T08:54:21.940" UserId="979" />
  <row Id="2325" PostId="1101" Score="0" Text="this is not constructive at this point. I am happy to let anyone read the question and links, my answer, and your comments, and decide what the word means." CreationDate="2014-09-26T08:57:40.643" UserId="21" />
  <row Id="2326" PostId="1159" Score="0" Text="Sorry if I did not made myself clear. My goal is to do PCA. I think SVD on sample data cannot help me to do PCA, right?" CreationDate="2014-09-26T12:36:08.193" UserId="3167" />
  <row Id="2327" PostId="1169" Score="1" Text="Overall, I like your answer but the opening sentence is not quite right. PCA isn't suited for many dimensions with low variance; rather, it is suited for many dimensions with *correlated* variance. For a given data set, the variance could be high in *all* dimensions but as long as there is high covariance, then PCA can still yield significant dimensionality reduction." CreationDate="2014-09-26T15:14:47.117" UserId="964" />
  <row Id="2328" PostId="1154" Score="0" Text="I can understand what's going on by what you have already. Are the pipes unidirectional or bidirectional?" CreationDate="2014-09-26T15:54:18.047" UserId="1241" />
  <row Id="2329" PostId="1151" Score="1" Text="Questions about how to become a data scientist [are off-topic here](http://meta.datascience.stackexchange.com/q/41/322)." CreationDate="2014-09-26T16:25:47.577" UserId="322" />
  <row Id="2330" PostId="1169" Score="0" Text="@bogatron: good catch, thanks. In fact, I was referring to high/low variance in _some_ dimensions, possibly not original ones. E.g. in [this picture](http://en.wikipedia.org/wiki/File:GaussianScatterPCA.png) these dimensions are defined by 2 arrows, not original x/y axes. PCA seeks to find these new axes and sorts them by the value of variance along each axis. Anyway, as you pointed out, it was a bad wording, so I tried to reformulate my idea. Hopefully, now it's more clear." CreationDate="2014-09-26T18:20:18.717" UserId="1279" />
  <row Id="2331" PostId="1169" Score="0" Text="That makes sense to me. +1." CreationDate="2014-09-26T19:13:37.270" UserId="964" />
  <row Id="2332" PostId="1166" Score="0" Text="@ffriend thank you very much, I am going to try it, just a question are these toolkit general purpose? In fact my target language is Persian" CreationDate="2014-09-26T19:30:03.517" UserId="3436" />
  <row Id="2333" PostId="1168" Score="0" Text="Thank you, I think I will use NLTK. The paper was good but not an excellent comparison" CreationDate="2014-09-26T19:43:51.783" UserId="3436" />
  <row Id="2334" PostId="1166" Score="0" Text="Short answer: no, but it is solvable. Natural languages differ in both - words and rules. You can't expect English lemmatizer to correctly find root of a French word. You also can't expect standard whitespace-based tokenizer to split German adjetcive/noun pairs that are written without whitespaces. But most NLP libraries are flexible enough to allow you extend them for your specific purposes. Sometimes it is done via custom modules, sometimes - via training data, and sometimes there's already ready-to-use library for you (e.g. see [Hazm](https://github.com/sobhe/hazm) for Persian support)." CreationDate="2014-09-26T20:27:59.073" UserId="1279" />
  <row Id="2335" PostId="1165" Score="1" Text="Hello Sean, thanks for your answer. I have just edited my question and narrow it down a bit. I specified that I am interested in the area of Education with the new revoluions like MOOCs as an example..." CreationDate="2014-09-27T16:58:50.050" UserId="3433" />
  <row Id="2336" PostId="1165" Score="0" Text="One of the most promising tools for machine learning with big data is [MLlib](https://spark.apache.org/docs/latest/index.html) from Apache Spark project. There's already a number of methods implemented, but much more is to be written yet. If you are also interested in education, take some algorithmic problem from it (e.g. predicting student's score or course efficiency) and design solution for that problem. Having told that, I should warn you that for using big data solutions your data should really be pretty large, and I'm not sure MOOCs will give that much data." CreationDate="2014-09-27T22:10:12.960" UserId="1279" />
  <row Id="2337" PostId="1106" Score="0" Text="I guess you are asking for two things. One is a rather well understood search problem. You can just use Solr or Elasticsearch to do the heavy lifting for you. They both can find relevant documnents in a collection by weighting hits. However if you want some deeper semantic understanding of the text (i.e. &quot;repair&quot; is not mentioned but a typical activity in a bike shop) then the pure search engine might fall flat." CreationDate="2014-09-28T11:28:48.223" UserId="3445" />
  <row Id="2338" PostId="1165" Score="0" Text="I would like to thank you first for your detailed answer. Yes you are right, the availability of the so much large data is very crucial. I will try to think about that and get back to you :) Thanks for your proposition of the subject :)" CreationDate="2014-09-28T14:59:34.200" UserId="3433" />
  <row Id="2339" PostId="1159" Score="0" Text="PCA is usually implemented by computing SVD on the covariance matrix. Computing the covariance matrix is an embarrassingly parallel task, so it should scale easily with the number of records." CreationDate="2014-09-29T00:18:59.397" UserId="924" />
  <row Id="2340" PostId="1154" Score="0" Text="Pipes are unidirectional. In my case there is a only one big receiver tank and many smaller ones that are pumped (one at a time) to the pipelines, so the liquid flows to the big tank." CreationDate="2014-09-29T03:07:22.030" UserId="3400" />
  <row Id="2341" PostId="413" Score="1" Text="I agree mostly, with a precision: Feature selection needs not be done by hand, it can be automatic. See for instance the Lasso method (http://en.wikipedia.org/wiki/Least_squares#Lasso_method)." CreationDate="2014-09-29T09:00:26.110" UserId="3410" />
  <row Id="2342" PostId="1103" Score="0" Text="@UzumakiNaruto The same goes here for really any domain, regardless of topic, etc. -- collect a LOT of relevant real-world data and annotate it manually... this will be the training data you use to train new models.&#xA;&#xA;Also, you might want to look into the work of Giuseppe Carenini at the Univ of British Columbia -- he's done some research applying NLP, etc. to things like business meeting notes: http://www.cs.ubc.ca/~carenini/" CreationDate="2014-09-29T17:12:25.793" UserId="819" />
  <row Id="2343" PostId="1152" Score="0" Text="@markusian re: &quot;the tweets vocabulary is quite limited&quot; -- that's not what I've seen in the real world. What kind of Twitter corpus are you dealing with? re: &quot;simply append the features for URLs to the features for the tweet&quot; What kind of features are you talking about? Something other than bag-of-words? re: &quot;this would put on the same level the words from the tweet and the words from the URL&quot; So just boost the score of the words in the tweets." CreationDate="2014-09-29T17:19:05.177" UserId="819" />
  <row Id="2344" PostId="876" Score="0" Text="I've realized that my data set is um... interesting. &quot;Interesting&quot; in that out of 10 explanatory variables, only one is actually independent." CreationDate="2014-09-30T00:05:12.950" UserId="1241" />
  <row Id="2345" PostId="1173" Score="1" Text="I'm not sure I'm understanding what you're looking for, because it seems like a simple log of what and when is all you need." CreationDate="2014-09-30T04:13:10.900" UserId="1241" />
  <row Id="2346" PostId="1180" Score="1" Text="Sorry, I'm still learning the correct terminology. I'm trying to determine similarity, and believe the Tanimoto  and Pearson coefficients and Euclidean distance calculation can be used interchangeably to do this (source: Programming Collective Intelligence)." CreationDate="2014-09-30T08:32:20.053" UserId="3459" />
  <row Id="2347" PostId="1180" Score="1" Text="Similarity is large, but distance is small, when things are &quot;alike&quot;. They're opposite in that sense, but, you could create some similarity metric out of distance, yes." CreationDate="2014-09-30T09:26:11.523" UserId="21" />
  <row Id="2348" PostId="1182" Score="0" Text="Thanks was looking for ideas for the data structures. Will check out the lib what they use." CreationDate="2014-09-30T15:21:02.593" UserId="3445" />
  <row Id="2349" PostId="1173" Score="0" Text="@AndyBlankertz in that case I was looking for in-memory, but sure I can push the events to a logfile or redis or similiar. Then I still need datastructure(s) for the efficient calculations. I would like to combine the typical values in a way I dont have to have buckets for all seperate." CreationDate="2014-09-30T15:22:49.710" UserId="3445" />
  <row Id="2350" PostId="1183" Score="0" Text="Hi Catalin, and welcome to DS! As it stands, your question is hard to answer. What does &quot;not similar&quot; mean? Are you looking to detect structural changes, like shifting levels? Outliers? Changes in seasonality? What is the &quot;data&quot; (any number, or counts, lots of zeros or few)? Please consider editing your question." CreationDate="2014-10-01T09:50:04.953" UserId="2853" />
  <row Id="2351" PostId="1186" Score="0" Text="You can classify all keywords beforehand and then just pull category from the index." CreationDate="2014-10-01T13:31:28.367" UserId="1279" />
  <row Id="2352" PostId="1186" Score="0" Text="@ffriend seems like an answer for one word query. But if search query is consist more words .. or combinations of words .. i have to create index for all combinations!!!" CreationDate="2014-10-01T13:35:16.603" UserId="3498" />
  <row Id="2353" PostId="1186" Score="1" Text="SVC is fast, so if you want to use it for query classification in a moderate-load application, it will work. But classification by a single (or even several words) is a bad idea in most cases. Take ambiguous words, for example: what if some word belongs to 2 categories with very little difference in probabilities? Are you going to throw just a little bit less probable category out of search? What you most probably want is an additional term in ranking formula while searching, not rejecting less probable categories at all." CreationDate="2014-10-01T13:56:54.813" UserId="1279" />
  <row Id="2354" PostId="1181" Score="2" Text="re: &quot;Is there a better way of using LDA to detect topics in text, there are so provide better results?&quot; &#xA;&#xA;Could you clarify your question a bit? What do you mean by &quot;better?&quot;" CreationDate="2014-10-01T14:33:18.317" UserId="819" />
  <row Id="2355" PostId="1174" Score="1" Text="Can you repost the important details in this question? e.g. what's the data look like, where'd it come from, is it labeled, etc." CreationDate="2014-10-01T14:43:41.097" UserId="403" />
  <row Id="2356" PostId="1183" Score="0" Text="i've updated the question" CreationDate="2014-10-01T15:30:48.633" UserId="3482" />
  <row Id="2357" PostId="1175" Score="0" Text="Thank you for this response. I am still a tad confused: 1) About consecutive updates - why is this better than seeing the same observation the same number of times, but out of order? and 2) I dont get the phrase used regarding &quot;for all importance weights of h, the update is equivalent to two updates with importance weight h=2&quot;. It seems like what is important is that the process is equivalent to h updates with weight =1." CreationDate="2014-10-01T17:24:15.557" UserId="1138" />
  <row Id="2358" PostId="1175" Score="0" Text="consecutive updates- its not that its better ( it isn't), but that it doesn't fit in the online &quot;memory-less&quot; setting of vowpal-wabbit ( in order to reorder the data you need to store the data). second is a typo? In the abstract it says:  &quot;that updating twice with importance&#xA;weight h is equivalent to updating once with&#xA;importance weight 2h&quot;" CreationDate="2014-10-01T17:34:35.127" UserId="1256" />
  <row Id="2359" PostId="1175" Score="0" Text="Sorry, I read your response: &quot;The idea being that presenting it consecutively reduces the problem because the error gradient... &quot; as saying that if you were relying on adding a record with weight n, n times (with weight 1), ignoring the computational burden, it was better to present the record to the algorithm n-times consecutively. So, this was the &quot;goal&quot; VW is trying to achieve, but with something equivalent that only requires a record to be present 1 times in the data." CreationDate="2014-10-01T18:22:29.540" UserId="1138" />
  <row Id="2360" PostId="1175" Score="0" Text="Regarding the typo, YES, seems I should have typed &quot;...h/2&quot;. I changed the question. This is stated in this fashion the way you added in the comment and as the question now states, later in the paper. Even with this new wording, I fail to see what the point it...likely obvious, I am just not latching onto it yet." CreationDate="2014-10-01T18:26:02.160" UserId="1138" />
  <row Id="2361" PostId="1175" Score="0" Text="So which bit? a) can't present the data out of order in online setting ( and computational load). b) why n consecutive updates is better than 1 single update of weight n: gradient descent - &quot;overshoot&quot;, c) why their proposed single update is better than n consecutive updates - computational load d) why importance weights are useful in a statistical learning system" CreationDate="2014-10-01T18:48:33.877" UserId="1256" />
  <row Id="2362" PostId="1175" Score="0" Text="I think I understand b) in that a huge weight multiplied against the gradient will really cause a large update and it will act as an outlier for tuning a learning rate. I think I understand c) in that having to repeat the update for a single observation with weight n, n-times will of course cause more computations. I know d) well - at least in the case of batch learning, where it is for example useful for unbalanced classes. I don't know of an example in true online learning (my VW experience is with a fixed offline data set that is fed into VW because it doesn't all fit in memory)." CreationDate="2014-10-01T19:05:33.057" UserId="1138" />
  <row Id="2363" PostId="1175" Score="0" Text="So, my confusion is for a) as well as the initial question about the invariance property - what it means and why it matters." CreationDate="2014-10-01T19:06:36.847" UserId="1138" />
  <row Id="2364" PostId="1175" Score="0" Text="Does invariance here simply mean that a single update (as they propose) achieves the same thing as seeing the record multiple times? Is that all it is saying?" CreationDate="2014-10-01T19:08:29.410" UserId="1138" />
  <row Id="2365" PostId="1175" Score="0" Text="a) VW basically streams the data- so how do you present the samples out of order, that requires you to store the data. And yes - thats all they mean by invariance." CreationDate="2014-10-01T20:15:48.957" UserId="1256" />
  <row Id="2366" PostId="1175" Score="0" Text="a) I thought that was what you were saying - that if you were presenting an observation multiple times (ignoring the computational cost) it was better that it be consecutive, as that would result in less instability of a large weight!" CreationDate="2014-10-01T20:21:47.670" UserId="1138" />
  <row Id="2367" PostId="1175" Score="0" Text="Oh geez if that is all it means, then that was pretty straightforward - I guess my typo did not help matters for my confusion :) thanks!" CreationDate="2014-10-01T20:22:23.357" UserId="1138" />
  <row Id="2368" PostId="1191" Score="0" Text="Do you use every userid and every itemid as a separate dimension?" CreationDate="2014-10-02T06:42:22.137" UserId="1279" />
  <row Id="2369" PostId="1191" Score="0" Text="When comparing items the number of dimensions is the number of users. Think of items as vectors of 1's and 0's where each dimension corresponds to some user and the value is 1 if the user accessed the item and 0 otherwise." CreationDate="2014-10-02T13:14:43.533" UserId="3506" />
  <row Id="2370" PostId="1194" Score="0" Text="I have tried varying the rank for the approximation. I tried ranks 40, 100, 300, 1000. I expect that with the rank 40 it would classify items into 40 available dimensions. If that classification into 40 classes is good, I would have no issue with that. However, it seems that some dimensions were simply not being used. A lot of dissimilar items ended up in the same dimension." CreationDate="2014-10-02T13:29:39.620" UserId="3506" />
  <row Id="2371" PostId="1191" Score="0" Text="The real question here is: does standard PCA work well with binary variables? And I'd say that the answer is &quot;no&quot;. Binary variables are not continuous (you can't get .73 in your user-item matrix), but instead categorical (only 0s and 1s are allowed, &quot;yes&quot;-s and &quot;no&quot;-s and nothing in between). IIRC, [MCA](http://en.wikipedia.org/wiki/Multiple_correspondence_analysis) is a standard analogue of PCA for categorical data. Though, my personal approach would be to use [RBMs](http://en.wikipedia.org/wiki/Restricted_Boltzmann_machine), which can also handle non-linearities." CreationDate="2014-10-02T21:14:39.957" UserId="1279" />
  <row Id="2372" PostId="1196" Score="0" Text="Simple way to deal with missing data is to use population's average for this variable. Intuition here is that if we don't know anything about it, then we would want to use something &quot;neutral&quot; - not better and not worse than an average. From statistical point of view, we may say that _if that value existed_, its prior would most probably be distributed normally with mean in population's expectation." CreationDate="2014-10-02T22:40:43.443" UserId="1279" />
  <row Id="2373" PostId="1191" Score="0" Text="It is possible that there are algorithms more suited to categorical data than SVD. However, SVD is often cited as the tool for dimensional reduction in the context of latent semantic analysis. See the response by buruzaemon, for example. In that case the SVD is applied to the term incidence matrix which is also made of 1's and 0's. Also some form of matrix factorization (may be SVD with some regularization) was successfully used in the Netflix competition to predict user ratings for movies (the ratings are integers 1-5). Therefore, I don't think it is easy to write SVD off in discrete case." CreationDate="2014-10-03T00:55:33.500" UserId="3506" />
  <row Id="2374" PostId="1191" Score="0" Text="Re using RBMs: Aren't RBMs for classification (supervised learning)? I am interested in clustering (unsupervised learning). Anyway RBM's are probably off limits for me because RBMs are not implemented in Mahout and I cannot use anything licensed by GPL for legal reasons." CreationDate="2014-10-03T00:59:01.023" UserId="3506" />
  <row Id="2376" PostId="1191" Score="0" Text="1. In LSA, SVD is not used for dimension reduction, but instead for finding semantic concepts. If you want dimension reduction, use PCA (or MCA). 2. For continuous variables it makes sense to talk about real values (e.g. .73) and order of values, for categorical - no. For Netflix it makes sense to say &quot;user likes X as much as 4.5&quot; and &quot;mark 4.5 is higher than 3.88&quot;. For values &quot;man&quot; and &quot;woman&quot; (even represented as 1 and 0) there's nothing between them (e.g. what would mean value .73?) and no specific order (&quot;man&quot; &gt; &quot;woman&quot; or vice versa?). Decide for yourself which case is yours." CreationDate="2014-10-03T15:40:06.363" UserId="1279" />
  <row Id="2377" PostId="1191" Score="0" Text="3. RBM may be used for data compression/dimension reduction, see relevant sections of my [earlier answer](http://stats.stackexchange.com/questions/114385/what-is-the-difference-between-convolutional-neural-networks-restricted-boltzma/117188#117188)" CreationDate="2014-10-03T15:44:04.323" UserId="1279" />
  <row Id="2381" PostId="1197" Score="0" Text="n-gram models are often the way to go when considering word order. see http://en.wikipedia.org/wiki/N-gram" CreationDate="2014-10-03T19:25:00.910" UserId="2969" />
  <row Id="2382" PostId="1199" Score="0" Text="Thanks for the input. This means greatly expanding the number of rhs variables, since you now need two variables for each one that has missing values. Also, I'd think you have to be careful about making 0 the default value for lack of information. But this is a good starting point, and I'll think more about it." CreationDate="2014-10-03T20:11:13.263" UserId="3510" />
  <row Id="2383" PostId="1205" Score="0" Text="This is fairly broad, and generally questions asking for recommended tools are off topic on StackExchange. I'd suggest stating more about the nature of your requirements and asking specific questions about tools if possible, to get more specific and useful responses." CreationDate="2014-10-04T10:15:00.067" UserId="21" />
  <row Id="2384" PostId="1189" Score="0" Text="The question here is quite unclear. What are you trying to do, build a confusion matrix? what have you tried? is there more to your goal?" CreationDate="2014-10-04T10:16:11.317" UserId="21" />
  <row Id="2386" PostId="1151" Score="0" Text="AirThomas, I had a similar question, is there another place where I can ask it where it wouldn't be off topic?" CreationDate="2014-10-05T00:28:53.647" UserId="3535" />
  <row Id="2387" PostId="1208" Score="0" Text="Do the test set predictions for the two classifiers correspond exactly? i.e. is every instance classified in the same way?" CreationDate="2014-10-05T01:30:30.080" UserId="51" />
  <row Id="2388" PostId="1208" Score="0" Text="yes the methods are exactly like each other. the only difference is from the feature type" CreationDate="2014-10-05T05:47:30.430" UserId="3530" />
  <row Id="2389" PostId="1214" Score="1" Text="I wanted to add the tag best-practices but I cannot since I don't have 150 reputation points. To be honest, I don't understand how a new comer can effectively be a contributor to the site with all such rules. I see a lot of questions for which I know the answers, but I can't answer or even up vote the answer if it is already there." CreationDate="2014-10-05T06:28:39.150" UserId="3540" />
  <row Id="2390" PostId="1213" Score="1" Text="Does it have to be a supervised approach? A convolutional denoising autoencoder seems like exactly what you're looking for, but it's unsupervised." CreationDate="2014-10-05T12:18:10.090" UserId="2969" />
  <row Id="2391" PostId="1215" Score="0" Text="The first line of your answer means to me you prefer windows but in the rest you count advantages of Linux, I didn't get Windows or Linux?" CreationDate="2014-10-05T12:40:26.250" UserId="3436" />
  <row Id="2392" PostId="1215" Score="0" Text="Linux. I prefer to use windows, but it's the wrong way to go for this problem. Edited the answer for clarity." CreationDate="2014-10-05T14:52:57.497" UserId="2969" />
  <row Id="2393" PostId="1191" Score="0" Text="If every item is assigned some semantic concepts, then that can be used for clustering. I believe that my case is analogous to the latent semantic analysis and the netflix data set because in all examples in the absence of data the default value in the matrix is zero. For netlix: zero doesn't mean that the user gave that movie a zero rating, it means that he didn't give any rating. Therefore netflix rating is also a categorical value. In fact I would experiment with the netflix dataset the same way by mapping all ratings to 1 and mapping absence of a rating to zero." CreationDate="2014-10-05T15:31:11.517" UserId="3506" />
  <row Id="2394" PostId="1191" Score="0" Text="Re RBMs: This is interesting. Thank you for sharing." CreationDate="2014-10-05T15:32:31.903" UserId="3506" />
  <row Id="2395" PostId="1214" Score="0" Text="I think this is more of a generic programming question, so StackOverflow might be better. You can include a snippet of what you are trying to do, and why it's slow, and ask for suggested optimizations." CreationDate="2014-10-05T16:46:40.337" UserId="21" />
  <row Id="2396" PostId="1210" Score="0" Text="I think this answer has a kernel of value but could probably benefit from a more specific example, and some editing." CreationDate="2014-10-05T16:48:11.980" UserId="21" />
  <row Id="2397" PostId="1215" Score="0" Text="@brentlace Thank you so much, I modified my question and asked a recommendation for a programming language. which you recommend?" CreationDate="2014-10-05T19:11:49.257" UserId="3436" />
  <row Id="2398" PostId="1215" Score="0" Text="I tend to do most if my work in python, but i don't actually do much nlp these days. My suggestion would be to use whatever you are comfortable with. Python would be a good place to start. Python also tends to play well with other languages, so if you find that it missing key capabilities, you should be able to integrate them." CreationDate="2014-10-05T19:14:44.210" UserId="2969" />
  <row Id="2399" PostId="1191" Score="0" Text="In Netflix 0 is just an encoding for NA (not available) values. It's not a continuous or categorical variable, it's just missing datum, and should be treated separately. E.g. you want your model to predict values from 1 to 5, but not zero - it would be nonsense (how even would you interpret it? predicting that user haven't given a rating?). So your case is _somewhat_ similar to LSA, but definitely not Netflix dataset." CreationDate="2014-10-05T22:05:34.757" UserId="1279" />
  <row Id="2400" PostId="1208" Score="0" Text="Not the methods, I meant the classified labels. Do they correspond exactly or is it just the aggregate percentages that match?" CreationDate="2014-10-06T04:06:11.803" UserId="51" />
  <row Id="2401" PostId="1076" Score="0" Text="@mike1886: if you already know how to update your model using new data, what else do you want? What is &quot;more of state of the art&quot; in your understanding? Just more recent and sounding cooler?" CreationDate="2014-10-06T05:56:47.770" UserId="1279" />
  <row Id="2403" PostId="1216" Score="2" Text="I think this might be a bit broad for StackExchange, and perhaps considered off-topic if it concerns career advice, but see what others think." CreationDate="2014-10-06T15:12:23.767" UserId="21" />
  <row Id="2407" PostId="1221" Score="0" Text="I do agree with using Linux through SSH, this way I can download any package and there is no cost for my Internet (In my country internet is not much cheap) but that way could I use something like PyCharm? or do I miss some GUI which is important for NLP tasks?" CreationDate="2014-10-06T20:01:40.433" UserId="3436" />
  <row Id="2408" PostId="1221" Score="0" Text="If you SSH into a Linux machine, you will have no GUI. However, you can use any GUI tools in Windows and simply transfer your files to you Linux machine via SSH. In fact, you can use PyCharm in Windows and configure it to transfer your files and execute your code on your Linux machine." CreationDate="2014-10-06T20:06:36.073" UserId="3466" />
  <row Id="2409" PostId="1221" Score="1" Text="You can dual boot and run both on the same machine; it's easy." CreationDate="2014-10-06T21:16:09.877" UserId="381" />
  <row Id="2410" PostId="1208" Score="0" Text="Aaah. I misunderstood, sorry. but it makes no difference, they are the same" CreationDate="2014-10-06T21:21:56.003" UserId="3530" />
  <row Id="2411" PostId="1191" Score="0" Text="I agree it isn't very appropriate to mix NAs with ratings. However, the data set is sparse--no user rates all movies. If we want to apply SVD to the dataset and exclude NA values, we would be applying it to an empty set or we will be narrowing the dataset down extremely to a subset of users and a subset of movies, so that each movie is rated by each user. So NAs have to be assigned some values to perform SVD at all. In this paper (http://research.cs.queensu.ca/TechReports/Reports/2006-527.pdf) the authors treated NAs as no-opinion with score 3 (see page 11). Sounds like it worked for them." CreationDate="2014-10-07T00:10:46.893" UserId="3506" />
  <row Id="2413" PostId="1208" Score="0" Text="I asked a similar question not too long ago: http://datascience.stackexchange.com/questions/992/why-might-several-types-of-models-give-almost-identical-results" CreationDate="2014-10-07T01:49:29.913" UserId="1241" />
  <row Id="2415" PostId="1191" Score="0" Text="Often people use mean of all ratings for specific film. E.g. if movie got ratings (2, 4, 2) from 3 users, all other user ratings (which are NAs in the dataset) are assigned (2+4+2)/3 = 2.66." CreationDate="2014-10-07T06:21:13.437" UserId="1279" />
  <row Id="2418" PostId="1216" Score="6" Text="Don't forget that the people you are comparing yourself against are those that have the knowledge to have well-read blogs, have high stack exchange reps, etc, ie, not a representative sample. You are comparing yourself with the best, not the average. If you are a smart IT guy and you want it badly enough, it is there for the taking. Data is growing exponentially, our ability to analyse and manage it, possibly more slowly. So, there are plenty of opportunities, just grab the bull by the horns." CreationDate="2014-10-07T19:36:54.663" UserId="3571" />
  <row Id="2419" PostId="1191" Score="0" Text="If a user bothered to rate the item, then that means that the user is somewhat interested in this item. I assume that users don't access items at random, but do so according to their personal tastes. Missing data is interpreted as negative feedback. Why would it be more appropriate to give mean rating for all users? The mean would be an estimator only conditionally that the users provide some rating, because this characterizes the sample space. It seems that the mean is appropriate for different purposes than mine, like modeling the rating function with the assumption that a rating is given." CreationDate="2014-10-07T23:28:25.813" UserId="3506" />
  <row Id="2420" PostId="1208" Score="0" Text="thanks @AndyBlankertz that helped a lot." CreationDate="2014-10-08T08:29:15.980" UserId="3530" />
  <row Id="2421" PostId="1191" Score="0" Text="It's up to your assumptions. There are ways to assess performance of both models, so you can always check what assumption was closer to the truth." CreationDate="2014-10-08T08:37:50.453" UserId="1279" />
  <row Id="2423" PostId="1213" Score="2" Text="Supervised is the wrong term here, I believe. The OP essentialy wants to denoise, which is what you typically do with unsupervised methods." CreationDate="2014-10-08T19:32:07.663" UserId="1193" />
  <row Id="2424" PostId="1228" Score="1" Text="Although this might answer the question, link-only answers are highly discouraged. Links may change, thus invalidating references. Try to make your answer self-contained, by adding further information to it." CreationDate="2014-10-09T02:30:56.957" UserId="84" />
  <row Id="2425" PostId="1214" Score="0" Text="Actually I think http://opendata.stackexchange.com/ would be a better fit." CreationDate="2014-10-09T02:36:56.137" UserId="381" />
  <row Id="2426" PostId="1230" Score="0" Text="Thanks, this was really helpful" CreationDate="2014-10-09T05:25:03.783" UserId="3587" />
  <row Id="2427" PostId="1231" Score="0" Text="Wonderful answer. Thanks." CreationDate="2014-10-09T05:25:36.827" UserId="3587" />
  <row Id="2428" PostId="1231" Score="0" Text="Do you see any problems with using the simple difference averaging approach I wrote above? It seems like an ok stand in for a similarity measure in a K-NN approach and it's just as lightweight as Euclidean Distance." CreationDate="2014-10-09T05:36:42.697" UserId="3587" />
  <row Id="2429" PostId="1231" Score="0" Text="No, it actually has a name: [Manhattan similarity](http://en.wikipedia.org/wiki/Taxicab_geometry)." CreationDate="2014-10-09T05:45:32.473" UserId="381" />
  <row Id="2430" PostId="1220" Score="0" Text="I mentioned about my being weak in mathematics, during school days. I have started liking mathematics ever since I have seen its real use in solving real life problems :).  So, you can suggest me ways to study mathematics. I like your answer." CreationDate="2014-10-09T10:38:08.233" UserId="3550" />
  <row Id="2431" PostId="1220" Score="0" Text="I always like to learn about the software problem I'm trying to solve, then learn the mathematics needed to solve the problem. However, it is possible that you won't be able to just pick up the new math and use it right away, depending on your skill level. Be honest with yourself and choose a software problem that has math you think you could pick up. Work on it daily, as part of your portfolio. Broaden your math knowledge with online courses if you find engaging software problems with math you don't understand. The key thing is habit - make time to study or code every day." CreationDate="2014-10-09T16:59:49.030" UserId="3466" />
  <row Id="2436" PostId="1087" Score="0" Text="A quick update .. Ive implemented this logic and the results  have come out pretty well. Built on Python NLTK  and gensim. im exploring opportunities to use information gain to add in some measure of &quot;usefulness&quot; using information gain or something like that . LSI is also an option here." CreationDate="2014-10-09T21:36:42.363" UserId="3232" />
  <row Id="2438" PostId="1199" Score="0" Text="Well, in my particular case the number of new variables was not a problem, since I did not have so many numerical features. And regarding the value 0 for the missing values, it made sense in the specific context of my problem. But maybe these ideas may be adapted somehow to your particular problem." CreationDate="2014-10-10T08:04:03.297" UserId="2576" />
  <row Id="2441" PostId="1246" Score="2" Text="Split dataset into mini-batches and fit model to each mini-batch sequentially." CreationDate="2014-10-10T13:53:58.887" UserId="1279" />
  <row Id="2442" PostId="1246" Score="0" Text="Thank you @ffriend. However, that wouldn't be a pure on-line implementation." CreationDate="2014-10-10T14:05:36.313" UserId="2576" />
  <row Id="2443" PostId="1096" Score="0" Text="This is as easy as it is going to get, if you want to DIY without spending money." CreationDate="2014-10-10T16:22:59.290" UserId="381" />
  <row Id="2444" PostId="1234" Score="0" Text="I really like your engine, it gave me great recommendations for related topics. Well done." CreationDate="2014-10-10T17:21:18.920" UserId="3526" />
  <row Id="2445" PostId="1234" Score="0" Text="Thanks man!! Keep using." CreationDate="2014-10-10T17:28:17.107" UserId="3596" />
  <row Id="2446" PostId="1246" Score="1" Text="What's the reason to use &quot;pure online&quot; implementation if your dataset is fixed? SGD only says that you don't need to iterate the whole dataset at once, but can split it into an arbitrary number of pieces (mini-batches) and process them one by one. Mini-batch of size 1 only makes sense when you have continuous and possibly endless source of data (like twitter feed, for example) and want to update the model after each new observation. But that's very rare case and definitely not for fixed datasets." CreationDate="2014-10-10T18:35:17.400" UserId="1279" />
  <row Id="2447" PostId="1216" Score="1" Text="Every company is different I guess, but in my company we don't do any insane statistics/mathematics. There are a lot of common sense problem solving though. I personally wish that my computer science background was stronger. I'd rank the skills in order of value like this: 1) Common sense, 2) Computer Science / Programming 3) Mathematics / Statistics." CreationDate="2014-10-11T03:54:46.093" UserId="3070" />
  <row Id="2449" PostId="1236" Score="0" Text="I think this will be a much better question if you define polysemes, and state what you have done so far to answer your question. This doesn't seem to be about data science at the moment." CreationDate="2014-10-11T09:44:12.857" UserId="21" />
  <row Id="2452" PostId="1255" Score="0" Text="So one-hot-encoding is still used, just on hashed values *which as you say saves space and can cause dimensionality reduction (given collisions). Is that correct?" CreationDate="2014-10-12T00:08:00.330" UserId="1138" />
  <row Id="2453" PostId="1255" Score="1" Text="One Host Encoding isn't a required part of hashing features but is often used alongside since it helps a good bit with predictive power.  One way to think of one hot encoding is transforming a feature from a set of N discrete values into a set N binary questions.  Perhaps it's not important for me know if feature J is 2 or 3 only that it's not 4.  One Hot makes that distinction specific.  This helps a lot with linear models whereas ensemble approaches (like RF) will scan break points in the feature to find that distinction." CreationDate="2014-10-12T00:15:58.410" UserId="92" />
  <row Id="2454" PostId="405" Score="0" Text="Nice answer, though it fails to cover the OP's point 2, which is the Python 2 vs 3 issue." CreationDate="2014-10-12T03:24:36.873" UserId="3571" />
  <row Id="2455" PostId="2257" Score="1" Text="I think you should clarify your question. What are you trying to predict exactly, what are your constraints, what have you tried, what are your concerns about the approach you cite?" CreationDate="2014-10-12T10:19:04.723" UserId="21" />
  <row Id="2458" PostId="2257" Score="0" Text="I edited the problem, add some clarification. Thank you!" CreationDate="2014-10-12T21:55:40.043" UserId="4619" />
  <row Id="2459" PostId="1236" Score="0" Text="@SeanOwen its always better if you dont comment, if you have no idea on subject.do you know what is polysemes?" CreationDate="2014-10-13T08:34:26.787" UserId="3598" />
  <row Id="2460" PostId="1076" Score="0" Text="Agree---scikit's online learning modules are pretty 'state-of-the-art', whatever that means" CreationDate="2014-10-13T13:56:02.590" UserId="1399" />
  <row Id="2461" PostId="1236" Score="1" Text="of course I do. You misunderstand my suggestion. It's for the benefit of readers who might help you with a resource on &quot;words with multiple meanings&quot; but don't recognize the word 'polyseme'." CreationDate="2014-10-13T20:48:26.457" UserId="21" />
  <row Id="2462" PostId="1250" Score="0" Text="i would like to upvote but I am not allowed." CreationDate="2014-10-13T22:01:16.370" UserId="3540" />
  <row Id="2463" PostId="2266" Score="0" Text="Thanks, Ben. The quality I mentioned, is the quality, or true worth, of a product in an shopping website. Actually, I'm going to measure/inference on its ranking itself, but try to identity two main factors towards the sales of a product: 1.quality(it that product good enough?) 2.ranking in page(a bias contributes to sales, but doesn't reflect product's quality)" CreationDate="2014-10-14T09:22:31.183" UserId="1048" />
  <row Id="2464" PostId="2266" Score="0" Text="Sure---I think you can include whatever predictors are useful (and indeed doing so may improve your model's fit). The point is you should view it as a logistic regression problem and understand which parameters you're doing inference about" CreationDate="2014-10-14T15:42:12.880" UserId="1399" />
  <row Id="2465" PostId="2274" Score="0" Text="Hi ffriend, thanks for your detailed answer. I did not go into much details into the independent variable on purpose, because the focus of my question is the fact that I am using a *single* variable to predict another and I wanted to know the most suitable data mining techniques for this case. You confirmed my feeling that simple statistics (or not so simple) could be appropriated in this case, but the best is to try out things. Regarding the &quot;weather&quot; variable I was actually planning to use one metric and continuous variable such as visibility or rain, just to keep things simple." CreationDate="2014-10-15T08:42:21.740" UserId="3159" />
  <row Id="2466" PostId="2274" Score="0" Text="In fact, using 2 or more variables for linear regression is almost as simple as using only one, but can lead to a much more accurate predictions. [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) (free PDF is available) is a great introduction to linear regression, its use cases and estimation metrics. If you are using specialized software like R, modelling different dependencies is deadly simple, boiling down to only few lines of code." CreationDate="2014-10-15T09:05:25.753" UserId="1279" />
  <row Id="2467" PostId="2278" Score="0" Text="please reformulate your question title. read here http://stackoverflow.com/help/how-to-ask for how to ask a good question" CreationDate="2014-10-15T09:33:36.683" UserId="906" />
  <row Id="2468" PostId="2277" Score="0" Text="Thanks for your comments regarding the meaning of the weather variables; although that was not my original question, it was really helpfull for my problem." CreationDate="2014-10-15T13:52:19.253" UserId="3159" />
  <row Id="2471" PostId="2286" Score="0" Text="thanks for this. But already have completed anaphora resolution part. I already added it in question description" CreationDate="2014-10-16T09:25:30.337" UserId="4662" />
  <row Id="2472" PostId="2286" Score="0" Text="OvisAmmon: also `,` and `and` will not be sentence boundary in all case" CreationDate="2014-10-16T10:03:14.787" UserId="4662" />
  <row Id="2473" PostId="2288" Score="0" Text="That's what I had in mind . They are indeed dependent on the behavior of each other.The problem is that I read about logistic regression  and saw an example with iris dataset. It is kind of a classificiation and the x_prediction  , y_prediction  , z_prediction are going to be float numbers not a value like (type 1 , type 2 etc.) Is that a problem for my case? Thanks for yoru answer" CreationDate="2014-10-16T15:04:30.480" UserId="4668" />
  <row Id="2474" PostId="2288" Score="0" Text="[Logistic Regression](http://en.wikipedia.org/wiki/Logistic_regression) predicts a binary response from categorical variables (type1, type2, etc). If all your variables are continuous, you may be better off using a [linear regression](http://en.wikipedia.org/wiki/General_linear_model) model" CreationDate="2014-10-17T10:20:38.800" UserId="3159" />
  <row Id="2475" PostId="2293" Score="1" Text="This is too vague to be answered. Time series are too different to just throw k-means on them and get out anything useful. It *heavily* depends on your data." CreationDate="2014-10-17T12:14:04.003" UserId="924" />
  <row Id="2476" PostId="2286" Score="0" Text="I'm not sure, that your last comment makes grammatical sense. In both examples comma and conjunction `and` are exactly sentence boundary markers as they split a complex sentence into two simple ones. More of that, Stanford NLP assigns POS tag `CC` to the conjunction `and` (you can try it online @ their site). So even with anaphora present as a prerequisite, basic thing you need to learn how to do is simple sentence borders. After that your task is simplified to binary classification query/not_query." CreationDate="2014-10-17T13:24:31.757" UserId="2573" />
  <row Id="2477" PostId="2287" Score="0" Text="What is the relationship between the variables? Does the value they take depend on t or not? It sounds like a dynamic Bayes net, or MRF, might be neccesary but you need to provide more info" CreationDate="2014-10-17T13:38:58.833" UserId="1399" />
  <row Id="2478" PostId="2288" Score="0" Text="I see thanks for your help , I cannot upvote your answer cause of no reputation but this is a solid answer ty :)" CreationDate="2014-10-17T14:37:03.243" UserId="4668" />
  <row Id="2479" PostId="1128" Score="0" Text="This is a really interesting question and I agree it has practical applications, but I also really think it belongs on Mathematics. It should get a lot more exposure there." CreationDate="2014-10-17T17:00:43.200" UserId="1156" />
  <row Id="2480" PostId="2296" Score="0" Text="How does t-digest compare to the p-square algorithm?" CreationDate="2014-10-17T17:16:09.790" UserId="4683" />
  <row Id="2481" PostId="2304" Score="0" Text="Has one of &quot;Java&quot; and &quot;Energy consumption forecasting&quot; higher priority than the other? (even if ideally you want both)" CreationDate="2014-10-18T17:36:39.233" UserId="3317" />
  <row Id="2482" PostId="1253" Score="8" Text="Basically because they deliver state-of-the-art results, and eliminate the need for feature engineering." CreationDate="2014-10-20T07:39:09.177" UserId="381" />
  <row Id="2483" PostId="2288" Score="0" Text="I am glad it helps; if I answered to your question, maybe you could mark it as &quot;answered&quot;?" CreationDate="2014-10-20T08:08:22.110" UserId="3159" />
  <row Id="2484" PostId="2296" Score="0" Text="Thanks for the answer: this is a simple model to compute extreme quantiles, and I think it will fit my needs. However for more complex time-series that do not have a nearly stationary distribution this approach may fail, and that's when I think we would need something adaptive such as a Markov chain." CreationDate="2014-10-20T09:32:25.270" UserId="3159" />
  <row Id="2485" PostId="2316" Score="0" Text="It's still here." CreationDate="2014-10-20T07:04:46.800" UserDisplayName="davidhigh" />
  <row Id="2487" PostId="2307" Score="0" Text="How can I generate a confusion matrix for the above mentioned input data to check various Performance measures such as variance, precision, recall, F1 measure ?" CreationDate="2014-10-20T15:31:10.347" UserId="3577" />
  <row Id="2488" PostId="2307" Score="0" Text="I do know that I can use the function confusionMatrix(predicted, actual) of the caret package, the only problem I am facing is with the predicted values. Since the data generated is ambiguous, I am not able to understand what the predicted values should be ?" CreationDate="2014-10-20T15:42:48.740" UserId="3577" />
  <row Id="2489" PostId="2307" Score="0" Text="In the worst case (uniform distribution), there is no &quot;predicted value&quot; because there are no clusters in the data. For data generated by other algorithms (e.g. linearly inseparable n-classes XOR), the algorithm does generate *n* classes, so they would be the basis for comparing predicted (i.e. perfect classification) vs actual (what ever your ML method yields)" CreationDate="2014-10-20T17:38:39.237" UserId="609" />
  <row Id="2490" PostId="2307" Score="0" Text="I did get the clusters using the above dataset through kmeans. Hence I wished to find the performance metrics so that after running kernel tricks, I can compare the performance metrics before and after applying kernel tricks" CreationDate="2014-10-20T17:43:39.267" UserId="3577" />
  <row Id="2491" PostId="2261" Score="0" Text="Tnx for the link :D" CreationDate="2014-10-20T17:46:48.557" UserId="4727" />
  <row Id="2492" PostId="2307" Score="0" Text="One more thing I noticed is that, the SSE generated using kmeans lies between 0.2 and 0.35, which makes me wonder whether the dataset can actually be considered 'bad' for kmeans?" CreationDate="2014-10-20T17:49:01.737" UserId="3577" />
  <row Id="2493" PostId="2307" Score="0" Text="Any given realization of a random data set could yield good clustering. But if you run it many times, as you should for statistical testing, and you'll find that it fails as often as it succeeds." CreationDate="2014-10-20T19:13:36.743" UserId="609" />
  <row Id="2494" PostId="2328" Score="1" Text="Be clearer about the structure of your data. Is it already gridded? Or do you need to interpolate it to a grid first? Because that's a whole other big question. Then (once you have a grid) can we talk about 3d graphics. Show us a chunk of your data." CreationDate="2014-10-21T11:44:40.520" UserId="471" />
  <row Id="2495" PostId="2329" Score="0" Text="That is just.. fantastic!" CreationDate="2014-10-21T13:35:38.493" UserId="3443" />
  <row Id="2496" PostId="2324" Score="0" Text="According to the authors, there is indeed a field aware characteristic to the model, relative to the standard implementation - it is stated in the kaggle forums. I was just not able to follow what it meant and what the difference actually was." CreationDate="2014-10-21T13:51:55.417" UserId="1138" />
  <row Id="2497" PostId="2308" Score="1" Text="This is too broad to be a useful question. Narrow it down by stating what you have studied, your interests, and some specific topics you are considering." CreationDate="2014-10-21T13:54:13.613" UserId="21" />
  <row Id="2499" PostId="2285" Score="0" Text="This doesn't make a good question, as there is no detail from which anyone could better guess than you what this term means. I'd elaborate with much more detail and your specific concern, or close this." CreationDate="2014-10-21T13:56:35.100" UserId="21" />
  <row Id="2501" PostId="2287" Score="0" Text="What are these variables you are predicting? it's not clear how they relate to the time series input." CreationDate="2014-10-21T13:58:24.423" UserId="21" />
  <row Id="2502" PostId="2303" Score="1" Text="This is probably off topic if you're just asking for tutorials and resources. Please elaborate with the specific issues you are facing with these tools." CreationDate="2014-10-21T13:59:41.253" UserId="21" />
  <row Id="2503" PostId="2302" Score="0" Text="Can you clarify what these data sets are that you are trying to generate? what have you tried in R so far?" CreationDate="2014-10-21T14:01:17.660" UserId="21" />
  <row Id="2504" PostId="2258" Score="0" Text="Yes, this is too open-ended if you're just looking for tutorials and resources. Maybe you can make this much more specific by specifying what you are trying to do, what you have tried so far and what concepts you found challenging." CreationDate="2014-10-21T14:01:58.440" UserId="21" />
  <row Id="2505" PostId="2299" Score="0" Text="I'm not sure this is a particular problem with R, or that it answers the question of how Python and R differ." CreationDate="2014-10-21T14:02:29.913" UserId="21" />
  <row Id="2510" PostId="2324" Score="0" Text="http://www.kaggle.com/c/criteo-display-ad-challenge/forums/t/10555/3-idiots-solution/55932#post55932" CreationDate="2014-10-21T14:12:45.363" UserId="1138" />
  <row Id="2511" PostId="197" Score="0" Text="Wanted to provide an update for this question/answer: we've been using JRip with some success, but our new leading contender is FURIA (https://www.cs.uni-paderborn.de/fileadmin/Informatik/eim-i-is/PDFs/furiadraft.pdf). It's generating the best rules for human review/use because it tries to generate an exhaustive ruleset. JRip makes nice rules, but it has a &quot;default&quot; rule for classification when no other rules apply. Default buckets don't work well in our project's business context, we need exhaustive rules." CreationDate="2014-10-21T17:36:54.457" UserId="275" />
  <row Id="2512" PostId="2308" Score="0" Text="thanks @SeanOwen. good, i will add some other information about my studies on Master and my interests :)" CreationDate="2014-10-21T19:33:49.923" UserId="4705" />
  <row Id="2513" PostId="2308" Score="0" Text="&quot;and beyond&quot; part of [this](http://www.meetup.com/spark-users/events/175940092/) and/or [this](http://www.slideshare.net/Hadoop_Summit/th-210p212meng) is probably a good point to start." CreationDate="2014-10-21T21:12:42.143" UserId="1279" />
  <row Id="2515" PostId="2324" Score="0" Text="Based on slie 14, it appears that they based their solution on [this](https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/Opera.pdf) paper (_[Ensemble of Collaborative Filtering and Feature Engineered Models for Click Through Rate Prediction](https://kaggle2.blob.core.windows.net/competitions/kddcup2012/2748/media/OperaSlides.pdf)_)." CreationDate="2014-10-22T00:20:57.583" UserId="381" />
  <row Id="2516" PostId="2308" Score="1" Text="You are paying to do a Masters, what do your tutors suggest?" CreationDate="2014-10-22T11:08:03.813" UserId="471" />
  <row Id="2518" PostId="2308" Score="0" Text="You might wanna state that Big Data is a leading trend in the Computer industry." CreationDate="2014-10-21T21:21:57.150" UserDisplayName="user4753" />
  <row Id="2522" PostId="2335" Score="0" Text="ok, it is probably the real solution. Thank you." CreationDate="2014-10-23T11:39:54.950" UserId="3281" />
  <row Id="2523" PostId="2332" Score="0" Text="thanks @ssantic , I will consider your proposal for to know the dimensions of this subject according to my ability" CreationDate="2014-10-23T14:55:27.333" UserId="4705" />
  <row Id="2525" PostId="2337" Score="0" Text="How do you decide that &quot;string&quot;, &quot;string2&quot; and &quot;string3&quot; are similar, but &quot;string&quot;, &quot;xstring&quot; and &quot;stringg&quot;, for example, are not?" CreationDate="2014-10-23T19:12:57.943" UserId="1279" />
  <row Id="2526" PostId="2337" Score="0" Text="As I understood it, string, string2 and string3 are placeholders; they are not necessarily similar." CreationDate="2014-10-23T20:05:41.947" UserId="381" />
  <row Id="2527" PostId="2337" Score="0" Text="By string similar I mean they are exactly the same, char by char. Additionally, there might be multiple pairs of exact substrings between the two strings." CreationDate="2014-10-24T05:58:02.363" UserId="4774" />
  <row Id="2528" PostId="2325" Score="0" Text="The implementation that you mentioned follows a tree-growing strategy, like Mondrian forests (http://arxiv.org/abs/1406.2673). Hence, the number of trees is constant while the number of splits is increased. My question focuses on increasing the number of trees for new samples while remaining untouched the previously trained trees." CreationDate="2014-10-24T09:20:03.073" UserId="4719" />
  <row Id="2529" PostId="2306" Score="0" Text="Can you make this question more specific? I am not sure it's productive to speculate on why software doesn't exist." CreationDate="2014-10-24T12:33:25.073" UserId="21" />
  <row Id="2530" PostId="1185" Score="2" Text="Questions asking for online resources are generally discouraged. This might be more useful if you state what aspects you want more insight on, and see if people can write up those insights possibly with links to papers." CreationDate="2014-10-24T12:39:43.657" UserId="21" />
  <row Id="2531" PostId="2325" Score="1" Text="Like [this](http://stats.stackexchange.com/questions/87237/trend-analysis-of-feature-importance-over-time-in-r)? Don't you also want to drop trees if appropriate?" CreationDate="2014-10-24T16:31:03.530" UserId="381" />
  <row Id="2532" PostId="2313" Score="0" Text="I'm not clear on something- are the classes predefined, or supposed to be defined by the data? I.e. are you talking about supervised or unsupervised learning?" CreationDate="2014-10-25T23:16:24.790" UserId="1241" />
  <row Id="2533" PostId="2287" Score="0" Text="@SeanOwen The variables are statistics taken from cores of a computer . So through time I take lets say 'snapshots' of the state of a core and it's variables" CreationDate="2014-10-26T15:42:06.923" UserId="4668" />
  <row Id="2534" PostId="2287" Score="0" Text="@BenAllison The variables are dependent on each other since they are statistics of a core in a computer multicore system." CreationDate="2014-10-26T15:42:42.360" UserId="4668" />
  <row Id="2535" PostId="2287" Score="0" Text="Yeah but what are you predicting? You say that given the values you want to predict the values but you have them." CreationDate="2014-10-26T17:16:18.677" UserId="21" />
  <row Id="2536" PostId="2343" Score="1" Text="I think this is well too vague to make a good question. You haven't given any parameters other than be implementable in 3-4 weeks. Add much more detail at least, but I think this is off topic." CreationDate="2014-10-26T20:12:29.093" UserId="21" />
  <row Id="2537" PostId="2346" Score="0" Text="I think this is just about programming or architecture? I don't see a requirement for engineering+statistics or things people would call ML. I would have said StackOverflow is the best place, but if you can narrow this down to more than just asking for product recommendations." CreationDate="2014-10-26T20:14:00.637" UserId="21" />
  <row Id="2538" PostId="2348" Score="0" Text="How to calculate classification accuracy with confusion matrix ? Thanks" CreationDate="2014-10-27T11:14:43.897" UserId="3503" />
  <row Id="2539" PostId="1189" Score="0" Text="Thanks, I want build confusion matrix to calculate accuracy." CreationDate="2014-10-27T11:17:27.580" UserId="3503" />
  <row Id="2540" PostId="2287" Score="0" Text="@SeanOwen I am using all these values to train a system , so that in the case of 3 new values of x , y , z it will predict what are the new values coming. &#xA;&#xA;The data set I have collected comes from many different benchmarks . So when the program is live I will need based on the current values of the system to predict what the values of the next interval will be." CreationDate="2014-10-27T14:06:23.097" UserId="4668" />
  <row Id="2541" PostId="2337" Score="0" Text="No, the question is if there is another string that has &quot;stringg&quot; then if and how is it different from string2 or string3? The question is about defining EXACTLY what you want to do. Like is there a minimum length of strings you're looking to cluster? Is there a maximum length?" CreationDate="2014-10-27T15:23:23.317" UserId="587" />
  <row Id="2542" PostId="2356" Score="0" Text="I added some more details so as to be more helpful" CreationDate="2014-10-27T16:04:43.740" UserId="4668" />
  <row Id="2543" PostId="2352" Score="0" Text="You really can't draw inferences from *simulated* data, and, in most cases, not even from real data. Since you have so many parameters to play with, you will always (through sheer chance) find some &quot;pattern&quot; that doesn't really exist." CreationDate="2014-10-27T21:10:31.910" UserId="4710" />
  <row Id="2544" PostId="2297" Score="0" Text="NASA approximates the position of planets and satellites and publishes these approximations online with a known margin of error. Using this margin of error, calculate the chances that some catastrophic event will occur in n years (eg, an asteroid hitting the Earth)" CreationDate="2014-10-27T21:14:07.560" UserId="4710" />
  <row Id="2546" PostId="2337" Score="0" Text="I've implemented longest common substring algorithm. It solved my problem but it's expensive computationally." CreationDate="2014-10-28T08:28:14.060" UserId="4774" />
  <row Id="2547" PostId="2362" Score="0" Text="It's a common problem, many techniques invented to try and address it - momentum, adaptive learning rates, batch solvers using second-order approximations etc. Fundamentally, gradient descent is a ropey choice for optimising a complex function, but the best option with neural networks is usually some form of it with some fixes to make it more bearable." CreationDate="2014-10-28T16:57:08.800" UserId="836" />
  <row Id="2548" PostId="2355" Score="0" Text="It is a known issue in GraphLab Create 1.0.1 and earlier that the visualization output will not show in an IPython Notebook running over HTTPS. If you are using HTTPS, try over HTTP instead. This will be fixed in a future release of GraphLab Create." CreationDate="2014-10-28T17:56:44.580" UserId="4834" />
  <row Id="2549" PostId="2297" Score="0" Text="That's very interesting, thanks for your input!" CreationDate="2014-10-29T06:19:44.117" UserId="2475" />
  <row Id="2550" PostId="264" Score="0" Text="@bayer, i think the clustering mentioned here is gaussian mixture model. GMM usually uses EM." CreationDate="2014-10-29T07:17:00.650" UserId="4841" />
  <row Id="2553" PostId="2353" Score="0" Text="I think you should explain the essence of what you are trying to do, and what you have investigated already. &quot;What's wrong with this code&quot; questions aren't that suitable for StackExchange." CreationDate="2014-10-29T08:10:52.140" UserId="21" />
  <row Id="2554" PostId="2359" Score="0" Text="What is the sort criteria? it's not clear. You say &quot;most relevant&quot; but to what?" CreationDate="2014-10-29T08:11:55.803" UserId="21" />
  <row Id="2555" PostId="264" Score="0" Text="I don't think that's what he means, cause GMM does not assume categorical variables." CreationDate="2014-10-29T09:06:51.230" UserId="1193" />
  <row Id="2556" PostId="2355" Score="0" Text="Thank you for your answer" CreationDate="2014-10-29T11:46:59.053" UserId="3281" />
  <row Id="2558" PostId="2346" Score="0" Text="I just made some edits that added questions that are more specific. I think that this question is on topic because once you've done your analyses, you're going to have to communicate the results to others, and that is the gist of Brandon's question." CreationDate="2014-10-29T16:02:34.727" UserId="1241" />
  <row Id="2559" PostId="2372" Score="0" Text="Eh, but the model is fixed here and serialized as PMML. The question is about scoring from a given fixed model." CreationDate="2014-10-29T17:37:22.810" UserId="21" />
  <row Id="2560" PostId="2313" Score="0" Text="the classes are already defined. I'm a little bit confused about the right way of classification. If I have 10 different categories (politics, biology, sports etc.) and I would like to predict new unlabeled data. How do I organize my training-data? I mean, do I build a training-set for predicting politics by creating a set of good samples (politic text) and bad samples (not politic text), the same for all other classes. That would be binary-classification. Or do I create single-class training-sets containing only good samples and use one-class algorithms ..." CreationDate="2014-10-29T21:46:16.737" UserId="4717" />
  <row Id="2561" PostId="2325" Score="0" Text="Thank you. This is more similar to what I am looking for. In this case, the use RF for feature selection of time-variant signals. However, the specific implementation and validity of the method is quite unclear, do you know if they published anything (Google didn't help)?" CreationDate="2014-10-30T13:59:27.840" UserId="4719" />
  <row Id="2562" PostId="2325" Score="1" Text="[Calculating Feature Importance in Data Streams With Concept Drift Using Online Random Forest](https://docs.google.com/presentation/d/19ccixj5ey5ldwpp63x9VGqpTR2Hr-D4nM3l1NRPSPFo/)" CreationDate="2014-10-30T18:01:29.780" UserId="381" />
  <row Id="2563" PostId="2378" Score="0" Text="What you need is a good book on linear algebra, and maybe convex optimization. I can't recommend the one I learned from but [this](http://www.crcpress.com/product/isbn/9781420095388) is relevant." CreationDate="2014-10-30T22:30:17.377" UserId="381" />
  <row Id="2564" PostId="2378" Score="0" Text="This is very broad for StackExchange. Maybe you can start with a specific statement about your understanding and a specific question from there." CreationDate="2014-10-31T08:29:30.863" UserId="21" />
  <row Id="2565" PostId="2325" Score="0" Text="Thanks for the link! I can see that they actually update all the previous trees using a tree-growing strategy, and I am interested in creating new DT's with the new data while keeping untouched the old trees." CreationDate="2014-10-31T10:15:07.017" UserId="4719" />
  <row Id="2566" PostId="2382" Score="0" Text="Poke around things like http://mesowest.utah.edu/cgi-bin/droman/meso_base.cgi?stn=WMGI2" CreationDate="2014-10-31T15:53:38.433" UserId="4710" />
  <row Id="2567" PostId="2381" Score="0" Text="Having trouble understanding your data set.  What do the 1's and 0's correspond to.  For example what does 1011 mean in plain english?" CreationDate="2014-10-31T16:52:03.127" UserId="4808" />
  <row Id="2568" PostId="2381" Score="0" Text="1011 means: Customer booked 2010, 2011 and 2013, but not 2012. It's basically a short time-series and each digit indicates if the customer booked that year (2010,2011,2012,2013)." CreationDate="2014-10-31T17:51:43.593" UserId="723" />
  <row Id="2569" PostId="2376" Score="0" Text="hello sean, thank u for ur reply. there is actually no problem with a binary classifier. I'm just wondering what kind of classification is right for the mentioned problem." CreationDate="2014-10-31T17:58:36.533" UserId="4717" />
  <row Id="2570" PostId="2381" Score="0" Text="Oh, of course I meant 1011 = booked 2010, 2012, 2013 and not 2011." CreationDate="2014-10-31T19:25:41.157" UserId="723" />
  <row Id="2571" PostId="2362" Score="0" Text="Hi Neil, thanks for the comment. Can you suggest other areas I could explore other than gradient descent algorithms?" CreationDate="2014-11-01T07:36:40.033" UserId="4824" />
  <row Id="2572" PostId="2362" Score="0" Text="I don't really think there are any serious contenders for nn optimisation on large complex networks. For small networks, a genetic algorithm search can work (see http://en.wikipedia.org/wiki/Neuroevolution_of_augmenting_topologies or examples)" CreationDate="2014-11-01T08:41:06.957" UserId="836" />
  <row Id="2573" PostId="2362" Score="0" Text="Thanks will take a look" CreationDate="2014-11-01T09:30:30.647" UserId="4824" />
  <row Id="2574" PostId="2382" Score="1" Text="This isn't a good place to ask for data sets." CreationDate="2014-11-01T16:08:44.920" UserId="21" />
  <row Id="2575" PostId="2382" Score="1" Text=". . . but you could try http://opendata.stackexchange.com/ - please check first whether the question is already answered, and please give more details in the question, there are probably lots of variations in energy production data sets, and it is not 100% clear what you are looking for. E.g. why is http://www.nrel.gov/gis/solar.html not suitable from the same site you get the wind data from?" CreationDate="2014-11-01T19:32:25.710" UserId="836" />
  <row Id="2577" PostId="2392" Score="0" Text="There's not much information here. You should provide more detailed diagnostic information, including how you are invoking OMP and with what settings." CreationDate="2014-11-03T08:27:15.523" UserId="21" />
  <row Id="2578" PostId="2391" Score="0" Text="I think you should clarify your problem a bit. So you think metrics1 and metrics2 predict metrics3, and want to know when metrics3 doesn't match the prediction well? that's just a regression problem." CreationDate="2014-11-03T08:28:38.440" UserId="21" />
  <row Id="2579" PostId="2391" Score="0" Text="@SeanOwen : Yes absolutely. metrics1 and metrics2 predict metric3. for example, temperature and pressure metrics predict a metric called constant for a fixed mass of gas. A change in the metric constant means there an anomaly detected in the mass of gas and an action has to be taken. Similarly, at times, we want to predict temperature or pressure metrics from metric constant as well. In the end, all these 3 metric values are streamed to the algorithm." CreationDate="2014-11-03T13:23:01.303" UserId="4887" />
  <row Id="2580" PostId="2394" Score="0" Text="&quot;Best&quot; is massively dependent on your application and probably subjective too." CreationDate="2014-11-03T15:58:56.823" UserId="471" />
  <row Id="2581" PostId="2394" Score="0" Text="Question was edited to be more specific." CreationDate="2014-11-03T16:56:03.183" UserId="3106" />
  <row Id="2582" PostId="2381" Score="0" Text="Haven't you answered your own question? What you provide in the question **is** a probability distribution. Another approach: model the number of hits by averaging/weighting the individual values (eg, chance of 2 hits is approximately .015 or so)" CreationDate="2014-11-03T17:59:57.047" UserId="4710" />
  <row Id="2583" PostId="2393" Score="0" Text="thank you for your reply! =)" CreationDate="2014-11-03T18:37:37.220" UserId="4717" />
  <row Id="2584" PostId="2393" Score="0" Text="You're more than welcome! Good luck!" CreationDate="2014-11-03T19:06:04.653" UserId="4897" />
  <row Id="2585" PostId="2381" Score="0" Text="I want to find a mechanism for this distribution. Therefore reduce all these parameters to only very few. For example just saying $P(1)=0.25$ wouldn't work since a binomial distribution doesnt match. However it almost does if you somehow include another parameter for consecutive hits." CreationDate="2014-11-04T05:54:35.810" UserId="723" />
  <row Id="2586" PostId="904" Score="0" Text="Did you created this dashboard as described above? I would love to have a look at the code!" CreationDate="2014-11-04T07:43:19.750" UserId="497" />
  <row Id="2587" PostId="2392" Score="0" Text="Edited with relevant piece of code and the error as well" CreationDate="2014-11-04T09:52:06.560" UserId="4889" />
  <row Id="2589" PostId="2397" Score="0" Text="If using decimal degrees make sure your metadata includes the Coordinate Reference System (most likely WGS84 lat-long, aka EPSG code 4326). Time stamps need to consider time zones, DST and leap seconds when applicable. Caveat emptor." CreationDate="2014-11-04T17:46:54.797" UserId="471" />
  <row Id="2590" PostId="2398" Score="1" Text="If the question is about finding collaborators... I think this is a bit off topic for this SE site." CreationDate="2014-11-04T19:14:01.473" UserId="21" />
  <row Id="2591" PostId="2398" Score="1" Text="This question appears to be off-topic because it is a request for finding people and not a data science question" CreationDate="2014-11-05T13:29:25.080" UserId="471" />
  <row Id="2592" PostId="2398" Score="0" Text="I would like to learn about data science.  Instead of asking 100 questions on here, why not find someone that would like to work as a team to share his/her insights." CreationDate="2014-11-05T17:40:43.333" UserId="4910" />
  <row Id="2596" PostId="2422" Score="0" Text="How many columns did you get after including the quadratic features? Was it 180? Also, what was the maximum memory usage without including the quadratic term?" CreationDate="2014-11-07T04:52:37.700" UserId="847" />
  <row Id="2597" PostId="2422" Score="0" Text="I'm trying to add all possible quadratic features. That should give about 4275 features I think. &#xA;Adding just the squared features works fine. That would be 180.&#xA;btw how do we check the max memory usage ?" CreationDate="2014-11-07T05:08:12.373" UserId="4947" />
  <row Id="2598" PostId="2422" Score="0" Text="see http://ubuntuforums.org/showthread.php?t=1161120" CreationDate="2014-11-07T05:55:48.770" UserId="847" />
  <row Id="2599" PostId="2318" Score="0" Text="Look pretty cool, but also a bit expensive. Anyway I like the idea." CreationDate="2014-11-07T07:41:25.573" UserId="82" />
  <row Id="2600" PostId="2405" Score="1" Text="Link-only answers are discouraged. Please in-line why you recommend this tool." CreationDate="2014-11-07T08:23:39.860" UserId="21" />
  <row Id="2601" PostId="2403" Score="0" Text="It's not clear what you are asking - is it better to learn tools or gather domain knowledge? probably too open-ended and opinion-based for StackExchange." CreationDate="2014-11-07T08:27:38.663" UserId="21" />
  <row Id="2602" PostId="2412" Score="0" Text="Questions just looking for resources are considered off-topic for StackExchange. Maybe you can refine this into specific questions about specific tools." CreationDate="2014-11-07T08:29:28.533" UserId="21" />
  <row Id="2603" PostId="2413" Score="0" Text="Thank you @Nitesh . I will try this technique" CreationDate="2014-11-07T09:20:29.970" UserId="4887" />
  <row Id="2604" PostId="2423" Score="0" Text="Thank you. &#xA;So adding all quadratic features is not possible. Guess I'll have to try some other methods" CreationDate="2014-11-07T09:36:33.640" UserId="4947" />
  <row Id="2606" PostId="2423" Score="0" Text="Yes, that would make more sense. I think 4275 is too many features, given the number of rows you are dealing with." CreationDate="2014-11-08T00:43:08.737" UserId="847" />
  <row Id="2607" PostId="2426" Score="0" Text="Could you provide more information here? One way would be to see if the data forms a bell (normal) curve, and look for extreme outliers?" CreationDate="2014-11-08T16:08:28.010" UserId="4710" />
  <row Id="2608" PostId="2426" Score="0" Text="Could you try to explain your data format again? I don't really understand it. Also, how big is your data set." CreationDate="2014-11-08T20:13:16.053" UserId="1241" />
  <row Id="2610" PostId="2428" Score="0" Text="Thanks Nitesh, The purpose (Cheat, Yes) is retained in the multi-layer classification rules to do?" CreationDate="2014-11-08T23:03:55.437" UserId="3503" />
  <row Id="2611" PostId="2428" Score="0" Text="@XuanDung: For classification where the response is not binary (multi-class), the confidence of other classes cannot be derived from the confidence of the predicted class (unlike in the binary class version)." CreationDate="2014-11-08T23:05:54.377" UserId="847" />
  <row Id="2612" PostId="2403" Score="0" Text="While your question is valid, this is not the right place for it. Career related questions are considered off topic here." CreationDate="2014-11-05T17:15:38.640" UserId="3466" />
  <row Id="2613" PostId="2428" Score="0" Text="I want to know why need to have (Cheat, yes) in this multi-class classification rule, while the predicted classification (Cheat, No) is selected, so (Cheat, Yes) is redundant." CreationDate="2014-11-08T23:12:37.233" UserId="3503" />
  <row Id="2614" PostId="2428" Score="0" Text="Its not just the predicted class, the probability with which its predicted is also important." CreationDate="2014-11-08T23:20:06.190" UserId="847" />
  <row Id="2615" PostId="2428" Score="0" Text="I still do not understand why need to have (Cheat, yes) in this multi-class classification rule, you can more explain clean, thanks." CreationDate="2014-11-08T23:26:49.070" UserId="3503" />
  <row Id="2616" PostId="2439" Score="1" Text="From a supervised learning perspective, is this a standard problem with 3190 instances and 60 columns where each of the columns is a categorical variable? If that's the case, why not feed in the transition probabilities as additional features and throw your favorite classifiers at it. It could be a neural net, but it could might as well be a GBM? Also, how do you know that its a 2nd order Markov chain? What method are you using to estimate the order (eg. BIC, AIC, etc.)?" CreationDate="2014-11-10T18:10:40.660" UserId="847" />
  <row Id="2620" PostId="2439" Score="0" Text="@nitesh the 2nd order Markov chain is based on the observation that DNA sequences are composed of Codons composed of 3 nucleotides." CreationDate="2014-11-10T21:27:57.150" UserId="136" />
  <row Id="2621" PostId="2439" Score="0" Text="@akellyril: Thanks for clarifying!" CreationDate="2014-11-10T21:40:00.920" UserId="847" />
  <row Id="2624" PostId="2432" Score="0" Text="I think this might be a bit broad, and maybe better suited for StackOverflow." CreationDate="2014-11-11T11:29:02.527" UserId="21" />
  <row Id="2625" PostId="2445" Score="0" Text="Do you have data already aggregated by date? Is it the case when your columns are something like: date_1, #calls, #sms, #internetConnections? How many days do you have such data for (in terms of number of rows)?" CreationDate="2014-11-11T17:56:29.393" UserId="847" />
  <row Id="2626" PostId="2445" Score="0" Text="@Nitesh yes exactly. I have 12 months, millions of records." CreationDate="2014-11-11T21:36:37.423" UserId="989" />
  <row Id="2627" PostId="2449" Score="0" Text="first of all thx. In Clustering: do you mean I should cluster all the days (24 features) and see how it works? And what about trends? I mean, during winter there is a different usage than in summer" CreationDate="2014-11-12T09:40:31.317" UserId="989" />
  <row Id="2628" PostId="2436" Score="0" Text="Since you can create categorical variables out of numeric ones, maybe you should drop &quot;categorical&quot; from the title." CreationDate="2014-11-12T11:22:51.003" UserId="1241" />
  <row Id="2629" PostId="2436" Score="0" Text="Creating categorical variables out of numeric ones is always a possibility, but I'd rather find specific categorical datasets." CreationDate="2014-11-12T11:36:25.860" UserId="2576" />
  <row Id="2630" PostId="2436" Score="0" Text="Why do you want that?" CreationDate="2014-11-12T11:39:27.560" UserId="1241" />
  <row Id="2631" PostId="2452" Score="0" Text="Thanks, @user1808924!" CreationDate="2014-11-12T15:05:28.797" UserId="4999" />
  <row Id="2632" PostId="1246" Score="0" Text="Apologies for my very late response. Please, check the text that I added to the original question." CreationDate="2014-11-12T15:31:05.363" UserId="2576" />
  <row Id="2633" PostId="2436" Score="0" Text="I don't want to introduce any assumption about the structure of the data. I have the impression that creating categorical features from numerical ones would have that effect. But I may be wrong." CreationDate="2014-11-12T15:32:54.937" UserId="2576" />
  <row Id="2634" PostId="2449" Score="1" Text="Yes, for clustering, consider clustering all the 12 months of data using all the features and see how it works. Since its unsupervised, it might as well be the case that clusters are created by trends (for example a cluster with all winter data and another with all summer data and so on). Thinking about this a little more carefully and reading your question again, I think multi-class classification might be a better approach." CreationDate="2014-11-12T16:30:20.480" UserId="847" />
  <row Id="2635" PostId="2436" Score="0" Text="I don't see why it would. In a regression, a variable indicating an income of &lt; 50k is no different from one indicating an income of &gt;= 50k." CreationDate="2014-11-12T17:05:41.460" UserId="1241" />
  <row Id="2636" PostId="2436" Score="0" Text="I think you should just pick _some_ dataset with a lot of categorical variables to start gaining experience. You can just ignore numerical variables. You can keep on looking for your ideal data set at the same time." CreationDate="2014-11-12T17:08:25.713" UserId="1241" />
  <row Id="2637" PostId="2451" Score="0" Text="You'll find lots of hits if you search stats.SE for [&quot;imbalanced&quot;](http://stats.stackexchange.com/search?q=imbalanced) data." CreationDate="2014-11-12T18:13:46.290" UserId="381" />
  <row Id="2638" PostId="2451" Score="0" Text="@Emre: This question is focused more on the solution to fraud like problems rather than handling imbalance in the dataset (also the fact that we could do both classification and regression and its unclear to me how to go about obtaining the best solution). I tried searching for &quot;imbalanced&quot; and found nothing that relates to this. Could you provide a link in case I have missed something that answers this precise question?" CreationDate="2014-11-12T18:31:57.743" UserId="847" />
  <row Id="2639" PostId="1079" Score="0" Text="This is off topic in this Stack Exchange. Your post belongs in: http://opendata.stackexchange.com/" CreationDate="2014-11-12T23:32:15.470" UserId="3466" />
  <row Id="2640" PostId="2268" Score="1" Text="Opinion based posts are off topic here. This site is for discussing solutions to specific technical problems." CreationDate="2014-11-13T00:15:36.367" UserId="3466" />
  <row Id="2641" PostId="2303" Score="0" Text="Here are some guidelines about posts here. http://datascience.stackexchange.com/help/dont-ask" CreationDate="2014-11-13T00:18:16.043" UserId="3466" />
  <row Id="2642" PostId="2456" Score="0" Text="Thanks a lot for sharing your ideas. U mentioned 'my NLTK'...did you write that part of library?" CreationDate="2014-11-13T02:28:58.310" UserId="1165" />
  <row Id="2643" PostId="2445" Score="0" Text="This needs a fair bit more information. What do you think drives similarity, and what things are you looking for similarity in. Times?" CreationDate="2014-11-13T04:39:18.413" UserId="21" />
  <row Id="2647" PostId="2448" Score="0" Text="I'm not sure this is directly related to data science, but just a broad design approach question?" CreationDate="2014-11-13T04:46:21.940" UserId="21" />
  <row Id="2648" PostId="2268" Score="0" Text="state of the art is the most advances in the field and are not opinion based, its based on history of that field." CreationDate="2014-11-13T06:21:45.490" UserId="3436" />
  <row Id="2650" PostId="2445" Score="0" Text="@SeanOwen sorry, I think the right term is &quot;patterns&quot;. Similar amount of calls or similar behaviours (Friday night have a peak like in Saturday night)" CreationDate="2014-11-13T14:41:58.083" UserId="989" />
  <row Id="2651" PostId="2452" Score="0" Text="@SeanOwen - if it prints the specified output, that will be valid PMML (it's an example from the spec). That's all I'm worried about." CreationDate="2014-11-13T15:12:20.990" UserId="4999" />
  <row Id="2652" PostId="2452" Score="0" Text="Ah, row can have any content: http://www.dmg.org/v4-2-1/Taxonomy.html#xsdElement_row  Let me delete/fix comments then." CreationDate="2014-11-13T15:43:17.987" UserId="21" />
  <row Id="2653" PostId="2464" Score="1" Text="[As explained in the tutorial](http://radimrehurek.com/gensim/tut2.html), you can express documents as vectors. Cluster those vectors." CreationDate="2014-11-13T18:54:37.147" UserId="381" />
  <row Id="2654" PostId="2448" Score="0" Text="Yes - I agree it's a rather broad design approach question. If you think I should take down and put elsewhere, happy to do so." CreationDate="2014-11-14T06:22:09.493" UserId="5004" />
  <row Id="2655" PostId="1246" Score="1" Text="Can you show your implementation? I see misunderstanding, but without code sample it will be hard to explain it." CreationDate="2014-11-14T08:21:17.407" UserId="1279" />
  <row Id="2656" PostId="2464" Score="0" Text="I know mate but I have to cluster them according to the topics created after I apply LDA on my collection. Each topic should be represented as a vector in order to compare each document with each topic and find the correspondent topic or topics for each doc." CreationDate="2014-11-14T11:03:49.373" UserId="5029" />
  <row Id="2657" PostId="2468" Score="0" Text="Thanks for a good answer. I guess I am definitely in the 100s of millions ( I have updated my question)." CreationDate="2014-11-14T15:59:55.560" UserId="1256" />
  <row Id="2659" PostId="2474" Score="1" Text="This format is called &quot;JSON&quot;. Curly braces form objects, while square ones represent arrays. Most programming languages have libraries for parsing JSON (e.g. for R see [this](http://stackoverflow.com/questions/2061897/parse-json-with-r) question), so parsing it becomes trivial task." CreationDate="2014-11-14T23:46:31.913" UserId="1279" />
  <row Id="2660" PostId="2474" Score="2" Text="I think this would be more appropriate at StackOverflow. It's not related directly to data science, but just parsing a common data format." CreationDate="2014-11-15T14:20:32.547" UserId="21" />
  <row Id="2661" PostId="2470" Score="0" Text="SVMs support binary classification by nature. Your classes can't be vectors. You're converting vectors to a scalar norm of some vector difference. These still aren't binary classes. I don't think SVM is the tool you want; maybe you can clarify the problem?" CreationDate="2014-11-15T14:22:14.240" UserId="21" />
  <row Id="2662" PostId="2463" Score="1" Text="Can you be more specific about your requirements? are you asking for an algorithm? software tool, library? how big is the data?" CreationDate="2014-11-15T14:22:53.063" UserId="21" />
  <row Id="2663" PostId="2473" Score="0" Text="SVMs do not operate on categorical features. Are you sure that's what you mean?" CreationDate="2014-11-15T14:24:33.907" UserId="21" />
  <row Id="2664" PostId="2469" Score="0" Text="#1 is unclear, and #4 is too open ended. Can you clarify what you mean in #2-3, and what you have tried so far?" CreationDate="2014-11-15T14:25:41.183" UserId="21" />
  <row Id="2666" PostId="1079" Score="0" Text="This question appears to be off-topic because it is seeking a data set, and I believe that is more specifically on-topic at opendata.stackexchange.com" CreationDate="2014-11-15T14:28:46.633" UserId="21" />
  <row Id="2667" PostId="2470" Score="0" Text="Each element in classes A and B are vectors of a given dimension d. In other words you have d features to consider to classify a vector as A or B." CreationDate="2014-11-15T21:40:10.550" UserId="5042" />
  <row Id="2668" PostId="2470" Score="0" Text="A class is like a label that can apply to a feature vector. It makes sense to say a vector can have label A but then what does the notation |A-C| mean? the norm of the difference of vectors that are in class A / C, is a scalar, and you say the scalar belongs to class A'?" CreationDate="2014-11-15T22:00:06.307" UserId="21" />
  <row Id="2669" PostId="2470" Score="0" Text="What I meant by |A-C| is actually the Mahalanobis transformation where I calculate the covariance and centroid of C and then I use those two parameters to transform my classes A and B into A' and B' which are the respective multivariate distances. Effectively now every element of A' and B' are scalars." CreationDate="2014-11-16T00:46:11.980" UserId="5042" />
  <row Id="2670" PostId="2470" Score="0" Text="Basically I want to know if this is a valid transformation in the sense that I am not informing the algorithm (double dipping?) because I know that B is closer to C than A." CreationDate="2014-11-16T00:48:16.663" UserId="5042" />
  <row Id="2671" PostId="1128" Score="1" Text="You can't be successful if you don't have a goal. What kind of insights do you want to get? You have to define that first. Otherwise: Just use element-wise addition and multiplication. You're done now! Or aren't you? If you understand why this doesn't do it - maybe you can make progress." CreationDate="2014-11-16T16:06:42.517" UserId="723" />
  <row Id="2673" PostId="1128" Score="0" Text="@Gerenuk : This would be a reasonable response if I wanted to test a specific hypothesis about specific time series.  I'm interested in the meta-problem... i.e. are there *any* systems that go beyond element-wise addition and multiplication?  If so, what are they, how do they go further, and what sort of (interesting) transformations do they facilitate?" CreationDate="2014-11-16T19:38:34.597" UserId="3328" />
  <row Id="2674" PostId="1128" Score="1" Text="@aSteve: I could make up many non-trivial algebras, too. It would be a very tedious process asking you every time why this particular algebra does not suit your needs. It's really much more fruitful if there is an notion of what you want to achieve." CreationDate="2014-11-16T20:55:24.630" UserId="723" />
  <row Id="2675" PostId="1128" Score="0" Text="@Gerenuk:  I'm trying to establish a taxonomy of algebras with existing (practical or academic) applications.  Every significantly distinct algebra is of interest - as is the context in which it was deemed valuable.  Ultimately, I would like to establish if an (eloquent) universal time-series algebra can be defined, and - if not - why not.  What is the minimum adequate sets of operators and constants?" CreationDate="2014-11-16T23:13:09.507" UserId="3328" />
  <row Id="2676" PostId="2463" Score="0" Text="Thanks for the response, i am looking for Algorithm and library but most importantly algorithm, the data is close be hundred of thousands of records." CreationDate="2014-11-17T09:57:28.420" UserId="5027" />
  <row Id="2677" PostId="2462" Score="0" Text="One of the problems with this is that you don't know which nucleotide starts the codon. So you'd need to create 58 categorical features. Worth a try though." CreationDate="2014-11-17T10:47:45.140" UserId="136" />
  <row Id="2679" PostId="2487" Score="0" Text="What structure is your source data? If it is just the string &quot;Camera&quot;, what kind of associations are you hoping to make - properties that a physical camera might have with variation, properties related to selling cameras? The latter is what your list looks like, but if you are starting with just a string word, you might just as well get into more open-ended word association (e.g. &quot;Camera&quot; is a word derived from a Latin root, it has X million Google hits, it is of interest to photographers etc etc)" CreationDate="2014-11-17T14:07:16.237" UserId="836" />
  <row Id="2680" PostId="2487" Score="0" Text="Thanks for your reply Neil. It would be just a string for &quot;camera&quot; and I would be looking for any words that would be associated to it, whether that be physical properties or non physical such as &quot;light&quot;." CreationDate="2014-11-17T15:20:58.313" UserId="5066" />
  <row Id="2681" PostId="2462" Score="0" Text="That is a good point. However, since 30 are in E and 30 are in I (or the reverse), we can assume that the first neucleotide starts the codon? I might be totally wrong here so please correct my hypothesis/ reasoning." CreationDate="2014-11-17T16:19:40.490" UserId="847" />
  <row Id="2682" PostId="2451" Score="1" Text="heckman sample selection correction.  The regression only applies to records where fraud is observed, controlling for the probability of fraud occuring. Same approach is (famously) taken to estimating labor supply equations when wages are only observed for workers. http://en.wikipedia.org/wiki/Heckman_correction" CreationDate="2014-11-17T16:36:26.473" UserId="5067" />
  <row Id="2683" PostId="2477" Score="0" Text="So you are suggesting we should treat it as a regression problem (never consider it as a classification one). My hunch was that there must be some way to combine regression and classification together to solve such problems better." CreationDate="2014-11-17T17:32:10.240" UserId="847" />
  <row Id="2684" PostId="2477" Score="0" Text="What is your class variable? Please answer that question." CreationDate="2014-11-17T19:46:54.460" UserId="3083" />
  <row Id="2685" PostId="2477" Score="0" Text="The response denotes the transaction value. That is converted to represent 0 if there is no fraud, and to the transaction value, if there is fraud found." CreationDate="2014-11-17T19:48:58.573" UserId="847" />
  <row Id="2686" PostId="2477" Score="0" Text="Seems like you'll have many classes then? Try regression, it's a suggestion.If you predict greater than a threshold, fraud, else, not fraud. Very simple and see if that works.you'll need to define the threshold." CreationDate="2014-11-17T19:53:38.543" UserId="3083" />
  <row Id="2687" PostId="2477" Score="0" Text="Or you may try logistic regression, and for those detected as fraud, model prediction using ols." CreationDate="2014-11-17T19:56:06.640" UserId="3083" />
  <row Id="2688" PostId="2469" Score="0" Text="I'm new for statistics and it was interesting for me, what kind of question i can ask, when i have such data." CreationDate="2014-11-17T20:36:04.127" UserId="3377" />
  <row Id="2689" PostId="2451" Score="0" Text="@justincress: Thanks for the comment. If you could go into more detail about how I could pose it as a supervised learning problem, it would be great." CreationDate="2014-11-17T21:02:45.883" UserId="847" />
  <row Id="2690" PostId="2487" Score="0" Text="This is mostly a request to find a database of particular data. I think it's not quite the right type of question for this StackExchange." CreationDate="2014-11-18T00:23:22.677" UserId="21" />
  <row Id="2693" PostId="2489" Score="0" Text="Could you paste the head of the cleaned data in the question itself? Also, what do you mean by reducing this to a single variable? If you can, please provide some more context. Examples might help too." CreationDate="2014-11-18T10:03:11.987" UserId="847" />
  <row Id="2694" PostId="2489" Score="0" Text="Say I have put up a question to get replies for 5 disabilities, a respondent has 3 disabilities and there are multiple cases like this. How do I club all of them in a single categorical variable representing these disabilities?" CreationDate="2014-11-18T10:07:12.170" UserId="5075" />
  <row Id="2695" PostId="2492" Score="1" Text="The issue becomes bigger when I have 11 such variables" CreationDate="2014-11-18T10:21:23.107" UserId="5075" />
  <row Id="2696" PostId="2492" Score="0" Text="Agreed. It becomes more and more complicated as the number of choices/ options increase. But you choose to restrict the number of categories to, for example, 5. This can be done by using the top 4 choices/ options as they are and treating everything else as &quot;other&quot;." CreationDate="2014-11-18T10:23:00.820" UserId="847" />
  <row Id="2697" PostId="2492" Score="0" Text="Any other way out?" CreationDate="2014-11-18T10:23:28.600" UserId="5075" />
  <row Id="2699" PostId="2492" Score="0" Text="I will probably edit my answer with the above comment." CreationDate="2014-11-18T10:25:54.510" UserId="847" />
  <row Id="2700" PostId="2492" Score="0" Text="How do I figure out the top 4 choices?" CreationDate="2014-11-18T10:31:22.530" UserId="5075" />
  <row Id="2701" PostId="2492" Score="0" Text="Collect data first and then use the collected data to figure out the top 4 choices by count." CreationDate="2014-11-18T10:35:53.553" UserId="847" />
  <row Id="2703" PostId="2495" Score="1" Text="This is very short on details, like your data size, how you're running it now, what parameters, etc." CreationDate="2014-11-18T16:47:36.130" UserId="21" />
  <row Id="2704" PostId="2493" Score="0" Text="This is quite open-ended. I would state more about your data, scale, what skills you have, what you have tried and what problems you're facing." CreationDate="2014-11-18T16:48:16.730" UserId="21" />
  <row Id="2706" PostId="2495" Score="0" Text="Did you code the SVM yourself or are you using a function from a package?" CreationDate="2014-11-18T18:20:57.340" UserId="1156" />
  <row Id="2707" PostId="2495" Score="0" Text="There seem to be no parallel implementations of SVM in R. The testing as one can guess, can be parallelized. See http://vikparuchuri.com/blog/parallel-r-model-prediction-building/." CreationDate="2014-11-18T20:13:20.350" UserId="847" />
  <row Id="2708" PostId="2496" Score="0" Text="This was for a homework assignment related to SVM, I didn't need to create a Visualization, I just was curious if it had ever been dome with anything outside 3 dimensions.  I am not entirely sure I know what you are referring to when you say multiple 3-D images, just so I am clear, I take it to mean creating several models with only 3 explanatory variables in them, correct?" CreationDate="2014-11-18T20:17:52.327" UserId="5023" />
  <row Id="2709" PostId="2496" Score="0" Text="Correct. I sometimes also show images with individual variables on two axis, and a weighted average of the remaining variables (in your case, 4 of them) on the third axis. This can show dense clusters of data that were not obvious by examination of separate 3-D pictures." CreationDate="2014-11-19T00:23:25.227" UserId="5083" />
  <row Id="2710" PostId="2499" Score="0" Text="Not looked very closely, but have you looked at Trulia's API?  The limit here seems to be 5000 a day. There is a chance that if you email them, you might be able to increase your limit. http://developer.trulia.com/" CreationDate="2014-11-19T06:37:26.630" UserId="847" />
  <row Id="2712" PostId="1246" Score="0" Text="I'm afraid I can not share my specific implementation (I'm under a non-disclosure agreement). However, I can post a pseudo-code summary of my on-line implementation, if that is ok." CreationDate="2014-11-19T09:25:43.130" UserId="2576" />
  <row Id="2713" PostId="2500" Score="1" Text="Do you have some other information about which user liked,  rated or bought which product?" CreationDate="2014-11-19T10:09:27.473" UserId="847" />
  <row Id="2714" PostId="2500" Score="1" Text="no only user and product data" CreationDate="2014-11-19T10:27:30.230" UserId="5091" />
  <row Id="2715" PostId="1246" Score="0" Text="pseudocode should be enough, thanks." CreationDate="2014-11-19T10:39:27.193" UserId="1279" />
  <row Id="2716" PostId="2493" Score="0" Text="Thanks. I'll edit the question and add more info" CreationDate="2014-11-19T11:31:08.043" UserId="5077" />
  <row Id="2717" PostId="2496" Score="0" Text="thanks for the tip, i will have to try that out." CreationDate="2014-11-19T13:15:11.243" UserId="5023" />
  <row Id="2718" PostId="2495" Score="0" Text="Data size is 10MB and I am running it on Revolution R 7.2.0" CreationDate="2014-11-19T16:11:27.683" UserId="3551" />
  <row Id="2719" PostId="2501" Score="1" Text="Can you tell us where you got the data set from?  I would suspect that the data-prep/normalization methods will depend on the source of the data." CreationDate="2014-11-19T18:57:01.573" UserId="375" />
  <row Id="2721" PostId="2500" Score="2" Text="Clustering the products is easy enough (look up k-means or Gaussian mixture models), but recommending them will be difficult unless you can propose a model which relates them to the users. Since you have no training data for a supervised model you need to get it from somewhere; e.g., market research. Otherwise you can make up some heuristics yourself and use it to build a Bayesian prior, but I suspect this more complexity than you are comfortable with.&#xA;&#xA;Worry about mahout later; you need an algorithm, a model first." CreationDate="2014-11-19T19:32:03.240" UserId="381" />
  <row Id="2722" PostId="2500" Score="1" Text="Without data associating the products and the customers, there is no way to build a list of recommended products on a per-user basis. It could be possible to group the items based on meta-data about the products - for example, each item would have a list of recommended items. This would be generated by products sharing tags, a price range, a brand name, or some other meta-data you have available. Do you have any additional meta-data?" CreationDate="2014-11-19T20:33:00.233" UserId="3466" />
  <row Id="2724" PostId="2479" Score="0" Text="I couldn't understand the output I got after implementing BBN in R. I changed the no.of nodes a lot to get more than 1 arc. Result :- Nodes: 8 arcs: 1 &#xA;    undirected arcs: 1 &#xA;    directed arcs: 0 &#xA;  average markov blanket size: 0.25 &#xA;  average neighbourhood size: 0.25 &#xA;  average branching factor: 0.00 &#xA;  learning algorithm: Grow-Shrink &#xA;  conditional independence test: Mutual Information (disc.) &#xA;  alpha threshold: 0.05 &#xA;  tests used in the learning procedure:65 &#xA;  optimized: TRUE. Moreover, the plot just gave me a circle of all nodes. What does it mean?" CreationDate="2014-11-19T21:28:29.957" UserId="5043" />
  <row Id="2725" PostId="2479" Score="0" Text="I looked for tutorials related to this and all of them uses a simple set of numerical columns to get the two types of BBN. How to find the effect of combination of several categorical variables over one target categorical variable using BBN?" CreationDate="2014-11-19T21:42:34.043" UserId="5043" />
  <row Id="2726" PostId="2499" Score="0" Text="@nfmcclure, read my post again :)" CreationDate="2014-11-19T22:34:36.093" UserId="5086" />
  <row Id="2730" PostId="2479" Score="0" Text="I don't use R for this BBN learning. I use BayesiaLab and Hugin. Maybe this is a separate question on howto use the R BBN package that you're using. Some code and references you're using would be nice on that new thread." CreationDate="2014-11-20T00:19:58.583" UserId="3083" />
  <row Id="2731" PostId="2501" Score="0" Text="Can you also tell us some other basic characteristics of the dataset like the # of users, # of items, total # of ratings in your dataset? Why are you taking the log of the ratings instead of the raw ratings?" CreationDate="2014-11-20T00:32:17.953" UserId="847" />
  <row Id="2732" PostId="2500" Score="0" Text="Thanks for you reply @Emre,@ sheldonkreger. So how similar products can be found out? as product attributes i have name,category,cost and ingredients (some product may have 1 ingredients where as some other have 10..) I am new to recommendation system and I would like to know which are all the best algorithms and tools available to implement this problem.." CreationDate="2014-11-20T04:17:44.077" UserId="5091" />
  <row Id="2733" PostId="2500" Score="0" Text="Since you're new I suggest [reading a book](http://www.springer.com/computer/ai/book/978-0-387-85819-7) and/or watching [these](https://www.youtube.com/watch?v=bLhq63ygoU8) [videos](http://www.youtube.com/watch?v=mRToFXlNBpQ) by the director of Research &amp; Engineering @Netflix." CreationDate="2014-11-20T06:02:13.817" UserId="381" />
  <row Id="2734" PostId="2501" Score="0" Text="Thank you, I add some additional information to the main post" CreationDate="2014-11-20T07:43:41.993" UserId="3281" />
  <row Id="2735" PostId="2504" Score="2" Text="I think unless you have some good intuition about the data properties, you end up exploring both options (perhaps on just 500,000 rows) and cross-validate. But perhaps there are visualisations or other analyses that can help you get that intuition." CreationDate="2014-11-20T08:43:52.910" UserId="836" />
  <row Id="2736" PostId="2490" Score="0" Text="Thanks, this was really helpful" CreationDate="2014-11-20T12:33:13.473" UserId="5027" />
  <row Id="2737" PostId="2504" Score="1" Text="I've actually been planning to do a big model comparison for my own research on real data this week. I'll clean up the results a bit and post them here. Also at least one CS student has studied the question: https://www.academia.edu/3526056/Algorithms_Comparison_Deep_Learning_Neural_Network_AdaBoost_Random_Forest" CreationDate="2014-11-20T14:53:25.473" UserId="1156" />
  <row Id="2738" PostId="2504" Score="1" Text="@NeilSlater Id like to see an answer addressing what that intuition might/could/should be" CreationDate="2014-11-20T14:54:14.623" UserId="1156" />
  <row Id="2739" PostId="2508" Score="0" Text="Several solutions are posted here: http://stackoverflow.com/q/3301694/2954547" CreationDate="2014-11-20T15:21:45.087" UserId="1156" />
  <row Id="2742" PostId="2505" Score="0" Text="I will have to try this out, thanks for the tip! I would vote both you and Oleg up but I don't have enough reputation points." CreationDate="2014-11-20T21:03:52.067" UserId="5023" />
  <row Id="2743" PostId="2495" Score="0" Text="See a similar question on stats.se: http://stats.stackexchange.com/questions/825/any-suggestions-for-making-r-code-use-multiple-processors" CreationDate="2014-11-20T23:32:10.670" UserId="847" />
  <row Id="2744" PostId="2507" Score="0" Text="Can you provide a representative selection of examples that demonstrate the variety of items the algorithm will need to parse?" CreationDate="2014-11-21T04:20:15.347" UserId="381" />
  <row Id="2745" PostId="1133" Score="0" Text="ALGLIB looks really interesting! I'm a Data Scientist working in a .NET shop, and am looking to learn C#. Could you tell me a little more about the library?" CreationDate="2014-11-21T09:38:54.500" UserId="1127" />
  <row Id="2746" PostId="1246" Score="0" Text="I just added the pseudo-code as an edit. Please, do not hesitate to ask for further detail if needed." CreationDate="2014-11-21T10:03:49.387" UserId="2576" />
  <row Id="2747" PostId="2507" Score="1" Text="There are a hundred ways to do this. Give some sense of what tools or language you need to do this in. Is there a data science aspect to this? seems like just log parsing." CreationDate="2014-11-21T10:32:26.920" UserId="21" />
  <row Id="2748" PostId="2510" Score="1" Text="Career questions are generally considered off-topic, but maybe you can modify this to ask about specific problems and areas you are interested in." CreationDate="2014-11-21T10:37:08.433" UserId="21" />
  <row Id="2749" PostId="2506" Score="0" Text="Neo4j itself does not provide visualization. I don't think this addresses the question, but maybe you can edit it to expand on how you would use this to make a visualization." CreationDate="2014-11-21T10:39:03.147" UserId="21" />
  <row Id="2750" PostId="2512" Score="0" Text="This is fairly terse. Maybe you can expand on why you think this is a good choice?" CreationDate="2014-11-21T10:40:17.307" UserId="21" />
  <row Id="2751" PostId="2512" Score="2" Text="ok I have made edits, hope this helps." CreationDate="2014-11-21T10:51:53.487" UserId="5110" />
  <row Id="2752" PostId="1246" Score="0" Text="In your code, what do you mean by &quot;sample&quot;? Single observation?" CreationDate="2014-11-21T10:56:47.013" UserId="1279" />
  <row Id="2753" PostId="1246" Score="0" Text="Yes. The training set consists of a set of training samples." CreationDate="2014-11-21T11:00:30.950" UserId="2576" />
  <row Id="2754" PostId="1133" Score="0" Text="Hi, in couple words... Old, solid, rich enough, made by one author. For many languages. I have used it several times for Dot Net." CreationDate="2014-11-21T12:36:56.283" UserId="97" />
  <row Id="2755" PostId="2516" Score="1" Text="+1 Good question and well explained. I don't think there will be a straightforward answer to this. Here are a few follow up questions. What is the cardinality of each of the columns? Also, what is the target column here? Are you interested in predicting Var1?" CreationDate="2014-11-21T18:38:30.060" UserId="847" />
  <row Id="2756" PostId="2496" Score="0" Text="I'm downvoting this answer because it's far too specific, to the point where I think it's incomplete. Rather than this _particular_ 6D visualization (which, I'm sorry to say, is not new at all), you can just apply the general principle of using color/shape/etc. to represent higher dimensions. It's also important to realize that you can in fact have _too many_ dimensions on one plot, and that mapping features to non-spatial scales needs case-by-case judgement." CreationDate="2014-11-21T18:58:37.920" UserId="1156" />
  <row Id="2757" PostId="2518" Score="0" Text="Mahout does item based similarity. However, item based similarity in Mahout is calculated through users ratings of these items. In the dataset as described in the question, an item based similarity is not possible (at least in the framework used in Mahout's item-based similarity)." CreationDate="2014-11-21T22:51:33.710" UserId="847" />
  <row Id="2758" PostId="2515" Score="1" Text="I think this is the way to go. Mini-batches with a well-chosen size can actually converge faster than either batch or online version (former only updates weights once per whole set, and latter cannot be vectorised, plus has additional weight update steps more often)" CreationDate="2014-11-22T08:39:12.487" UserId="836" />
  <row Id="2759" PostId="2510" Score="1" Text="@Chuck D. When you say _with an eye toward entering the field of data science_, do you mean working in an industry position?" CreationDate="2014-11-22T19:28:34.770" UserId="4621" />
  <row Id="2760" PostId="2516" Score="1" Text="I could see using a well-indexed MySQL database to do this." CreationDate="2014-11-22T19:30:43.100" UserId="4710" />
  <row Id="2761" PostId="2510" Score="0" Text="@RobertSmith: Yes, I'm definitely interested in an industry position." CreationDate="2014-11-22T22:51:09.397" UserDisplayName="user5007" />
  <row Id="2762" PostId="2510" Score="1" Text="Then I have a quick recommendation. If you don't have previous experience in statistics, it would be good to pursue a master's degree (assuming you won't be left with a huge debt.) to get the basics. A PhD simply takes too long and it would be more valuable for you to have those years as work experience. Keep in mind that neither a MSc nor a PhD in a traditional department will teach you most of what you need in practice. That's why the people answering your question are suggesting departments geared toward data science. You need to learn quite a bit on your own." CreationDate="2014-11-23T01:11:44.383" UserId="4621" />
  <row Id="2763" PostId="2521" Score="0" Text="This doesn't answer your question but the first four lines of Athlete.csv list the fields for that CSV file. So it seems they only pull specific fields for athletes, and the extra fields are ignored. This isn't mentioned explicitly on the page you note, but they do say &quot;we provide *some* of the core DBpedia&quot; (emphasis added). I assumed they meant specific records, but apparently it also means only specific fields." CreationDate="2014-11-23T01:23:39.870" UserId="4710" />
  <row Id="2764" PostId="2526" Score="0" Text="Yeah, I had put that question on that thread - But I did not receive this mail??" CreationDate="2014-11-23T06:07:08.073" UserId="5126" />
  <row Id="2765" PostId="2495" Score="0" Text="If you really have to stay in R: http://amplab-extras.github.io/SparkR-pkg/" CreationDate="2014-11-23T17:52:09.700" UserId="381" />
  <row Id="2766" PostId="2503" Score="1" Text="You'll find lots of leads if you look up &quot;encoding categorical variables&quot;; e.g. http://stats.stackexchange.com/questions/21770/" CreationDate="2014-11-23T18:27:27.070" UserId="381" />
  <row Id="2767" PostId="2510" Score="0" Text="@SeanOwen, I've been thinking about how to reword this but I'm at a loss as to how to bring this back on-topic. I'm not to the point where I'm ready to ask about specific schools - I'm just looking for a broader perspective as to whether there is a general consensus concerning education credentials in the field. Do you have any further suggestions that could help me out? Thanks." CreationDate="2014-11-23T22:35:19.777" UserDisplayName="user5007" />
  <row Id="2768" PostId="2515" Score="0" Text="Thank you both. Apologies for stubbornly rejecting mini batches before, but I was unsure of the implications of this method on the convergence rate. Neil, is your affirmation coming from your own experience, or are there any theoretical/empirical published results?" CreationDate="2014-11-24T09:56:03.613" UserId="2576" />
  <row Id="2769" PostId="2518" Score="0" Text="I believe that in a proper case, the data should contain username, product name and rating to generate the recommendation system. Consider an overview of this scenario, you just need a couple of categorical &amp; numerical variables. Can't we use category,product name and cost to replicate this case in here? It may not be a good way to do it but,do you think it is not possible to use item-based similarity here?" CreationDate="2014-11-24T14:44:18.647" UserId="5043" />
  <row Id="2773" PostId="2518" Score="0" Text="Of course its possible to use the item categories/ features to create similarity. However, that representation will be sparse and might not yield meaningful similarities. Notice that when we have ratings, then for each item's vector, a component denotes a particular user's ratings. This inherently ensures that each user's rating is weighted equally when calculating similarity. In any case, one would have to do extensive preprocessing in order to get this dataset to be ready to be fed into Mahout." CreationDate="2014-11-24T17:58:38.407" UserId="847" />
  <row Id="2776" PostId="2530" Score="0" Text="In this case, you are suggesting the data to be fed something along the following lines, then? category_i, item_id_j, value_{i,j}. Here category_i denotes the i-th category and item_id_j denotes the j-th item and value_{i,j} denotes the value for the item category combination. For example, color can be one category and its value can be red for a given item. However, to feed it in, one would have to transform the dataset to read something like: color_red, item_i, 1 as one row and may be color_blue, item_i, 0 as another row. This way, we run into values that are both unary and numeric." CreationDate="2014-11-24T19:41:07.037" UserId="847" />
  <row Id="2777" PostId="2523" Score="1" Text="By KS, do you mean the Kolmogorov-Smirnov statistic? AUROC is probably the area under the ROC curve?" CreationDate="2014-11-24T19:49:59.380" UserId="847" />
  <row Id="2778" PostId="2525" Score="0" Text="Please provide more context here. Consider providing an example or the task that you are interested in. Why is it that you think there is a connection in the first place? May be cite a source?" CreationDate="2014-11-25T03:53:05.710" UserId="847" />
  <row Id="2779" PostId="2525" Score="0" Text="The task is to find clusters (non parametric) for given set of data points. So I am interested to find mean components in the case of GMM. The claim is these mean components lie in the span of eigen vectors of second order moment matrix . Can you give me some intuition behind this claim." CreationDate="2014-11-25T05:11:41.057" UserId="4686" />
  <row Id="2782" PostId="2451" Score="0" Text="I don't really know how to recast econometrics into data science speak. It's just a two stage regression model where the first step is a probit and the second is OLS.  To the extent that the models are specified directly (instead of learned from data) doesn't that make the algorithm &quot;supervised&quot; ? Not my expertise, sorry, but I think the heckman correction is exactly what you're looking for." CreationDate="2014-11-25T14:23:42.120" UserId="5067" />
  <row Id="2783" PostId="851" Score="1" Text="@ffriend: Please post your comment as an answer so it can be accepted." CreationDate="2014-11-25T17:29:25.573" UserId="847" />
  <row Id="2784" PostId="2538" Score="0" Text="More information would be very helpful. What are you forecasting? what is the error distribution?" CreationDate="2014-11-25T23:18:12.757" UserId="21" />
  <row Id="2785" PostId="2518" Score="0" Text="Thanks for your comments Nitesh Srinath. But it seams ( I am new to mahout and still experimenting) mahout is good at collaborative filtering. And what i need is something related to contend based filtering. I do not have any user ratings/preference value available. So is there any way to implement content based filtering in mahout or is there any other tools/libraries available.." CreationDate="2014-11-26T04:21:52.047" UserId="5091" />
  <row Id="2786" PostId="2506" Score="0" Text="https://gephi.github.io/  It support connecting to neo4j" CreationDate="2014-11-26T04:53:29.717" UserId="5091" />
  <row Id="2787" PostId="2538" Score="0" Text="@SeanOwen I am forecasting wind-speed at a particular location, The error series is obeying normal distribution.  please intimate me if you need any other information." CreationDate="2014-11-26T04:53:40.640" UserId="5099" />
  <row Id="2789" PostId="2540" Score="0" Text="Thank you very much for your full response. I check them." CreationDate="2014-11-26T06:49:38.047" UserId="3436" />
  <row Id="2790" PostId="2523" Score="0" Text="Seems like starting from Wikipedia and going through the original references would be a good place to start." CreationDate="2014-11-26T10:08:51.270" UserId="587" />
  <row Id="2792" PostId="2506" Score="0" Text="@SeanOwen is there a Javascript graph lib that works well with Neo4J that you will recommend. by the way, i have started learning Neo4j" CreationDate="2014-11-27T08:33:33.127" UserId="5027" />
  <row Id="2794" PostId="2506" Score="0" Text="check this http://neo4j.com/developer/javascript/" CreationDate="2014-11-27T08:55:50.247" UserId="5091" />
  <row Id="2798" PostId="2543" Score="1" Text="This is too broad as a question here, as you're asking what your question is. Please refine it with detail about what you are trying to accomplish." CreationDate="2014-11-27T20:09:31.973" UserId="21" />
  <row Id="2799" PostId="2553" Score="0" Text="+1. A reference: Good, in his *Resampling Methods* (3rd ed., 2006, p. 19) notes that the bootstrap may be unstable for sample sizes n&lt;100. Unfortunately, I don't have the book at hand, so I can't look up his argumentation or any references." CreationDate="2014-11-27T21:29:16.843" UserId="2853" />
  <row Id="2800" PostId="2553" Score="0" Text="@goangit, thank you for your reply. I don't have the software you used and don't quite follow why 0.05 and 0.95 is used in your first solution. Nonetheless, your answer inspired me to program a bootstrap which I will provide details of as &quot;an answer&quot;. Comments on the validity of what I've done from anyone would be appreciated. Thanks" CreationDate="2014-11-27T21:41:36.307" UserId="5180" />
  <row Id="2802" PostId="2553" Score="0" Text="@Steve, the software used here is R, it's freely [available](http://www.r-project.org/), in both senses of the word. The cutoffs used in the two tail case provide the central 90% confidence interval." CreationDate="2014-11-27T23:46:10.697" UserId="5153" />
  <row Id="2804" PostId="2362" Score="0" Text="NN initialization is also an open issue." CreationDate="2014-11-28T06:07:40.173" UserId="5198" />
  <row Id="2805" PostId="806" Score="0" Text="&quot;The implicit goal of AUC is to deal with situations where you have a very skewed sample distribution, and don't want to overfit to a single class.&quot; I thought that these situations were where AUC performed poorly and precision-recall graphs/area under them were used." CreationDate="2014-11-26T20:11:42.207" UserId="1241" />
  <row Id="2806" PostId="2553" Score="0" Text="@goangit, thanks. I've heard of R but can't justify the time to learn a new sw package, Mathematica is what I use, but this is not a Mathematica question. Could you explain what the (5%,-0.29) and (95%, 2.95) actually mean ? Is this saying that we are 90% confident that the ratio of the means will fall in the interval -0.29 to 2.95 ?" CreationDate="2014-11-28T13:06:23.920" UserId="5180" />
  <row Id="2807" PostId="2566" Score="0" Text="Thank You very much for valuable suggestion" CreationDate="2014-11-28T16:19:00.843" UserId="5099" />
  <row Id="2808" PostId="2567" Score="0" Text="If you are allowed to change your output to binary (depending on which party won), [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logistic_regression) is a good fit. It still considers independent output which may not be what you want." CreationDate="2014-11-29T17:20:27.880" UserId="4621" />
  <row Id="2810" PostId="2578" Score="0" Text="This question is potentially opinion-based, but I think it's specific enough to be valid. I'd like to see if a moderator has any thoughts on this. This exchange is still young, so we are still setting the standards." CreationDate="2014-12-02T19:48:17.897" UserId="3466" />
  <row Id="2811" PostId="2576" Score="0" Text="For Web entities there's WHOIS." CreationDate="2014-12-03T05:17:35.610" UserId="381" />
  <row Id="2812" PostId="712" Score="1" Text="@indico for most practical problems kernel SVM training complexity is closer to quadratic. Platt's cubic SMO has been out of use for quite some time. That's still too high for truly large data sets, but it's not as bad as you portray. Linear SVM is highly efficient, with sublinear complexity." CreationDate="2014-12-03T10:12:08.473" UserId="119" />
  <row Id="2813" PostId="2574" Score="1" Text="These don't relate to R or MapReduce, which seem to be requirements here." CreationDate="2014-12-03T13:51:27.717" UserId="21" />
  <row Id="2814" PostId="2584" Score="0" Text="You might instead reproduce your answer here, although ideally it wouldn't have been cross-posted as a question initially." CreationDate="2014-12-03T13:52:22.533" UserId="21" />
  <row Id="2815" PostId="2568" Score="1" Text="What debugging have you done? This sounds like a pure code review question, and there are other SE sites for that." CreationDate="2014-12-03T14:31:55.107" UserId="21" />
  <row Id="2816" PostId="2574" Score="0" Text="On spark You can write in python. Use RPy library for integrating R an Python." CreationDate="2014-12-03T14:40:02.177" UserId="5224" />
  <row Id="2817" PostId="2585" Score="0" Text="Hi Robert, thanks for the reply! Good point about the representation of the training/test set but this is time series data so as new data comes in I cannot guarantee that values will be similar in range to what they were before. I have the same exact feeling as you that I have no good reason to think it won't affect a SVM in any circumstance." CreationDate="2014-12-03T14:59:36.297" UserId="802" />
  <row Id="2818" PostId="2593" Score="0" Text="One solution which goes in the direction I am looking for is [fluentd](http://fluentd.org)." CreationDate="2014-12-03T16:36:50.713" UserId="5266" />
  <row Id="2819" PostId="2585" Score="0" Text="Have you compared predictions using MinMaxScaler and standardization?" CreationDate="2014-12-03T17:40:51.910" UserId="4621" />
  <row Id="2820" PostId="2585" Score="0" Text="I have and the results are similar, but that doesn't really tell me if things are getting screwy with the MinMaxScalar." CreationDate="2014-12-03T18:12:11.077" UserId="802" />
  <row Id="2821" PostId="2585" Score="0" Text="Sure. In any case, it would be better to use standardization if you're not getting something valuable from `MinMaxScaler`." CreationDate="2014-12-03T19:47:44.527" UserId="4621" />
  <row Id="2822" PostId="2595" Score="0" Text="great insight, thanks!" CreationDate="2014-12-03T20:28:58.367" UserId="802" />
  <row Id="2823" PostId="2584" Score="0" Text="Thank you. I guess I don't have much of a choice here. It's not just about cameras, basically parent category is, lets say, electronics with all sub-cats: cameras, smartphones, laptops etc. But so far there is no better solution than to build manually. Online catalogs would help me, but like I said, most of them contain more irrelevant info than relevant-at least in my case." CreationDate="2014-12-04T08:33:09.717" UserId="5241" />
  <row Id="2824" PostId="2584" Score="0" Text="Electronics in general is a huge category with long tail distribution. A lot would depend on your specific needs. For example, taking the &quot;assorted&quot; category in: if you do that, you will be swamped with tons of Chinese small name brands, and your best bet would be reliable feature recognition (memory size, color, domain keywords, etc.). If your target metric is purely numerical, &quot;Panasonics&quot; would suddenly constitute a shrinking minority. But if it's &quot;major electronics&quot;, try compiling a brand dictionary from web sites first." CreationDate="2014-12-04T10:02:06.847" UserId="5249" />
  <row Id="2825" PostId="2584" Score="0" Text="Btw, I would be interested to see the results of your experiments - if you intend making them public." CreationDate="2014-12-04T10:08:15.097" UserId="5249" />
  <row Id="2826" PostId="2597" Score="0" Text="This presupposes you know the total number of contestants and that number isn't infinite? And so when a new guy shows up to the race track you have to re-train the entire model?" CreationDate="2014-12-04T14:53:32.377" UserId="5247" />
  <row Id="2827" PostId="2600" Score="0" Text="That's fairly open ended. Can you say more about your requirements and what you have tried?" CreationDate="2014-12-04T15:21:23.807" UserId="21" />
  <row Id="2828" PostId="2600" Score="0" Text="With many LP (or MILP) solvers, you can easily add the convex and the constraints. For example, in scipy.optimize.anneal I could try f(x) = sum(x) as simply summing the input, e.g. in the case of a Graph's nodes. But two things are not clear: How do we pass in graphs to begin with and how do we impose further constraints, i.e. the requirement that every connected edge must contain different node colors?" CreationDate="2014-12-04T15:45:15.177" UserId="5273" />
  <row Id="2829" PostId="2602" Score="0" Text="Fair warning: Links aren't accepted as answers on this site. I recommend editing or deleting before you get any downvotes!" CreationDate="2014-12-04T18:56:36.163" UserId="3466" />
  <row Id="2830" PostId="2597" Score="0" Text="I updated my answer." CreationDate="2014-12-04T19:53:53.070" UserId="381" />
  <row Id="2831" PostId="2606" Score="0" Text="I'd also take relevant classes from the [statistics](http://handbook.uts.edu.au/directory/maj01111.html) or [maths](http://handbook.uts.edu.au/directory/maj01110.html) [departments](http://handbook.uts.edu.au/directory/stm90681.html). And do buy that book Charlie recommended!" CreationDate="2014-12-04T22:45:47.173" UserId="381" />
  <row Id="2832" PostId="2607" Score="0" Text="I was thinking more along the lines of [Bayesian nonparametric ranking](http://papers.nips.cc/paper/4624-bayesian-nonparametric-models-for-ranked-data.pdf)." CreationDate="2014-12-04T23:59:52.197" UserId="381" />
  <row Id="2833" PostId="2607" Score="0" Text="Sure. That is definitely a good fit. It could be a bit difficult to implement, though." CreationDate="2014-12-05T01:51:55.813" UserId="4621" />
  <row Id="2834" PostId="2604" Score="0" Text="What are the parameters for the custom search link you provided? Does it search in a list of websites, keywords, etc.?" CreationDate="2014-12-05T07:42:26.397" UserId="227" />
  <row Id="2835" PostId="2604" Score="0" Text="@AmirAliAkbari It searches through sources like Data.gov, Quandl, and other major data warehouses." CreationDate="2014-12-05T12:39:34.210" UserId="5279" />
  <row Id="2837" PostId="2609" Score="1" Text="This type of question is considered off topic for StackExchange." CreationDate="2014-12-05T15:48:27.397" UserId="21" />
  <row Id="2838" PostId="2493" Score="0" Text="Even after the editing of the question, I'm still not sure what you're trying to do. Do you want to cluster the paragraphs?" CreationDate="2014-12-05T15:48:53.937" UserId="819" />
  <row Id="2840" PostId="334" Score="0" Text="This discussion was useful to me. I have worked on the &quot;edges&quot; of data science with BI and analytics and have taken data mining and statistics courses for a Master's Degree several years ago. Right now I primarily work on the enterprise information management aspect of corporate data but would like to switch to a data science job. Therefore I am looking at best way to &quot;present myself&quot; on a resume and with training/experiences to potential future employer." CreationDate="2014-12-04T18:01:58.977" UserDisplayName="user5278" />
  <row Id="2841" PostId="2601" Score="1" Text="If you have a specific, reproducible example then that would probably add value. Also, are you sure that you're talking about a *package* in R? I see no package called `sann`. I do see a `sann` function in the `ConsPlan` package...  I'm pretty sure that's what you meant, so I'm going to edit the question. Please let me know if I'm off base." CreationDate="2014-12-05T15:53:58.770" UserId="2723" />
  <row Id="2842" PostId="2601" Score="1" Text="Upon further investigation, you actually could've gotten this `sann` function from a few different packages. The definition of `sann` varies between them, based on the developers comments in various R mailing lists. Were you perhaps using the `optim` package?" CreationDate="2014-12-05T16:13:40.923" UserId="2723" />
  <row Id="2843" PostId="2593" Score="0" Text="Are you interested only in libraries available within Google Go?" CreationDate="2014-12-05T16:38:27.373" UserId="2723" />
  <row Id="2844" PostId="2590" Score="0" Text="Can you provide some reproducible code/data to work with? I've completed a cross-sell model recently and I'd like to give it a shot." CreationDate="2014-12-05T16:46:19.343" UserId="2723" />
  <row Id="2845" PostId="2624" Score="0" Text="thank you, that's very useful. +1." CreationDate="2014-12-05T17:35:15.847" UserId="2723" />
  <row Id="2846" PostId="2612" Score="2" Text="Which tools have you already looked at, and why don't they solve your problem?" CreationDate="2014-12-05T18:24:02.693" UserId="3466" />
  <row Id="2847" PostId="2626" Score="0" Text="If price is no object you could check out Netezza..." CreationDate="2014-12-05T23:27:40.047" UserId="5247" />
  <row Id="2848" PostId="2615" Score="0" Text="I've looked for spaces after each of the state names. There aren't any." CreationDate="2014-12-06T00:12:42.247" UserId="2647" />
  <row Id="2849" PostId="2629" Score="1" Text="It means different things to different people. Give it time and people may come to an agreement. Until then: [Six categories of Data Scientists](http://www.datasciencecentral.com/profiles/blogs/six-categories-of-data-scientists), [16 analytic disciplines compared to data science](http://www.datasciencecentral.com/profiles/blogs/17-analytic-disciplines-compared)." CreationDate="2014-12-06T08:12:06.730" UserId="381" />
  <row Id="2850" PostId="2629" Score="1" Text="This is quite open-ended and opinion based, which is viewed as off topic for StackExchange." CreationDate="2014-12-06T13:29:27.300" UserId="21" />
  <row Id="2851" PostId="2354" Score="0" Text="I'd recommend getting rosetta by going directly go GitHub.  That ensures you get the latest version.     https://github.com/columbia-applied-data-science/rosetta" CreationDate="2014-12-06T23:42:17.337" UserId="5313" />
  <row Id="2852" PostId="2544" Score="0" Text="Thanks a lot, IharS" CreationDate="2014-12-07T04:42:14.747" UserId="5172" />
  <row Id="2853" PostId="2543" Score="0" Text="Hi Sean, I'm just trying to explore the possible options with the data available" CreationDate="2014-12-07T04:45:02.923" UserId="5172" />
  <row Id="2854" PostId="2622" Score="0" Text="The `Data Science Toolkit` is very interesting but not what I am looking for. I am looking for some high performant stream based protocol which allows me to stream (and buffer) data from n data-miners to m data-processors." CreationDate="2014-12-07T11:21:16.863" UserId="5266" />
  <row Id="2855" PostId="2593" Score="0" Text="@Hack-R if it is a more complex protocol which requires some heavy logic I would prefer that a library would be available in Go but I would even more prefer if the library would be a available for other languages too. What do you think of a message queue like [nsq](http://nsq.io)." CreationDate="2014-12-07T11:22:41.060" UserId="5266" />
  <row Id="2856" PostId="2625" Score="0" Text="This is a *pure* python solution?" CreationDate="2014-12-07T11:23:28.320" UserId="5266" />
  <row Id="2857" PostId="2633" Score="1" Text="Install it on your own computer. It has more than a gig right?" CreationDate="2014-12-07T12:00:56.413" UserId="381" />
  <row Id="2858" PostId="2633" Score="0" Text="yep, but my system has only 4GB of RAM. I tried using ubuntu in VM. It's very low." CreationDate="2014-12-07T12:59:35.613" UserId="5172" />
  <row Id="2859" PostId="2637" Score="0" Text="yes in fact i have the same questions in my mind. No other choice left except setting up a physical machine with good amount of memory or creating a large instance in AWS." CreationDate="2014-12-07T13:49:30.050" UserId="5172" />
  <row Id="2860" PostId="2642" Score="2" Text="Could you clarify, in what way the equations should be part of the ML problem or solver? Error gradient backpropagation in neural networks is pretty much entirely the chain rule applied repeatedly to a set of partial differential equations. Would that count?" CreationDate="2014-12-07T22:41:31.310" UserId="836" />
  <row Id="2861" PostId="2643" Score="0" Text="[Spark Cluster on Google Compute Engine](https://greenido.wordpress.com/2014/05/13/spark-cluster-on-google-compute-engine/)" CreationDate="2014-12-07T23:04:13.033" UserId="381" />
  <row Id="2862" PostId="2637" Score="0" Text="Or you could upgrade your computer; memory is cheap." CreationDate="2014-12-07T23:05:40.490" UserId="381" />
  <row Id="2863" PostId="2639" Score="0" Text="thanks. I am familiar with adaboost, and boosting in general. The problem I'm describing has 2 differing qualities though - 1. It is an estimator not classifier we want to get (seems simple to get around), and 2. looking for an online learning method, as all the estimators R_i are online. Would you suggest online boosting? I have no relevant exp, would appreciate some pointers." CreationDate="2014-12-08T07:09:13.367" UserId="5314" />
  <row Id="2866" PostId="2575" Score="0" Text="I have never seen such a thing in Hive." CreationDate="2014-12-08T14:23:45.440" UserId="21" />
  <row Id="2867" PostId="660" Score="0" Text="Is this a single machine cluster? Can you provide the stderr log?" CreationDate="2014-12-08T14:34:52.917" UserId="2723" />
  <row Id="2869" PostId="2654" Score="0" Text="http://prediction.io/" CreationDate="2014-12-09T02:46:24.813" UserId="381" />
  <row Id="2870" PostId="2642" Score="1" Text="By PDE, I meant some set of [equations](http://en.wikipedia.org/wiki/Convection%E2%80%93diffusion_equation) which are generally used to describe physical phenomena and using that as a learning/modeling process. The solution to such equations gives us some sort of error estimation to gauge the strength of these models and thereby, reduce the unpredictability in certain situations." CreationDate="2014-12-09T04:20:40.940" UserId="1131" />
  <row Id="2871" PostId="2643" Score="0" Text="@Emre, I've also looked into that blog, But there are no steps provided for cluster formation. It is single node." CreationDate="2014-12-09T06:20:56.250" UserId="5172" />
  <row Id="2872" PostId="2643" Score="0" Text="You indicated that you wanted 1 slave, so [setting the second parameter to 1](https://github.com/sigmoidanalytics/spark_gce#usage) should do the trick." CreationDate="2014-12-09T06:31:02.437" UserId="381" />
  <row Id="2873" PostId="2648" Score="1" Text="Yes, that would probably be it. I could also add additional weights to some words, that I already know that are informative. Thanks for your help and useful links." CreationDate="2014-12-09T09:33:49.713" UserId="2750" />
  <row Id="2874" PostId="2642" Score="0" Text="@Sidha: That doesn't clarify it for me. Gradient descent, (and the variants of it), is an attempt to find a solution to the minimum of an error function, and *uses* partial differential equations in its formulation. Please clarify, does that count (because if it does, a large percentage of ML could be said to use PDEs)? If you are looking for a machine learning process that uses PDEs in some other way, it needs to be clearer, because simply &quot;uses some PDEs&quot; applies to anything that can be numerically optimised with gradient descent." CreationDate="2014-12-09T10:23:06.047" UserId="836" />
  <row Id="2875" PostId="2651" Score="5" Text="-1: Where have you looked already? Found anything?" CreationDate="2014-12-09T13:59:00.947" UserId="471" />
  <row Id="2876" PostId="2642" Score="1" Text="Gradient descent uses partial derivatives, agreed. What I intended to say was a system of PDE's and not in an optimization problem. Please check the link in the previous comment. This [quora discussion](http://www.quora.com/Are-Differential-Equations-relevant-to-Machine-Learning) mentions some usage in optical flow applications in computer vision. But I am wondering if there are more applications out there using this approach." CreationDate="2014-12-09T15:18:47.340" UserId="1131" />
  <row Id="2877" PostId="2658" Score="0" Text="Do you have any programming skills? There are many ways to do this, I can suggest something hopefully based on something you have used in the past." CreationDate="2014-12-09T20:51:47.973" UserId="3466" />
  <row Id="2878" PostId="2658" Score="0" Text="I have a good knowledge in Java. I have used R for a couple of data mining tasks. Currently I am studying Python for using NLP." CreationDate="2014-12-09T20:53:30.183" UserId="5043" />
  <row Id="2879" PostId="2660" Score="0" Text="This basic strategy would also work if you found an NLP toolkit in another language you know, like Java. I'm just not familiar with those." CreationDate="2014-12-09T22:00:30.657" UserId="3466" />
  <row Id="2880" PostId="2293" Score="1" Text="For outlier detection, have a look at the algorithms in ELKI. That seems to be the most complete collection of outlier detection." CreationDate="2014-12-09T22:37:32.467" UserId="924" />
  <row Id="2881" PostId="2633" Score="2" Text="Talk to your professors and tell them you need a bigger machine to use. Most CS departments will help you out." CreationDate="2014-12-10T01:39:13.597" UserId="3466" />
  <row Id="2886" PostId="2575" Score="0" Text="That answer does not solve the problem :)" CreationDate="2014-12-10T09:57:34.930" UserId="5224" />
  <row Id="2887" PostId="2575" Score="0" Text="Is there a way of implementing a function on my own on Hive or maybe should I use different tool - maybe should I check Apache Spark?" CreationDate="2014-12-10T10:13:32.133" UserId="5224" />
  <row Id="2888" PostId="2668" Score="2" Text="Do you also want to know about Machine Learning toolkits such as e.g. FACTORIE?" CreationDate="2014-12-10T11:40:40.087" UserId="5370" />
  <row Id="2889" PostId="2575" Score="0" Text="Heh, yeah that's why I only commented. What I mean is that, having sniffed around Hive a long time, I've never heard of this, so it's unlikely to exist, but I don't know for sure that this is the answer. Spark does not have it either." CreationDate="2014-12-10T12:11:32.657" UserId="21" />
  <row Id="2890" PostId="2662" Score="0" Text="*facepalm* I meant kNN. XD I will change the title. Sorry for that. Started learning about these things not long ago and I'm still confusing the terms. +1 from me for the &quot;You are mixing up kNN classification and k-means&quot;, which pointed out the mistake I've made in my question!" CreationDate="2014-12-10T12:24:45.520" UserId="5356" />
  <row Id="2891" PostId="2661" Score="0" Text="Sorry, I wrote k-means but meant kNN (see question again but view it as kNN not k-means). Thanks for the fast reply though!" CreationDate="2014-12-10T12:29:17.380" UserId="5356" />
  <row Id="2892" PostId="2546" Score="0" Text="Thanks Laurik. Unfortunately not only numbers, and I don't know also what future messages will be. So, I really need AI." CreationDate="2014-12-10T13:32:29.570" UserId="3024" />
  <row Id="2893" PostId="2668" Score="0" Text="That sounds relevant to my question, feel free to share more." CreationDate="2014-12-10T15:40:50.120" UserId="3466" />
  <row Id="2894" PostId="2675" Score="0" Text="Software/Hardware classification is like the sample task that I tried to work with the categorization. There are several other categories I have on my mind which would take a deep understanding of what is wrong with the products, by reading the customer's case, and tag the appropriate category. I started reading NLPTK using python but I would like to know the kind of functions that I should be looking for to address this case" CreationDate="2014-12-10T15:44:21.057" UserId="5043" />
  <row Id="2895" PostId="2671" Score="0" Text="Yeah, I actually updated only &quot;k-means-&gt;kNN&quot;. Forgot to change the rest but you managed to see through. I am actually interested exactly in the last paragraph of your answer - EXACT same distance (no matter how probable that is in real-life data evaluation). I don't get &quot;But you would still simply pick the class that is most prevalent among that group of observations.&quot; What criterion/criteria do I have to observe is such a situation? PS: I'll update my question again so that it fits &quot;kNN&quot; exactly." CreationDate="2014-12-10T16:16:23.637" UserId="5356" />
  <row Id="2896" PostId="2662" Score="0" Text="I've updated the answer to also talk about kNN classification." CreationDate="2014-12-10T16:21:38.553" UserId="924" />
  <row Id="2897" PostId="2662" Score="0" Text="Oh, really cool. Thanks. Exactly what I wanted to know! Sorry again for asking about the wrong thing. :D" CreationDate="2014-12-10T16:24:43.320" UserId="5356" />
  <row Id="2898" PostId="2676" Score="0" Text="Ok thanks, that's somehow more clear, especially for the XOR. For the gravity thing, I don't know if it's a good example, I guess machine learning doesn't really apply as we know how gravity works we don't really need machine learning, right? The question was more about teaching something with a small array of inputs, and apply it to a larger array." CreationDate="2014-12-10T16:26:27.383" UserId="5369" />
  <row Id="2899" PostId="2675" Score="0" Text="This isn't a simple matter of looking for magic functions. What you want to do is build a classifier using supervised machine learning. These are the steps... 1. manually annotate a sample of your data as a training set, 2. extract features from your data to train on (for text, this might be something like ngrams), 3. build the classifier model using a machine learning library, 4. apply the classifier model to new data. Some libraries like Stanford Classifier will help you with steps 2 &amp; 3." CreationDate="2014-12-10T17:27:06.407" UserId="819" />
  <row Id="2900" PostId="2679" Score="0" Text="Thanks! Interestingly enough they chose to use multiple Hinton Diagrams to plot their weights. I still think it's hard to interpret as soon as you have too many layers/connections but it's good to see it in action at least." CreationDate="2014-12-10T20:52:55.593" UserId="5316" />
  <row Id="2903" PostId="2678" Score="0" Text="Prior to training the classifier, I do format the data in tuples of `({tokenized content}, category)`. As I have the training set websites in a database and already categorized, it is not a problem. The labels that the classifier will be able to apply will only be those it has seen from the annotated training set, correct?" CreationDate="2014-12-11T00:18:36.850" UserId="5199" />
  <row Id="3904" PostId="2682" Score="1" Text="Hi, I do have the same query regarding scaling issue of Neo4j. And now I decided to go for Titan DB. Titan is an open source distributed graph database build on top of Cassandra." CreationDate="2014-12-11T07:18:18.570" UserId="5091" />
  <row Id="3905" PostId="2678" Score="0" Text="Re: &quot;The labels that the classifier will be able to apply will only be those it has seen from the annotated training set, correct?&quot; Correct. In supervised learning, the classifier won't be able to create new/unseen categories. If you want to do that, you should look into something like clustering or topic modeling." CreationDate="2014-12-11T11:36:17.293" UserId="819" />
  <row Id="3906" PostId="2678" Score="0" Text="Thank you very much for the information! As you answered my question as well, I'll be accepting this as the answer." CreationDate="2014-12-11T12:10:29.753" UserId="5199" />
  <row Id="3907" PostId="3685" Score="1" Text="Very interesting idea! Really, visualizing a deep net like a social network is something I've not thought about. The main difference between the models is that these graphs code information in their nodes while neural networks do it within their connections. But it could be modified, e.g. by setting the social network node values to the outgoing connections weights of the neural network." CreationDate="2014-12-11T12:49:38.227" UserId="5316" />
  <row Id="3908" PostId="3685" Score="0" Text="I'm glad that you like the idea. Feel free to upvote/accept. And don't forget to review SoNIA software, with link to which I recently updated my answer. Finally, if you use (or plan to use) R, here's another relevant interesting info for you: http://sna.stanford.edu/rlabs.php." CreationDate="2014-12-11T12:57:07.617" UserId="2452" />
  <row Id="3909" PostId="2678" Score="0" Text="My apologies for bringing this back after accepting the above answer, but I reckoned that I would have better odds of getting an answer to my update if I asked you directly. So as to avoid long comments, I would greatly appreciate it if you could take a look at my edit in the original post." CreationDate="2014-12-11T16:28:45.077" UserId="5199" />
  <row Id="3912" PostId="3689" Score="1" Text="This is great info about Scala in general, but I'm looking for information about specific data science tools or frameworks which have Scala integration, and why Scala is a good language for those tools to use." CreationDate="2014-12-11T20:31:54.600" UserId="3466" />
  <row Id="3914" PostId="3691" Score="0" Text="thank you for your input, if you have an idea what would be better then NB, please let me know." CreationDate="2014-12-12T00:17:13.170" UserId="6387" />
  <row Id="3916" PostId="3688" Score="0" Text="@fordprefect Multinomial Naive Bayes uses a multinomial distribution for the probabilities of some feature given a class: $p(f_i|c)$. The OP wants a classifier to manage multiple outputs as TheGrimmScientist described." CreationDate="2014-12-12T01:03:52.467" UserId="4621" />
  <row Id="3917" PostId="3691" Score="1" Text="the &quot;existing solutions&quot; section of [this slide deck](http://www.slideshare.net/sympapadopoulos/a-graphbased-clustering-scheme-for-identifying-related-tags-in-folksonomies) holds everything I would know to reply with, plus more (assuming you're not needing specifically a classifier and just want a way to use tags).  I hope it's useful to you." CreationDate="2014-12-12T01:48:22.737" UserId="6391" />
  <row Id="3918" PostId="3691" Score="0" Text="@TheGrimmScientist Wouldn't it be reasonable to use the first approach? If you have a vector of features f1, f2, f3 and let's say, 3 labels for this vector, we can partition that into 3 vectors (all containing the same features f1, f2, f3) with different labels as outputs. Then it is possible to use Naive Bayes as usual. I'm not sure if that is what you had in mind." CreationDate="2014-12-12T02:57:27.040" UserId="4621" />
  <row Id="3919" PostId="3693" Score="1" Text="Why collect the names at all?" CreationDate="2014-12-12T08:33:39.923" UserId="381" />
  <row Id="3920" PostId="3690" Score="0" Text="Writing a conversion script seems the most straightforward way to do this." CreationDate="2014-12-12T16:18:43.763" UserId="227" />
  <row Id="3921" PostId="1243" Score="0" Text="Is it possible to run A/B tests in your situation? That could give you some training data, or even just a general idea." CreationDate="2014-12-12T21:54:47.650" UserId="3466" />
  <row Id="3923" PostId="3690" Score="0" Text="@AmirAliAkbari - Have you done something like that with R and OpenCV? If so, can you provide an example in the form of an answer? If not, can you point out some promising approaches for writing such a conversion script?" CreationDate="2014-12-12T22:52:44.337" UserId="6390" />
  <row Id="3924" PostId="37" Score="5" Text="You're talking about unstructured data, not big data.  Unstructured data usually leads to NoSQL solutions and big data in application, but they are still different." CreationDate="2014-12-12T22:55:16.847" UserId="6391" />
  <row Id="3925" PostId="2650" Score="0" Text="It would be nice if you could provide us with an example of gradient application in data mining/machine learning. This would make your answer self contained and more useful to others." CreationDate="2014-12-12T23:41:48.410" UserId="84" />
  <row Id="3926" PostId="2631" Score="2" Text="If you have such a question, you probably have an intuition on why using clustering for outlier detection would be a nice strategy. If you add such information to your post, it shall definitely increase the visibility and the interest from others in answering your question." CreationDate="2014-12-12T23:43:45.277" UserId="84" />
  <row Id="3928" PostId="2586" Score="0" Text="Please, provide us with the main (and just the main!) topics discussed in each subject. This would make your post self contained, instead of forcing others to flick through your course webpages." CreationDate="2014-12-12T23:49:42.287" UserId="84" />
  <row Id="3929" PostId="2579" Score="0" Text="I Found this blog post very useful : http://shanon-shanghai.blogcn.com/articles/mahout-based-recommender-system-introduction.html#5-3" CreationDate="2014-12-11T06:16:59.327" UserId="5091" />
  <row Id="3930" PostId="2667" Score="0" Text="[Bulbflow- A python interface to graph databases](http://bulbflow.com/overview/); [Bulb Docs](http://bulbflow.com/docs/); [A snarky answer](http://bit.ly/1zvehXv). Note this question has been solved already on [stackoverflow.com](http://stackoverflow.com/questions/24990607/bulbs-python-connection-to-a-remote-titandb-rexster), which is where it belongs because it specifically relates to programming." CreationDate="2014-12-10T14:13:56.900" UserId="5247" />
  <row Id="3931" PostId="2676" Score="0" Text="There are different strategies for neural networks to work with unknown data, and there are also different strategies to let a neural network grow. But all of them wouldn't really fit to the question." CreationDate="2014-12-13T09:00:51.660" UserId="3132" />
  <row Id="3932" PostId="3693" Score="0" Text="Records need to be unique, and everybody knows their own name (that sounds sarcastic, but it's just simplicity)." CreationDate="2014-12-13T13:43:37.490" UserId="2742" />
  <row Id="3933" PostId="3694" Score="0" Text="I wasn't aware of those alternative naming schemes. Thank you. On the conflict issue, a high vote answer on [SO suggests](http://stackoverflow.com/questions/297960/hash-collision-what-are-the-chances) this about SHA-1, &quot;To address the birthday paradox, a database with 10^18 (a million million million) entries has a chance of about 1 in 0.0000000000003 of a collision.&quot;" CreationDate="2014-12-13T13:49:43.550" UserId="2742" />
  <row Id="3934" PostId="2625" Score="0" Text="No, using mongodb for caching.  I think Mongodb is written in Java if that's what you mean?" CreationDate="2014-12-13T16:40:30.897" UserId="5247" />
  <row Id="3935" PostId="3699" Score="0" Text="What algorithm, meaning, what loss function are you regularizing?" CreationDate="2014-12-13T20:32:15.557" UserId="21" />
  <row Id="3936" PostId="2659" Score="0" Text="It might have been better to make a new question since now the answers don't match your new, reasonably different question." CreationDate="2014-12-13T20:33:44.490" UserId="21" />
  <row Id="3937" PostId="3701" Score="0" Text="I get the point, but just to be clear, it sounds like you're mixing two options. The first is the option to take student ID together with the name, which would be likely unique and somewhat obscure to anyone who would obtain the data from the internet. Plus a second option to take this encoded ID+Lname and has that as well. Plus a further measure to use a salt of their last name. In other words, pre-computer I imagine the ID+Lname could be an acceptable obfuscation?" CreationDate="2014-12-14T01:17:24.630" UserId="2742" />
  <row Id="3938" PostId="3701" Score="0" Text="No, I did not say to take the PIN and the name; just the PIN, as it is already unique. Using the last name as the salt was just a suggestion; you can use any user-specific information." CreationDate="2014-12-14T01:36:44.303" UserId="381" />
  <row Id="3939" PostId="3702" Score="0" Text="Before I suggest the answer directly -- what did you find that makes you think it's discriminative? are you sure k-NN always involves Bayes's theorem or priors?" CreationDate="2014-12-14T11:26:50.307" UserId="21" />
  <row Id="3940" PostId="3702" Score="0" Text="The problem is that I'm not sure about the definition of these two types classifiers, my book isn't clear. That being said, I think that the generative classifier must be able to generate data poins as well, and to do that it learns the joint probability $p(x,C_{k})$. The discriminative classifier does not learn the joint probability, and so does KNN. While I see that it does use Baye's theorem, it only computes the posterior. Tell me if this is correct or not, but also please tell me the definitive difference between the two classifiers because I'm confused." CreationDate="2014-12-14T11:32:40.510" UserId="6419" />
  <row Id="3941" PostId="3702" Score="0" Text="Also from what I understand in the book, it always uses Baye's theorem to compute the posteriors. The class priors $p(C_{k})$ are given as just $\frac{N_{k}}{N}$ where the numerator is the number of points in class k, and N is the total number of points in our set." CreationDate="2014-12-14T11:51:57.360" UserId="6419" />
  <row Id="3942" PostId="2678" Score="1" Text="RE: &quot;would I have to feed the entirety of my data into TF-IDF at once?&quot; Yes, that's how it works. RE: &quot;I have been looking at the scikit-learn TfidfVectorizer to do this, but I am a bit unsure as to its use as examples are pretty sparse.&quot; Here's an example I wrote: https://github.com/charlieg/A-Smattering-of-NLP-in-Python#building-a-term-document-matrix -- it's probably best if you use a corpus of documents as input, rather than some dict+string tuple you created." CreationDate="2014-12-14T14:51:04.237" UserId="819" />
  <row Id="3943" PostId="3699" Score="0" Text="I'm using the logistic loss function." CreationDate="2014-12-14T15:11:50.157" UserId="6418" />
  <row Id="3944" PostId="3699" Score="1" Text="Take a look at [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV) which is generally used for parameter estimation to improve score. Hope it helps." CreationDate="2014-12-15T02:51:13.053" UserId="1131" />
  <row Id="3945" PostId="3703" Score="1" Text="Unfortunately, it looks as if `r-opencv` is relatively unmaintained (last updates 9 months ago, followed by 5 years ago). The PMML option and associated link appear to be great resources. Thanks!" CreationDate="2014-12-15T02:53:26.640" UserId="6390" />
  <row Id="3946" PostId="3703" Score="0" Text="I see. Well, depending on how you look at it, this might be an opportunity to revive the project (of course, if you're interested and can devote some time to it). I'm glad the other information is helpful to you. Thanks for accepting the answer, which you can also upvote, if you feel generous today :-). Good luck with your project! And keep an eye on [Azure Machine Learning](http://azure.microsoft.com/en-us/services/machine-learning), which I haven't included in my answer, as I consider this offering currently 1) still too raw; 2) not cost-effective; 3) not flexible enough (vs. pure R-based)." CreationDate="2014-12-15T03:53:06.580" UserId="2452" />
  <row Id="3948" PostId="2509" Score="0" Text="Thanks a lot. I didn't knew alpha-algorithms. I'll check in that direction." CreationDate="2014-12-15T12:17:05.737" UserId="3024" />
  <row Id="3951" PostId="2609" Score="1" Text="Is it? That's quite interesting question." CreationDate="2014-12-15T22:12:04.203" UserId="5224" />
  <row Id="3953" PostId="3711" Score="1" Text="Maybe Laplace Smoothing is what you are looking for? http://en.wikipedia.org/wiki/Additive_smoothing" CreationDate="2014-12-16T18:59:17.717" UserId="3132" />
  <row Id="3955" PostId="1190" Score="1" Text="This question appears to be off-topic because it is about statistics and should be on crossvalidated, not stack overflow!" CreationDate="2014-12-17T10:23:51.570" UserId="471" />
  <row Id="3958" PostId="3707" Score="1" Text="Link-only answers are discouraged. Can you expand on the content in this link that you think is relevant?" CreationDate="2014-12-17T13:49:50.817" UserId="21" />
  <row Id="3959" PostId="156" Score="0" Text="Freebase is [closing down](https://groups.google.com/forum/#!msg/freebase-discuss/s_BPoL92edc/Y585r7_2E1YJ) and its database will move to [Wikidata](https://www.wikidata.org) soon." CreationDate="2014-12-17T14:39:02.783" UserId="43" />
  <row Id="3962" PostId="3718" Score="0" Text="You have mentioned neither *types* of data you collect, nor its volume's *order of magnitude*. This information would be helpful in providing a more targeted advice." CreationDate="2014-12-18T02:35:11.440" UserId="2452" />
  <row Id="3963" PostId="3719" Score="1" Text="Also consider Graphlab (python based): http://graphlab.com/products/create/overview.html&#xA;Here's a good blog post about it as well:&#xA;http://bugra.github.io/work/notes/2014-04-06/graphs-databases-and-graphlab/&#xA;I can't help you with the Titan vs oriebtDB discussion though.  Hopefully someone will chime in with that." CreationDate="2014-12-18T18:03:46.183" UserId="375" />
  <row Id="3966" PostId="3730" Score="2" Text="This one should go to DataScience as it is about managing large scale projects." CreationDate="2014-12-19T16:06:58.140" UserId="1237" />
  <row Id="3967" PostId="3730" Score="0" Text="I agree with @StasK: this question should be migrated to either Data Science or, alternatively, to StackOverflow (I just flagged it)." CreationDate="2014-12-19T16:37:38.783" UserId="2452" />
  <row Id="3968" PostId="3734" Score="0" Text="I flagged this question for migration to Data Science SE site, as I think it's not a statistics question, but a data mining (data science) one." CreationDate="2014-12-20T08:10:50.377" UserId="2452" />
  <row Id="3969" PostId="3733" Score="1" Text="So you want to estimate the function and its extrema too? Or you have a black box operating as a function which you need to strategically query to determine the extremum?" CreationDate="2014-12-20T20:42:59.690" UserId="381" />
  <row Id="3970" PostId="3719" Score="0" Text="Also possible to use Spark and GraphX" CreationDate="2014-12-20T21:00:53.057" UserId="3466" />
  <row Id="3971" PostId="3733" Score="0" Text="I have a black box operating as a function and I need to strategically query it in order to determine the optimum. Of course, I am not looking for the global maximum but anything that is smarter than a grid search over a space of parameters." CreationDate="2014-12-20T22:16:39.243" UserId="847" />
  <row Id="3972" PostId="3735" Score="1" Text="Thank you Aleksandr. I didn't consider it, but there are factors instead of characters in the data set. I will coerce as character and run again!" CreationDate="2014-12-21T06:04:58.283" UserId="6506" />
  <row Id="3973" PostId="3735" Score="0" Text="@Chris: My pleasure, Chris! Would love to hear about the results. Don't forget to accept/upvote, if my answer is helpful :-)." CreationDate="2014-12-21T06:57:55.040" UserId="2452" />
  <row Id="3974" PostId="1253" Score="2" Text="I would suggest not to use NLP and ML together in such context, as if they were comparable entities. They are IMHO two distinctly separate fields. Despite the fact that NLP uses a lot of ML approaches, methods, algorithms and tools, NLP is an applied field, whereas ML is a (more) generic one." CreationDate="2014-12-21T10:34:29.063" UserId="2452" />
  <row Id="3975" PostId="3739" Score="0" Text="The work is (for now) academic, rather than purely practical. I may do some forecasting very late on or in future, but I'm more interested in exploring the past data for now. The clustering is a goal in and of itself, as well as some ideas I want to explore past that point." CreationDate="2014-12-22T05:24:51.753" UserId="5246" />
  <row Id="3976" PostId="3739" Score="0" Text="Sorry, hit enter prematurely. I've looked into autocorrelation to some extent and ran it on a subset of my data a whole ago but it wasn't really clear to me what I could get out of it. The data is pretty noisy. The seasonality patterns are sometimes pretty obvious on visualisation, but inexact in their timings - so I may be looking for similar patterns but not on a nice, even schedule. I was told that autocorrelation was likely to be problematic on such data, but happy to have another look if there's value in it. I don't want to *just* find seasonality, but understanding it is a goal." CreationDate="2014-12-22T05:47:47.340" UserId="5246" />
  <row Id="3977" PostId="3739" Score="0" Text="Work through that tutorial at least up to and including 2.5.  It uses R which is especially good for your academic environment.  It will teach you autocorrelation which sounds like exactly what you're looking for (can't tell if it wasn't a fit because you didn't know what you were looking at, or the data is actually too noisy).  If noise is the issue, exponential smoothing is one way to help with that, which will be taught as a part of the holt-winters model.  Even if all of that doesn't give you the answer, it will certainly make your next step way clearer." CreationDate="2014-12-22T06:45:44.040" UserId="6391" />
  <row Id="3979" PostId="3738" Score="0" Text="+1 Very nice question, and it is great to see so much enthusiasm! I think you could nail down your question a little bit, so it's more inviting for others to read, and then give you an answer." CreationDate="2014-12-22T09:56:32.873" UserId="84" />
  <row Id="3981" PostId="3738" Score="0" Text="@Rubens Thanks! I'll re-work it when I'm home this evening, I can see where it'd be useful to include some more information about how I've gotten to this point and why. I was worried about it getting too long, but I'll separate out the background and question a bit more to avoid it getting unreadable." CreationDate="2014-12-22T10:04:44.587" UserId="5246" />
  <row Id="3982" PostId="3742" Score="0" Text="R is a good way to go! Can you please add a data sample and desired output you want to get?" CreationDate="2014-12-22T14:52:17.677" UserId="97" />
  <row Id="3983" PostId="3742" Score="0" Text="Columns are separated by commas and lines by semicolon:&#xA;ID, timeframe, fruit_amt, veg_amt;&#xA;4352, before, 0.25, 0.75;" CreationDate="2014-12-22T15:17:29.737" UserId="6491" />
  <row Id="3984" PostId="3742" Score="0" Text="Trying again.  Columns are separated by commas and lines by semicolon:&#xA;ID, date, timeframe, fruit_amt, veg_amt;&#xA;4352, 05/23/2013, before, 0.25, 0.75;&#xA;5002, 05/24/2014, after, 0.06, 0.25;&#xA;4352, 04/16/2014, after, 0, 0;&#xA;4352, 05/23//2013, after, 0.06, 0.25;&#xA;5002, 05/24/2014, before, 0.75, 0.25;&#xA;I want to have all the observations for a single ID (ex, 4352 or 5002) on one row (making the data set much wider).  My data set is much wider than this to begin with (I've got about 50 different columns per current observation) but this is the basic idea." CreationDate="2014-12-22T15:23:09.600" UserId="6491" />
  <row Id="3985" PostId="3742" Score="0" Text="You could better include it into your post and format in proper way..." CreationDate="2014-12-22T15:40:50.030" UserId="97" />
  <row Id="3988" PostId="3746" Score="0" Text="thank you very much!" CreationDate="2014-12-22T19:35:16.133" UserId="3132" />
  <row Id="3989" PostId="3746" Score="0" Text="You are welcome." CreationDate="2014-12-22T19:39:25.617" UserId="3466" />
  <row Id="3990" PostId="3723" Score="1" Text="Thanks so much Aleksandr Blekh &amp; Nitesh for your insight. I'll definitely look into these options. One thing i should have been more clear about is that i'm capturing live data from a laser-micrometer, sampling at about 60 samples per second.  Right after i asked this question, I found a software called  KST (https://kst-plot.kde.org/). It works nicely except that it isn't stable and isn't particularly easy to pull older data, zoom in/out, etc." CreationDate="2014-12-22T23:13:29.220" UserId="6469" />
  <row Id="3991" PostId="1081" Score="0" Text="@mateuz I think you're saying the same thing that I am saying, just in a different way. Data is the approximation of some element which we have decided to measure in some way." CreationDate="2014-12-23T00:28:37.500" UserId="3152" />
  <row Id="3992" PostId="3723" Score="0" Text="@ClaytonPipkin: You're very welcome. I'm glad to help. Don't forget to upvote both answers and accept the best, if satisfied. KST looks interesting, but keep in mind that it's an application, hence, it doesn't have a native ability to visualize data for the Web." CreationDate="2014-12-23T00:38:24.840" UserId="2452" />
  <row Id="3995" PostId="3751" Score="2" Text="Deep Learning is highly related to the theory of neural networks, so it would be good to start learning the basics of artificial neural networks first. See also http://datascience.stackexchange.com/questions/2651/deep-learning-basics" CreationDate="2014-12-23T09:43:34.677" UserId="3132" />
  <row Id="3996" PostId="3745" Score="0" Text="This is too open-ended to be a good question. Instead of asking for recommendations, can you specify more about what you want to know, what you know so far, and what your question is?" CreationDate="2014-12-23T15:41:27.437" UserId="21" />
  <row Id="3997" PostId="3751" Score="2" Text="This is probably too vague to make a good question. Add some detail about what your background is, what specifically about AI you're wondering if you need to know, etc." CreationDate="2014-12-23T15:42:10.027" UserId="21" />
  <row Id="3999" PostId="3751" Score="0" Text="I am a fresh graduate from a computer science program and have taking classes in algorithm and AI. To my understanding deep learning is a branch under AI, we were taught the basic concept of AI but I was wondering which are should I work on mastering and where should I move on to ." CreationDate="2014-12-24T02:28:02.930" UserId="6536" />
  <row Id="4002" PostId="3751" Score="2" Text="https://www.coursera.org/course/neuralnets" CreationDate="2014-12-25T10:27:24.407" UserId="2752" />
  <row Id="4003" PostId="3761" Score="0" Text="thank you @Javierfdr , I would have a question:&#xA;Do you suggest the use of genetic algorithm after selecting the number of hidden neurons (and so for start the training with the selected architecture) or for each hidden neuron number trial, in order to have a better starting point before select the hidden neuron number?" CreationDate="2014-12-25T20:37:09.897" UserId="6559" />
  <row Id="4004" PostId="3739" Score="0" Text="I had a read through the tutorial, but it mostly goes over things I already know. I'm actually working in Python and I'm a bit too far in to things to switch to R, although I've intended to grab rpy at some point in case there were some things I couldn't find in any Python libraries. I've re-written my question in case it helps any - like I say, the clustering is a goal in and of itself, I'm not looking for an entirely different direction to go in. I'm afraid the tutorial doesn't really answer my question." CreationDate="2014-12-25T23:37:03.877" UserId="5246" />
  <row Id="4005" PostId="3764" Score="0" Text="It seems that `scikit-learn` currently does not offer a DTW implementation, hence my reference to the older `mlpy`." CreationDate="2014-12-26T10:11:18.890" UserId="2452" />
  <row Id="4006" PostId="3738" Score="0" Text="It may not be a &quot;pure statistics&quot; question but it needs a pure statistics answer. You will struggle until you can think about it in pure statistics terms." CreationDate="2014-12-26T11:02:20.280" UserId="471" />
  <row Id="4007" PostId="3761" Score="0" Text="You can do both @Daniel, it depends on what can you actually compute. The input optimization with GA will tend to make a specific architecture of NN to work better. So if you can do it for every architecture you are trying it you will have more specific results to analyze. So if your final methodology will be: Optimize initial weights with a GA and use them to a given NN architecture, then yes you will need to do it for each NN to test which works better with this hybrid approach. Please notice that the objective function of the GA will be the calculation of the whole NN for a given input." CreationDate="2014-12-26T15:42:28.977" UserId="5143" />
  <row Id="4008" PostId="3761" Score="0" Text="Also notice @Daniel that if you are using binary input data you must look for Genetic Algorithms, and if using not binary input data then Evolutionary Algorithms will work better (based in the same principle but better architected for non input data)" CreationDate="2014-12-26T15:45:22.660" UserId="5143" />
  <row Id="4009" PostId="3761" Score="0" Text="Thank you @Javierfdr ! I have performed my test, in the question now there is the result. Can you confirm/comment my deduction? Thanks" CreationDate="2014-12-26T20:43:46.460" UserId="6559" />
  <row Id="4010" PostId="3761" Score="0" Text="@Daniel in general it looks like a good solution. Both 12 neurons and 18 neurons give you a low difference error between the sets. I would chose 12 neurons because it will tend to overfit less than a higher number of neurons. Also, are you training the network long enough? You might use a sort of early stopping to stop learning when a certain threshold of the difference of learnings in two consecutive steps is not surpassed. Look at these nice rule of thumbs for picking number of hidden neurons http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-10.html" CreationDate="2014-12-26T23:30:06.450" UserId="5143" />
  <row Id="4011" PostId="3767" Score="0" Text="Thank you Max, it works fine now" CreationDate="2014-12-27T12:46:10.343" UserId="6572" />
  <row Id="4012" PostId="3764" Score="1" Text="A number of these are resources I've been looking at - I've implemented a modified version of the work in points 2 and 4, for instance - so we're probably on the same-ish page now. And the vast majority of what I know is based on Eamonn Keogh's papers or articles based on them. But there are some here I hadn't read, and the one about bike share time series clustering is interesting - thanks! I'm not seeing anything that specifically answers my question, but do point it out if I've missed something while reading." CreationDate="2014-12-27T13:43:28.367" UserId="5246" />
  <row Id="4013" PostId="3764" Score="1" Text="Also, if you're still finding this interesting, Keogh's papers are really worth a read. They're surprisingly easy to read and quite practical given the focus on using many data sets, and providing enough information that someone could re-create all experiments. The most recent one is interesting,and is what I was working my way through when I got sidelined by my question. http://www.cs.ucr.edu/~eamonn/selected_publications.htm" CreationDate="2014-12-27T13:46:44.573" UserId="5246" />
  <row Id="4014" PostId="3738" Score="0" Text="@Spacedman - I welcome answers in whatever manner people feel is the best way to answer it, with the caveat that I may have further questions if the answer is heavy on formulas or references to statistical concepts that I don't understand yet." CreationDate="2014-12-27T13:55:40.887" UserId="5246" />
  <row Id="4015" PostId="3764" Score="1" Text="@JoDouglass: You're welcome! I didn't intend to answer your question directly (due to my limited knowledge of the topic), but hoped that it would be helpful, which appears to be the case. Thank you for nice comments and the reference - I will browse the papers and try to get a better idea. There is so much to learn, it's overwhelming a bit." CreationDate="2014-12-27T16:05:26.760" UserId="2452" />
  <row Id="4016" PostId="3764" Score="1" Text="Overwhelming is right, I was kicking myself for choosing this topic for a while! I feel like I'm getting there, though - and it's been really interesting to learn about. I have a number of things up and running as sort of rough versions of what I need to do, and I think it's more about figuring out how to process my data before running it through my models, now. That bike share link is interesting to me as it's the first I've seen discussing averaging of time series since reading the recent Keogh paper I mentioned." CreationDate="2014-12-27T16:30:37.450" UserId="5246" />
  <row Id="4017" PostId="3764" Score="1" Text="@JoDouglass: When I said &quot;overwhelming&quot;, I meant the whole data science domain (including AI/ML and statistics, specifically). I'm yet to find a **resource**, which presents a _high-level_ discussion of various _approaches_ and/or _methods_ as **themes**, integrated into a _comprehensive_, yet _parsimonious_, **framework**." CreationDate="2014-12-27T17:09:15.957" UserId="2452" />
  <row Id="4018" PostId="3764" Score="1" Text="That's fair! I have no background in it myself, and feel like I jumped into the deep end by starting with time series. I would have dearly loved a resource like that when I started out a year ago - and I still would now." CreationDate="2014-12-27T17:43:33.170" UserId="5246" />
  <row Id="4019" PostId="3764" Score="0" Text="@JoDouglass: I just posted a corresponding question [here](http://datascience.stackexchange.com/q/3769/2452) and [on Quora](http://qr.ae/zb6WL)." CreationDate="2014-12-27T17:55:03.700" UserId="2452" />
  <row Id="4026" PostId="2629" Score="0" Text="Here's my opinion:  http://thegrimmscientist.com/2014/05/05/what-is-data-science/" CreationDate="2014-12-28T05:30:05.677" UserId="6391" />
  <row Id="4028" PostId="3772" Score="0" Text="When you say you're using a moving window, do you mean you are performing some manipulation of the data within that window or just using the previous 6 values?" CreationDate="2014-12-28T20:15:35.377" UserId="4621" />
  <row Id="4029" PostId="3772" Score="0" Text="Yes exactly, and the previous value of the serie I am trying to predict." CreationDate="2014-12-28T22:54:46.593" UserId="303" />
  <row Id="4030" PostId="3773" Score="0" Text="The question about previous value was overall usefullness. If the previous value is a function of past rates, how is this different from just taking a bigger window." CreationDate="2014-12-28T22:57:06.720" UserId="303" />
  <row Id="4031" PostId="3773" Score="0" Text="For the algorithm: regularized polynomial regression (Followed Andrew Ng course on Coursera)" CreationDate="2014-12-28T23:03:44.080" UserId="303" />
  <row Id="4032" PostId="3773" Score="0" Text="For the time-serie parts: I (more or less) know how to pre-process data. I was wondering if there is algorithm for time series and not a specific number of features." CreationDate="2014-12-28T23:06:45.067" UserId="303" />
  <row Id="4033" PostId="3773" Score="0" Text="Great, I can comment here! Apologies for not being able to ask you questions in the comments before. The time series that you're predicting - does it have any historic data, or is it made up purely of your predictions?" CreationDate="2014-12-28T23:23:49.420" UserId="5246" />
  <row Id="4034" PostId="3773" Score="0" Text="@Imorin - I would agree that if you're only looking at the 6 previous values in a time series, you're possibly not using all available information. Have you had a look at Rob Hyndman's blog on forecasting? There's a lot about forecasting and time series, and there's a post from Oot 4 2010 where he discusses cross validation as a method to check on the usefulness of variables in a forecasting model. Time series are mentioned at the end. There are also often helpful discussions in the comments.  http://robjhyndman.com/hyndsight/forecasting/" CreationDate="2014-12-29T00:30:54.210" UserId="5246" />
  <row Id="4038" PostId="3774" Score="0" Text="I strongly suspect non-linear dependencies. That is why I wanted to try Machine Learning on multiple features, their respectives powers and cross products. Your answer is still very helpfull as I just understood that the machine learning process I wanted to do here is strictly the same as fitting a model to my time-serie. (wich I am very familiar with to say the least)" CreationDate="2014-12-29T14:25:10.170" UserId="303" />
  <row Id="4039" PostId="449" Score="2" Text="Keep in mind that Sean Owen is a co-author of the new O'Reilly book on Spark. :-)" CreationDate="2014-12-29T17:05:50.890" UserId="3466" />
  <row Id="4041" PostId="3774" Score="0" Text="Sure. If there is a non-linear dependency between series, then a feature based approach is a good approach." CreationDate="2014-12-29T20:12:19.303" UserId="4621" />
  <row Id="4048" PostId="3783" Score="0" Text="Before anything, to model this as a &quot;data science&quot; problem, you need to define a metric to determine how &quot;close&quot; the encryption is to the original text. I am not sure this is feasible, since the computing time required to decrypt/encrypt isn't really a good metric for how good/bad an encryption algorithm is." CreationDate="2014-12-30T14:14:02.160" UserId="6529" />
  <row Id="4051" PostId="3783" Score="0" Text="You need a model for the encryption algorithm, then you set about estimating the model parameters." CreationDate="2014-12-30T19:10:48.970" UserId="381" />
  <row Id="4053" PostId="3783" Score="0" Text="Unless you are working with weak or broken encryption, then the amount of processing and data you would need to differentiate between two modern schemes is likely to be prohibitive. Most of the pad generating algorithms are statistically random, and very hard to differentiate from truly random sources without knowing the decryption key." CreationDate="2014-12-30T21:25:39.883" UserId="836" />
  <row Id="4054" PostId="3740" Score="0" Text="It's hard to say about Gaussian NB and tf*idf, but I have successfully used multinomial (and sometimes binomial one). Multinomial distribution is much easier to reason about, and it clearly maps to text classification task (value of a feature = number of occurrences of some word in text, probability to get specific text ~ multinomial distribution over all words in this text). Imbalance in dataset is one of the most important components in NB classifier (recall `P(C)`). You _can_ omit it, if it improves results, but it won't be NB anymore, but more like MLE." CreationDate="2014-12-30T22:58:11.760" UserId="1279" />
  <row Id="4055" PostId="2382" Score="0" Text="This question is more appropriate for opendata.stackexchange.com" CreationDate="2014-12-31T12:39:30.103" UserId="21" />
  <row Id="4056" PostId="3788" Score="0" Text="Well even if you do it in Python or R you will have to change the data into coordinates. What are you trying to do? Try gis.stackexchange.com for more specialised advice in this." CreationDate="2014-12-31T17:00:27.617" UserId="471" />
  <row Id="4057" PostId="3792" Score="0" Text="Know it is at least being researched (in case of Amazon) don't know state of implementation." CreationDate="2015-01-01T14:35:24.813" UserId="1256" />
  <row Id="4061" PostId="3797" Score="0" Text="You are likely querying the last known output of batch, not running a batch process." CreationDate="2015-01-03T18:52:57.727" UserId="21" />
  <row Id="4062" PostId="3797" Score="0" Text="OK. so how do I merge the lats known output of batch with the streaming data stored inside spark discrete RDD?" CreationDate="2015-01-03T19:02:11.967" UserId="6644" />
  <row Id="4063" PostId="3804" Score="2" Text="I think that the optimal solution depends on various factors, such as the data volume, project's performance requirements and intended types of analysis." CreationDate="2015-01-04T05:03:15.673" UserId="2452" />
  <row Id="4064" PostId="3804" Score="0" Text="I'm crawling around 5,000 posts or so, and intend to perform some unsupervised topic modelling on them." CreationDate="2015-01-04T13:50:06.517" UserId="6660" />
  <row Id="4065" PostId="3808" Score="1" Text="Why don't you have a validation set? Is it just because the implementation doesn't create one for you? Not knowing a lot about bayesian regularisation, my naive answer would be &quot;so set aside a validation or test set manually&quot; . . . in fact, as long as you are not *tuning* your params using the set, but performing a single pre-decided comparison of generalisation success, then I would call that a &quot;test set&quot;" CreationDate="2015-01-04T14:36:15.570" UserId="836" />
  <row Id="4066" PostId="3808" Score="0" Text="maybe I can compare the validation mse(with the name 'validation' because it is used for parameters tuning) of the trainlm with the test mse (called test because the paramerets are tuned in another way)?" CreationDate="2015-01-04T16:34:44.153" UserId="6559" />
  <row Id="4067" PostId="3808" Score="0" Text="@NeilSlater I forgot to tag" CreationDate="2015-01-04T17:33:19.287" UserId="6559" />
  <row Id="4068" PostId="3808" Score="0" Text="I would not know how to assess whether it was safe to compare those two metrics across the types of NN training you are using, maybe you should wait for a full answer. However, I'd just hold out a test set, not used to train or optimise params for either network, and compare results from both networks on that. If you can afford to keep enough labeled data aside, this is the simplest and least susceptible to errors in assessment." CreationDate="2015-01-04T18:41:24.753" UserId="836" />
  <row Id="4069" PostId="3770" Score="0" Text="Care to expand a bit on what you mean by &quot;unknown property&quot;?" CreationDate="2015-01-04T21:20:51.923" UserId="6391" />
  <row Id="4070" PostId="2580" Score="1" Text="Mahout is slow, and didn't make a lot of progress recently. It's dead, IMHO." CreationDate="2015-01-04T22:24:01.307" UserId="924" />
  <row Id="4071" PostId="3804" Score="2" Text="Based on your comment, I think that the storage method is not very important and may be based on your convenience and/or tools availability. Let's see what other people think." CreationDate="2015-01-04T23:41:09.030" UserId="2452" />
  <row Id="4073" PostId="3804" Score="1" Text="This needs more clarification, and does not seem data science specific -- not the scraping and storage part." CreationDate="2015-01-05T11:28:10.083" UserId="21" />
  <row Id="4074" PostId="3807" Score="2" Text="This is probably not suitable as a question as-is. You should indicate what you have tried and what your specific question is about the input format and package you are using." CreationDate="2015-01-05T11:29:56.683" UserId="21" />
  <row Id="4075" PostId="2580" Score="0" Text="AFAIK it is not dead, they are creating bindings to Spark (cf MLLib) https://mahout.apache.org/users/recommender/intro-cooccurrence-spark.html" CreationDate="2015-01-05T18:50:21.613" UserId="1256" />
  <row Id="4076" PostId="3814" Score="1" Text="I think that adding clarifying words to terms, _ambiguous_ in a particular **context** (such as _sample_), is inevitable, if you want to communicate with the maximum _clarity_ and prevent any _miscommunication_." CreationDate="2015-01-05T19:49:28.460" UserId="2452" />
  <row Id="4077" PostId="3760" Score="0" Text="From the shape of the curve, I want to say to run with 6 or 7 since you 'should' be looking for the 'shoulder'.  However, your validation and testing scores look to be doing surprisingly well even at the highest number of hidden nodes.  Any chance you could try the test for even more nodes (upwards of 35 or 40) to see if you can start breaking your validation and testing sets?" CreationDate="2015-01-05T22:27:50.000" UserId="6391" />
  <row Id="4078" PostId="3821" Score="0" Text="If you are trying to forecast the time series based on its history, use the [entropy rate](http://en.wikipedia.org/wiki/Entropy_rate)." CreationDate="2015-01-06T05:10:56.850" UserId="381" />
  <row Id="4080" PostId="3814" Score="0" Text="@AleksandrBlekh you mean writing something like &quot;I analyzed a statistical sample of rock samples&quot;? Honestly I'm not sure there is a distinction; just hierarchical data." CreationDate="2015-01-06T16:47:53.833" UserId="1156" />
  <row Id="4081" PostId="3825" Score="0" Text="Is it possible to use a programming language or any other software to build such system &lt;- sure. but why are you not using mahout? maybe you could rephrase the question a little, but overall what you are about to do with symptoms is not that different from what recommenders do with products." CreationDate="2015-01-06T21:34:46.957" UserId="3132" />
  <row Id="4082" PostId="3825" Score="0" Text="I guess so.Technically, I am not recommending symptoms here, I am just pointing the possible symptoms a user might report on. That is why I termed it as a deterministic system as opposed to recommendation system." CreationDate="2015-01-06T21:51:38.253" UserId="5043" />
  <row Id="4083" PostId="3825" Score="0" Text="The format of the data needed for mahout is bit different from the one that I have now. If I arrange the data set in the format **Account Name**, **Symptom**,**Count** ;would it be enough to work with Mahout?" CreationDate="2015-01-06T21:53:21.897" UserId="5043" />
  <row Id="4084" PostId="3825" Score="0" Text="Yes, the input format is UserId, ItemId, &quot;Preference&quot;. Maybe just give it a try: https://mahout.apache.org/users/recommender/userbased-5-minutes.html" CreationDate="2015-01-06T22:19:48.633" UserId="3132" />
  <row Id="4086" PostId="3814" Score="0" Text="@ssdecontrol: Obviously, I didn't mean that. No need to combine together the same terms in the same sentence. What I meant is what you expressed by saying &quot;rock samples&quot;. Using &quot;rock&quot; here makes it clear that geological, not statistical, meaning is implied. So, you can rewrite your phrase as follows: &quot;I collected rock samples from &lt;...&gt; site (N=100). I used stratified random sampling for collection&quot;. No ambiguity, everybody's happy." CreationDate="2015-01-06T23:29:04.670" UserId="2452" />
  <row Id="4087" PostId="3814" Score="0" Text="@AleksandrBlekh that's definitely more elegant. But I'm legitimately wondering if &quot;statistical sample&quot; is too cumbersome. The qualification should be bilateral IMO. Maybe &quot;random sample&quot; would be unambiguous enough" CreationDate="2015-01-07T00:51:23.233" UserId="1156" />
  <row Id="4088" PostId="3814" Score="0" Text="@ssdecontrol: I see your point and agree that &quot;statistical sample&quot; is an overkill. However, other than that particular word combination, I still stand by my opinion. I don't see how &quot;sample&quot; can qualify some other term, while the reverse is IMHO true in most cases." CreationDate="2015-01-07T01:00:22.587" UserId="2452" />
  <row Id="4089" PostId="3818" Score="0" Text="Spreading the day column into 5 independent binary output columns is a good idea, thank you. I'm not sure I understand what you mean by `plot the distribution of spend as a function of day`." CreationDate="2015-01-07T02:08:50.653" UserId="6669" />
  <row Id="5081" PostId="3787" Score="0" Text="I think if you model temperature, than a numeric variable should be considered. In you specific case, I would think that building an empirical distribution from conditioned data would work much better than considering temperature as a nominal variable." CreationDate="2015-01-07T07:56:26.207" UserId="108" />
  <row Id="5082" PostId="3818" Score="0" Text="typo: binary input column." CreationDate="2015-01-07T17:22:57.637" UserId="6669" />
  <row Id="5083" PostId="3825" Score="0" Text="Is there a way to work with Mahout for the data that has userName, SymptomName as opposed to Ids[numerical val]? Because, when I tried to run the program for such a data, it threw **Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;** *Number Format Exception error*" CreationDate="2015-01-07T17:31:06.840" UserId="5043" />
  <row Id="5084" PostId="3787" Score="0" Text="@rapaio can you expand a bit?  The only definitions of 'empirical distribution' I know are either doing the same thing I did in the example or are not applicable to the current problem." CreationDate="2015-01-07T19:40:05.437" UserId="6391" />
  <row Id="5085" PostId="3816" Score="1" Text="ideally also ensure the test set is properly separate from the training set. Eg if doing handwriting recognition have the test set have writing done by people who's writing (on other letters) is not in the training set." CreationDate="2015-01-08T01:09:26.593" UserId="2680" />
  <row Id="5086" PostId="3787" Score="0" Text="You built an empirical distribution for a nominal variable. My proposal was to build an E.D. for a continuous variable, for example a KDE ( see http://en.wikipedia.org/wiki/Kernel_density_estimation ). Using a KDE the probability from each sample value 'concentrated' in each point is 'spreaded' in its neighburhood. The result is that for each possible temperature value you will get a probability from the sample points in its neghbourhood and you don't need to build intervals, which are arbitrary and thus have good chances to loose a lot of valuable information." CreationDate="2015-01-08T06:53:31.653" UserId="108" />
  <row Id="5087" PostId="4830" Score="0" Text="How does time series come into the question?" CreationDate="2015-01-08T14:18:59.350" UserId="21" />
  <row Id="5090" PostId="4834" Score="1" Text="There's a good rundown of missing data methods that might help get you started: http://www.stat.columbia.edu/~gelman/arm/missing.pdf" CreationDate="2015-01-08T15:17:46.693" UserId="1156" />
  <row Id="5091" PostId="4834" Score="0" Text="And another approach that I haven't tried myself but comes from a very respected author and sounds good in the abstract: http://gking.harvard.edu/files/gking/files/measure.pdf" CreationDate="2015-01-08T15:21:03.827" UserId="1156" />
  <row Id="5092" PostId="4836" Score="0" Text="Not a direct answer and not tutorials either, but if you're interested in hierarchical models, you might find [my recent answer](http://datascience.stackexchange.com/a/4833/2452) on the topic helpful (a collection of research papers)." CreationDate="2015-01-08T18:12:36.247" UserId="2452" />
  <row Id="5093" PostId="4834" Score="0" Text="Sorry, I wanted just to add paragraphs for clarity in the question -- but Stack Overflow only accepts edits if they are longer than 6 characters.  So I also rephrased the problem description to 'A/B measurements'.  Why did you call them 'B/A measurements', by the way?" CreationDate="2015-01-08T19:51:25.907" UserId="1367" />
  <row Id="5094" PostId="3804" Score="0" Text="I agree with @SeanOwen that this question needs clarification: at least, the OP should include his size, performance and expected use (topic modeling) requirements in the question (taking it from his own comment); and he should make it clear why is this a Data Science question (maybe rephrase the question to: does storage format of textual data matter for later modeling?)" CreationDate="2015-01-08T20:14:22.880" UserId="1367" />
  <row Id="5095" PostId="2346" Score="0" Text="I think the question is clearly too wide: consider how many answers there is to &quot;What are the practical problems I might run into?&quot;.  In the end, the collection of all answers to that question *is* the system you want to create. :)  IMHO, you should narrow this down to &quot;How should I define an exception? How can I know if my definition is a good one?&quot;; it is the most related to Data Science, the others are for Programmers SE." CreationDate="2015-01-08T21:20:54.687" UserId="1367" />
  <row Id="5096" PostId="4830" Score="0" Text="Well, it depends what one is trying to do with the textual data right. For e.g. I have a use-case where i am tracking how elaborate explanation does a user give when asked the same question by different ppl but over a period of time." CreationDate="2015-01-08T23:06:21.697" UserId="5179" />
  <row Id="5097" PostId="4837" Score="0" Text="I think this is hard to answer if you have the formula in front of you but are saying you just don't understand it. Can you break down specifically what you are having trouble with?" CreationDate="2015-01-08T23:42:28.413" UserId="21" />
  <row Id="5098" PostId="4836" Score="2" Text="Generally questions that just ask for off-site resources are considered off topic. Maybe you can narrow down what you are looking for in a tutorial. What do you know, what do you want to know, do you want code in a particular language, etc." CreationDate="2015-01-08T23:43:36.520" UserId="21" />
  <row Id="5099" PostId="4837" Score="0" Text="just added a sample dataset. Does this help? I thought cost function for linear regression is same." CreationDate="2015-01-09T01:01:55.267" UserId="7712" />
  <row Id="5100" PostId="3767" Score="1" Text="@RobertoDotti, then mark the question as answered (and post your answer if topepo's wasn't what you did)?" CreationDate="2015-01-09T01:22:23.070" UserId="6391" />
  <row Id="5101" PostId="4846" Score="0" Text="I played a bit on the Dell's website and was able to configure two systems, which approach your main desired parameters (6-core, 64G of RAM, Ubuntu 12.04LTS) - albeit disks size is smaller (it's tricky) - and the total came to just a little above 3K USD. Models that allow you to do that are: Dell Precision Tower 5810 Workstation and Dell Precision Tower 7810 Workstation. Note that 7810 has additional benefit of optional second processor with a relatively cheap upgrade." CreationDate="2015-01-09T09:35:21.827" UserId="2452" />
  <row Id="5102" PostId="4833" Score="1" Text="Thanks for those links, turns out the 2nd one down was almost exactly what I was after." CreationDate="2015-01-09T10:23:40.893" UserId="474" />
  <row Id="5104" PostId="4833" Score="1" Text="@DaveChallis: My pleasure! Glad to be able to help." CreationDate="2015-01-09T11:24:46.347" UserId="2452" />
  <row Id="5105" PostId="4848" Score="0" Text="this means Lambda architecture a little bit of airy fairy thing. easy to talk on slides and looks  pretty but then in reality it not so easy to implement." CreationDate="2015-01-09T15:11:47.707" UserId="6644" />
  <row Id="5106" PostId="4848" Score="0" Text="or a better analogy is the mice deciding to &quot;bell the cat&quot;. great architecture... but who is going to do it?" CreationDate="2015-01-09T15:13:18.613" UserId="6644" />
  <row Id="5107" PostId="4834" Score="0" Text="Thanks for the comments I will take a look at these links.  B/A measurements was a typo...thanks for the fix." CreationDate="2015-01-09T15:14:56.857" UserId="7713" />
  <row Id="5108" PostId="4835" Score="0" Text="Sean-Thank you for the response, much appreciated!  I thought it seemed like an OK path forward but I did not have anything to base that on.  Will read up on Laplace Smoothing." CreationDate="2015-01-09T15:16:07.760" UserId="7713" />
  <row Id="5110" PostId="3802" Score="0" Text="Interesting links. Thanks" CreationDate="2015-01-09T15:34:39.513" UserId="303" />
  <row Id="5112" PostId="3805" Score="0" Text="usefull library but not really an answer to my question" CreationDate="2015-01-09T15:37:03.023" UserId="303" />
  <row Id="5113" PostId="3801" Score="0" Text="Seems to be a practical answer. Not sure if this is applicable to financial time series because of arbitrage." CreationDate="2015-01-09T15:38:33.107" UserId="303" />
  <row Id="5114" PostId="3802" Score="0" Text="@lmorin: You're welcome." CreationDate="2015-01-09T15:38:33.467" UserId="2452" />
  <row Id="5115" PostId="3771" Score="0" Text="Isn't that a [differenced first-order autoregressive model](http://people.duke.edu/~rnau/411arim.htm) or ARIMA(1,1,0)?" CreationDate="2015-01-09T16:45:29.570" UserId="2452" />
  <row Id="5117" PostId="3771" Score="0" Text="One more note: [this discussion](http://stats.stackexchange.com/q/50807/31372) and, especially, Matt Krause's answer might be helpful." CreationDate="2015-01-09T17:42:19.213" UserId="2452" />
  <row Id="5118" PostId="1216" Score="0" Text="You might want to read [my related answer](http://datascience.stackexchange.com/a/742/2452)." CreationDate="2015-01-10T09:28:17.187" UserId="2452" />
  <row Id="5119" PostId="4844" Score="0" Text="I think this is a bit off-topic, as you're asking about hardware requirements for software. But it's also quite open ended since you're covering OS, to stats environments, to distributed cluster computing frameworks and databases. It's not clear what your use case requires from the sentence of description." CreationDate="2015-01-10T11:13:41.343" UserId="21" />
  <row Id="5120" PostId="1216" Score="0" Text="If you're a good programmer then you probably already use quite a bit of math.  I can't imagine a programmer that is good and doesn't use math on a daily basis.  What is the highest level of math you've used? What programming language do you use and what do you use it for?  You certainly don't need a PhD to do data science, but the math is essential." CreationDate="2015-01-10T19:51:37.717" UserId="4697" />
  <row Id="5121" PostId="4827" Score="1" Text="Questions about installing R packages aren't about Data Science and so aren't on-topic here." CreationDate="2015-01-10T22:40:02.057" UserId="471" />
  <row Id="5122" PostId="3781" Score="0" Text="Do you consider your data set time series?" CreationDate="2015-01-11T00:08:06.653" UserId="2452" />
  <row Id="5123" PostId="4844" Score="1" Text="@SeanOwen, you will see that I already accepted Aleksandr Blek's answer before you put the question on hold. It was very well thought out and supplied me with heaps of material to work from - exactly what I was after. Please take the hold off unless you are on a power trip of course." CreationDate="2015-01-11T01:36:30.707" UserId="7722" />
  <row Id="5124" PostId="4846" Score="1" Text="Thanks @Aleksandr Blekh, this provided me with so much more to take into consideration. I realized that I have to up my budget to ~$6K and I am going to go for a Lenovo P700 Thinkstation. Most of the Lenovo hardware is certified for Ubuntu. I am going for a single core that can support 64GB RAM (8x8), with ample cache 30MB and fast speed 3.4GHz. I also opted for an SSD drive for the OS and APPS, but mechanical HDD's for the data side." CreationDate="2015-01-11T01:41:04.203" UserId="7722" />
  <row Id="5125" PostId="4846" Score="0" Text="@RUser: I am very glad that my answer was helpful. As for your choice, I can't say much, as I haven't had direct experience with the brand, just read some mixed reviews (in terms of quality control), but not too many to generalize. I hope that this option will work for you well. Would be interested in and appreciate your feedback after you set the system up and perform some R analyses." CreationDate="2015-01-11T01:50:31.493" UserId="2452" />
  <row Id="5129" PostId="4844" Score="0" Text="Just ran across [this comparative analysis article](http://www.tomsitpro.com/articles/thinkstation-p300-vs-hp-dell-workstation,1-1955.html) and thought that might want to read it, despite your focus/decision on Lenovo system." CreationDate="2015-01-11T05:10:51.427" UserId="2452" />
  <row Id="5130" PostId="4844" Score="0" Text="@RUser I stand by my comment but happy for other mods to weigh in. Easy on the accusations, it's inappropriate. It is good you got a usable answer but it doesn't necessarily mean the question is on topic here." CreationDate="2015-01-11T09:42:52.970" UserId="21" />
  <row Id="5131" PostId="4855" Score="0" Text="What classifier are you using?" CreationDate="2015-01-11T15:43:03.903" UserId="1156" />
  <row Id="5132" PostId="4855" Score="0" Text="logistic regression in apache mahout" CreationDate="2015-01-11T17:28:09.863" UserId="6636" />
  <row Id="5133" PostId="4855" Score="0" Text="The problem is not with zeroes per se but with too many of one class and not enough of the other; logistic regression is usually fitted with maximum likelihood (although I can't speak specifically for the Mahout implementation) and that causes problems for the algorithm. A search for &quot;rare events logistic regression&quot; on the Stats.StackExchange might help you. I'd write a bigger answer but right now I'm posting from my phone and only have a few minutes" CreationDate="2015-01-11T18:31:33.873" UserId="1156" />
  <row Id="5135" PostId="4856" Score="0" Text="how would you add a &quot;derivative&quot; to a model ?" CreationDate="2015-01-11T23:46:54.457" UserId="303" />
  <row Id="5136" PostId="4835" Score="0" Text="This is also a well known technique in compositional data analysis (data in the interior of a simplex), for handling zero-valued components. The idea is that, in many physical applications such as geology (where the methods were first developed), zeroes were more likely due to detector limitations than being true &quot;structural&quot; zeroes. So adding some small value to the zeroes allows analysts to apply log transforms to the composition without having missing values or infinities" CreationDate="2015-01-12T04:23:49.017" UserId="1156" />
  <row Id="5137" PostId="3825" Score="0" Text="It sounds like you just need to write some wrapper functions, or a wrapper program." CreationDate="2015-01-12T04:31:34.333" UserId="1156" />
  <row Id="5139" PostId="4867" Score="0" Text="The apache mahout does indeed support three class classifiers. And the positive negative values were features, but I will indeed give multiple classifiers a shot. Thanks for the input" CreationDate="2015-01-12T11:45:16.833" UserId="6636" />
  <row Id="5140" PostId="4866" Score="0" Text="That is a really good answer.  Just a formatting issue: could you please add newlines before points 3 and 4?  Currently they are lost in a single paragraph ..." CreationDate="2015-01-12T14:14:17.683" UserId="1367" />
  <row Id="5141" PostId="4853" Score="0" Text="Hi user2313838, thanks for your reply! I am not familiar with spatial indices. From what I read it is a way to store spatial information regarding shapes/lines/etc..Could you expand on how the usage of spatial indices can extend into the question?" CreationDate="2015-01-12T23:17:26.133" UserId="7739" />
  <row Id="5142" PostId="1216" Score="0" Text="@Amstell: I studied mathematics till high school only, but I have used maths in  programming (Visual basic and C#), however, not at an advance level. I understand that math is essential. I am currently working on R Language, due to my interest in learning statistics." CreationDate="2015-01-13T04:04:09.450" UserId="3550" />
  <row Id="5143" PostId="4853" Score="0" Text="An example would be a [Ball tree](http://en.wikipedia.org/wiki/Ball_tree) structure." CreationDate="2015-01-13T04:04:24.223" UserId="7740" />
  <row Id="5144" PostId="4853" Score="0" Text="I revised that first part. The spatial index would be used to efficiently query for nearby items provided your metric was based on some kind of vector representation. It may or may not be relevant to your application." CreationDate="2015-01-13T04:27:40.623" UserId="7740" />
  <row Id="5145" PostId="1216" Score="0" Text="@Kurio27 I would say you've got quite a bit of learning ahead of you then, but using R to learn statistics will be a great resource. In my experience, hard coding statistical models builds a lot of understanding and intuition and I couldn't recommend it enough.  Stick with it and don't be discouraged by those saying you don't have a shot.  I was 30 when I started learned advanced mathematics (i.e. dyn. programming) having barely passed high school and not touching basic math until I was 28. I'm 32 now and feel very comfortable with a lot of techniques I had no idea about 2 years ago.Good luck!" CreationDate="2015-01-13T04:55:07.690" UserId="4697" />
  <row Id="5146" PostId="3781" Score="2" Text="If you want to classify data, you should not use a clustering method." CreationDate="2015-01-13T10:30:57.763" UserId="1193" />
  <row Id="5147" PostId="327" Score="0" Text="Yes, a good programmer can do the same in C. BUT a bad programmer can do it in Python as fast as an experienced programmer can do it in C." CreationDate="2015-01-13T13:38:41.387" UserId="7784" />
  <row Id="5148" PostId="327" Score="0" Text="That's true @Pithikos" CreationDate="2015-01-13T14:38:23.670" UserId="115" />
  <row Id="5149" PostId="3801" Score="0" Text="I think the answers to your questions are still valid. For the timeseries model you may want to look at ARCH (AutoRegressive Conditional Heteroskedasticity) models." CreationDate="2015-01-13T17:27:15.343" UserId="6648" />
  <row Id="5150" PostId="2514" Score="0" Text="Just to clarify, I meant unseen item types in the population being sampled; obviously there are none in the sample itself." CreationDate="2015-01-13T18:58:15.040" UserId="5095" />
  <row Id="5151" PostId="4856" Score="0" Text="Hi @Imorin, I've updated the answer with a bit more information. Hope that helps." CreationDate="2015-01-13T22:56:34.027" UserId="7738" />
  <row Id="5152" PostId="1154" Score="0" Text="If you are interested in visualizing not only tanks' positions, but also substance flows between the tanks, I would use **Sankey diagrams** (_network flow diagrams_). For more details and `R` examples, check [this blog post](http://blog.ouseful.info/2013/07/23/generating-sankey-diagrams-from-rcharts). If you don't use `R`, I'm sure that other programming languages, frameworks and ecosystems have similar libraries (for example, `d3.js`, if you want your diagrams to be Web-enabled)." CreationDate="2015-01-14T00:15:55.493" UserId="2452" />
  <row Id="5153" PostId="4864" Score="1" Text="[If you *do* want to post related questions at multiple SE sites](http://stats.stackexchange.com/questions/133340/giving-100-to-whoever-can-identify-the-algorithms-behind-these-440-instances), please link them together." CreationDate="2015-01-14T07:35:10.523" UserId="2853" />
  <row Id="5154" PostId="4873" Score="3" Text="Asking for all the plugins/API is waaay too broad for this. StackExchange is not a replacement search engine for the lazy. I could show you one way to do it, but you are asking for every possible way. Refine your question or it'll probably get closed." CreationDate="2015-01-14T11:50:54.030" UserId="471" />
  <row Id="5155" PostId="4873" Score="0" Text="Hi Spacedman, Which is the best api and bet way to get the data from DBpedia" CreationDate="2015-01-14T12:33:05.950" UserId="5091" />
  <row Id="5156" PostId="4874" Score="0" Text="Thank you logc. Yes here you are limiting attribute as 'EuropeanCountries' from all countries but what I would like to have is given a country name and then list (in json or just print) all information about that country from dbpedia." CreationDate="2015-01-14T13:33:53.857" UserId="5091" />
  <row Id="5157" PostId="4874" Score="0" Text="I am aware of that, that is why I said 'a bit of work is needed'. If I have time, I will edit the answer; otherwise, consider it a starting point :)" CreationDate="2015-01-14T14:01:37.580" UserId="1367" />
  <row Id="5158" PostId="4874" Score="0" Text="Edited to fit the question more precisely" CreationDate="2015-01-14T16:39:28.750" UserId="1367" />
  <row Id="5160" PostId="3781" Score="0" Text="Please include a printout of `process_set` and `process_set.hpc` (the first few lines as printed by `pandas` are enough).  Also, please clarify if you want to **cluster** or **classify** this dataset.  You cannot classify if you do not have pre-existing labels." CreationDate="2015-01-14T16:55:48.497" UserId="1367" />
  <row Id="5161" PostId="4877" Score="0" Text="I have done point-biserial correlations before, but I don't think this works for what I want to achieve. As you wrote - it calculates correlation between variables - in their entirety. I am looking for correlation for parts/segments of a variable, not all values, e.g. when continuous variable X &gt; 3000, not for all values of variable X. Basically something that automatically, or via some machine learning algorithm, &quot;discovers&quot;/data-mines different combinations of variable segments that highly correlate to the DV. Maybe I can try to make my question clearer?" CreationDate="2015-01-14T20:30:21.177" UserId="102" />
  <row Id="5162" PostId="4879" Score="0" Text="How is the output related to the input, other than being of the same dimension? What are you trying to do?" CreationDate="2015-01-15T04:18:36.803" UserId="381" />
  <row Id="5163" PostId="4866" Score="0" Text="@logc: Thank you. You can correct the formatting issues and such yourself using the edit button under the post." CreationDate="2015-01-15T07:53:14.430" UserId="6496" />
  <row Id="5164" PostId="4873" Score="0" Text="The best API for DBpedia is the RDF download, so you can process it locally without hitting on an API at all." CreationDate="2015-01-15T08:01:18.573" UserId="924" />
  <row Id="5165" PostId="4870" Score="0" Text="Aleksandr Blekh and @Max Gibiansky, thanks for the starting guidelines." CreationDate="2015-01-15T08:40:45.303" UserId="7750" />
  <row Id="5167" PostId="4881" Score="0" Text="Can you clarify what you mean? not all models assume hidden/latent variables. There is always a distribution of output given input, just because the data exists." CreationDate="2015-01-15T13:42:42.337" UserId="21" />
  <row Id="5168" PostId="4866" Score="0" Text="No, I couldn't because edits have to be at least 6 characters long.  :)" CreationDate="2015-01-15T15:09:38.520" UserId="1367" />
  <row Id="5169" PostId="4879" Score="0" Text="Thanks, I've edited the question." CreationDate="2015-01-15T17:08:07.347" UserId="7806" />
  <row Id="5170" PostId="4884" Score="0" Text="Hi, this topic is very broad. Please revise your question to be specific to machine learning methods in one field. A list for every field is too broad for this site." CreationDate="2015-01-15T20:26:54.640" UserId="3466" />
  <row Id="5171" PostId="2311" Score="1" Text="Deep learning cannot figure out the _labels_ (how could it be?), but only learn features. You still need some amount of labeled data and supervised learning to solve tasks like classification or regression." CreationDate="2015-01-16T08:28:22.303" UserId="1279" />
  <row Id="5173" PostId="4870" Score="0" Text="You are welcome, @hossain. Thank you for kind words, Max." CreationDate="2015-01-16T10:11:23.243" UserId="2452" />
  <row Id="5174" PostId="4891" Score="1" Text="I don't think this is true, but I think I know what you're getting at: in-memory works a lot faster for iterative computation and a lot of ML is iterative." CreationDate="2015-01-16T17:18:15.257" UserId="21" />
  <row Id="5175" PostId="4882" Score="0" Text="I think you'd have to narrow this down by talking about your data, scale and purpose." CreationDate="2015-01-16T17:19:06.810" UserId="21" />
  <row Id="5176" PostId="4887" Score="0" Text="Thanks @Javierfdr ! I tought that would not have such method. I've already searched at acm, ieee, elsevier, data direct, and so., but even so, I decided to ask here to confirm such expectation. If we narrow to the deep package inspection field, would you recommend any particular method?" CreationDate="2015-01-17T12:56:06.803" UserId="6560" />
  <row Id="5177" PostId="4884" Score="0" Text="I apologize for asking such obvious question, but the answer from @javierfdr was exacly the confirmation which I wanted. Anyway, please tell me what might I do. Is a case to delete the my question? Thanks in advance!" CreationDate="2015-01-17T15:37:34.240" UserId="6560" />
  <row Id="5178" PostId="4887" Score="0" Text="Could you please reply with the papers you've mentioned?" CreationDate="2015-01-17T15:39:11.350" UserId="6560" />
  <row Id="5179" PostId="3788" Score="0" Text="Address data doesn't necessarily have to be a graph. If you want to store relationships between the nodes (not just distances) then a graph makes sense." CreationDate="2015-01-17T17:02:06.187" UserId="7842" />
  <row Id="5180" PostId="4881" Score="0" Text="@SeanOwen: I have edited the question. Please have a look." CreationDate="2015-01-18T10:15:24.887" UserId="7811" />
  <row Id="5182" PostId="4882" Score="0" Text="This is like asking for &quot;machine learning tools.&quot; A generative model is any model that posits a probabilistic data-generating process" CreationDate="2015-01-18T22:21:46.273" UserId="1156" />
  <row Id="5183" PostId="1137" Score="2" Text="You might want to check my answers on the _Cross Validated_ site on _using DTW for time series analysis_: [DTW for clustering and/or classification](http://stats.stackexchange.com/a/131284/31372) and, if applicable, [DTW for irregular time series](http://stats.stackexchange.com/a/133826/31372)." CreationDate="2015-01-19T08:51:37.100" UserId="2452" />
  <row Id="5184" PostId="4904" Score="0" Text="Nice answer! (+1)" CreationDate="2015-01-19T15:31:39.983" UserId="2452" />
  <row Id="5185" PostId="4904" Score="0" Text="Thanks, Aleksandr!" CreationDate="2015-01-19T15:52:57.700" UserId="2723" />
  <row Id="5186" PostId="4904" Score="0" Text="It is my pleasure!" CreationDate="2015-01-19T16:34:14.103" UserId="2452" />
  <row Id="5188" PostId="4904" Score="0" Text="Thank you.. Is there any methods to perform feature space enrichment?" CreationDate="2015-01-19T17:39:32.613" UserId="7873" />
  <row Id="5189" PostId="4879" Score="0" Text="neural networks have been trained for filtering." CreationDate="2015-01-19T20:34:23.407" UserId="3132" />
  <row Id="5190" PostId="4904" Score="0" Text="Sure. There are many such methods. For instance the Gabor filter is a bandpass filter edge detection algorithm commonly used for feature generation in facial recognition and texture classification. This can be used in combination with classification algorithms such as support vector machines." CreationDate="2015-01-20T02:48:02.920" UserId="2723" />
  <row Id="5192" PostId="4904" Score="0" Text="Can I use that for feature enrichment in image classification?" CreationDate="2015-01-20T07:45:58.367" UserId="7873" />
  <row Id="5193" PostId="4904" Score="0" Text="@SarathaPriya Sure. Facial recognition and texture classification are actually specific types of image classification." CreationDate="2015-01-20T15:08:28.887" UserId="2723" />
  <row Id="5194" PostId="4887" Score="0" Text="@fabraz This post I published could be helpful for your question http://datascience.stackexchange.com/questions/4914/when-to-use-what-machine-learning" CreationDate="2015-01-20T15:28:54.607" UserId="5143" />
  <row Id="5195" PostId="4882" Score="0" Text="It sounds like you're just asking how to combine probabilities from several generative models. You'd multiply and normalize them. Is there more to it?" CreationDate="2015-01-21T06:47:54.730" UserId="21" />
  <row Id="5196" PostId="711" Score="2" Text="I don't get it - isn't this a question that should be posted on CrossValidated? I continue to be confused about what goes where between DataScience and CrossValidated." CreationDate="2015-01-21T13:31:03.130" UserId="6672" />
  <row Id="5197" PostId="711" Score="0" Text="@fnl: svms have some competition as classifiers from less mathematically &quot;pure&quot; engineered solutions, so I think DataScience is in a better position to make the comparison here. Although I share your confusion!" CreationDate="2015-01-21T13:59:40.567" UserId="836" />
  <row Id="5199" PostId="4920" Score="0" Text="I understand what you say @ssdecontrol, actually having a comprehensive list of solution to typical problems as you mention could also be very useful. Now, the main difference between the two approaches is that what I'm proposing is directly linked to the technical questions you might ask yourself when you are already trying alternatives,and in that point you already made some assumptions.So, if you have assumed that your features are not-gaussian, should I use PCA for dimensionality reduction? No. Your approach is wider: What to use for dim. reduction -&gt; PCA, but assume gaussian features. Thx" CreationDate="2015-01-21T15:46:48.233" UserId="5143" />
  <row Id="5200" PostId="4916" Score="0" Text="That framework you mentioned would be a great thing to have! Is there anything similar being written?" CreationDate="2015-01-21T15:49:49.083" UserId="5143" />
  <row Id="5201" PostId="4916" Score="0" Text="@Javierfdr: Nothing that I'm aware of. However, I keep looking." CreationDate="2015-01-21T16:05:02.417" UserId="2452" />
  <row Id="5202" PostId="4920" Score="0" Text="@Javierfdr my point is that the technical questions are a distraction if you don't have a substantive question in mind." CreationDate="2015-01-21T16:08:09.970" UserId="1156" />
  <row Id="5203" PostId="4916" Score="0" Text="@AleksandrBlekh the more I think about it the more I think the search for a _statistical_ framework is misguided. See Frank Harrell's answer on your question, and my answer to this one. But Harlow's book  sounds really interesting and I'm gonna pick it up from the library this week." CreationDate="2015-01-21T16:12:28.297" UserId="1156" />
  <row Id="5204" PostId="4916" Score="0" Text="@ssdecontrol: I respectfully disagree. Assuming that such framework doesn't exist (which is most likely the case at the present time) and realizing that it's not an easy task to create one, I strongly believe that it's very much possible, nevertheless. As for the answers you've mentioned (I always read all of them), I read both, but they don't prove that creating such framework is impossible - just difficult, as I've mentioned. That's not something that should stop people from thinking about it and even working toward that. Enjoy Harlow's book." CreationDate="2015-01-21T16:29:22.677" UserId="2452" />
  <row Id="5205" PostId="4921" Score="0" Text="I've answered a similar question a while ago. You can check my answer here: http://datascience.stackexchange.com/a/843/2452." CreationDate="2015-01-22T11:42:20.250" UserId="2452" />
  <row Id="5206" PostId="4927" Score="0" Text="Thank you so much for mentioning this option. I will definitely give it a try. However, I want an image that has exactly like this AMI, but can be run with VirtualBox on my laptop." CreationDate="2015-01-23T03:08:12.367" UserId="7923" />
  <row Id="5207" PostId="4927" Score="0" Text="I watched a tutorial recently about Docker, tested it  and found it easy to understand. What part did you find not user-friendly?" CreationDate="2015-01-23T03:24:57.253" UserId="4621" />
  <row Id="5208" PostId="1227" Score="0" Text="Are you sure you can't code that logic using a many-to-many relationship between your models. Hopefully, when you start writing your server-side code, your favorite framework will allow you to declare models with some fields. Therefore, you will be able to instruct that the model `Team` should a have a field called `players` that requires a many-to-many relationship to your other model `Player`. This allows you to retrieve which players are in each team and which teams are associated to each player." CreationDate="2015-01-23T03:47:47.867" UserId="4621" />
  <row Id="5209" PostId="4927" Score="0" Text="@JeanVids: You're very welcome. I understand your desire to have a local VM - that was the reason I've tried Docker on my computer. I will let you know, if I find a VirtualBox VM image focused on data science (hopefully, R-based)." CreationDate="2015-01-23T03:49:21.143" UserId="2452" />
  <row Id="5210" PostId="4927" Score="0" Text="@RobertSmith: It is easy to understand until things don't work. I've tried to install and use Docker toolset some time ago, but I was unable to get it installed completely (used several tutorials, including the official one, plus lots of Internet searching, but to no avail). I don't remember exactly, but the issue was pretty technical, which resulted in inability to even run Docker virtualization software on my computer." CreationDate="2015-01-23T03:54:19.150" UserId="2452" />
  <row Id="5211" PostId="4927" Score="0" Text="@AleksandrBlekh Maybe Docker has been improving lately because I had very good experience. It was very easy to install (at least, on Linux) and I was able to test the main features without any issues." CreationDate="2015-01-23T04:13:50.663" UserId="4621" />
  <row Id="5212" PostId="4927" Score="1" Text="@RobertSmith: I understand. Perhaps, the problem was that I was trying to set it up on my Windows machine. Anyway, I will give it a try some time later. Thanks for your comments." CreationDate="2015-01-23T04:48:43.470" UserId="2452" />
  <row Id="5213" PostId="4927" Score="1" Text="@AleksandrBlekh Yes, that might be the main problem. Unfortunately there are many issues when installing this sort of thing on Windows." CreationDate="2015-01-23T05:08:41.310" UserId="4621" />
  <row Id="5214" PostId="4925" Score="0" Text="Although this question could be viewed as borderline offtopic I somehow find it a good one for the site IMHO." CreationDate="2015-01-23T10:21:51.133" UserId="21" />
  <row Id="5215" PostId="4921" Score="0" Text="This is too broad to be an effective question here. Maybe you can list a couple projects you are interested in, what you've tried, and name the obstacle you face in pursuing each." CreationDate="2015-01-23T10:23:36.417" UserId="21" />
  <row Id="5216" PostId="4929" Score="0" Text="Interesting project (+1). Thank you for sharing! It might be easier to use it than to figure out why Docker didn't want to work on my Win 7 laptop (see above). However, it still might be a good idea to learn Docker, considering recent trends." CreationDate="2015-01-23T10:45:15.893" UserId="2452" />
  <row Id="5217" PostId="4925" Score="3" Text="In addition to the awesome comments, there's a (somewhat older) blog post comparing several different solutions: http://jeroenjanssens.com/2013/12/07/lean-mean-data-science-machine.html" CreationDate="2015-01-23T13:29:46.980" UserId="587" />
  <row Id="5218" PostId="4922" Score="0" Text="Thank you for your specific response to a rather vague question. &#xA;This was EXACTLY what I was looking for! (I don't have enough Rep to &quot;Vote Up&quot;" CreationDate="2015-01-23T15:13:54.130" UserId="7909" />
  <row Id="5219" PostId="4924" Score="0" Text="Thank you very much.&#xA;I would &quot;Vote Up&quot; this response, but I do not have enough Rep." CreationDate="2015-01-23T15:14:33.967" UserId="7909" />
  <row Id="5220" PostId="4929" Score="0" Text="Nice information. Comparing to vm tools, it needs some time to understand how docker operates. If you are already familiar with vm, it's a good idea to use this toolbox. Thank you for sharing." CreationDate="2015-01-23T15:55:01.110" UserId="1003" />
  <row Id="5222" PostId="4901" Score="0" Text="By the way, what is the &quot;download ratio&quot; of a video?" CreationDate="2015-01-23T20:27:10.383" UserId="4621" />
  <row Id="5223" PostId="4929" Score="0" Text="Thank you for sharing. It is definitely interesting. But I don't see how someone can use it without a graphical interface. I would need R-studio, and PyCharm for Python.( iPython notebook is there). I will need to play with a bit to understand it completely." CreationDate="2015-01-24T00:03:08.927" UserId="7923" />
  <row Id="5224" PostId="4901" Score="0" Text="download ratio = downloaded bytes / file size in byte" CreationDate="2015-01-24T05:45:16.223" UserId="7867" />
  <row Id="5225" PostId="4901" Score="0" Text="Okay, but then most videos have a download ratio of 1? In which situations you have a different ratio? Maybe if the user can't complete the download?" CreationDate="2015-01-24T20:49:30.420" UserId="4621" />
  <row Id="5226" PostId="4937" Score="0" Text="so how do apps know where to query. should they choose impala with 10 years of data? or choose oracle with 2 years of data?" CreationDate="2015-01-24T23:11:15.330" UserId="6644" />
  <row Id="5227" PostId="4937" Score="0" Text="also, both systems would need business friendly data views. so won't this approach lead to duplicate development?" CreationDate="2015-01-24T23:11:57.050" UserId="6644" />
  <row Id="5228" PostId="4937" Score="1" Text="I presume that there are different use cases for the data warehouse versus active archive, so no I don't see that there is necessarily any duplication. You query the data source that fits your use case." CreationDate="2015-01-24T23:31:20.353" UserId="21" />
  <row Id="5229" PostId="4887" Score="0" Text="It's really helpful! Thank you!" CreationDate="2015-01-25T00:18:06.840" UserId="6560" />
  <row Id="5230" PostId="4917" Score="0" Text="I find a &quot;Goal&quot; horizontal line at 74% confusing. If the goal is 100%, then what is the meaning of a &quot;goal at 74%&quot; line?" CreationDate="2015-01-25T09:13:05.293" UserId="2452" />
  <row Id="5231" PostId="4938" Score="0" Text="thanks for your answer. So the SVM packages in eg R or Python do not use quadratic programming methods when the data is non-linearly separable?" CreationDate="2015-01-25T10:07:58.323" UserId="7944" />
  <row Id="5232" PostId="4938" Score="0" Text="Not sure about what svm libraries you use. I use libsvm and different svm tools may use different svm solvers. To find better svm solvers is another research topic. QP is the basic way to solve svm." CreationDate="2015-01-25T10:37:58.553" UserId="1003" />
  <row Id="5233" PostId="4941" Score="3" Text="Providing a summary of external links' content would be useful." CreationDate="2015-01-25T16:40:07.080" UserId="227" />
  <row Id="5234" PostId="4914" Score="0" Text="Really broad. I think each sub-question must be a separate question in order to have meaningful answer." CreationDate="2015-01-25T16:50:46.647" UserId="227" />
  <row Id="5235" PostId="4842" Score="0" Text="The question seems to be more about the cleaning part than the anonymization part. Any ideas how to do the cleaning?" CreationDate="2015-01-25T16:56:47.333" UserId="227" />
  <row Id="5236" PostId="4928" Score="0" Text="Thank you. how can I implement the SIFT features in matlab? and May I know how to implement Bag of Phrases?" CreationDate="2015-01-25T17:54:55.853" UserId="7873" />
  <row Id="5237" PostId="4904" Score="0" Text="Thank you. Can you tell me how Bag of phrases is generated in image?" CreationDate="2015-01-25T17:56:12.247" UserId="7873" />
  <row Id="5238" PostId="4928" Score="0" Text="I don't recommend you to implement your own SIFT algorithm for purposes other than educational. I'm not a MATLAB user but I know VLFeat (http://www.vlfeat.org/matlab/vl_sift.html) provides a MATLAB API, which is what other libraries tend to use. You might want to create another question regarding &quot;bag of phrases&quot;." CreationDate="2015-01-25T19:46:52.310" UserId="4621" />
  <row Id="5239" PostId="4945" Score="0" Text="Nice pictures! Did you create them yourself? If yes, maybe you can share the code for drawing them?" CreationDate="2015-01-27T08:34:15.850" UserId="816" />
  <row Id="5240" PostId="4901" Score="0" Text="Under the scenario of internet streaming, a video is divided into several chunks and downloaded to end device one by one, so we have download ratio = download bytes / file size in bytes" CreationDate="2015-01-27T11:59:01.587" UserId="7867" />
  <row Id="5242" PostId="4957" Score="2" Text="Are there any specific reasons to do the analysis inside Excel? Maybe exporting the data to a data science tool is more appropriate." CreationDate="2015-01-27T16:04:52.920" UserId="227" />
  <row Id="5244" PostId="4955" Score="0" Text="You can start with converting each email into tf-idf vector http://scikit-learn.org/stable/modules/feature_extraction.html. Finding the right number of K is a tricky problem, this might help http://stackoverflow.com/questions/1793532/how-do-i-determine-k-when-using-k-means-clustering" CreationDate="2015-01-27T16:14:45.930" UserId="7983" />
  <row Id="5245" PostId="4955" Score="0" Text="Very nice use case. But I think you are mixing &quot;feature extraction&quot;, &quot;classification accuracy&quot; and &quot;supervised learning&quot;. I mean not being able to parse an email easily does not imply you can not do supervised learning. It's all about knowing the classes in advance and having classified train data, and both can be gathered in your case." CreationDate="2015-01-27T16:20:42.827" UserId="227" />
  <row Id="5246" PostId="4957" Score="0" Text="I basically have Excel spreadsheets where we do a lot of our analysis and fed up of exporting the data to other tools. I just want to do the analysis in Excel." CreationDate="2015-01-27T16:21:27.297" UserId="7982" />
  <row Id="5247" PostId="4954" Score="0" Text="Can you explain more? Showing the way to find &quot;average monthly search&quot; for a given term is also appreciated." CreationDate="2015-01-27T16:25:52.127" UserId="227" />
  <row Id="5248" PostId="4957" Score="1" Text="Mathematica has some machine learning algorithms and can read Excel spreadsheets directly." CreationDate="2015-01-27T17:38:34.210" UserId="7980" />
  <row Id="5249" PostId="4954" Score="0" Text="Amir, see my update about Google Keyword Planner." CreationDate="2015-01-27T18:24:11.343" UserId="7961" />
  <row Id="5250" PostId="4901" Score="0" Text="Great. What correlations do you expect? Are you sure file size is helping you to find a correlation?" CreationDate="2015-01-28T01:35:26.287" UserId="4621" />
  <row Id="5251" PostId="4961" Score="0" Text="I work with many clients who do a lot of their work in Excel. You can mess up your R scripts like you can mess up in Matlab or anything else. I totally disagree." CreationDate="2015-01-28T09:55:23.497" UserId="7982" />
  <row Id="5252" PostId="4962" Score="1" Text="I guess you haven't had the experience I had with a lot of my clients working in Excel. &quot;Nobody does serious ML in Excel&quot;, really??? That's the best you can do?" CreationDate="2015-01-28T09:56:31.473" UserId="7982" />
  <row Id="5253" PostId="4966" Score="0" Text="That's what I was asking for." CreationDate="2015-01-28T10:39:38.717" UserId="7982" />
  <row Id="5254" PostId="4966" Score="0" Text="@Dimitris: You're welcome. Glad that you like my answer." CreationDate="2015-01-28T11:03:23.553" UserId="2452" />
  <row Id="5256" PostId="4914" Score="1" Text="This question could be qualified as too broad or not too broad, depending on how you look at it. If the question would imply a _detailed_ description of tasks and methods, that would be surely broad not only for a question, but even for a single book. However, I don't think that this question implies that _interpretation_. I believe that this question seeks a **framework** or a **taxonomy**, matching tasks with _approaches_ or _methods_ (_algorithms_ and _concepts_ should be ignored due to granularity issues). From that perspective, this answer is not too broad and, thus, is IMHO valid." CreationDate="2015-01-28T13:16:53.377" UserId="2452" />
  <row Id="5257" PostId="4914" Score="0" Text="@AleksandrBlekh Exactly a framework of the kind you mention is the intention of the question. I'm editing it to clarify. Thank you" CreationDate="2015-01-28T14:22:17.073" UserId="5143" />
  <row Id="5258" PostId="4962" Score="0" Text="If you want to make your work difficult, be my guest :) Once I was asked to deliver an ML presentation at a company that used Excel. They liked the content but were initially taken aback by the software. Well, I had better things to do than try to craft a silk purse out of a sow's ear. Give pandas a try; you'll quickly come to like it." CreationDate="2015-01-28T15:12:16.767" UserId="381" />
  <row Id="5259" PostId="4962" Score="1" Text="I have worked with Pandas and I regularly use R however I cannot dismiss Excel simply because there is a perception out there that its old, cumbersome and must go. I think there is a lot you can do with it and our clients rely on it." CreationDate="2015-01-28T16:52:08.723" UserId="7982" />
  <row Id="5260" PostId="4961" Score="0" Text="The fact that other people do it doesn't mean it is the right to thing to do. It is not about messing up because as you said, it is quite possible to make mistakes in R or Python. However, the difference is that you are more likely to  mess up in Excel than in other tools more suitable to analytics, statistics or machine learning. I added some more links in case you need convincing." CreationDate="2015-01-28T16:57:48.333" UserId="4621" />
  <row Id="5261" PostId="4961" Score="0" Text="I have been using and customising Excel for all sorts of things for the last 15 years. I am sure you can produce 100 links discussing how bad it can be but from personal experience I know you can control it and use all its capabilities without messing up anything. Visual programming which is what Excel offers is a brilliant concept. Just because something is trendy like Python at the moment doesn't necessarily mean is the best option." CreationDate="2015-01-28T17:08:28.527" UserId="7982" />
  <row Id="5263" PostId="4961" Score="0" Text="And how do you know you're not messing up anything? People make mistakes precisely because they are not easy to spot. Again, this happens in all sorts of environments, but it is more likely in Excel. However, if you want to keep using Excel for heavy calculations, go ahead." CreationDate="2015-01-28T18:18:18.003" UserId="4621" />
  <row Id="5264" PostId="4961" Score="0" Text="I know I am not messing up anything because I design the entire thing and have control. So far I haven't had any difficult situation in Excel. I guess Microsoft Research looking into using Excel Add-ins says something to you." CreationDate="2015-01-28T19:20:41.647" UserId="7982" />
  <row Id="5265" PostId="4941" Score="0" Text="I will accept the answer after you provide a summary from the external link. Thanks." CreationDate="2015-01-28T19:49:34.730" UserId="3466" />
  <row Id="5266" PostId="4914" Score="0" Text="@Javierfdr: You're welcome." CreationDate="2015-01-28T20:43:01.783" UserId="2452" />
  <row Id="5267" PostId="4952" Score="0" Text="That depends on how you receive/process the training data. If you receive the training examples in a batch (without a time associated with each example) and you want to build a classifier that you will apply to all future observations, then no, you do not need to update the distribution parameters. But if you are attempting to do [online learning](http://en.wikipedia.org/wiki/Online_machine_learning) where the classifier is updated after each example, then you may want to update the parameters (e.g., using the `N` previous observations)." CreationDate="2015-01-29T00:52:13.443" UserId="964" />
  <row Id="5268" PostId="4967" Score="0" Text="Use both variables and let the model figure out the coefficients?" CreationDate="2015-01-29T03:57:08.760" UserId="381" />
  <row Id="5269" PostId="4955" Score="0" Text="@SaurabhSaxena Running TF-IDF in real time as and when we receive a new alert email is impossible, as TF-IDF needs to run on the whole document corpus and not just the single new email. Thanks for the links and info." CreationDate="2015-01-29T06:09:22.493" UserId="7979" />
  <row Id="5270" PostId="4955" Score="0" Text="@AmirAliAkbari I do not understand. The use case I have, has to have the mix of &quot;feature extraction&quot; of important info from the mail, &quot;supervised clustering&quot; of emails into groups for email routing and &quot;classification&quot; of new emails into proper cluster with accuracy. Accuracy is important here because a CPU Load on app server alert email should be classified as MySQL DB server disk space alert mail. :)" CreationDate="2015-01-29T06:13:31.570" UserId="7979" />
  <row Id="5271" PostId="4972" Score="0" Text="Interesting. On a somewhat different problem I ended up using an expectation maximization approach, where where you  ascribe a &quot;probability of belonging to a cluster&quot; rather than strict for every sample, and a &quot;probability of electrical output given that you belong to said cluster&quot;, and maximize over probabilities.&#xA;&#xA;Thanks for the link (I'd upvote if I had the reputation for it), I'll look into it..." CreationDate="2015-01-29T09:41:02.003" UserId="7999" />
  <row Id="5272" PostId="4963" Score="0" Text="Just a comment - there's another good tool comparable with Spark which is called &quot;Apache Flink&quot;. In some cases it's better, so you might want to check it out as well." CreationDate="2015-01-29T11:19:36.933" UserId="816" />
  <row Id="5274" PostId="4972" Score="0" Text="Probability also makes sense, though I find it an overkill for simple tasks. When I face same tasks which involve regression and hour of day do matter I usually rely on simple 'hours to midnight' transformation - in majority of problems out there it works best of all. I just calculate the amount of hours between current day hour and the midnight (if hour&gt;12 return 24 - hour, else return hour). More complicated transformations usually cause my models to overfit." CreationDate="2015-01-29T16:09:23.243" UserId="7969" />
  <row Id="5275" PostId="4914" Score="0" Text="@SeanOwen I modified the main question. Please tell me if is still broad and I would need to make it sharper. Thx!" CreationDate="2015-01-29T16:41:34.967" UserId="5143" />
  <row Id="5277" PostId="4987" Score="0" Text="Could you please explain more detaily. I will give the list of my data(an array) to scale() function and it will return an array. Then should I divide my real list of data to the other array(which is the result of scale) ?" CreationDate="2015-01-29T20:52:30.333" UserId="8018" />
  <row Id="5278" PostId="974" Score="1" Text="I've just been investigating this area and wrote a blog post about what I found.  I used an LSH, but I think my sparsity level was higher than you are looking for.  http://tttv-engineering.tumblr.com/post/109569205836/scaling-similarity" CreationDate="2015-01-30T11:04:54.227" UserId="8030" />
  <row Id="5280" PostId="4990" Score="0" Text="This is not related to my question." CreationDate="2015-01-30T12:43:44.360" UserId="3430" />
  <row Id="5281" PostId="4952" Score="0" Text="I believe my question was not clearly stated initially." CreationDate="2015-01-30T12:45:16.903" UserId="3430" />
  <row Id="5282" PostId="4990" Score="0" Text="The model you're describing is an instance of a cascading classifier. Therefore, this is a legitimate practice used fairly frequently, so this information is very related to your question." CreationDate="2015-01-30T21:11:36.943" UserId="4621" />
  <row Id="5283" PostId="4990" Score="0" Text="I noticed you changed your question. If the first model was computed without looking at the labels and then its output was taken as features for the second classifier, that's not cheating. You don't say if the first model was built from cross validation, though." CreationDate="2015-01-30T21:17:57.050" UserId="4621" />
  <row Id="5284" PostId="4997" Score="1" Text="Can you please provide us with some information on both datasets/links? This will indeed ease the burden of those looking for specific types of data set. Take a look at other posts to see what kind of information your references are missing." CreationDate="2015-01-30T21:31:13.203" UserId="84" />
  <row Id="5286" PostId="4981" Score="0" Text="ok, so if one of the input is 2d then it is expanded, yes, this is exactly what i wanted to know...  thanks a lot" CreationDate="2015-01-31T09:56:02.603" UserId="8013" />
  <row Id="5287" PostId="4978" Score="0" Text="It's not clear what you mean. Is the input to SVM a vector? yes, that's true for most ML algorithms. Multiple vectors? yes, training happens on &gt; 1 data point. 2D? yes. Somehow, several 2D vectors at once? not clear what that means. Please clarify." CreationDate="2015-01-31T11:32:41.617" UserId="21" />
  <row Id="5288" PostId="5001" Score="2" Text="Have you searched at the Open Data Stack Exchange? http://opendata.stackexchange.com/" CreationDate="2015-01-31T00:07:33.683" UserId="8005" />
  <row Id="5291" PostId="5001" Score="0" Text="I very much doubt that you'll find that kind of data as **general** open data sets. You will have much more chances to find what you're looking for in **industry-focused** databases or **market research** data sets." CreationDate="2015-01-31T20:19:43.320" UserId="2452" />
  <row Id="5292" PostId="4978" Score="1" Text="I have got my answer from climbs. Thank you!" CreationDate="2015-02-01T05:05:48.883" UserId="8013" />
  <row Id="5293" PostId="5002" Score="2" Text="Thanks Aleksandr. I agree with your answer. But I just want to hear opinion of other experts in base of their experiences and their sense with various interfaces. For example you maybe work with various frameworks. I want to know which of them has better interface in your opinion. Definitely this question is about some frameworks with same tasks. For example a selection between Flink and Spark just in your opinion. As your answer detailed comparison is so lengthy and this is not my purpose." CreationDate="2015-02-01T07:09:09.393" UserId="7977" />
  <row Id="5294" PostId="5002" Score="2" Text="I edited my question based on your answer. Thanks:)" CreationDate="2015-02-01T07:16:00.560" UserId="7977" />
  <row Id="5295" PostId="5002" Score="2" Text="@OmidEbrahimi: You're very welcome :-). I very much understand the reasoning behind your question. It's just that this type of questions (opinion-based) are not in favor on StackExchange (and I actually disagree with that, but most of the time try to follow the guidelines). Despite that and surprisingly, opinion-based questions and answers are no such rarity on StackExchange. I guess, it's more of issue of moderators' discretion and personal (subjective) tolerance for a particular question/topic. My main point is that SE is unlikely the best outlet for a _comprehensive_ treatment of the topic." CreationDate="2015-02-01T07:40:34.867" UserId="2452" />
  <row Id="5296" PostId="4990" Score="0" Text="There is only one model. I did reworded my question since it seems people are confused." CreationDate="2015-02-01T12:40:21.317" UserId="3430" />
  <row Id="5297" PostId="5000" Score="0" Text="would you consider a function which more accurately reflects the data you have rather than a linear regression model ?" CreationDate="2015-02-01T13:26:46.637" UserId="7980" />
  <row Id="5298" PostId="5000" Score="0" Text="I tried different models, but so far all of them sometimes output negative values. This is not really wrong - if you think of linear regression as a line it is possible that it is not able to build a function which will be ALWAYS positive, or even carefully 'touch' the axis (to output 0s). But once again, whatever I tried, for new data I sometimes keep getting negatives." CreationDate="2015-02-01T16:39:48.950" UserId="7969" />
  <row Id="5299" PostId="5010" Score="0" Text="Is it possible to do this using HUE?" CreationDate="2015-02-01T18:35:26.717" UserId="5224" />
  <row Id="5300" PostId="5009" Score="0" Text="Is it possible to do this using HUE?" CreationDate="2015-02-01T18:36:06.860" UserId="5224" />
  <row Id="5301" PostId="5000" Score="0" Text="Hard to know for definite without sight of the data, but one could imagine a piecewise linear approximation that could be shaped not to go negative, or a high order polynomial perhaps. :)" CreationDate="2015-02-01T18:58:39.890" UserId="7980" />
  <row Id="5302" PostId="5011" Score="0" Text="Thank you Jake for the second point, as it is really accurate - I've somehow forgotten that you should not touch the test set in model building phase :)." CreationDate="2015-02-01T22:31:12.023" UserId="8017" />
  <row Id="5303" PostId="5011" Score="0" Text="Regarding the first one - the thing is I cannot assume that every &quot;sub-training&quot; set is statistically representative. For many classification problems with highly imbalanced class labels, it is recommended NOT to use the real fraction of class labels expected in future, but rather to make them equally big (by under/over sampling for example). Thus, I cannot assume that resampled training set characteristics are representative over the real population." CreationDate="2015-02-01T22:37:40.467" UserId="8017" />
  <row Id="5304" PostId="4941" Score="1" Text="I just updated the answer to add a summary" CreationDate="2015-02-02T07:32:25.093" UserId="7950" />
  <row Id="5305" PostId="4986" Score="1" Text="What range you normalise to would depend on what you wish to do with the transformed data." CreationDate="2015-02-02T11:50:30.607" UserId="7980" />
  <row Id="5306" PostId="5011" Score="0" Text="@Matek It is quite common to resample the training set and include the resampled sets in an aggregated predictor, in Bagging for instance. So the principle that resampling can still be considered representative is established in some contexts. Whilst for your final classifier you might want to use all available data to build your mode, train and test sets, during model exploration any input from the test set during training will pollute your model and skew your results. Class balancing is often an attempt to minimise a loss function on predictions rather than maximise prediction accuracy." CreationDate="2015-02-02T12:07:55.633" UserId="7980" />
  <row Id="5307" PostId="5015" Score="0" Text="i will try to take a further look at logistic regression. Do you know any good source/webpage which explains logistic regression as easy as possible?" CreationDate="2015-02-02T18:13:46.427" UserId="8063" />
  <row Id="5308" PostId="5015" Score="0" Text="Is pretty well explained in this set of notes http://cs229.stanford.edu/notes/cs229-notes1.pdf" CreationDate="2015-02-02T19:13:16.487" UserId="8065" />
  <row Id="5309" PostId="2360" Score="0" Text="Will you clarify one point, the value transform from one scale to another in relation to the parentheses are:`target scale`(`current scale`)" CreationDate="2015-02-02T23:36:31.750" UserId="2742" />
  <row Id="5310" PostId="5024" Score="0" Text="This is too broad. What specifically are you hoping to learn? what problems are you trying to solve?" CreationDate="2015-02-03T11:49:19.860" UserId="21" />
  <row Id="5311" PostId="5014" Score="0" Text="Their calculation looks simply wrong. The F1 column is not the harmonic mean, not even to the number of decimal places shown." CreationDate="2015-02-03T11:55:51.240" UserId="21" />
  <row Id="5312" PostId="5024" Score="0" Text="I'm trying to learn how to use Genetic Algorithms and try to learn when to apply, for example answers like Stephan are welcome to improve the investigation with Genetics and R." CreationDate="2015-02-03T11:58:43.380" UserId="8076" />
  <row Id="5313" PostId="5018" Score="0" Text="Looks like it does not work somehow on mine Hue.. Probably I have mb older version or something" CreationDate="2015-02-03T17:39:12.493" UserId="5224" />
  <row Id="5314" PostId="5025" Score="0" Text="Thank you so much Stephan! I will do a deep sight in this packages." CreationDate="2015-02-03T18:57:22.063" UserId="8076" />
  <row Id="5315" PostId="5019" Score="0" Text="Hmmm, interesting, thank you! I will try it, are there any good links on GLM\Poisson regression articles which you could suggest?" CreationDate="2015-02-03T21:13:15.067" UserId="7969" />
  <row Id="5316" PostId="5022" Score="0" Text="Thanks, your answer and some extra research seems to indicate to me that pure statistical data would be better. I'm not worried much about ranking...this is more along the lines of analyzing a trade for players in a fantasy sports league to determine if trading player A for player B would be recommended or not. Hope that helps you better understand my intention :)" CreationDate="2015-02-04T00:02:09.220" UserId="8074" />
  <row Id="5317" PostId="5019" Score="0" Text="I'm not aware of any particularly outstanding general guides on Poisson regression. As a first step, I would recommend looking up GLMs in whatever programming language you are comfortable with.  Most implementations provide support for Poisson regression and getting started is often as easy as plugging in your training data and specifying a &quot;Poisson&quot; error distribution or a &quot;log&quot; link function." CreationDate="2015-02-04T02:40:43.307" UserId="182" />
  <row Id="5318" PostId="5016" Score="0" Text="SVMs are more suitable for our use case but my team wanted to explore more into deep networks and how will these help us. DBNs solved our water scheduling problem with .1% more accuracy and lesser time as compared to shallow networks so we are wondering if such an approach might help in this case as well. How would you suggest we deal with point clouds data for input to network where it has (x,y,z,r,g,b) values for each point ?" CreationDate="2015-02-04T06:30:18.020" UserId="8051" />
  <row Id="5319" PostId="5039" Score="0" Text="No luck with this solution.  I'm also only using 100 examples/cases." CreationDate="2015-02-04T06:51:14.040" UserId="985" />
  <row Id="5320" PostId="5039" Score="0" Text="Can you expect the output values from each iteration of the optimization? Is it ever non-zero?" CreationDate="2015-02-04T07:02:22.573" UserId="182" />
  <row Id="5321" PostId="5042" Score="0" Text="yes I have other features like num of room, num of stars.. but my question ih how the model predict the number oclicks for each num if hotel and it dont know num of hotel if i dont use these feature in training model?" CreationDate="2015-02-04T07:25:32.930" UserId="8088" />
  <row Id="5322" PostId="5039" Score="0" Text="yeah, if I understand what you're saying. I'm starting with initial_theta = np.array([0,0,0]) and when calculating line by line I get a result of final_wealth = 0.1335, then I tried initial_theta = np.array([-0.1,0,0]) and got final_wealth = 0.0433.  So that part is working properly (and same as my MATLAB solution).  But I also tried changing the initial_theta to non-zero values and running the optimization function and ended up with that same non-zero initial_theta.  So it seems that the optimization isn't running any iterations..." CreationDate="2015-02-04T07:26:39.603" UserId="985" />
  <row Id="5323" PostId="5039" Score="0" Text="Sorry, that should say &quot;inspect&quot; rather than &quot;expect&quot;. It seems like you caught the drift though." CreationDate="2015-02-04T07:28:38.170" UserId="182" />
  <row Id="5324" PostId="5043" Score="1" Text="I have change the description of ambiguous description of Y, actually it is a continuous value from zero to one, not a binary value of zero or one." CreationDate="2015-02-04T07:32:28.260" UserId="7867" />
  <row Id="5325" PostId="5043" Score="0" Text="In that case I would suggest to examine scatter plots between each of your X variables versus y (instead of histograms). Additionally since your target data is bound between 0 and 1, you may still want to consider trying logistic regression." CreationDate="2015-02-04T07:41:43.963" UserId="182" />
  <row Id="5326" PostId="5039" Score="0" Text="fyi, the optimization is exceeding the max iterations, yet the result is not changing at all from the initial_theta" CreationDate="2015-02-04T07:51:36.820" UserId="985" />
  <row Id="5328" PostId="5014" Score="0" Text="Sounds like i should contact one of the authors of the paper." CreationDate="2015-02-04T12:49:27.307" UserId="8063" />
  <row Id="5329" PostId="5043" Score="2" Text="@RyanJ.Smith you can't do logistic regression on real values. You can do beta regression on numbers bound to the [0,1] interval, or you can do linear regression on the logit transform" CreationDate="2015-02-04T13:07:21.407" UserId="1399" />
  <row Id="5330" PostId="5033" Score="0" Text="@Monozygotic thank you for reply. . Euh sincerely i unserstand a little bit.. Sorry but in wich step i include the id_hotel? It is unknown feature for the model" CreationDate="2015-02-04T00:26:12.163" UserId="8088" />
  <row Id="5331" PostId="5030" Score="0" Text="This is effectively a link-only answer and those are discouraged." CreationDate="2015-02-04T15:47:07.533" UserId="21" />
  <row Id="5332" PostId="5016" Score="0" Text="Remember DBNs and CNNs are not the same thing, they sound similar, but they are different architectures. Your input dimension is clearly to big, so you should segment it in small windows and pretrain the CNN with those windows first.&#xA;Then you should do a sweep over each lidar image, augmenting in that way your dataset, from 4500 to way more. &#xA;&#xA;If yoh have a single label per lidar candidate, you probably want to pool the windows for each candidate." CreationDate="2015-02-04T16:06:03.680" UserId="8065" />
  <row Id="5333" PostId="5032" Score="0" Text="To add to this, this resource includes a visual demonstration of how an ROC curve is constructed by varying the thresholds (about 2/3 down the page): http://www.r-bloggers.com/roc-curves-and-classification/" CreationDate="2015-02-04T17:21:33.547" UserId="525" />
  <row Id="5334" PostId="5034" Score="0" Text="Add all of these comments in their respective answers, you are just adding clutter in this way" CreationDate="2015-02-04T17:53:10.530" UserId="8065" />
  <row Id="5335" PostId="5043" Score="0" Text="Good catch, Ben.  I had forgotten that though the output from logistic regression is continuous, the response is originally assumed to be Bernoulli (binary)." CreationDate="2015-02-04T17:53:46.163" UserId="182" />
  <row Id="5336" PostId="111" Score="0" Text="So...Mobile App would write directly to DynamoDB or go via a web layer(REST API) that would write to DynamoDB? Any pros/cons or comments on that?" CreationDate="2015-02-04T18:33:51.763" UserId="8098" />
  <row Id="5337" PostId="5043" Score="0" Text="+1 for &quot;It is also possible that there is not a strong relationship between any of your features and your targets.&quot; This is the first explanation that would come to my mind. Prediction is often *hard*." CreationDate="2015-02-04T20:02:27.120" UserId="2853" />
  <row Id="5338" PostId="5034" Score="0" Text="I'm not allowed to leave comments yet..." CreationDate="2015-02-05T02:26:36.137" UserId="8075" />
  <row Id="5339" PostId="5034" Score="0" Text="Could you maybe tell me which part of my answer should have been placed in a comment instead? This was actually the first answer posted to the question, so I couldn't place anything as a comment to other answers anyway. I don't really see what's wrong with the answer..." CreationDate="2015-02-05T02:45:06.033" UserId="8075" />
  <row Id="5340" PostId="5038" Score="2" Text="What type of data is `y`?  Is it an int or a float?  Do you use `from __future__ import division`?" CreationDate="2015-02-05T09:36:42.253" UserId="8110" />
  <row Id="5341" PostId="5024" Score="0" Text="Hi Sean, I've focused more my question, is it OK?" CreationDate="2015-02-05T11:19:36.340" UserId="8076" />
  <row Id="5342" PostId="5034" Score="0" Text="You have two answers to the same question, your second answer should be a comment to the first, or just edit the first answer" CreationDate="2015-02-05T17:20:13.240" UserId="8065" />
  <row Id="5343" PostId="5058" Score="0" Text="I will be performing linear regression as part of my analysis, but would like to take it further by using ML." CreationDate="2015-02-05T21:32:42.050" UserId="6683" />
  <row Id="5344" PostId="5056" Score="0" Text="Thanks, that makes sense. Would I also use the features `athlete`, `tracktype`, `coach` and `location` as factors (although, in my actual data I have hundreds of athletes, coaches (and other features))?" CreationDate="2015-02-05T21:36:24.430" UserId="6683" />
  <row Id="5346" PostId="5064" Score="0" Text="This is a [spoke-hub](http://en.wikipedia.org/wiki/Spoke-hub_distribution_paradigm) or [star network](http://en.wikipedia.org/wiki/Star_network). Do you have a more specific question?" CreationDate="2015-02-06T06:02:47.590" UserId="381" />
  <row Id="5348" PostId="5038" Score="0" Text="y is an int... changed it to a float and solved the issue! thanks a ton!" CreationDate="2015-02-06T07:14:06.950" UserId="985" />
  <row Id="5349" PostId="5039" Score="1" Text="TheBlackCat solved it, but thanks for your help Ryan!" CreationDate="2015-02-06T07:15:07.750" UserId="985" />
  <row Id="5351" PostId="5039" Score="0" Text="Make sure that the correct answer gets posted as an answer and marked correct, not just left as a comment above." CreationDate="2015-02-06T09:13:21.833" UserId="182" />
  <row Id="5352" PostId="5038" Score="0" Text="Great!  I have added the answer as an answer below, please mark it as correct so others can find it more easily." CreationDate="2015-02-06T09:42:57.257" UserId="8110" />
  <row Id="5353" PostId="5056" Score="0" Text="Of course. Better to use as many features as you have at the beginning and then check bias\variance of the result - i.e split your training data into 2 sets, try to train algorithm on first subset and check the outcome on the second set. If it will 'overlearn' with specified amount of features you may want to think about dropping out one of the features (for example, coach) to check if it improves the performance of your model." CreationDate="2015-02-06T14:46:07.693" UserId="7969" />
  <row Id="5354" PostId="5056" Score="0" Text="You may also want to try to transform your original features in some way (like I mentioned for # of race (first second etc). For example one good idea would be to transform quantitative features (like height or weight) into cathegorical or binary features (&gt; 6 feet and &lt; 6 feet or 'between 60 and 70 KG, between 70 and 80 KG'). General advice is that you could get a really good results with rather simple algorithm (Linear Regression for example) but with a good selection of features." CreationDate="2015-02-06T14:50:30.430" UserId="7969" />
  <row Id="5355" PostId="5053" Score="0" Text="What do you mean by `chi-square difference`? Is this a goodness of fit test using Pearson's chi-squared test? It is not clear from Vik's code because I think he wrote a function to perform that operaton." CreationDate="2015-02-06T20:03:48.263" UserId="4621" />
  <row Id="5356" PostId="5064" Score="0" Text="Yes but a multi-hop star network can have a single node sending info to multiple recipients. Also, in spoke-hub a single node can receive from multiple nodes. I am looking for a model where there are radial lines coming out from a single node. Each radial line comprises cascaded nodes. Also, I want to know if a social network can mimic such network in practice." CreationDate="2015-02-06T20:12:38.660" UserId="8127" />
  <row Id="5357" PostId="5048" Score="0" Text="How do you read line by line in OCR?" CreationDate="2015-02-07T01:22:14.147" UserId="4621" />
  <row Id="5358" PostId="5053" Score="0" Text="@RobertSmith please look at the function `corpora::chisq` and this link for description: http://rpackages.ianhowson.com/rforge/corpora/man/chisq.html" CreationDate="2015-02-07T09:58:12.067" UserId="8059" />
  <row Id="5359" PostId="5056" Score="0" Text="All very useful, thanks. When I start delving deeper into this I'm sure I'll be back with more questions!" CreationDate="2015-02-07T10:51:00.023" UserId="6683" />
  <row Id="5360" PostId="5053" Score="0" Text="Great. Thank you!" CreationDate="2015-02-07T16:47:27.623" UserId="4621" />
  <row Id="5361" PostId="5079" Score="1" Text="Sure. That is a good idea. The general term for this kind of techniques is &quot;model order reduction&quot;. SVD is routinely used for this purpose (also called singular value decomposition.) Keep in mind that is advisable to demean your matrix before using SVD. Also, most metrics have issues in high dimensional spaces, so be careful there." CreationDate="2015-02-08T06:47:25.977" UserId="4621" />
  <row Id="5362" PostId="5082" Score="0" Text="Thanks, looks interesting, but probably not suitable for my needs. First, I don't have a boolean input, and second, I can have a large set of items to base recommendations on, so I can't precalculate frequently used sets of base items." CreationDate="2015-02-08T16:44:16.137" UserId="8158" />
  <row Id="5363" PostId="5079" Score="0" Text="In response to your update, take a look at [this answer](http://stats.stackexchange.com/a/93281/2676) about demeaning your matrix. You're using a 30x30 matrix, right? It shouldn't be slow at all. However, a  20,000x20,000 matrix is probably too much. In the answer I linked above, it talks about an incremental SVD. It is also a good idea to read the approaches used in the Netflix competition to pick up some tricks to deal with huge matrices in this context." CreationDate="2015-02-08T20:10:22.670" UserId="4621" />
  <row Id="5364" PostId="5084" Score="0" Text="I also thought about it in those terms. The problem is that even in chain-of-command structures, there is communication between people within the same rank and there is also communication from one-to-many between different levels of the tree." CreationDate="2015-02-08T20:20:41.830" UserId="4621" />
  <row Id="5365" PostId="5084" Score="0" Text="Yeah, I don't think this kind of graph will occur naturally in a system. Though in a military, officers at the same level cannot issue orders to others on the same level. And officers can't issue orders to juniors that are under a different officer." CreationDate="2015-02-08T20:33:35.193" UserId="90" />
  <row Id="5366" PostId="5084" Score="0" Text="Sure, although in the restrictive case of communicating orders, one officer can issue orders to several juniors." CreationDate="2015-02-08T20:39:30.977" UserId="4621" />
  <row Id="5367" PostId="5084" Score="0" Text="Sure, multiple reciever nodes are still allowed if I understood what you asked correctly - _Basically, I have a source node and some information propagating radially from it and each recipient receives the information from a single sender._" CreationDate="2015-02-08T20:56:00.980" UserId="90" />
  <row Id="5368" PostId="1175" Score="0" Text="Sean, silly, but can I confirm with you, this whole idea is really about making importance weights (as shown in the input format required by VW) work (and not overshoot)?" CreationDate="2015-02-08T20:59:02.630" UserId="1138" />
  <row Id="5369" PostId="1175" Score="1" Text="yes that's right" CreationDate="2015-02-08T21:48:08.620" UserId="1256" />
  <row Id="5370" PostId="5084" Score="0" Text="Well, I didn't ask it but I understood it differently. I thought that the picture in the question disallowed several recipients for a single sender." CreationDate="2015-02-09T01:53:20.803" UserId="4621" />
  <row Id="5371" PostId="5034" Score="0" Text="Ah I see. I'll keep that in mind for next time. :)" CreationDate="2015-02-09T05:23:35.310" UserId="8075" />
  <row Id="5372" PostId="5081" Score="1" Text="Thanks, nice answer! Unfortunately I can not give point to that because I do not have enough reputation." CreationDate="2015-02-09T07:36:58.740" UserId="3346" />
  <row Id="5373" PostId="5081" Score="0" Text="@user115415: You're quite welcome! No problem - feel free to upvote/accept, when you reach reputation of 15 or more (should be pretty soon :-)." CreationDate="2015-02-09T08:23:36.520" UserId="2452" />
  <row Id="5374" PostId="5080" Score="5" Text="I've flagged this to be closed as &quot;unclear what you are asking&quot;, because the answer to your question as posed is obviously &quot;yes&quot;. Of course they do. What are you really asking? Because if that **is** what you are really asking then I dread to think what questions you ask of your data as a data scientist..." CreationDate="2015-02-09T12:15:27.513" UserId="471" />
  <row Id="5377" PostId="5091" Score="0" Text="You may want to change the question title, since you question title doesn't really match what you want to ask." CreationDate="2015-02-10T02:08:09.487" UserId="1003" />
  <row Id="5379" PostId="5094" Score="0" Text="thank you very much for you reply, it helps me, but is it possible  to count the number of people currently working in the same company?" CreationDate="2015-02-10T09:33:35.477" UserId="8088" />
  <row Id="5380" PostId="5094" Score="0" Text="Yes, after you extract the company names for each people, you can reference the hadoop/spark word count examples to count the number of people in the same company." CreationDate="2015-02-10T09:46:02.293" UserId="1003" />
  <row Id="5381" PostId="968" Score="1" Text="I'm voting to close this question as off-topic because its a simple R programming question and should be on http://stackoverflow.com or http://stats.stackexchange.com/" CreationDate="2015-02-10T10:26:30.023" UserId="471" />
  <row Id="5382" PostId="5094" Score="0" Text="Thank you, Do you think that Hive regex, or Pig regex, can be useful to extract company's names?" CreationDate="2015-02-10T12:45:09.037" UserId="8088" />
  <row Id="5383" PostId="5094" Score="0" Text="Of course. If you already know the rules to extract all company names, just use the regex to extract the names." CreationDate="2015-02-10T13:56:17.170" UserId="1003" />
  <row Id="5384" PostId="5094" Score="0" Text="Ok, i'am trying to use Hive Regex to do this, but I have no idea after this how to count the number of people currently working in the same company. Should I use map reduce algorithm ?" CreationDate="2015-02-10T14:01:23.120" UserId="8088" />
  <row Id="5386" PostId="5094" Score="0" Text="Please try to search &quot;hadoop word count&quot; on Google, and you will get ideas." CreationDate="2015-02-10T14:07:01.053" UserId="1003" />
  <row Id="5389" PostId="5084" Score="0" Text="As Mr. Smith pointed out, I am disallowing several recipients for single sender. Anyways I think I should still select this answer. I do not think this question is wellposed and it may not occur in practice. The motivation for asking this question was the following. For such topology, the observation at the bottommost or the leaf nodes are independent given the source symbol. Most of the social networks that I have seen have links going all over the place." CreationDate="2015-02-10T20:47:02.977" UserId="8127" />
  <row Id="5390" PostId="968" Score="1" Text="As R is a primary data science tool, I don't find this off topic IMHO." CreationDate="2015-02-11T02:06:43.697" UserId="21" />
  <row Id="5391" PostId="5108" Score="0" Text="What's the category of the &quot;Advice for new faculty members&quot;?" CreationDate="2015-02-11T08:15:00.033" UserId="587" />
  <row Id="5392" PostId="5108" Score="0" Text="@LauriK: Definitely not the one that has been selected." CreationDate="2015-02-11T08:22:39.830" UserId="2452" />
  <row Id="5393" PostId="5111" Score="1" Text="Thanks a lot; although this is more about finance and statistics, I think it'd come in handy in due time. Also, man, that's a lot of books :-)" CreationDate="2015-02-11T13:01:45.940" UserId="8214" />
  <row Id="5394" PostId="5111" Score="0" Text="@Chiffa: You're very welcome! &quot;Lots of books&quot;: I prefer to under-promise and over-deliver, not vice versa :-). Seriously, though, it is just my initial selection, which is more of a tip of an iceberg. If you noticed, I've included several ML-specific books, as I understand that's your current focus." CreationDate="2015-02-11T13:10:20.860" UserId="2452" />
  <row Id="5395" PostId="5111" Score="0" Text="Indeed; those are the ones I'm going to look into first." CreationDate="2015-02-11T13:12:24.973" UserId="8214" />
  <row Id="5396" PostId="5112" Score="1" Text="Thank you for the answer (+1). I don't think that the hierarchical structure of the classification path necessarily implies the use of hierarchical clustering (HC) approach. They might be using some fancy ontology-based or other technique, different from HC. I'm not sure that they are doing a good job - as I said, I've seen multiple similar errors in the books section, which is one of Amazon's major e-commerce channels." CreationDate="2015-02-11T13:34:45.507" UserId="2452" />
  <row Id="5397" PostId="5113" Score="0" Text="What do you do with that split? Can you give us more context on that?" CreationDate="2015-02-11T13:44:08.033" UserId="108" />
  <row Id="5398" PostId="5114" Score="0" Text="So, rule of thumb it is. Thanks. As for cross-validation, I haven't read that far yet, so it might cover it." CreationDate="2015-02-11T14:15:23.477" UserId="8214" />
  <row Id="5399" PostId="5112" Score="1" Text="@AleksandrBlekh Yep, it's just a thought." CreationDate="2015-02-11T14:16:15.483" UserId="8113" />
  <row Id="5400" PostId="5114" Score="0" Text="As we know, 80/20 rule aka Pareto principle is based on on the [Pareto distribution](http://en.wikipedia.org/wiki/Pareto_distribution). Consequently, the 80/20 split is based on the specific value of the Pareto index ($\alpha \approx$ 1.161). What I'm curious about is whether the two-way and three-way data splits have an **analytical solution**, based on the parameters you've mentioned (&quot;sample size, distributions and relationships between your variables&quot;) as well as Pareto distribution's parameters." CreationDate="2015-02-11T14:16:22.953" UserId="2452" />
  <row Id="5402" PostId="5111" Score="0" Text="@Chiffa: Good luck! Feel free to accept a particular answer, if you are satisfied with it." CreationDate="2015-02-12T00:52:17.167" UserId="2452" />
  <row Id="5403" PostId="5099" Score="0" Text="That's along the lines my train of thought is going, thanks." CreationDate="2015-02-12T02:02:42.473" UserId="6683" />
  <row Id="5404" PostId="5097" Score="0" Text="I think most common ones are `numpy`, `scipy`, `scikit-learn` and `pandas`." CreationDate="2015-02-12T04:47:22.887" UserId="3070" />
  <row Id="5405" PostId="5080" Score="0" Text="Basically, you are asking if someone can find you a job.  That's spam." CreationDate="2015-02-12T04:55:28.853" UserId="8116" />
  <row Id="5406" PostId="5111" Score="0" Text="@AleksandrBlekh wow, thanks for crediting me :)" CreationDate="2015-02-12T09:07:32.143" UserId="8202" />
  <row Id="5407" PostId="5111" Score="0" Text="@AlexanderDidenko: You're welcome. :-) Feel free to suggest other relevant sources - I will be glad to update my answer." CreationDate="2015-02-12T09:21:11.847" UserId="2452" />
  <row Id="5409" PostId="5118" Score="0" Text="@bogatron: speaking of positive and negative examples can be confusing since the dataset has 4 different classes, the subset with weather = sunny just happens to have just two." CreationDate="2015-02-12T13:50:24.163" UserId="8191" />
  <row Id="5410" PostId="5118" Score="0" Text="You are correct - I mistakenly looked at the `Parents` attribute which only has two values (`Yes/No`) and just happened to work out to the same entropy value for `Weather=Sunny`. The answer is correct now." CreationDate="2015-02-12T15:05:15.893" UserId="964" />
  <row Id="5411" PostId="5118" Score="0" Text="bogatron: Thanks for updating !" CreationDate="2015-02-12T15:51:59.077" UserId="8191" />
  <row Id="5412" PostId="5118" Score="0" Text="Unfortunately, I can't upvote you since I don't have enough points." CreationDate="2015-02-12T15:53:19.787" UserId="8191" />
  <row Id="5413" PostId="5135" Score="0" Text="Thanks for the insight. You're right the survival table is just the training data set. I guess what confused me a little is the fact that the example just used the survival table to predict against the test data, without explicitly stating the algorithm used - it basically used the survival table as a lookup table to predict the outcome for each test case  passenger. My question should have been whether this specific approach was an example of using Naive Bayes." CreationDate="2015-02-13T09:19:59.850" UserId="8234" />
  <row Id="5414" PostId="5117" Score="0" Text="Thank you.  Shall up-vote once I clear the up-vote threshold." CreationDate="2015-02-13T09:30:48.003" UserId="8221" />
  <row Id="5415" PostId="5118" Score="0" Text="@ee2Dev Thank you.  Shall up-vote once I clear the up-vote threshold." CreationDate="2015-02-13T09:41:44.280" UserId="8221" />
  <row Id="5416" PostId="5118" Score="0" Text="user1744649: Thanks if my answer was helpful you can also accept my answer." CreationDate="2015-02-13T09:47:08.627" UserId="8191" />
  <row Id="5417" PostId="5135" Score="0" Text="No, it is not. It is even simpler than Naive Bayes; it looks like a simplified version of k-nearest neighbours classification. They group together or cluster the training examples. Each cluster contains the examples in the training set which have the same values in the input variables. Each cluster's class is computed as the rounded mean of the surviving value for the individuals in the cluster, and the results are stored in a lookup table. &#xA;&#xA;The lookup table assigns an output value to each test example depending on its input values." CreationDate="2015-02-13T10:00:59.633" UserId="2576" />
  <row Id="5418" PostId="5135" Score="0" Text="Cool, thanks a lot." CreationDate="2015-02-13T10:52:48.140" UserId="8234" />
  <row Id="5421" PostId="5137" Score="0" Text="I don't think many people are going to start by watching an hour+ long video. How about describing your fears in a bit more detail." CreationDate="2015-02-14T16:36:52.697" UserId="7720" />
  <row Id="5422" PostId="5101" Score="0" Text="Thank you for your interesting suggestions." CreationDate="2015-02-14T20:27:04.030" UserId="3346" />
  <row Id="5423" PostId="5080" Score="0" Text="Basically I asked a question that was clear  enough to somebody and I received two good answers. Of course anybody is free to think what their mind suggest them to think." CreationDate="2015-02-14T20:29:34.690" UserId="3346" />
  <row Id="5424" PostId="5129" Score="0" Text="I think questions that are purely about career and off site learning resources for a tool are off-topic for this site." CreationDate="2015-02-14T22:02:41.447" UserId="21" />
  <row Id="5425" PostId="5137" Score="1" Text="Good news: there are lots more tools like this." CreationDate="2015-02-14T22:03:31.490" UserId="21" />
  <row Id="5427" PostId="5140" Score="0" Text="thank you very much for your answer. In the case of Naive Bayes, don't we have the likelihood p(x,y) over both the examples x and the class labels y? (as in http://cs229.stanford.edu/notes/cs229-notes2.pdf), whereas in logistic regression it is only over y: p(y|x) (http://cs229.stanford.edu/notes/cs229-notes1.pdf)" CreationDate="2015-02-15T13:17:42.890" UserId="8273" />
  <row Id="5429" PostId="5147" Score="0" Text="Can you be a lot more specific? what do you have in mind when you ask about methodologies? modeling, scoring, evaluation?" CreationDate="2015-02-15T17:36:43.477" UserId="21" />
  <row Id="5431" PostId="5148" Score="0" Text="That seems promising, I will have a look at those. As for methodology, I mean a step-by-step phased process that one can use for framing guidance." CreationDate="2015-02-15T17:39:12.710" UserId="7720" />
  <row Id="5432" PostId="5147" Score="0" Text="Specifically I mean a step-by-step phased process that one can use for framing guidance. But I am interested in anything close to that too." CreationDate="2015-02-15T17:40:09.477" UserId="7720" />
  <row Id="5433" PostId="5148" Score="0" Text="Yeah check out the resources I linked, the first one talks about a very high level methodology. But once you have your high level pieces figured out, you need to look for process for each of your sub processes." CreationDate="2015-02-15T17:48:17.760" UserId="90" />
  <row Id="5434" PostId="5147" Score="0" Text="I edited it to clarify what I meant. Can you take me off hold now?" CreationDate="2015-02-15T17:48:54.930" UserId="7720" />
  <row Id="5435" PostId="5147" Score="0" Text="It is clear what a methodology is but the topic is still quite broad. Are you talking about approaches to modeling? Feature selection? visualization?" CreationDate="2015-02-15T17:52:40.743" UserId="21" />
  <row Id="5436" PostId="5140" Score="0" Text="In the case of Naive Bayes, that is the joint probability $p(x,y)$ and they use in page 10 what they call the &quot;joint likelihood&quot;. In the logistic regression example, the notation in page 18 might be confusing because they have $p(y|x;\theta)$. For a moment, forget about $x$ and you have what I wrote above. However, $\theta$ depends on your covariates $x$ and that is why they have $h_{\theta}(x)$. In the same way, you can also write $\pi$ as $\pi(x)$ in the notation I used and you'd have the same description. By the way, notice I fixed a typo a moment ago." CreationDate="2015-02-15T17:53:04.597" UserId="4621" />
  <row Id="5437" PostId="5147" Score="0" Text="No, it is to solve real life business problems, which is of course driving the boom in Data Science. I am not so interested in pure academic applications - although they are fun, they do not require a methodology usually. &#xA;&#xA;Do I need to state that too?" CreationDate="2015-02-15T17:55:37.647" UserId="7720" />
  <row Id="5438" PostId="5140" Score="0" Text="Another change I did was to use Bernoulli distributions to model the binary outcomes in the logistic regression case, since that is what you meant based on your references." CreationDate="2015-02-15T18:01:51.590" UserId="4621" />
  <row Id="5439" PostId="5147" Score="0" Text="To help clarify, there are methodologies in the programming world, like Extreme Programming, Feature Driven Development, Unified Process, and many more. I am looking for their equivalents, if they exist." CreationDate="2015-02-15T18:02:51.027" UserId="7720" />
  <row Id="5440" PostId="5156" Score="0" Text="I guess it's really a method of measuring similarity to a cluster that I'm looking for. The clustering is based on a graph of similarities between nodes, so I don't really have any centroid measurements." CreationDate="2015-02-16T14:27:01.477" UserId="474" />
  <row Id="5441" PostId="5145" Score="0" Text="Thanks to both of you.  Can one of you provide me with a URL to free site like wikipedia where I can read up about the methodology?" CreationDate="2015-02-16T15:13:33.030" UserId="8281" />
  <row Id="5442" PostId="5155" Score="0" Text="I'm not sure about type of data you perform clustering on, but maybe [this related answer](http://datascience.stackexchange.com/a/4833/2452) of mine will be helpful in terms of figuring out a method for measuring similarity, optimal for your situation." CreationDate="2015-02-16T15:20:35.737" UserId="2452" />
  <row Id="5443" PostId="5155" Score="0" Text="If your clusters have an exemplar, you could compare the incoming samples against it, otherwise you could compare it against the centroid, which you could keep track of. I'm not sure what the problem is, as this seems obvious." CreationDate="2015-02-16T18:13:04.430" UserId="381" />
  <row Id="5444" PostId="5155" Score="0" Text="@Emre - that's part of the problem, I don't have an exemplar or centroid for each cluster easily available, though perhaps this is something I can calculate (in case it helps, I'm using MCL clustering - http://micans.org/mcl/, so input is a similarity matrix rather than feature vectors)" CreationDate="2015-02-16T18:20:38.513" UserId="474" />
  <row Id="5445" PostId="5155" Score="1" Text="You can always calculate the centroid from the raw features, so I'd give it a try." CreationDate="2015-02-16T18:30:44.193" UserId="381" />
  <row Id="5446" PostId="2504" Score="1" Text="I still don't have enough reputation to put a short comment on your original question and this is not really an answer. In any case, I wanted to say that I think this paper is pretty relevant to this issue: Fernández-Delgado, M., Cernadas, E., Barro, S., &amp; Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems? The Journal of Machine Learning Research, 15, 3133–3181. Retrieved from http://dl.acm.org/citation.cfm?id=2697065" CreationDate="2015-02-16T19:33:26.393" UserId="506" />
  <row Id="5448" PostId="5150" Score="2" Text="I wouldn't call these methodologies though. They are algorithms, that can be applied as part of a methodologie." CreationDate="2015-02-17T05:17:45.623" UserId="7720" />
  <row Id="5451" PostId="128" Score="0" Text="Is HDP supposed to be data-driven in regards to the number of topics it will select? On practical side, I tried to run Blei's HDP implementation and it just ate all memory until I killed the process. I have 16GB RAM and just over 100K short documents to analyze." CreationDate="2015-02-18T09:25:21.440" UserId="7848" />
  <row Id="5452" PostId="5178" Score="0" Text="If you tell me three things I might have an answer! 1. Does your medium size data gets bigger? If so how periodically. 2. Do you use programming languages/frameworks to do pattern matching and data analysis? 3. Does anyone else use this data? If so do they change it?" CreationDate="2015-02-13T10:40:38.030" UserDisplayName="Dave Rose" />
  <row Id="5453" PostId="5178" Score="0" Text="I voted to &quot;Leave Open&quot; when reviewing Close Votes queue because I don't think this question is for specific situation. However, can anybody who is familiar with Software Recommendations SE tell us this question would fit into that site?" CreationDate="2015-02-13T10:52:42.327" UserDisplayName="scaaahu" />
  <row Id="5454" PostId="5178" Score="1" Text="@scaaahu I don't think this is necessarily a software question; an acceptable answer could also describe a workflow or combination of tools and systems. (Anyways, being on topic somewhere else shouldn't play into the decision to close a question here.)" CreationDate="2015-02-13T11:01:58.770" UserDisplayName="ff524" />
  <row Id="5455" PostId="5178" Score="2" Text="Just to protect against data corruption with image data, I periodically run a script that re-computes a checksum file with all files and their md5 checksums. The checksum file is then kept in git. Now I can immediately see with git diff if any of the checksums have changed. And I can also see which files have been removed &amp; added. And if there are e.g. any signs of data corruption, then I can use the regular backups to restore old versions. Not perfect but better than nothing." CreationDate="2015-02-13T11:30:36.463" UserDisplayName="Jukka Suomela" />
  <row Id="5456" PostId="5178" Score="0" Text="@JukkaSuomela I think you should post that as an answer, not a comment." CreationDate="2015-02-13T11:48:49.980" UserDisplayName="ff524" />
  <row Id="5457" PostId="5178" Score="0" Text="@Johann Why not just files, without version control (but with backups)?" CreationDate="2015-02-13T12:22:54.293" UserId="289" />
  <row Id="5458" PostId="5178" Score="0" Text="@PiotrMigdal: Are you seriously asking why people should use version control, instead of just having a bunch of files with backups?-)" CreationDate="2015-02-13T13:01:50.247" UserDisplayName="Jukka Suomela" />
  <row Id="5459" PostId="5178" Score="1" Text="@JukkaSuomela I think it's a reasonable question when you've got very large datasets, if those datasets change frequently... in those cases, backup often *is* what's used as version control." CreationDate="2015-02-13T13:29:18.427" UserDisplayName="jakebeal" />
  <row Id="5460" PostId="5178" Score="1" Text="I'm voting to close this question as off-topic because it deals with data/databases rather than something _specific_ to academia. The questions is great, and (IMHO) should be moved to DataScience.SE or (perhaps) Databases.SE." CreationDate="2015-02-13T14:02:35.853" UserId="289" />
  <row Id="5461" PostId="5178" Score="0" Text="@PiotrMigdal I don't know if this is off-topic, but this question does also not fit well with Databases.SE or DataScience.SE. I would like to know what other researchers/Institute do in practice to deal with this kind of problem - I've updated the question accordingly." CreationDate="2015-02-15T19:22:05.807" UserDisplayName="Johann" />
  <row Id="5462" PostId="5178" Score="0" Text="@DaveRose 1. Yes, I will hopefully add more experimental data and processed images, but not very often (maybe a few iterations); 2. Yes, that is part of my thesis; 3. Yes, others will hopefully use and change the data." CreationDate="2015-02-15T19:32:18.047" UserDisplayName="Johann" />
  <row Id="5463" PostId="5178" Score="0" Text="@JukkaSuomela That actually sounds pretty good (at least much better than anything I've found so far)." CreationDate="2015-02-15T19:33:45.380" UserDisplayName="Johann" />
  <row Id="5464" PostId="5178" Score="0" Text="@PiotrMigdal Going without version control kind of pains me for the reasons I've stated in the question (especially: &quot;did my data change with out me noticing?&quot; and then 2 days of diffing by hand). So I am looking for something smarter." CreationDate="2015-02-15T19:35:56.057" UserDisplayName="Johann" />
  <row Id="5465" PostId="5178" Score="0" Text="@Johann 1. How you store or version control it is in domain of data science (regardless if you use it in academia, industry or for a hobby project). I really want to ensure the best answers and it is good to go where there are many experts in data. 2. My point was only that it might be not for git. (And, all in all, git _is_ a filesystem). Do you want to diff per file, line, or what?" CreationDate="2015-02-15T19:53:37.380" UserId="289" />
  <row Id="5466" PostId="5178" Score="0" Text="@PiotrMigdal 1) You are probably right, though I am still curious how scientist - without a background in data science - handle that situation, 2) No doubt, git cannot handle that kind of data. A diff per file would be enough, just to see if my data has changed (or I changed it inadvertently). The point is to have control/documentation over how my data changed through what action by whom." CreationDate="2015-02-16T05:16:23.600" UserDisplayName="Johann" />
  <row Id="5467" PostId="5178" Score="1" Text="@Johann Data scientist have different backgrounds. Mine is in quantum mechanics, for example. The whole point here is that: 1. StackExchange discourages so-called [boat questions](http://meta.stackexchange.com/questions/14470/what-is-the-boat-programming-meme-about) and 2. its better to get best practices rather than how it is solved by people who had to solve it but had no idea." CreationDate="2015-02-18T12:33:42.243" UserId="289" />
  <row Id="5468" PostId="5179" Score="0" Text="There is a popular open source version of Dropbox, OwnCloud. I haven't tried it, though." CreationDate="2015-02-13T21:35:47.000" UserDisplayName="Davidmh" />
  <row Id="5469" PostId="5152" Score="4" Text="+1 for starting with a simple model and sub-sampling" CreationDate="2015-02-18T13:55:32.467" UserId="8319" />
  <row Id="5470" PostId="5178" Score="0" Text="@PiotrMigdal Thats a good point - thanks!" CreationDate="2015-02-18T13:59:44.580" UserId="8320" />
  <row Id="5476" PostId="5109" Score="0" Text="StackExchange sites should not be a link-dump of increasingly aging and missing resources." CreationDate="2015-02-18T15:25:17.420" UserId="471" />
  <row Id="5484" PostId="2284" Score="0" Text="&quot;Where **he was** born?&quot; is not a question. &quot;Where **was he** born?&quot; is. Are you trying to parse poor English?" CreationDate="2015-02-18T18:02:01.703" UserId="471" />
  <row Id="5485" PostId="3734" Score="0" Text="You can consider to disk based processing instead of whole in memory approach. Just read a batch of instances that fit in your memory update your clusters then read another batch from disk and go on like this. This is slow but the most realiable way. You can conside to use HDF5 like data structures as well, to read structured data from disk." CreationDate="2015-02-18T20:39:44.777" UserId="464" />
  <row Id="5486" PostId="5165" Score="0" Text="Agreed, too broad." CreationDate="2015-02-18T23:07:25.207" UserId="3466" />
  <row Id="5487" PostId="5160" Score="0" Text="Can you please provide some info about your previous question, and link? Perhaps you should just revise that question. Hard to say without any context." CreationDate="2015-02-18T23:08:46.253" UserId="3466" />
  <row Id="5489" PostId="5124" Score="0" Text="You know one for Java? I am not familiar with python and R that well." CreationDate="2015-02-19T07:20:16.607" UserId="8235" />
  <row Id="5492" PostId="5109" Score="0" Text="Yes, I think that this is generally considered off-topic; it's open-ended and opinion-based." CreationDate="2015-02-19T10:06:09.440" UserId="21" />
  <row Id="5494" PostId="5124" Score="0" Text="Check out Weka. I'm not familiar with Java though. Seems like a good question to ask here if it hasn't been asked already" CreationDate="2015-02-19T18:07:02.923" UserId="8236" />
  <row Id="5499" PostId="5181" Score="0" Text="This sounds very promising. Thanks for your reply." CreationDate="2015-02-20T21:52:17.773" UserId="8127" />
  <row Id="5500" PostId="5188" Score="0" Text="Thank you . I understand the first paragraph an dthe second paragraph, but to me, they are saying the opposite things. You say I can't infer that in the first sentence but that I can in the last. Can you explain? Thank you!" CreationDate="2015-02-20T23:00:07.250" UserId="8313" />
  <row Id="5502" PostId="5178" Score="0" Text="@Johann Are you sure you need to version control your dataset? If you are processing a big set of images, you usually want to version control the procedures you followed that led to a modified set of images. Therefore, you usually don't need to keep track the images themselves. If you want to restore this modified set in the future, obviously you only need to take the original dataset and apply the procedures you did according to some commit in your code." CreationDate="2015-02-21T00:50:06.050" UserId="4621" />
  <row Id="5503" PostId="5188" Score="0" Text="what exactly is the goal of your analysis ? Are you interested in saying something about the 'complexity' of condition a in comparison to condition b ? When is a condition complex ? If it's hard to diagnose ? if the symptoms are severe ? can s.o. have both conditions ?" CreationDate="2015-02-21T06:48:01.813" UserId="8191" />
  <row Id="5504" PostId="5199" Score="0" Text="I think that it is impossible to answer this question as it's stated. You may have a better chance of getting a reasonable answer, if you will provide more details about your project(s), goals, specific packages, users (including their number) and requirements for the setup. Keep in mind that, in addition to _local machines_ and _local servers_, you have various **cloud** options, such as _Amazon Web Services (AWS)_." CreationDate="2015-02-21T07:47:30.790" UserId="2452" />
  <row Id="5505" PostId="5199" Score="0" Text="This question is not really Data Science related, as it stands now.  Maybe you can elaborate and make the connection clearer?  Otherwise, you may want to move it to Programmers Stack Exchange." CreationDate="2015-02-21T11:37:13.047" UserId="1367" />
  <row Id="5506" PostId="5201" Score="0" Text="Answered here, but as you can see, there is nothing Data Science specific about the answer.  Hope it helps, anyway." CreationDate="2015-02-21T12:02:18.130" UserId="1367" />
  <row Id="5507" PostId="5206" Score="0" Text="This is extremely helpful, and works. One complication I have which maybe wasn't clear is that each cell in each row df['salary'] is a list, not a string (i.e. `[£26,658, to, £32,547], [Competitive, with, Excellent, Benefits]`, not `[£26,658 to £32,547],[Competitive with Excellent Benefits]` . So when I run your function I will get `TypeError: expected string or buffer`. I tried solving this via `df['salary']=df['salary'].apply(''.join(df['salary']))` just after you define df in your second line of code of the second section, but get `TypeError: sequence item 0: expected string, list found`." CreationDate="2015-02-22T14:04:04.213" UserId="8375" />
  <row Id="5508" PostId="5206" Score="0" Text="first, `apply` takes a function as argument, so you should write `df['salary'].apply(' '.join)`. Note also that I would do `' '.join` as opposed to `''.join` to avoid accidentally concatenating strings that should not be.&#xA;&#xA;Secondly, the only reason you have the lists is because you did `df['salary']=df['salary'].astype(str).str.split()`. My solution doesn't require this step, so you can just work with the un-splitted strings as they are. `re.findall` will scan through an entire string and extract everything that looks like it's a reference to money." CreationDate="2015-02-22T14:29:47.107" UserId="8376" />
  <row Id="5510" PostId="5196" Score="0" Text="I would begin by looking at the time required to make forecasts and interacting with server times. Can they be optimized further? If not, then you should try distributed/parallel computing solutions." CreationDate="2015-02-22T15:29:37.450" UserId="1131" />
  <row Id="5514" PostId="5206" Score="0" Text="I am still getting TypeError: expected string or buffer even when I removed `df['salary']=df['salary'].astype(str).str.split().` When I print df['salary'] it certainly looks now like a column of strings. Even when I did `df['salary']=df['salary'].astype(int32)` based on stackoverflow.com/questions/21841402/… I get `ValueError: invalid literal for long() with base 10: '\xc2\xa336,000 - \xc2\xa340,000 per year plus excellent benefits'.` I read my CSV in as UTF-8 by the way." CreationDate="2015-02-23T13:27:09.893" UserId="8375" />
  <row Id="5515" PostId="5199" Score="0" Text="Thanks for the feedback.  I'll try to add more detail." CreationDate="2015-02-23T17:07:47.243" UserId="8368" />
  <row Id="5516" PostId="5201" Score="0" Text="Thanks.  Which section of Stack Exchange is the correct one for discussing these issues?  I understand that they are only tangentially related to the topic of Data Science." CreationDate="2015-02-23T17:22:14.830" UserId="8368" />
  <row Id="5517" PostId="5201" Score="0" Text="I would move or re-post the question on http://programmers.stackexchange.com . Stack Overflow is for code that is not running as expected, while Programmers SE is for the design stage.  Make it clear you ask for advice on the setup for your team, and not on the setup of Python alone! Otherwise they might suggest to move the question to http://serverfault.com or something ..." CreationDate="2015-02-23T17:25:20.940" UserId="1367" />
  <row Id="5520" PostId="5211" Score="0" Text="Thank you, I'm just looking for different approaches to solve classification problems. The 3 tips that you gave me seems to be the way to go. At first I wanted to see if could translate a single-class classification into a multiple-class classification problem. But knowing that I need instances for each class might be a problem" CreationDate="2015-02-23T21:14:27.530" UserId="8381" />
  <row Id="5521" PostId="5211" Score="0" Text="The animal/human classification is just an example. My goal is to find a way to tweak or improve current malware classification methods for android" CreationDate="2015-02-23T21:15:52.313" UserId="8381" />
  <row Id="5522" PostId="5211" Score="0" Text="Ok, good luck! Feel free to ask again if you get stuck somewhere!" CreationDate="2015-02-23T21:16:56.930" UserId="8191" />
  <row Id="5523" PostId="1151" Score="0" Text="@Air: Unless you can refer to the official rule on DS, which states otherwise, career-related questions are not off-topic and valid. Pointing to several opinions in a Meta discussion doesn't change that." CreationDate="2015-02-24T07:27:08.453" UserId="2452" />
  <row Id="5526" PostId="1151" Score="0" Text="@AleksandrBlekh You do realize you are responding to a comment that is nearly six months old? And that opinions in a Meta discussion *are* as close as you will get to an official rule on topicality?" CreationDate="2015-02-24T14:55:25.263" UserId="322" />
  <row Id="5527" PostId="1151" Score="0" Text="@Air: Does it matter, if that comment is six months old, when your today's one confirms that you still think the same? Fortunately, it's just your opinion, nothing more. As are all other opinions, expressed on Meta." CreationDate="2015-02-24T15:05:09.293" UserId="2452" />
  <row Id="5529" PostId="1151" Score="0" Text="@AleksandrBlekh I'm struggling to find a point to this exchange. If the consensus has changed since I originally commented, all you need do is provide a citation for the new consensus and flag the comment is obsolete. Unless your goal is simply to have an argument (no thanks), there's no reason for me to be involved here." CreationDate="2015-02-24T19:54:05.333" UserId="322" />
  <row Id="5530" PostId="1151" Score="0" Text="@Air: My point is that I believe that your initial comment is formulated too strongly (nothing personal, though). My goal is not to have an argument. However, I don't think that several opinions in Meta establish any consensus, even if they would be the same (which they are not). Especially, since, as many Meta discussions, that  one isn't representative. Generally, I don't see how a Meta discussion could be an accurate measure/indicator of a consensus, unless some minimum level of representativeness is guaranteed. Unless there is a more formal voting process/tool, which I'm unaware of." CreationDate="2015-02-24T20:07:26.970" UserId="2452" />
  <row Id="5531" PostId="435" Score="0" Text="While a good question I notice that it appears on multiple Stack Exchange forums... http://stackoverflow.com/questions/24260299/relational-data-mining-without-ilp  ; Not that I'm a stickler about stuff like that, but I think that we're not supposed to do it" CreationDate="2015-02-25T15:36:26.273" UserId="2723" />
  <row Id="5532" PostId="435" Score="0" Text="Also, it would be very helpful if you could be a little more specific with regards to what you're classifying, the barrier you're encountering and in an ideal world provide us with some sample data to look at" CreationDate="2015-02-25T16:29:08.187" UserId="2723" />
  <row Id="5533" PostId="5235" Score="0" Text="Thanks a lot Alex, the scheduler does have a memory. It takes into account previous allocations in its current decisions.  I will attempt to apply hidden Markov model" CreationDate="2015-02-26T05:20:29.907" UserId="8410" />
  <row Id="5534" PostId="5225" Score="0" Text="The suggested keywords does provide me new search terms to find existing research papers, which I did not find previously. Thanks" CreationDate="2015-02-26T06:24:30.687" UserId="7887" />
  <row Id="5535" PostId="5220" Score="0" Text="Also I've thought the modules you've indicated could possibly generate datas that are suits for predicting. Your answer has been a confirmation for me. I got the gist. Thanks!" CreationDate="2015-02-26T06:58:34.847" UserId="8386" />
  <row Id="5536" PostId="5233" Score="2" Text="Just a little thought here. Why don't you just train one SVM for each response variable?" CreationDate="2015-02-27T09:15:18.167" UserId="2576" />
  <row Id="5537" PostId="5244" Score="1" Text="See [this][1] question on Cross Validated.&#xD;&#xA;&#xD;&#xA;&#xD;&#xA;  [1]: http://stats.stackexchange.com/questions/39232/is-r-viable-for-production-deployed-code" CreationDate="2015-02-27T17:16:34.460" UserId="8392" />
  <row Id="5541" PostId="5212" Score="0" Text="I think this is too broad. You should say more about your requirements or constraints to narrow this down." CreationDate="2015-02-28T04:07:58.203" UserId="21" />
  <row Id="5543" PostId="5243" Score="1" Text="Requests for resources are generally viewed as off-topic. Maybe you can steer this towards asking about a specific problem you'd like to know how to solve?" CreationDate="2015-02-28T04:16:44.690" UserId="21" />
  <row Id="5544" PostId="5215" Score="0" Text="I tried this and now I have gone down a rabbithole of encoding problems after this error when trying to convert to string (I read my CSV as encoding=UTF-8') - `UnicodeEncodeError: 'ascii' codec can't encode character u'\xa3' in position 0: ordinal not in range(128)` (http://stackoverflow.com/questions/3588083/unicodeencodeerror-ascii-codec-cant-encode-character-u-xa3). Then, when I try to encode or decode df['salary'] it says AttributeError: 'Series' object has no attribute 'encode'. Basically what I think is that the pound signs which are now in UTF-8 cannot be converted to strings?" CreationDate="2015-02-28T13:12:33.810" UserId="8375" />
  <row Id="5545" PostId="2633" Score="0" Text="You can always create another google account and use GCE from that." CreationDate="2015-02-28T06:16:06.353" UserId="8466" />
  <row Id="5546" PostId="5248" Score="0" Text="Importing 10^10 rows of data into R just to calculate kendall coefficient is just simply imposible and not smart. That's why I wrote a question here. I am aware on how to specify implementation of kendall coefficient, just simply check the code of `cor` function in R :) I think the best idea would be to implement user defined function to calculate that kendall coefficient. Do you maybe know where could I upload that function later on for public use? By the way, have you used RHive :) do you recommend any good materials for start?" CreationDate="2015-02-28T16:03:05.993" UserId="5224" />
  <row Id="5548" PostId="5251" Score="1" Text="I don't think this answer contains a single correct statement." CreationDate="2015-02-28T18:55:36.690" UserId="783" />
  <row Id="5549" PostId="5251" Score="0" Text="What exactly is wrong about legal departments being scared of the GPL, and R+CRAN being mostly GPL code? **These are facts, even if you don't like them.**&#xA;Your comments don't contain any *precise* rebuttal..." CreationDate="2015-02-28T19:41:09.777" UserId="924" />
  <row Id="5550" PostId="5248" Score="0" Text="@MarcinKosinski Instead of importing 10^10 rows, why not just sample it? As for UDF's, if you have one or develop one with other people you can put it into a JIRA ticket for addition to Hive via the links in my solution, or become a committer and commit code to the project (or give your code to a committer): https://www.apache.org/dev/committers.html" CreationDate="2015-02-28T20:12:19.093" UserId="2723" />
  <row Id="5551" PostId="5251" Score="0" Text="It's plain and simple FUD and trolling. I have used GPL code in production for the almost decades I have been working.  And as for citing one particular paper: yawn." CreationDate="2015-02-28T21:22:20.950" UserId="515" />
  <row Id="5552" PostId="5251" Score="0" Text="So have I. But I have also seen company polcies that forbid the use of any GPL code. YMMV." CreationDate="2015-02-28T21:27:04.133" UserId="924" />
  <row Id="5553" PostId="5251" Score="0" Text="You may have heard of RedHat, a 12 billion dollar Fortune 500 company selling GPL'ed Linux to just about every corporation out there.  So no, your sample of one does not exactly generalize.  But you're a troll, and I'll stop here now.  For a better answer follow the very first comment above to something I had to say in the matter over on CrossValidated a few years ago." CreationDate="2015-02-28T22:03:40.300" UserId="515" />
  <row Id="5554" PostId="5251" Score="0" Text="I have not been saying it's impossible. I have been saying he needs to check with *his* legal department. I see this as the flaw of legal departments of some companies, not of R - but you have to live with reality... (and they may even be paranoid enough to allow Linux, but no other GPL software...) But I have understood, **it's not what you want to hear**. A pity. But maybe the original poster does have the same problem." CreationDate="2015-02-28T22:22:01.853" UserId="924" />
  <row Id="5555" PostId="5256" Score="7" Text="I would also mention &quot;data analyst&quot;, which is a _more generic_ and, thus, in my opinion, better term than &quot;data miner&quot;." CreationDate="2015-03-01T02:09:19.823" UserId="2452" />
  <row Id="5556" PostId="5259" Score="0" Text="Sven, thanks -- this is brilliant. The key is the order() command, which I had under-appreciated until now. Is this how everyone does it?" CreationDate="2015-03-01T07:32:02.993" UserId="3510" />
  <row Id="5557" PostId="5259" Score="0" Text="@DavidEpstein I cannot tell you how others manage their data. This is how I would do it." CreationDate="2015-03-01T07:48:34.687" UserId="106" />
  <row Id="5558" PostId="5259" Score="0" Text="Well, thanks very much; I appreciate the tip." CreationDate="2015-03-01T07:52:24.180" UserId="3510" />
  <row Id="5560" PostId="5248" Score="0" Text="What's the bias of coefficient calculated on 1000 rows-long sample when population has 10^10 observation?" CreationDate="2015-03-01T17:08:55.447" UserId="5224" />
  <row Id="5561" PostId="5244" Score="0" Text="I saw this post, and still decided to ask here, hoping that this new forum might gather more people with practical know-how than Cross Validated." CreationDate="2015-03-01T21:17:51.110" UserId="8310" />
  <row Id="5562" PostId="5248" Score="0" Text="@MarcinKosinski I'm not sure, but I pull samples of data that are anywhere from 10,000 - 30,000 rows from SQL Server and Hive on a daily basis. Sampling data in R from Hive and other databases has been a standard approach in my team at 2 different companies where I've worked. I expected that even if you had a Kendall Coefficient function in Hive it would be impractical to run it on all 10^10 rows without sampling, at least if it's something you plan to do more than once. That would be very time- and resource-intensive without buying you much." CreationDate="2015-03-02T02:39:41.620" UserId="2723" />
  <row Id="5563" PostId="5264" Score="1" Text="By grid world, do you mean discrete spaces? Please elaborate." CreationDate="2015-03-02T08:08:49.713" UserId="8491" />
  <row Id="5564" PostId="4875" Score="0" Text="Maybe I tend to always want to keep it simple, but why not just use Logistic Regression with some interaction terms?" CreationDate="2015-03-01T03:58:26.763" UserDisplayName="user8483" />
  <row Id="5566" PostId="5257" Score="0" Text="An upvote is something a person does, so presumably an upvote is a translation of an individual rating. There seem to be only two reasonable choice: either 5-star, or 4- and 5-star, ratings map to an upvote. Is there more to it than this?" CreationDate="2015-03-02T08:55:08.700" UserId="21" />
  <row Id="5567" PostId="5264" Score="1" Text="Yes, discrete spaces" CreationDate="2015-03-02T09:27:20.433" UserId="8013" />
  <row Id="5569" PostId="4991" Score="1" Text="It would be ideal to post a short summary of the article in this answer." CreationDate="2015-03-02T23:53:40.360" UserId="3466" />
  <row Id="5571" PostId="5268" Score="0" Text="maybe cross validate. randomly holdout 1 month of each backtesting year. Or do 1 year(or up to today) forward testing as validation." CreationDate="2015-03-03T13:15:58.133" UserId="7746" />
  <row Id="5572" PostId="5268" Score="0" Text="Unfortunately,  our users got unhappy when we tried to reserve time periods for validation as they (believe) they do their own validation so need all the data :-(.   Statistically speaking, is the screen more likely to be overfit if we randomly pulled months from the full set of results and checked if the screen still performed well?" CreationDate="2015-03-03T16:51:36.540" UserId="8344" />
  <row Id="5574" PostId="1165" Score="1" Text="Our start up is looking for academics to work on big data via stock analysis, if that field interests you, feel free to reach out." CreationDate="2015-03-03T21:28:45.627" UserId="8344" />
  <row Id="5575" PostId="1165" Score="0" Text="Oh, and I know that Universities are very interested in retention rates of students, so creating a model to predict retention rates based on the classes taken might be one proposal you could make to your university..." CreationDate="2015-03-03T21:31:32.303" UserId="8344" />
  <row Id="5576" PostId="2588" Score="0" Text="Nice answer (+1). I'm curious about your opinion (as a comment or an answer) on [this question](http://datascience.stackexchange.com/q/5108/2452) of mine, which I believe is relevant to this discussion." CreationDate="2015-03-03T23:27:18.197" UserId="2452" />
  <row Id="5577" PostId="5275" Score="1" Text="Virtually every job has time and resource constraints. As you said, this has been done before (e.g. mathematicians working on the ENIAC) The fact that data science is widespread doesn't mean it is a new job." CreationDate="2015-03-04T05:10:35.523" UserId="4621" />
  <row Id="5579" PostId="5268" Score="0" Text="if randomly pull, the sample performance tends to have a similar mean but more volatility which considered as a worse performance in most performance measures. Don't think this has anything to do with detecting overfit. I think forward testing might be a good way to go(as Meta trader does), normally traders will do this anyway" CreationDate="2015-03-04T11:34:19.617" UserId="7746" />
  <row Id="5580" PostId="5268" Score="0" Text="I wish all our users were wise enough to do it that way. As you'd think telling people that the developers of the system always trade virtually before investing in a screener would be enough! :-(.  I'm still hoping that someone has a few statistical tricks that will help at least a few people avoid disaster." CreationDate="2015-03-04T15:40:50.973" UserId="8344" />
  <row Id="5581" PostId="1227" Score="0" Text="Have you chosen any particular technology? This could be done with Neo4J and Cypher." CreationDate="2015-03-04T20:22:36.583" UserId="3466" />
  <row Id="5582" PostId="5264" Score="0" Text="Do these answers help you? Or is there something which is still unclear." CreationDate="2015-03-04T21:02:06.673" UserId="8491" />
  <row Id="5584" PostId="5251" Score="1" Text="Despite my respect for Dirk as an incredible contributor to R community, I disagree with his stance here. Even if (and it well may be) this answer is incorrect, partially or completely, I don't think it's a good idea to label and/or generalize people (especially, based on a single answer). Anony-Mousse is a nice contributor to SE community, especially CV and SO, so a bit more respect would nice. But, even regardless of anyone's contributions, I think that being more considerate results in constructive discussion, which, in turn, leads to higher quality professional environment." CreationDate="2015-03-05T03:03:16.783" UserId="2452" />
  <row Id="5585" PostId="5282" Score="0" Text="On StackExchange, such short answers are usually (with rare exceptions) preferred in a form of a comment. Posting answer implies a more detailed/comprehensive treatment of a topic. Just FYI." CreationDate="2015-03-05T03:06:06.607" UserId="2452" />
  <row Id="5586" PostId="5278" Score="0" Text="Thanks so much for your help." CreationDate="2015-03-05T05:38:33.900" UserId="8318" />
  <row Id="5589" PostId="5282" Score="0" Text="@Alex S Kinman multi hidden layer is common in deep learning architecture." CreationDate="2015-03-05T13:40:38.047" UserId="7746" />
  <row Id="5590" PostId="5282" Score="0" Text="@Aleksandr Blekh what if a question doesn't need a long answer at all? do you have a reference where your comment based on, meta, faq etc? thx" CreationDate="2015-03-05T13:43:04.033" UserId="7746" />
  <row Id="5591" PostId="5282" Score="0" Text="@Xin , I don't know enough about deep neural networks. But for traditional neural networks, the universal function approximation theorem states that a multilayer network with a single hidden layer can approximate any continuous decision function. I am curious, are there any results proving that DNN have more representational power than regular ANN?" CreationDate="2015-03-05T14:03:04.347" UserId="8517" />
  <row Id="5592" PostId="5282" Score="0" Text="@Xin: The following relevant SE Meta answers should give you the right impression, hopefully: [on what is considered a good answer](http://meta.stackexchange.com/a/129021/263279) and [on converting a comment to an answer by expanding the former](http://meta.stackexchange.com/a/248125/263279). I'm sure that there many more guidelines on Meta, but that should be enough." CreationDate="2015-03-05T14:07:20.373" UserId="2452" />
  <row Id="5593" PostId="5282" Score="0" Text="This is not a linearly separable problem because a single hyperplane cannot describe the decision boundary. A `4x2x1` network can describe the boundary. The two hidden layer neurons describe the two `AND` surfaces and the output neuron is the logical `OR` of the two `AND` surfaces." CreationDate="2015-03-05T14:55:48.873" UserId="964" />
  <row Id="5594" PostId="5282" Score="0" Text="Also, the statement that &quot;the number of neurons in the hidden layer doesn't need to exceed the number of inputs&quot; is false. Consider the simple problem of class 1 covering the x-y plane with a triangular region cut out for class 2. There are only two inputs for this network but it requires at least three hidden layer neurons to describe the triangular decision surface. One can easily construct examples that require *many* more hidden layer neurons than inputs (e.g., if class 2 lies in multiple triangular regions)." CreationDate="2015-03-05T15:03:07.343" UserId="964" />
  <row Id="5595" PostId="5282" Score="0" Text="@Bogatron : I actually drew the decision boundary for the variables (2 by 2) , and there was a linear decision boundary. What am I doing wrong ?" CreationDate="2015-03-05T17:07:16.317" UserId="8517" />
  <row Id="5596" PostId="5282" Score="0" Text="I don't know what you mean with &quot;2 by 2&quot;. The decision boundary for `y = (x1 ∧ x2) ∨ (x3 ∧ x4)` is in a 4-D space. &quot;Linearly separable&quot; implies that a single 4-D hyperplane can represent the decision boundary but that is not the case. There is no linear decision boundary that can correctly classify all 16 `y` values for this logical function." CreationDate="2015-03-05T18:00:57.187" UserId="964" />
  <row Id="5597" PostId="5282" Score="0" Text="@bogatron (not disputing you, just trying to figure out my error) in the 3-D feature space with dimensions [x1,x2,(x3*x4)] , there is a plane representing the decision boundary. Why won't that translate into a 4-D hyperplane in the [x1,x2,x3,x4] space ?" CreationDate="2015-03-05T23:10:59.267" UserId="8517" />
  <row Id="5599" PostId="2391" Score="0" Text="Take a look at information I've shared in [this related answer](http://datascience.stackexchange.com/a/5200/2452). I hope that it'll be helpful." CreationDate="2015-03-06T04:29:42.800" UserId="2452" />
  <row Id="5601" PostId="5282" Score="1" Text="@AlexSKinman I deleted my previous comment because the 3-D case I gave was a bad example (that one is indeed linearly separable). I tried training a 4x1 network on the `y = (x1 ∧ x2) ∨ (x3 ∧ x4)` problem numerous times and could not get it to learn the function (the best it would do was accuracy of 0.9375 (i.e., one input was always misclassified). Using a 4x2x1 network, the function was easily learned. If you believe you can construct a separating hyperplane, perhaps you could provide its coefficients (i.e., the `a` coefficients for `f(x) = a1*x1 + a2*x2 + a3*x3 + a4*x4 - a0`)." CreationDate="2015-03-06T06:05:28.453" UserId="964" />
  <row Id="5602" PostId="3699" Score="0" Text="as @Sidhha said, grid search is good way to go. I found Michael Nielsen's book([chap3](http://neuralnetworksanddeeplearning.com/chap3.html#how_to_choose_a_neural_network's_hyper-parameters)) is useful to understand how to choose hyper parameter in practice." CreationDate="2015-03-06T07:26:51.637" UserId="7746" />
  <row Id="5603" PostId="5289" Score="0" Text="Could you elaborate further on your answer? I myself *accept* what you said, which seems reasonable to happen, but don't actually understand why it is so." CreationDate="2015-03-06T09:33:35.240" UserId="84" />
  <row Id="5604" PostId="5024" Score="0" Text="I found the glmulti package useful for GA-like fitting.  One of the problems with &quot;best package&quot; questions is that without a good understanding of the nature of the problem, the data, and the goal - the means to get to the answer are unknonw." CreationDate="2015-03-06T16:48:06.033" UserId="8552" />
  <row Id="5605" PostId="5298" Score="1" Text="It's a very strange question idd. Maybe he ment CNNs. I agree with your answer though you *can* make connections like regression if you really need to. But in the end it's just 2 different things." CreationDate="2015-03-09T22:03:59.677" UserId="5316" />
  <row Id="5606" PostId="1185" Score="0" Text="It's not what you are asking. But you can take a look at this link of [Data Science](http://subasish.github.io/pages/data_sc_best_papers/) best papers." CreationDate="2015-03-09T00:24:48.157" UserId="8582" />
  <row Id="5607" PostId="5299" Score="0" Text="Well that does make some sense but doesn't it look like it's a vague connection?" CreationDate="2015-03-10T07:20:19.210" UserId="7960" />
  <row Id="5608" PostId="5298" Score="0" Text="+1 for helping with the approach. I think the clarification question you used would definitely be a good way to go about this" CreationDate="2015-03-10T07:21:34.103" UserId="7960" />
  <row Id="5609" PostId="5298" Score="0" Text="I guess I agree with the strange question thing. Yup, he could've meant CNNs. Idk as I was not asked this in any of my interviews. I read this question somewhere." CreationDate="2015-03-10T07:22:55.437" UserId="7960" />
  <row Id="5610" PostId="5299" Score="0" Text="It is vague but there just is no obvious connection. It's one of those interview questions that try to capture many things at once but just fail" CreationDate="2015-03-10T10:17:55.457" UserId="5316" />
  <row Id="5611" PostId="5261" Score="0" Text="Why do you include the line `sentence &lt;- str_replace_all(sentence, wordsDF[x,1], &quot; &quot;)`? I can't see why it's necessary and it's definitely slowing things down" CreationDate="2015-03-10T15:58:08.337" UserId="1156" />
  <row Id="5613" PostId="5310" Score="0" Text="I agree. I primarily tested with the weights initialized to 1 (I have my reasons) and it's how I first noticed the large variance. Then I initialized the weights using NW to check if the results would be better. There was some improvement but the large variance was still present." CreationDate="2015-03-12T13:28:36.473" UserId="8626" />
  <row Id="5614" PostId="5310" Score="0" Text="If you repeat training with the same set of initial weights, I would expect the same result (unless you have some sort of asynchronous processing going on). One other point: the difference in MSE between 0.5 and 0.5*10^-6 is only about 0.5, which isn't necessarily a large difference, depending on your training set size, number of outputs, and initial MSE." CreationDate="2015-03-12T15:00:01.067" UserId="964" />
  <row Id="5615" PostId="5310" Score="0" Text="The variance was still being observed with weights and biases all initialized to 1, which is why I felt confused when this happened. The data set is also consistent and it contains 10,000 values. There is less variance now even with such initialization, however everynow and then it tends to happen and it seems very strange. Is it possible that the error surface being created is different everytime  and hence the network sometimes gets stuck in local minimum by chance?" CreationDate="2015-03-12T15:44:52.797" UserId="8626" />
  <row Id="5616" PostId="5310" Score="0" Text="What makes you say that 0.5 vs. 0.5*10-6 is a large variance? What is the MSE at the start of training? Also, I'm suspicious about *all* weights being initialized to 1. If all the weights in an MLP are initialized to the same value, then I would expect all final weights for a given layer to converge to a common value." CreationDate="2015-03-12T15:53:26.390" UserId="964" />
  <row Id="5617" PostId="5310" Score="0" Text="I have one output since it's approximating a one dimensional cosine function, 10,000 values for training set size. The initial MSE (after the first epoch using Levenberg Marquardt BP) is 14.6268. The neural network has one hidden layer with 4 neurons and it is approximating cos(Pi). All the weights and biases are initialized to 1. It might be worth noting that the data division is happening at random (I'm calling dividerand in MATLAB) every time. Again, the dataset is consistent, and each point is evenly spaced out (that is the cosine function is being plotted with a fixed step)" CreationDate="2015-03-12T19:12:01.543" UserId="8626" />
  <row Id="5618" PostId="5308" Score="0" Text="Thanks for the reply, I'm going to check them all and update my situation later on." CreationDate="2015-03-13T04:08:40.653" UserId="8609" />
  <row Id="5619" PostId="5308" Score="1" Text="@KevinBelloMedina: You're welcome. Note that development of MALLET has been moved to GitHub: https://github.com/mimno/Mallet." CreationDate="2015-03-13T05:02:56.983" UserId="2452" />
  <row Id="5621" PostId="189" Score="0" Text="If you're still interested, the corresponding [CRAN Task View](http://cran.r-project.org/web/views/Optimization.html) contains references to a large number of relevant R packages." CreationDate="2015-03-13T07:37:36.660" UserId="2452" />
  <row Id="5622" PostId="4855" Score="0" Text="Are the vectors where you encounter this error *all* zeroes? If so, and you're not fitting an intercept/bias term, this is your problem" CreationDate="2015-03-13T13:30:24.820" UserId="1399" />
  <row Id="5623" PostId="4855" Score="0" Text="And you could fix it by simply including a bias" CreationDate="2015-03-13T13:30:36.933" UserId="1399" />
  <row Id="5624" PostId="5093" Score="1" Text="Something is wrong here. IQR is the gap between the 25th and 75th percentile of your data. By definition this must contain 50% of your data. Is there a parameter on the filter that regulates what multiple of IQR you should consider extreme? Typical values would be 2-3 * IQR" CreationDate="2015-03-13T15:34:25.067" UserId="1399" />
  <row Id="5626" PostId="5318" Score="0" Text="You could normalize the distribution of elements over clusters for each object, then compare them using the [KLD](http://en.wikipedia.org/wiki/Kullback–Leibler_divergence)" CreationDate="2015-03-14T00:34:27.393" UserId="381" />
  <row Id="5627" PostId="5316" Score="0" Text="Look at Termine, and papers associated with the project: http://www.nactem.ac.uk/software/termine/" CreationDate="2015-03-14T04:37:07.673" UserId="609" />
  <row Id="5628" PostId="3719" Score="1" Text="No, it's not; GraphX is not a database." CreationDate="2015-03-14T04:59:30.633" UserId="381" />
  <row Id="5630" PostId="5305" Score="1" Text="I'm voting to close this question as off-topic because questions about finding data are generally off topic here or belong at OpenData SE. Also I think you would want to define what data you need as there is a single &quot;the Twitter data set&quot;." CreationDate="2015-03-14T22:06:09.250" UserId="21" />
  <row Id="5632" PostId="5313" Score="0" Text="I realize you're not sure where to start, but this is quite broad. Maybe you can at least clarify what you want: a paper, a tool, a use case? what is your level of background?" CreationDate="2015-03-14T22:10:08.497" UserId="21" />
  <row Id="5634" PostId="5301" Score="1" Text="I suggest that we close this question on this site. It is a duplicate, cross-posted on Cross Validated and migrated to StackOverflow, where it already has an accepted answer: http://stackoverflow.com/q/28962097/2872891. Anyone, wanting to add their answer, can do so on SO." CreationDate="2015-03-15T02:16:51.593" UserId="2452" />
  <row Id="5635" PostId="5325" Score="0" Text="thanks for your answer. So in the case of regression, there is no idea of maximising a margin?" CreationDate="2015-03-15T20:43:33.710" UserId="8653" />
  <row Id="5636" PostId="5325" Score="0" Text="You've used the $\epsilon$-insensitive formulation by Vapnik, whose goal is to find the flattest (simplest) solution that minimizes the $\epsilon$-insensitive loss, $\mathcal L(x) = (|x| - \epsilon)^+$. I suppose you could say that decreasing $w$ would increase the margin--distance to the line--for a given $\epsilon$ (think of a straight line with $\epsilon$ margin in the vertical direction and mentally rotate it so its slope tends to zero), but I don't usually think of it this way." CreationDate="2015-03-15T21:27:03.533" UserId="381" />
  <row Id="5637" PostId="4852" Score="0" Text="If you have to use a data store with limitations on the number of columns, you could map an NxN structure onto one of MN X N/M. And then wrap it in a retrieval mechanism that retrieves M rows at a time." CreationDate="2015-03-16T13:15:26.073" UserId="7980" />
  <row Id="5638" PostId="1068" Score="0" Text="So far, it [&quot;parses&quot;](https://spark.apache.org/docs/1.0.2/mllib-decision-tree.html) CSV files by splitting using &quot;,&quot;. This fails [for example](http://en.wikipedia.org/wiki/Comma-separated_values#Example) if there are commas inside some field delimited with quotes. Not a good start, but I will keep checking spark along other libraries." CreationDate="2015-03-16T17:21:12.963" UserId="1281" />
  <row Id="5639" PostId="5324" Score="0" Text="Thanks for researching!" CreationDate="2015-03-16T20:29:51.433" UserId="8191" />
  <row Id="5640" PostId="1128" Score="0" Text="I have no particular expertise in this field, but would not a set of shift and scaling, and perhaps copy,  operators be enough to transform any time series into any other time series ?" CreationDate="2015-03-16T21:30:19.597" UserId="7980" />
  <row Id="5641" PostId="5318" Score="0" Text="First you need to define &quot;similarity&quot;, then define some statistical model in order to define probabilities. Without those, this can go nowhere." CreationDate="2015-03-16T22:58:15.713" UserId="471" />
  <row Id="5642" PostId="5328" Score="0" Text="Are you trying to detect anomalous behavior? It would help if you elaborate what exactly are you trying to achieve here." CreationDate="2015-03-17T06:03:03.197" UserId="1131" />
  <row Id="5643" PostId="5328" Score="0" Text="Yes we can say it anomalous, Actually the idea is to capture some specific malware behaviour." CreationDate="2015-03-17T07:13:25.703" UserId="4993" />
  <row Id="5644" PostId="5328" Score="0" Text="You should be looking for [Outlier analysis](http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html#example-covariance-plot-outlier-detection-py) in that case." CreationDate="2015-03-17T07:18:20.020" UserId="1131" />
  <row Id="5645" PostId="5332" Score="0" Text="Imagine pruning a tree all the way back to the root, would that make it more general ?" CreationDate="2015-03-17T07:19:55.070" UserId="7980" />
  <row Id="5646" PostId="5331" Score="0" Text="Are you asking how to get a list of pixel co-ordinates from an image of a character ?  I'm not sure what language you might be using, but in Mathematica this entire list would be returned by: Position[ImageData@Binarize@myCharacterImage, 0]" CreationDate="2015-03-17T07:32:35.543" UserId="7980" />
  <row Id="5647" PostId="5328" Score="0" Text="The question is not about how to find outliers, The question is how to tune outliers detection system performance in terms of detection." CreationDate="2015-03-17T08:35:37.430" UserId="4993" />
  <row Id="5648" PostId="5328" Score="0" Text="Would that not translate to using different kernels (linear, rbf, sigmoid) and tuning parameters for one-class SVM? Correct me if I understand this wrongly." CreationDate="2015-03-17T08:48:19.377" UserId="1131" />
  <row Id="5649" PostId="5328" Score="0" Text="Yes kernels methods helps in tuning classification, but the problem with malware behavioural detection technique is that these are not adaptable with input data environment. and perform poor if test data is not taken from the same environment which provide training data. this is mainly because of dynamic nature of network traffic." CreationDate="2015-03-17T10:10:53.760" UserId="4993" />
  <row Id="5651" PostId="1053" Score="0" Text="Turns out that since 4 days ago [Spark supports data frames](https://spark.apache.org/releases/spark-release-1-3-0.html) and probably loading from csv files, which is something. I will spend some time checking that." CreationDate="2015-03-17T11:45:25.617" UserId="1281" />
  <row Id="5652" PostId="5328" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/22019/discussion-between-sidhha-and-nahraf)." CreationDate="2015-03-17T14:10:01.300" UserId="1131" />
  <row Id="5653" PostId="5338" Score="1" Text="Thanks a lot for your answer! Maybe my question is not very clear, but I struggle more with the calculation of the Y value than with the diverging color scheme. I have the absolute values of the metric, but that doesn't really help me showing the &quot;speed&quot; of change, and if the &quot;speed&quot; is positive compared to the last &quot;step&quot;/&quot;time bucket&quot;." CreationDate="2015-03-17T14:55:41.753" UserId="8686" />
  <row Id="5654" PostId="5338" Score="0" Text="A line gradient solution with D3 can be found at http://bl.ocks.org/d3noob/3e72cafd95e1834f599b and (better) http://codepen.io/ekrembk/pen/ights/ for example..." CreationDate="2015-03-17T15:03:08.393" UserId="8686" />
  <row Id="5655" PostId="5338" Score="0" Text="Have a look at http://codepen.io/anon/pen/PwxaRy Yet, the problem with the Y values remains... Do you have an idea?" CreationDate="2015-03-17T15:18:51.850" UserId="8686" />
  <row Id="5657" PostId="5338" Score="1" Text="You're very welcome! I couldn't display the result of your most recently referred code, as it's anon. Anyway, I don't see calculation of speed/rate of change as a problem. Rate of chage $R_i$ (for a time delta $\Delta t_i$) = $\frac{Y_{i+1} - Y_i}{\Delta t_i}$. So your $Y$ for the trend chart should be the $R_i$. What is the problem with that?" CreationDate="2015-03-17T15:45:03.317" UserId="2452" />
  <row Id="5658" PostId="5338" Score="0" Text="I already calculate the change rates. That's not the problem. Normally, the change rates (with my metric (except errors)) will be &gt;= 0. What I want to with my graph is the &quot;speed&quot; that the values of the metric change from `t0` to `t1`. If the &quot;speed&quot; slows down, I want to show a negative trend. This is IMHO not possible with just the change rate." CreationDate="2015-03-17T15:50:04.433" UserId="8686" />
  <row Id="5659" PostId="5338" Score="1" Text="@Tobi: Very easy (if I understand you correctly) - you just need to calculate [second derivative](http://en.wikipedia.org/wiki/Second_derivative) (rate of change of rate of change) at your predefined intervals. Those will be your $Y$ values." CreationDate="2015-03-17T16:04:28.490" UserId="2452" />
  <row Id="5660" PostId="5338" Score="0" Text="Sorry for being a total noob at this. You mean as described at http://en.wikibooks.org/wiki/Numerical_Methods/Numerical_Differentiation#Second_Derivative" CreationDate="2015-03-17T16:29:07.390" UserId="8686" />
  <row Id="5661" PostId="5338" Score="1" Text="@Tobi: No need to apologize. Nobody knows everything :-). Yes, I believe that's the _quadratic approximation_. There are lots of resources on the topic. Have a look at [this discussion](http://math.stackexchange.com/q/810454), for example. Good luck and let me know, if I can be of more help!" CreationDate="2015-03-17T16:42:52.530" UserId="2452" />
  <row Id="5662" PostId="1125" Score="0" Text="Check [this relevant answer](http://stats.stackexchange.com/a/136760/31372) of mine on Cross Validated. I hope that it will be of your interest." CreationDate="2015-03-17T17:24:15.250" UserId="2452" />
  <row Id="5663" PostId="5331" Score="0" Text="I've already written a script in python which returns a list of points (x,y) on button-1 click (left click). (As we draw in paint, we keep left click pressed ) ..... I'll have to draw all the characters of the keyboard in order to generate the training data, Which is tedious. So, I am interested if any standard such databse already exists?" CreationDate="2015-03-17T18:23:19.520" UserId="8681" />
  <row Id="5664" PostId="5331" Score="0" Text="For example, you might consider automatically converting an existing dataset, for instance the NMIST digits you mention, using something like the above function applied to every character image file. It would scale to thousands of images quite easily." CreationDate="2015-03-17T19:07:38.503" UserId="7980" />
  <row Id="5665" PostId="5331" Score="0" Text="Do a google image search for handwritten characters and you will get a lot of examples." CreationDate="2015-03-17T20:03:05.083" UserId="7980" />
  <row Id="5667" PostId="5303" Score="1" Text="Please post your PCA matrix which maps your data to PCA vectors.  I'd like to figure out what the two paths mean." CreationDate="2015-03-17T20:56:11.513" UserId="1077" />
  <row Id="5669" PostId="1128" Score="0" Text="That sort of set sounds plausible, to me.  I'd like to find, either, an explicit set that is used in some practical situation.... or a specific theoretic set, ideally accompanied by an (informal?) &quot;proof&quot; that this set is &quot;sufficient&quot; - in, erm, any sense that that makes sense." CreationDate="2015-03-17T22:13:20.233" UserId="3328" />
  <row Id="5670" PostId="5331" Score="0" Text="First, I don't know how to sample those data. Even if I do it, my array ( obtained from the image) will contain entries from left to right scanning of the image. I want entries according to user strokes, not those obtained by left to right ( n up-down) scanning of the image sample. Could you suggest a way to get entries as user strokes them?" CreationDate="2015-03-18T03:13:39.353" UserId="8681" />
  <row Id="5671" PostId="5331" Score="0" Text="I.e. Entries as per user hand moves while writing a character." CreationDate="2015-03-18T03:14:24.663" UserId="8681" />
  <row Id="5672" PostId="5343" Score="0" Text="thank you for the reply. Can you recommend any good Aho-Corasick implementation/flavor that can handle extended matching features (* and ? ) and preferably SIMD instructions? Naive implementations I tried were quite slow and didn't have extended matching features." CreationDate="2015-03-18T07:10:24.150" UserId="8597" />
  <row Id="5673" PostId="5343" Score="0" Text="Patterns are to be changed moderately often, so I could recompile the automaton once it happens." CreationDate="2015-03-18T07:28:48.913" UserId="8597" />
  <row Id="5674" PostId="5344" Score="0" Text="What a muddle. Are you aware that your source is discussing gradient descent in the context of [reinforcement learning](http://en.wikipedia.org/wiki/Reinforcement_learning)? This is used when you are facing situations that change over time; esp. in response to actions you have taken. If this is not your case, I refer you to the [illustrated explanation in Wikipedia](http://en.wikipedia.org/wiki/Gradient_descent)." CreationDate="2015-03-18T08:19:22.737" UserId="381" />
  <row Id="5675" PostId="5344" Score="0" Text="cool, good looks on that. but it doesn't really change the nature of my question." CreationDate="2015-03-18T08:22:23.997" UserId="8285" />
  <row Id="5677" PostId="5338" Score="0" Text="Sorry, I'm still having problems with the calculation. Could you please give me an examle with the above provided sample data?" CreationDate="2015-03-18T12:44:43.900" UserId="8686" />
  <row Id="5678" PostId="5338" Score="0" Text="@Tobi: What language are we talking about? JavaScript? Also, does your column `change_rate_delta` contains already pre-calculated values, or `change_rate` should be used?" CreationDate="2015-03-18T20:19:38.400" UserId="2452" />
  <row Id="5679" PostId="5345" Score="1" Text="How about Sense - A Next-Generation Platform for Data Science(http://blog.sense.io/introducing-sense-a-platform-for-data-science/). quote &quot;Sense brings together the most powerful tools — R, Python, Julia, Spark, Impala, Redshift, and more — into a unified platform to accelerate data science from exploration to production.&quot;" CreationDate="2015-03-19T02:32:19.980" UserId="1003" />
  <row Id="5680" PostId="5351" Score="0" Text="Just a clarification: when I said &quot;I don't know why one would prefer other options&quot; that statement implied exclusion of Emacs fans - they have special preferences and obviously gravitate toward Emacs-based R solutions :-)." CreationDate="2015-03-19T06:15:06.457" UserId="2452" />
  <row Id="5683" PostId="5352" Score="0" Text="Taking into accounjt this fact http://blog.revolutionanalytics.com/2015/01/revolution-acquired.html we can expect further support to R from Microsoft" CreationDate="2015-03-19T09:29:19.500" UserId="97" />
  <row Id="5684" PostId="5331" Score="0" Text="Sorry, I misunderstood your needs. There appears to be some work on stroke order on Chinese characters.  http://commons.wikimedia.org/wiki/Commons:Stroke_Order_Project" CreationDate="2015-03-19T10:39:12.730" UserId="7980" />
  <row Id="5685" PostId="5331" Score="0" Text="@image_doctor : thanks. You are awesome. :)&#xA;Though I'll still need some help for Roman script." CreationDate="2015-03-19T14:45:02.187" UserId="8681" />
  <row Id="5686" PostId="5353" Score="0" Text="thank you for the great links!" CreationDate="2015-03-19T19:52:54.130" UserId="8597" />
  <row Id="5688" PostId="5338" Score="0" Text="JavaScript/Node.js is my preferred language. As far as I understood, it should be possible to do the calc based on the raw metric values provided with the sample data. But I think I'm doing something wrong. Thanks again for your effort!" CreationDate="2015-03-20T08:37:33.423" UserId="8686" />
  <row Id="5689" PostId="5344" Score="0" Text="value of feature_j is just the mean of the value of the j-th element in all the input vectors" CreationDate="2015-03-20T09:27:26.953" UserId="2576" />
  <row Id="5690" PostId="5344" Score="0" Text="so for the weight associated with the word 'j', find the corresponding value in each of the training example files, and then take the mean of that, and that's what I multiply the loss function by in the update?" CreationDate="2015-03-20T09:33:38.270" UserId="8285" />
  <row Id="5694" PostId="5330" Score="0" Text="thanks. It turns out the issue was `Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF,]` was not doing what I intended. I had thought it would interpret the T/F in that column in the data frame temp and use that as inclusion/exclusion criteria. Changing that to `Expt1_C1_EXONS_dxd_res_wDesc[temp$FGF==&quot;TRUE&quot;,]` works and takes almost no time. Examining the results of the incorrect version, it produced a huge file with all NA's. So my question is now, why didn't it take my temp$FGF values and what was it actually doing in that case? Thanks" CreationDate="2015-03-18T12:01:30.120" UserDisplayName="user8706" />
  <row Id="5695" PostId="5354" Score="0" Text="Thanks. I will try gwidgets too." CreationDate="2015-03-20T17:02:36.027" UserId="8662" />
  <row Id="5696" PostId="5358" Score="0" Text="This is entirely dependent on the software you are using to manage your documents. What are you using?" CreationDate="2015-03-20T17:26:29.933" UserId="3466" />
  <row Id="5697" PostId="5357" Score="4" Text="This question appears to be primarily opinion based. Please consider rephrasing. Maybe ask what kinds of data science tools are available for C/C++, or what kinds of applications use these languages." CreationDate="2015-03-20T17:30:39.807" UserId="3466" />
  <row Id="5700" PostId="5357" Score="1" Text="@sheldonkreger  That is what I'm asking, I'll make that more clear, thanks" CreationDate="2015-03-20T17:44:09.740" UserId="2723" />
  <row Id="5701" PostId="5357" Score="0" Text="Awesome, thanks." CreationDate="2015-03-20T17:46:43.700" UserId="3466" />
  <row Id="5703" PostId="5360" Score="0" Text="I don't think there is a way to achieve this without having a unified access-check mechanism using an architecture similar to what I described above. You need to find a way to, EVERY time a file is requested - regardless of which system it is on - validate that the user requesting it has the correct permission. How you implement that will depend entirely on what kind of software you are using now." CreationDate="2015-03-21T04:22:01.043" UserId="3466" />
  <row Id="5704" PostId="5345" Score="1" Text="@scyen: Sense and similar products (or, rather, the approach) are indeed interesting, however, they are not &quot;IDE alternatives for R programming&quot;, but large, complex and often expensive **platforms** for data science work. Note that this question is specifically about development environments / IDEs." CreationDate="2015-03-21T23:53:10.237" UserId="2452" />
  <row Id="5706" PostId="5366" Score="0" Text="Links aren't considered valid answers on this site. Please answer the original question in your post and use links to supplement your answer." CreationDate="2015-03-23T17:40:37.060" UserId="3466" />
  <row Id="5712" PostId="5358" Score="0" Text="I don't have vote-to-close karma on this SE yet, but this seems way off-topic." CreationDate="2015-03-24T12:34:52.410" UserId="8798" />
  <row Id="5713" PostId="4949" Score="0" Text="If you feel like you have an answer (as in your EDIT), consider posting it as an actual answer. It's OK to answer your own questions on Stack Exchange, and it makes things clearer." CreationDate="2015-03-24T12:37:08.230" UserId="8798" />
  <row Id="5714" PostId="5358" Score="0" Text="@JoeGermuska: Yeah, you can ask a moderator to close it. I can't delete the question because it has an answer. Would be helpful if the [What topics can I ask about here?](http://datascience.stackexchange.com/help/on-topic) would give a hint what was on topic and what wasn't. Why do you think this was off-topic when it's not defined what is *on* topic?" CreationDate="2015-03-24T12:38:43.823" UserDisplayName="user8743" />
  <row Id="5715" PostId="5357" Score="1" Text="I've used Waffles (C++) to incorporate machine learning into existing C++ engines." CreationDate="2015-03-24T18:11:02.200" UserId="1077" />
  <row Id="5716" PostId="5365" Score="0" Text="Because languages like R and Python are very slow/inefficient compared to languages like C. Thus when dealing with lots of data and computations, if you can do something in C it's faster than if you can do it in R. I do love and use Hadley's packages tho!" CreationDate="2015-03-24T18:25:56.267" UserId="2723" />
  <row Id="5717" PostId="5357" Score="0" Text="@Pete if you can incorporate that into an answer I'd be likely to mark it as the solution" CreationDate="2015-03-24T18:28:05.220" UserId="2723" />
  <row Id="5718" PostId="5366" Score="0" Text="OK, thanks, will do." CreationDate="2015-03-24T18:36:36.600" UserId="8762" />
  <row Id="5719" PostId="5383" Score="1" Text="Sure it will. If you suddenly get a new group of people on your site with a specific fetish, you can be sure that certain items will spike in their values." CreationDate="2015-03-25T09:59:06.760" UserId="587" />
  <row Id="5722" PostId="5357" Score="1" Text="Meta toolkit is available in C++ : https://meta-toolkit.github.io/meta/. There's a course on Coursera that uses it, it's still in week 1, so you may want to take a look. The course is called &quot;Text Retrieval and Search Engines&quot;." CreationDate="2015-03-25T12:29:46.440" UserId="587" />
  <row Id="5723" PostId="5388" Score="0" Text="Excellent answer (+1). Just one suggestion: if possible, provide literature or, at least, general references for the _shallow NLP technique_ that you've mentioned." CreationDate="2015-03-25T13:17:54.010" UserId="2452" />
  <row Id="5724" PostId="5357" Score="0" Text="@LauriK OK, why are you guys putting the best answers in comments instead of as actual answers?? ;)  Seriously, I upvoted the answers because they were great comments, but the actual answers I was looking for like this are in comments!" CreationDate="2015-03-25T13:25:00.010" UserId="2723" />
  <row Id="5725" PostId="5370" Score="1" Text="Thanks, Andre. I do use Pybrain a lot; for me Python is a middleground in between R and C, but I still wanted to learn C for both speed and wider application of the code. I selected this as the solution because I had not thought of using C/C++ to write R extensions, which is a really wonderful idea that I'm absolutely going to do. Thanks!!" CreationDate="2015-03-25T13:27:40.333" UserId="2723" />
  <row Id="5726" PostId="5388" Score="0" Text="Thank you so much. Two questions, can I do this with nltk?  Could I use tf-idf to do the same, then take the most unique words (highest scores) as my key words?" CreationDate="2015-03-25T13:30:04.140" UserId="8643" />
  <row Id="5729" PostId="5388" Score="0" Text="@ Aleksandr Blekh, thanks. I have added additional reading links for learning more about shallow and deep NLP. Hope this helps" CreationDate="2015-03-25T13:56:20.720" UserId="8465" />
  <row Id="5730" PostId="5388" Score="0" Text="@ William Falcon, thanks. 1) Yes, you can use nltk 2) Absolutely, TF-IDF can be used If you are trying to find the concept or theme at document(s) level." CreationDate="2015-03-25T13:59:47.023" UserId="8465" />
  <row Id="5731" PostId="819" Score="0" Text="PostgreSQL also has HSTORE columns and (recently) JSON which provide an outlet for schemaless data associated with rows which are otherwise typed." CreationDate="2015-03-25T20:07:27.683" UserId="8798" />
  <row Id="5732" PostId="5357" Score="0" Text="@Hack-R thanks :)" CreationDate="2015-03-25T20:11:32.137" UserId="587" />
  <row Id="5734" PostId="739" Score="0" Text="Ask Quora might help." CreationDate="2015-03-26T03:41:01.120" UserId="1048" />
  <row Id="5736" PostId="5384" Score="0" Text="These look like great references. Thanks for pointing them out! After a first glance, I think they'll do great to get me started in studying this." CreationDate="2015-03-26T14:33:20.330" UserId="8692" />
  <row Id="5742" PostId="5383" Score="2" Text="User behavior defines item similarity, or at least that's what I take from your question. how can user behavior not change item similarity?" CreationDate="2015-03-27T11:27:06.387" UserId="21" />
  <row Id="5743" PostId="5393" Score="0" Text="The question isn't actually complete..." CreationDate="2015-03-27T11:27:55.127" UserId="21" />
  <row Id="5744" PostId="5401" Score="0" Text="Requests for book recommendations and the like are generally off topic on StackExchange" CreationDate="2015-03-27T11:28:34.987" UserId="21" />
  <row Id="5745" PostId="5407" Score="0" Text="Thanks. Most of the data I'm analyzing is in multi-dimensional form. NoSQL solutions and other cloud based solutions are not really an option for me. Thus, it doesn't make sense to look towards Azure. However, I'll check out the blog post. Unfortunately, SSAS hasn't really improved all that much over the years and I am on 2008 R2." CreationDate="2015-03-27T11:45:11.783" UserId="8842" />
  <row Id="5746" PostId="5407" Score="0" Text="By the way, can you add more insight on the limited statistical methods and algorithm variety? I assumed you could develop your own methods and algorithms with SSAS just like you could with R? Or do you mean the availability of those packages is not as common as the open source community?" CreationDate="2015-03-27T11:51:04.613" UserId="8842" />
  <row Id="5747" PostId="5407" Score="0" Text="@Fastidious: You're welcome. Re: your first comment (I'll address the second one in the next comment). I don't understand your rationale behind rejecting Azure ML. While I'm not a big fan of Microsoft solutions, but for those who are tied to that technology stack, Azure ML seems like a sensible (while, for some, still might not be the best) option. Azure ML has direct ties with Azure, which, being a general cloud platform, supports whatever you throw at it. Wrap whatever environment and tools you use into (or install them separately on) a virtual machine and launch it. It is not cheap, though." CreationDate="2015-03-27T11:55:06.370" UserId="2452" />
  <row Id="5748" PostId="5407" Score="0" Text="@Fastidious: Absolutely. The richness of R ecosystem in terms of community and packages is unparalleled. Even considering recent Microsoft's acquisition of R-focused company Revolution Analytics, it is not going to change the overall situation (while it might improve some MS offerings, add integrations, etc.). R's foothold is way too solid. So, in theory, you can develop your data science projects on SSAS, but you'll experience, at least, two main obstacles: 1) limited variety of packages; 2) limited options for language and platform interoperability. If your needs are modest, it might be OK." CreationDate="2015-03-27T12:07:21.623" UserId="2452" />
  <row Id="5749" PostId="5407" Score="0" Text="The rational is because of strict policies of passing data outside of our network via API's and of course the fact the data is terabytes of data or gigabytes of data per day. So, storage costs, bandwidth, processing power and so forth are deep considerations depending on the costs. Then of course there is API development costs too. I can't just jump to a cloud solution at the drop of a hat hah. :)" CreationDate="2015-03-27T12:27:38.920" UserId="8842" />
  <row Id="5750" PostId="5407" Score="0" Text="@Fastidious: I see. Well, this situation requires a detailed and careful analysis of all factors involved, which hopefully will produce an optimal solution. Good luck!" CreationDate="2015-03-27T13:20:36.140" UserId="2452" />
  <row Id="5751" PostId="5393" Score="0" Text="my full quesetion doesn't appear to be showing. Is there a page error? This is what I see on edit." CreationDate="2015-03-27T15:25:37.753" UserId="8823" />
  <row Id="5752" PostId="5393" Score="0" Text="I have been looking at Stochastic Single Value Decomposition in Mahout for implementing a distributed LSA algorithm. However, I am having trouble finding the best way to set k and p such that k+p&lt;min(m,n). Is there an optimal way to set k and p? I know that p should not exceed 10% of k and that k is the rank (typically from 20 to 200 according to the documents). Can I relate it to the number of dependent vectors?" CreationDate="2015-03-27T15:25:43.987" UserId="8823" />
  <row Id="5753" PostId="5393" Score="0" Text="Fixed it. You had a &lt; that was not followed by space so became an unclosed tag." CreationDate="2015-03-27T15:37:44.240" UserId="21" />
  <row Id="5754" PostId="5393" Score="0" Text="ok thanks, please let me know if I need more" CreationDate="2015-03-27T15:39:32.450" UserId="8823" />
  <row Id="5755" PostId="5407" Score="1" Text="No worries. We do use R and feed data to it via SQL Server. I just wanted to see if we can still leverage everything in SQL too." CreationDate="2015-03-27T17:55:40.993" UserId="8842" />
  <row Id="5756" PostId="5394" Score="0" Text="thanks, but I'm still wondering does the small contribution of $UV$ imply that, to fit my data, I actually doesn't need this non-linear part?" CreationDate="2015-03-28T07:31:39.480" UserId="7867" />
  <row Id="5757" PostId="5401" Score="0" Text="@SeanOwen: Actually, I'm not sure that book recommendations should be considered off-topic here. My logic and experience of Cross Validated, which is a serious forum, suggests that book recommendations are OK, as long as the requests are related to the topic of discussion. Please let me know, if there exist a corresponding DS Meta discussion, so that I could share my opinion." CreationDate="2015-03-28T13:58:35.710" UserId="2452" />
  <row Id="5758" PostId="5412" Score="0" Text="Your algorithm is incomprehensible from your description so far. I can see you want to label some words as relevant or not relevant, and use that to train a NN from the sentences, in order to find relevant words in other sentences. But I cannot see what representation you are using for the sentence. Could you explain perhaps with a little pseudo-code?" CreationDate="2015-03-28T16:42:41.010" UserId="836" />
  <row Id="5759" PostId="5413" Score="0" Text="also, jpeg images cannot be the same title, so wouldn't the ML program recognize every image as its own class if all titles different?" CreationDate="2015-03-28T22:36:22.733" UserId="8860" />
  <row Id="5760" PostId="5412" Score="0" Text="added some explanation." CreationDate="2015-03-29T03:37:39.607" UserId="7712" />
  <row Id="5761" PostId="5414" Score="0" Text="This is right. I asked someone at Ersatz Labs too. Same answer: group class by folder. Thanks." CreationDate="2015-03-29T20:16:16.590" UserId="8860" />
  <row Id="5762" PostId="5414" Score="0" Text="by the way, for that Kaggle link. That is a TON of classes for only 30K images. Interesting. Thanks for sharing that." CreationDate="2015-03-29T20:18:31.723" UserId="8860" />
  <row Id="5763" PostId="5387" Score="0" Text="Thanks @LauriK for your comment. Would you mind elaborating what language or program that automated scripts are written on? I assume you have scripts written on sql. How do you automate them? what software your dashboards are made of? Please give a detailed example as I am trying to adopt something similar to what you have." CreationDate="2015-03-29T21:26:17.557" UserId="7923" />
  <row Id="5764" PostId="5412" Score="1" Text="I am still not sure that I understand your idea. However, the usual way to test such ideas is to build them and score them against other approaches. A sliding window with offsets is similar to n-grams, and it is definitely possible to train a neural network to predict likely next words from n-grams." CreationDate="2015-03-30T07:24:30.700" UserId="836" />
  <row Id="5765" PostId="5387" Score="0" Text="@JeanVids edited the answer." CreationDate="2015-03-30T11:19:12.430" UserId="587" />
  <row Id="5766" PostId="5412" Score="0" Text="You may want to take a look at a course that is specifically about this: https://www.coursera.org/course/textretrieval" CreationDate="2015-03-30T11:23:37.113" UserId="587" />
  <row Id="5767" PostId="5358" Score="0" Text="Duplicate of [Solr Permissions / Filtering Results depending on Access Rights](http://stackoverflow.com/questions/9222835/solr-permissions-filtering-results-depending-on-access-rights)." CreationDate="2015-03-30T14:23:08.267" UserDisplayName="user8743" />
  <row Id="5769" PostId="5358" Score="0" Text="I'm voting to close this question as off-topic because poster requested to close as off-topic." CreationDate="2015-03-31T08:10:38.127" UserId="21" />
  <row Id="5771" PostId="5284" Score="0" Text="http://blog.cloudera.com/blog/2009/07/file-appends-in-hdfs/" CreationDate="2015-04-01T06:50:37.207" UserId="118" />
  <row Id="5772" PostId="5284" Score="0" Text="http://stackoverflow.com/questions/9162943/how-does-hdfs-with-append-works" CreationDate="2015-04-01T06:51:04.357" UserId="118" />
  <row Id="5773" PostId="2454" Score="0" Text="Take a look at AWS lambda service http://aws.amazon.com/ru/lambda/" CreationDate="2015-04-01T06:53:12.893" UserId="118" />
  <row Id="5775" PostId="5429" Score="0" Text="time to read more on Monte-Carlo and multidimensional statistical analysis...thanks for nice explanation..!!!" CreationDate="2015-04-02T04:58:45.300" UserId="6465" />
  <row Id="5776" PostId="5429" Score="0" Text="@hadooper How about an up-vote?" CreationDate="2015-04-02T06:00:55.527" UserId="609" />
  <row Id="5777" PostId="5429" Score="2" Text="It requires 15 rep points...!!!" CreationDate="2015-04-02T06:07:21.577" UserId="6465" />
  <row Id="5779" PostId="5429" Score="0" Text="@hadooper: Just gave you the vote you needed to vote, cheers!" CreationDate="2015-04-02T14:40:22.390" UserId="158" />
  <row Id="5781" PostId="5435" Score="2" Text="`**` is an old-fashioned symbol (originating from the original Algol and Fortran languages in the 1950's) for exponentiation." CreationDate="2015-04-02T21:55:54.550" UserId="2825" />
  <row Id="5782" PostId="5435" Score="0" Text="So in other words it is simply an exponent, ie. O(N^3)? @whuber" CreationDate="2015-04-02T21:58:22.793" UserId="8936" />
  <row Id="5783" PostId="5435" Score="0" Text="Not quite: &quot;3&quot; is the exponent, &quot;\*\*&quot; is the exponentiation operator.  &quot;N\*\*3&quot; nowadays is often written (in text) as &quot;N^3&quot;, it's read aloud as &quot;N cubed,&quot; and can be computed as N\*N\*N." CreationDate="2015-04-02T21:59:50.127" UserId="2825" />
  <row Id="5784" PostId="5435" Score="0" Text="Right, that's what I meant. :) Thank you! @whuber" CreationDate="2015-04-02T22:16:45.733" UserId="8936" />
  <row Id="5785" PostId="5401" Score="0" Text="@SeanOwen Oh. I didn't know that. This was my first question on StackExchange. I will try to be more careful next time." CreationDate="2015-04-02T23:47:41.343" UserId="8839" />
  <row Id="5786" PostId="5402" Score="0" Text="Thank you for your kind answer." CreationDate="2015-04-02T23:48:45.907" UserId="8839" />
  <row Id="5787" PostId="5437" Score="1" Text="Thanks a lot Aleksandr Blekh. I am looking into your suggestions now." CreationDate="2015-04-02T23:52:36.297" UserId="3314" />
  <row Id="5788" PostId="5437" Score="0" Text="@user62198: You're very welcome. Good luck!" CreationDate="2015-04-02T23:55:17.020" UserId="2452" />
  <row Id="5789" PostId="5440" Score="0" Text="Thanks.  That's definitely useful." CreationDate="2015-04-03T02:48:13.807" UserId="8938" />
  <row Id="5790" PostId="5437" Score="0" Text="By &quot;automatically recode those values&quot;, do you mean that I consider the missing values as a separate factor level ? Could you please comment ? Thanks.." CreationDate="2015-04-03T02:51:48.080" UserId="3314" />
  <row Id="5791" PostId="5441" Score="0" Text="All useful terms, but I think agree that performativity is the most applicable, at least in terms of trying to express the concept in writing.  Obviously, there is a fair bit of heterogeneity of terminology across fields, though.  So, I'm not sure that any of these are really going to help me with any literature research.  But that's probably the term that I'm going to try to use." CreationDate="2015-04-03T02:52:42.213" UserId="8938" />
  <row Id="5792" PostId="5437" Score="0" Text="@user62198: No. I meant the recoding in `R` parlance: `''` =&gt; `NA`." CreationDate="2015-04-03T02:56:38.767" UserId="2452" />
  <row Id="5793" PostId="5440" Score="0" Text="@jsmith54: My pleasure." CreationDate="2015-04-03T02:57:15.770" UserId="2452" />
  <row Id="5794" PostId="5429" Score="0" Text="upvoted...!!!!!" CreationDate="2015-04-03T05:00:15.530" UserId="6465" />
  <row Id="5795" PostId="5438" Score="0" Text="Yes, &quot;feedback loop&quot; is how I would always describe this." CreationDate="2015-04-03T09:28:23.903" UserId="21" />
  <row Id="5797" PostId="5437" Score="0" Text="Thanks much. Your answer was very helpful." CreationDate="2015-04-03T13:18:31.290" UserId="3314" />
  <row Id="5798" PostId="5438" Score="0" Text="I think feedback loop is definitely useful to say, but the literature on performativity and reflexivity seem to address more of the complex interplay that I was trying to discuss.  So I think it's useful to say something like, &quot;A model that can act on the concept to change the concept can thus change itself in a sort of feedback loop.  Let's call this property (performativity/reflexivity).  This is distinct from concept drift, because the model is the cause of the drift.&quot;" CreationDate="2015-04-03T15:08:39.797" UserId="8938" />
  <row Id="5799" PostId="5437" Score="0" Text="@user62198: My pleasure. I'm glad I could help. Feel free to upvote as well :-)." CreationDate="2015-04-03T18:01:10.613" UserId="2452" />
  <row Id="5800" PostId="339" Score="14" Text="&quot;Speed: R software initially had problems with large computations (say, like nxn matrix multiplications). But, this issue is addressed with the introduction of R by Revolution Analytics. They have re-written computation intensive operations in C which is blazingly fast. Python being a high level language is relatively slow.&quot;&#xA;&#xA;I'm not an experienced R user, but as far as I know pretty much everything with low-level implementations in R also has a similar low-level implementation in numpy/scipy/pandas/scikit-learn/whatever. Python also has numba and cython. This point should be a tie." CreationDate="2015-04-03T22:05:28.860" UserId="2885" />
  <row Id="5801" PostId="5442" Score="0" Text="Their data is almost certainly not public." CreationDate="2015-04-03T22:06:40.433" UserId="2885" />
  <row Id="5802" PostId="5444" Score="3" Text="I would never fault someone for not knowing Hadoop but even in small data situations I feel as if R is superior.  There are simply a miriad of things you can do with R that you can't do with Excel.  It concerns me this individual has not &quot;discovered&quot; that in his 15+ years" CreationDate="2015-04-03T23:30:42.300" UserId="8944" />
  <row Id="5803" PostId="5444" Score="0" Text="@JHowIX: Are your familiar with the term &quot;good enough&quot;? I'm also a big fan of R and would prefer it to many tools, Excel included, any day. However, the fact that R can do more doesn't imply that Excel (or any other tool suitable for a task) is inferior in a particular work context. So, while your concern is valid (I refer to that by using word &quot;disturbing&quot;), it might be that the person haven't had an opportunity/need to do that. Remember, that you're talking about time, when R existed, but was popular mostly in academia and data science (termed data analysis or such) was not as hot as today." CreationDate="2015-04-03T23:39:50.293" UserId="2452" />
  <row Id="5804" PostId="5442" Score="0" Text="https://datafloq.com/read/big-data-obama-campaign/516" CreationDate="2015-04-03T23:40:22.117" UserId="8860" />
  <row Id="5808" PostId="5443" Score="0" Text="Most data science job ads ask for specific skills, like R, Hadoop, whatever. Did you neglect to mention this in your ad? Unless your new Data Scientist is going to work in a bubble then he or she will have to work with the team, and probably need to work with the standard team software..." CreationDate="2015-04-04T08:50:54.213" UserId="471" />
  <row Id="5809" PostId="5443" Score="1" Text="well if they won't use `\LaTeX{}` then i wouldn't hire 'em.  just kidding..." CreationDate="2015-04-04T17:47:03.633" UserId="8953" />
  <row Id="5811" PostId="5457" Score="0" Text="While active learning (AL) seems like a close enough concept, I think that there is a significant difference between AL and situation, described by the OP - the former implies **intentional supervising efforts**, while the latter implies the **lack** of such efforts and, even, intentions. Therefore, my preference is either _side effect_, or _feedback loop_ terms." CreationDate="2015-04-05T08:45:36.657" UserId="2452" />
  <row Id="5812" PostId="5457" Score="0" Text="I'd say that active learning, while definitely related, tends to refer to a fairly specific technique intentionally deployed, rather than the unintentional consequence of an arbitrary model type acting on the concept.  It strikes me as similar to reinforcement learning.  These techniques are likely useful, once you've realized you have a feedback loop in your system, but I agree with Aleksandr that they express a design intention I was implying does not exist in my example." CreationDate="2015-04-05T18:53:31.557" UserId="8938" />
  <row Id="5814" PostId="5443" Score="1" Text="@Spacedman:  I provided the story for anecdotal context but am really more interested in people's views on excel than I am hiring tips.  Our team is free to use whatever tools we like." CreationDate="2015-04-05T23:15:49.570" UserId="8944" />
  <row Id="5815" PostId="5458" Score="0" Text="Have you already manually assigned these 12 category labels to some or all of your dataset?  If you haven't, then this becomes more a problem of unsupervised clustering than supervised classification." CreationDate="2015-04-06T01:20:22.777" UserId="182" />
  <row Id="5817" PostId="5435" Score="0" Text="@whuber `**` is very much still alive and used in Python, PERL and [PHP](http://stackoverflow.com/questions/1211514/raising-to-power-in-php) for exponentiation. That's because `^` generally denotes bitwise-xor." CreationDate="2015-04-06T11:51:26.427" UserId="8501" />
  <row Id="5818" PostId="5449" Score="0" Text="Hi Glen, thanks for your comments.  Take a look at the following link.  Its from Swami Chandrasekaran who led the Watson team at IBM, so a pretty experienced data scientist in my opinion.  He has programming as basically the third thing a data scientist needs to know, behind &quot;Fundamentals&quot; and Statistics.  According to his roadmap, once you know how to program, you are 15% of the way to being a data scientist.  Based on this, I might disagree slightly with the statement that true data scientists come in a &quot;non-programming&quot; flavor.  http://nirvacana.com/thoughts/becoming-a-data-scientist/" CreationDate="2015-04-06T13:19:53.803" UserId="8944" />
  <row Id="5819" PostId="5449" Score="0" Text="Well, I only say that based on experience. Most statistics and data science courses even don't cover programming outside of what you need for the popular statistical programs. Due to that, most of the guys I run into in the stats world are not good at programming. It's like an afterthought when they enter the real world and realize it helps." CreationDate="2015-04-06T13:47:59.090" UserId="3417" />
  <row Id="5820" PostId="5442" Score="0" Text="@TyrionLannister if anything is public, it could be worth asking on [opendata.se](http://opendata.stackexchange.com/)." CreationDate="2015-04-06T14:04:27.047" UserId="8953" />
  <row Id="5822" PostId="5435" Score="2" Text="@smci That doesn't make it any less old-fashioned!" CreationDate="2015-04-06T14:57:20.900" UserId="2825" />
  <row Id="5823" PostId="5460" Score="0" Text="&lt;if var1 in (1, 2)&gt;&#xA;This is really what I need. THANK YOU! )" CreationDate="2015-04-06T18:55:00.523" UserId="8951" />
  <row Id="5824" PostId="5460" Score="0" Text="@Ted awesome.  if that solved your problem, click the checkmark to accept this answer so everybody knows you're good to go.  and remember there's a wealth of `r` information over at [stackoverflow](http://stackoverflow.com/questions/tagged/r)." CreationDate="2015-04-06T19:00:05.147" UserId="8953" />
  <row Id="5828" PostId="5464" Score="0" Text="Wait, what exactly are you asking? This sounds more like a blog post than stackexchange question tbh." CreationDate="2015-04-07T09:51:16.250" UserId="587" />
  <row Id="5829" PostId="5464" Score="0" Text="@LauriK well 'what constitutes data science/big data' has been asked here before and there doesn't seem to be a black/white answer.  so i'm giving a specific example and asking where it falls in the gray." CreationDate="2015-04-07T10:49:25.250" UserId="8953" />
  <row Id="5830" PostId="5428" Score="0" Text="can you link to the wikipedia articles you are referring to?  you don't mean the canonical form of a graph and how it's related to graph isomorphism? as opposed to isometric, where the scale of 3d axes-&gt;2d are the same/are all separated by 120deg angles." CreationDate="2015-04-07T18:54:29.023" UserId="8953" />
  <row Id="5831" PostId="5428" Score="0" Text="http://en.wikipedia.org/wiki/Graph_canonization" CreationDate="2015-04-07T19:40:45.430" UserId="3466" />
  <row Id="5832" PostId="5428" Score="0" Text="http://en.wikipedia.org/wiki/Graph_isomorphism" CreationDate="2015-04-07T19:41:15.707" UserId="3466" />
  <row Id="5833" PostId="5428" Score="0" Text="I did indeed mean graph isomorphism, thanks for asking. I have updated the question." CreationDate="2015-04-07T19:42:41.330" UserId="3466" />
  <row Id="5834" PostId="5471" Score="0" Text="Not sure that a web development question belongs on this site. R is popular here, but this question does not seem to be about doing data science." CreationDate="2015-04-07T20:07:57.800" UserId="3466" />
  <row Id="5835" PostId="5471" Score="1" Text="The question is linked to reproducible research in data science as one can seamlessly integrate R source code with markdown and produce html output." CreationDate="2015-04-07T20:32:56.700" UserId="8644" />
  <row Id="5836" PostId="5471" Score="0" Text="I agree with the OP - this question is more about reproducible research than web development and has significant data science component. Therefore, it definitely belongs to this site in my book." CreationDate="2015-04-07T21:55:19.187" UserId="2452" />
  <row Id="5840" PostId="5446" Score="0" Text="Thanks Alexsandr. I'll take a look. I think the most fascinating part of the Obama campaign and big data, in general, is the ability to personalize. Because they had such rich data sets about us, the President was able to give you a different campaign than he gave me. A campaign volunteer probably read a different script to my neighbor than to me." CreationDate="2015-04-08T02:32:53.363" UserId="8860" />
  <row Id="5841" PostId="5461" Score="0" Text="Thanks. I'll check out her work." CreationDate="2015-04-08T02:35:57.617" UserId="8860" />
  <row Id="5842" PostId="5446" Score="0" Text="I think the hardest part is to generate or find these data. The analysis is the easy part." CreationDate="2015-04-08T02:36:12.227" UserId="8860" />
  <row Id="5843" PostId="5446" Score="0" Text="@TyrionLannister: You're welcome. While personalization can certainly benefit from data science, I doubt it's used to such level of granularity - IMHO it is simply not feasible. I also don't think that analysis in political informatics is easy (I mean here a **comprehensive** analysis, not some trivial one). I partially agree with you about data collection, though - it is definitely not an easy part, but doable and rather simple (especially, considering big budgets that can be thrown at such task); analysis, on the other hand, is challenging and of critical importance (to the stakeholders)." CreationDate="2015-04-08T02:55:35.977" UserId="2452" />
  <row Id="5845" PostId="5477" Score="1" Text="I was looking for a pure R-based solutions actually. I will explore the ones that you pointed at. Thank you." CreationDate="2015-04-08T07:46:40.917" UserId="8644" />
  <row Id="5846" PostId="5477" Score="1" Text="@MihaTrošt: My pleasure - I'm glad to help." CreationDate="2015-04-08T07:56:30.417" UserId="2452" />
  <row Id="5853" PostId="5481" Score="0" Text="This system wouldn't do any processing itself? Just a repository of data sets like CKAN?" CreationDate="2015-04-08T17:57:40.183" UserId="471" />
  <row Id="5854" PostId="5471" Score="0" Text="If its really about reproducible research it needs more explanation. As it stands its just &quot;How do I build a static web site using R?&quot;. Close." CreationDate="2015-04-08T18:01:18.453" UserId="471" />
  <row Id="5856" PostId="5481" Score="1" Text="You could always spin up a NoSQL Environment that allows analyst and other data guys to drop data in and play with. Then when data products are discovered, it can be transitioned to the production system that either NoSQL or even a RDBMS. We considered Hadoop because it allows less ETL development and has additional components such as HIVE and the works to play with." CreationDate="2015-04-08T22:52:31.737" UserId="3417" />
  <row Id="5857" PostId="5471" Score="0" Text="I think it's on-topic but merely too broad. A better question would identify what has been tried, and what specifically went wrong." CreationDate="2015-04-09T01:00:19.963" UserId="21" />
  <row Id="5859" PostId="5458" Score="0" Text="Yes, I manually assigned 12 category." CreationDate="2015-04-09T11:24:14.117" UserId="8969" />
  <row Id="5860" PostId="5370" Score="1" Text="I second the notion of learning Python. I work with large datasets and data scientist utilizing R to analyze those datasets. Although I learned C at a very young age, Python is the one language that is truly giving me value as a programmer and assisting these data scientist. Therefore, look to compliment the team, not yourself." CreationDate="2015-04-09T15:13:21.893" UserId="3417" />
  <row Id="5861" PostId="5489" Score="0" Text="IMHO, this question better fits _Cross Validated_ or _StackOverflow_ SE sites." CreationDate="2015-04-09T15:29:58.890" UserId="2452" />
  <row Id="5862" PostId="5490" Score="0" Text="given that &#xA;    fit-&lt; glm(formula = Case ~ X + Y, family = &quot;binomial&quot;, data = data)&#xA;&#xA;what is fitted(fit)? Could it be what i'm looking for?" CreationDate="2015-04-09T17:55:57.983" UserId="9033" />
  <row Id="5863" PostId="5370" Score="1" Text="similarly python is sped up by writing  in cython (again basically C). I have to say I have yet to use it myself. There is very little that cannot be done using existing libraries (eg scikit-learn, pandas  in python[which are written in cython so you don't have to!])." CreationDate="2015-04-09T21:19:11.783" UserId="1256" />
  <row Id="5864" PostId="5490" Score="0" Text="@Ciochi: No. In your example above, a fitted `glm` model object would be the `fit` object. See UPDATE section in my answer." CreationDate="2015-04-10T00:19:10.900" UserId="2452" />
  <row Id="5865" PostId="5490" Score="0" Text="@Ciochi: So, my suggestion is to use standard access functions (see UPDATE) for extracting traditional information and `str()` and low-level (direct) access (via `$`), if you need other information, not accessible via high-level functions. I hope that this clarifies things." CreationDate="2015-04-10T00:29:36.703" UserId="2452" />
  <row Id="5866" PostId="5490" Score="0" Text="thanks for the answers, i appreciate it. I've already read the Update section you've added, but i still dont get it, mostly because i'm pretty noob on statistics, i just use basic stuff like t-test etc. Indeed, i dont get what the fit object is. Is it a new variable?" CreationDate="2015-04-10T00:42:13.957" UserId="9033" />
  <row Id="5867" PostId="5490" Score="0" Text="@Ciochi: You're welcome. Feel free to accept/upvote my answer, if it is helpful. Yes, `fit` is new variable that gets created and initialized with the return value of the `glm()` function (the return value is an object of class `glm`)." CreationDate="2015-04-10T01:49:19.313" UserId="2452" />
  <row Id="5869" PostId="5490" Score="0" Text="I still have some problems in getting the value of this new variable fit. Reading elsewhere it seems to me that this variable fit is a variable of estimated probabilities, thus ranging from 0 to 1, as the values i get when i run fitted(fit)." CreationDate="2015-04-10T10:49:28.203" UserId="9033" />
  <row Id="5870" PostId="5490" Score="0" Text="@Ciochi: Sorry, but I don't quite understand what your current issue is." CreationDate="2015-04-10T10:52:04.373" UserId="2452" />
  <row Id="5871" PostId="5490" Score="0" Text="The problem is that i need to use those values for an upcoming proceeding. Being the new variable fit called Z, i need to to present data as Z: Controls vs Cases, mean ± sd vs mean ± sd, P&lt;0.01, for example." CreationDate="2015-04-10T11:01:59.230" UserId="9033" />
  <row Id="5872" PostId="5490" Score="0" Text="@Ciochi: I think that you might receive more attention and help with this question at the _Cross Validated_ SE site. I can ask local moderator to migrate it, if you want." CreationDate="2015-04-10T11:08:26.110" UserId="2452" />
  <row Id="5873" PostId="5490" Score="0" Text="yeah, thank you." CreationDate="2015-04-10T12:13:00.417" UserId="9033" />
  <row Id="5874" PostId="5408" Score="0" Text="How many attributes are you considering in your dataset? And how many examples?" CreationDate="2015-04-10T13:14:06.380" UserId="2576" />
  <row Id="5877" PostId="5492" Score="1" Text="&quot;Generate insight&quot; is far too broad; until you know what you're asking it's not useful to ask about tools." CreationDate="2015-04-10T21:33:25.563" UserId="21" />
  <row Id="5878" PostId="5493" Score="0" Text="Well, weighted average or majority vote, but are you asking for more than that? Can you clarify what you are referring to in the first part?" CreationDate="2015-04-10T21:34:13.117" UserId="21" />
  <row Id="5881" PostId="5493" Score="0" Text="My question more precisely is: when I find out which labels are for whom my RF classifier uncertain is (by voting), I could train another (say, SVM) classifier exclusively for these uncertain labels. I do this by filtering out the RF-certain labels, and the remaining subset (with the unsure labels) is the training set for my SVM model. But how could I technically combine the RF and the SVM for an unseen new dataset with all the possible labels? In R one could classify (&quot;predict&quot;) a given unlabeled dataset with only one model." CreationDate="2015-04-11T04:28:46.710" UserId="9011" />
  <row Id="5882" PostId="5492" Score="0" Text="Hi Sean, Generating insight by here I mean- finding insights such as certain descriptive statistics- mean payment days, median payment days, default by how many days, creating a delay band i.e. 1-10 days, 11-20 days etc. region wise details, BU wise details." CreationDate="2015-04-11T05:02:45.353" UserId="9052" />
  <row Id="5897" PostId="5443" Score="1" Text="Yes, see [here](http://www.microsoft.com/mac/excel). For the joke impaired, see [here too](https://twitter.com/bigdataborat/status/372350993255518208)." CreationDate="2015-04-11T14:12:35.270" UserId="515" />
  <row Id="5898" PostId="5504" Score="0" Text="I think the question is incomplete (&quot;By looking&quot; ... ?) and it's likely too open ended to ask &quot;what can I learn&quot;. Can you narrow this down specifically, to maybe your second point? what are you hoping to compare and what have you tried?" CreationDate="2015-04-12T10:34:32.737" UserId="21" />
  <row Id="5899" PostId="5506" Score="0" Text="Thanks for your answer, can you explain more about your example of acquiring other features (variables) for the states and then average each feature among objects assigned to each cluster? I didn't completely understand it" CreationDate="2015-04-12T16:05:36.780" UserId="9079" />
  <row Id="5900" PostId="5506" Score="0" Text="Maybe get population size of each state during the given calendar year, percent ethnicity for a variety of race/ethnicity combinations, average education, average household income.  Migration could affect names as well.  Next, average the population sizes of the states within a cluster, do this for each cluster, then show a bar chart with the mean (average) population size for each cluster.   A high bar means large average population size of states in that cluster, whereas small bars reflect clusters having states with low population size.  (each bar represents a cluster)." CreationDate="2015-04-12T16:34:33.040" UserId="9086" />
  <row Id="5901" PostId="310" Score="0" Text="as you probably saw, the [Wikipedia article on P-U learning](http://en.wikipedia.org/wiki/PU_learning) has a reference to a paper where this has been applied to gene identification. Maybe it's worth figuring out / asking the authors what software they used." CreationDate="2015-04-12T20:27:06.180" UserId="462" />
  <row Id="5902" PostId="310" Score="0" Text="There is some discussion on PU learning in scikit learn here: http://stackoverflow.com/questions/25700724/binary-semi-supervised-classification-with-positive-only-and-unlabeled-data-set (using a 'one class' support vector machine)" CreationDate="2015-04-12T20:33:17.840" UserId="462" />
  <row Id="5903" PostId="5481" Score="2" Text="From a _storage perspective_, processed data is no different from unprocessed one, so I think that **any** storage approach is appropriate, as long as it satisfies **other requirements** (compatibility with other systems, software, business processes, APIs, etc.), all focused on your IT environment." CreationDate="2015-04-13T04:05:52.303" UserId="2452" />
  <row Id="5904" PostId="5507" Score="0" Text="I think that the kernel doesn't space out the values. My values stay together even after applying the kernel. I'm going to add a picture of how my values are." CreationDate="2015-04-13T09:52:01.180" UserId="2586" />
  <row Id="5906" PostId="5505" Score="0" Text="Thanks for the answer. &#xA;I have access to some local systems, can you give me a start on how to set up a cluster using these systems without any cloud services ?&#xA;Looks like everywhere AWS is being used." CreationDate="2015-04-13T10:15:11.803" UserId="4947" />
  <row Id="5907" PostId="5505" Score="1" Text="@abhivij: You're welcome. Setting up a cluster is not a rocket science, but might be [not trivial](http://stackoverflow.com/q/17923256/2872891), depending on the requirements and your current skills. You can read [this blog post](http://www.smart-stats.org/wiki/parallel-computing-cluster-using-r) and [this blog post](https://beckmw.wordpress.com/2014/01/21/a-brief-foray-into-parallel-processing-with-r) as a starting point. (to be continued)" CreationDate="2015-04-13T10:37:50.190" UserId="2452" />
  <row Id="5908" PostId="5505" Score="1" Text="@abhivij: (cont'd) Also, you'd have to refer to documentation on multiprocessing `R` packages that you will decide to use, for example [this tutorial](http://cran.r-project.org/web/packages/doMPI/vignettes/doMPI.pdf). A more high-level overview and example of an R-based cluster can be found in [this working paper](http://biostats.bepress.com/cgi/viewcontent.cgi?article=1016&amp;context=uwbiostat). Hope this helps." CreationDate="2015-04-13T10:38:14.633" UserId="2452" />
  <row Id="5909" PostId="1100" Score="0" Text="Interesting problem. Since you posted this over 6 months ago can I get you to confirm that you're still interested in this before I spend time taking a stab at it?" CreationDate="2015-04-13T13:40:19.757" UserId="2723" />
  <row Id="5910" PostId="5261" Score="0" Text="I would like to take a stab at this. Do you have any example data or should I select some built-in dataset to benchmark speed improvement?" CreationDate="2015-04-13T13:42:26.980" UserId="2723" />
  <row Id="5912" PostId="5521" Score="0" Text="Thanks! But one thought about using a cluster centroid, it would require each cluster has similar volume/or radius. if all potential clusters are significantly differ in volume, I do not sure the method can work in the case?" CreationDate="2015-04-14T15:01:49.750" UserId="9097" />
  <row Id="5913" PostId="5525" Score="1" Text="This is almost a link-only answer since it doesn't provide any answer to the question directly here, and are discouraged on SE." CreationDate="2015-04-14T15:22:06.767" UserId="21" />
  <row Id="5915" PostId="5524" Score="0" Text="what package does `rpart` come from?" CreationDate="2015-04-14T15:40:41.600" UserId="471" />
  <row Id="5920" PostId="5527" Score="0" Text="Thanks a lot! your are really inspiriting! are there standard/classical methods to check/quantify if the classes are well separated or overlaps?" CreationDate="2015-04-15T15:47:07.267" UserId="9097" />
  <row Id="5923" PostId="5531" Score="0" Text="Pete, sorry, but I remember now that I've used your solution and ... it didn't worked as expected. Indeed, time elapsed (incl. nr of days, but also other things) between two years is not constant.&#xA;Due to that, when I have applied your solution, in the best case, the minutes/seconds parts were changed, which is not expected." CreationDate="2015-04-15T19:10:02.527" UserId="3024" />
  <row Id="5924" PostId="5531" Score="0" Text="Please add what happened on to your question.  Include a small representation of your data.  When I apply this solution to my data, it only changes the year, leaving the mintues/seconds unchanged." CreationDate="2015-04-15T19:13:40.713" UserId="1077" />
  <row Id="5925" PostId="5530" Score="1" Text="We are striving to keep quality very high here. Can you please clean up the punctuation and capitalization issues in your post? If you need help, let me know. Thanks!" CreationDate="2015-04-15T19:47:38.023" UserId="3466" />
  <row Id="5926" PostId="5530" Score="1" Text="I also want to mention that I think this is a great question for this site. Thanks for posting." CreationDate="2015-04-15T19:48:59.080" UserId="3466" />
  <row Id="5927" PostId="5527" Score="0" Text="I would say classes are well separated if you can train a classifier on them and it gets very high accuracy. But if you can do that you probably don't need to do any sampling for k-NN... Or if when you apply k-NN, the top-k classes are consistently the same... you could try k-NN on a subset of your data, then try the &quot;sampling&quot; way, and try to quantify your loss." CreationDate="2015-04-15T23:55:14.153" UserId="9114" />
  <row Id="5928" PostId="5531" Score="0" Text="Thanks Pete,&#xA;&#xA;I don't know what I did previously, but now it works. I'll create another answer to give the source code if it can help somebody in the future. In fact, the &quot;complex&quot; stuff is around how many days are in a year, for which I'm computing delta between two datetime.date objects." CreationDate="2015-04-16T07:35:23.000" UserId="3024" />
  <row Id="5929" PostId="5536" Score="0" Text="thanks Matic DB.....I will try it out once again...." CreationDate="2015-04-16T13:22:35.447" UserId="3314" />
  <row Id="5931" PostId="5536" Score="0" Text="Thanks. @Matic DB...i was able to get the id ..Below is my solution" CreationDate="2015-04-16T15:53:59.910" UserId="3314" />
  <row Id="5932" PostId="2643" Score="0" Text="What do you want to use for a file system and cluster manager instead of Hadoop?  Options include Cassandra for files, Mesos or Yarn for cluster management, etc." CreationDate="2015-04-16T19:28:39.760" UserId="9146" />
  <row Id="5936" PostId="5549" Score="0" Text="I have answered a **very similar question** here: http://datascience.stackexchange.com/a/5200/2452. My _related answer_ on _Cross Validated_ might also be helpful: http://stats.stackexchange.com/a/142725/31372." CreationDate="2015-04-17T23:29:30.693" UserId="2452" />
  <row Id="5937" PostId="5547" Score="0" Text="What's the value of `y.shape`?" CreationDate="2015-04-19T09:01:42.143" UserId="173" />
  <row Id="5938" PostId="5547" Score="1" Text="And are you sure you're executing the code you're displaying? In the code sample the last line is `net1.fit(X, y)` while the traceback indicates that the problem happens when executing `net1.fit(X[i], y[i])`." CreationDate="2015-04-19T09:10:23.857" UserId="173" />
  <row Id="5939" PostId="5555" Score="0" Text="I don't think anybody is able to answer your question satisfactorily. Firstly, the situation depends on too many factors. Secondly, in order to assess potential improvement (or lack of it) from the two factors you've mentioned, a research study has to be designed and performed. Alternatively, you can try to find similar studies and review their results." CreationDate="2015-04-20T00:00:21.937" UserId="2452" />
  <row Id="5941" PostId="5550" Score="0" Text="Thanks for your comments." CreationDate="2015-04-20T16:52:01.413" UserId="3314" />
  <row Id="5942" PostId="5549" Score="0" Text="Thanks again for your suggestions and comments." CreationDate="2015-04-20T16:52:16.960" UserId="3314" />
  <row Id="5943" PostId="5561" Score="0" Text="Could you provide an example which reproduces the errors (with minimal data samples)? Also, posting full tracebacks is extremely helpful to diagnose a problem. Otherwise we can only guess." CreationDate="2015-04-20T17:12:58.540" UserId="173" />
  <row Id="5944" PostId="5560" Score="0" Text="Thanks walczak." CreationDate="2015-04-20T17:19:24.087" UserId="4993" />
  <row Id="5945" PostId="5549" Score="0" Text="You're very welcome." CreationDate="2015-04-20T21:36:50.910" UserId="2452" />
  <row Id="5946" PostId="5550" Score="0" Text="You are welcome. Hoping this helps..." CreationDate="2015-04-21T07:54:48.467" UserId="3024" />
  <row Id="5947" PostId="19" Score="1" Text="That depends on whether it is just being thrown in as a buzzword." CreationDate="2015-04-21T20:52:25.963" UserId="9237" />
  <row Id="5949" PostId="5574" Score="0" Text="Often you rather want to predict (stock market), which is unrelated to Nash. Also crowds/people do not follow Nash and usually you want to control the crowd, rather than game it. And Nash relies on an abstract model for you simply do not have the numbers in real life. That's my impression, but I'd be interested to see confirmed cases of usage which go beyond an academic paper." CreationDate="2015-04-22T06:23:34.550" UserId="723" />
  <row Id="5950" PostId="5581" Score="0" Text="300 might potentially be a small sample in this context, how were the passengers distributed? Were they all on the same plane, on the same route ? Did they fly in the same week/month? Without sight of the data it's hard to say exactly what you might be able to extract from it. Maybe you could make your question more specific?" CreationDate="2015-04-22T09:29:49.153" UserId="7980" />
  <row Id="5951" PostId="5582" Score="0" Text="You might find taking your first steps in machine learning easier and faster  if you use a specialised graphical environment  specifically for ML, such as WEKA. Most of the popular algorithms present  in WEKA have Python equivalents, so once you have explored how different algorithms behave you can implement them with appropriate parameters using the python libraries. There is an accompanying book for WEKA that introduces ML concepts." CreationDate="2015-04-22T09:39:14.373" UserId="7980" />
  <row Id="5952" PostId="5581" Score="0" Text="Sir, this is the website i scraped [ http://www.airlinequality.com/Forum/af.htm ]  Have a look at it. :)" CreationDate="2015-04-22T12:39:02.897" UserId="9174" />
  <row Id="5953" PostId="5581" Score="0" Text="is it possible to do something ? @image_doctor" CreationDate="2015-04-22T13:05:59.440" UserId="9174" />
  <row Id="5954" PostId="5581" Score="0" Text="Something needs definition, what type of question are you seeking to answer ?   Unless you want to analyse the textual comments of the reviewers, you only have a handful of unspecific features which tell you how a selection of passengers perceive the airline in some very general categories. You might be able to extract the country of origin of the reviewer and see how that affects perception.  Without other data you can't compare this airline to the performance of any other airline. Apart from distributions of scores for things like value for money, etc. what are you searching for ?" CreationDate="2015-04-22T13:14:13.203" UserId="7980" />
  <row Id="5956" PostId="5586" Score="1" Text="&quot;Big Data&quot; is often collected without any thought as to the questions you might ask of it. This is in effect a little big data question." CreationDate="2015-04-22T15:49:05.327" UserId="471" />
  <row Id="5957" PostId="5566" Score="1" Text="So are you currently doing a degree in Comp Biol? How far into it? When did you work at all these research labs? Can we see your CV?" CreationDate="2015-04-22T15:54:03.900" UserId="471" />
  <row Id="5963" PostId="5566" Score="2" Text="You could begin your hunt with [DataKind](http://www.datakind.org/projects/)." CreationDate="2015-04-21T08:46:01.687" UserId="1131" />
  <row Id="5964" PostId="5566" Score="1" Text="You can search for inspirations by browsing the projects carried out within [Data Science for Social Good](http://dssg.io/) initiative." CreationDate="2015-04-21T15:22:00.447" UserId="173" />
  <row Id="5965" PostId="5586" Score="0" Text="@Spacedman, you're right. The difference is in the means of data collection, I suppose." CreationDate="2015-04-22T21:02:52.010" UserId="8152" />
  <row Id="5966" PostId="5574" Score="0" Text="I have done a bit of study on the topic, which is not authoritative, but I think Nash-Equilibrium gives us to choose the best decision when we have an idea about the decisions of our competitors. Secondly it is not an abstract model, all the abstractions are removed by assigning a pay-off function which is quantifiable." CreationDate="2015-04-23T07:09:32.993" UserId="9227" />
  <row Id="5967" PostId="5588" Score="0" Text="Just an observation, this can be cast as a classification problem, if the cost  is binned or the prices  fit into common ranges. :)" CreationDate="2015-04-23T07:47:14.557" UserId="7980" />
  <row Id="5968" PostId="5443" Score="0" Text="I'd say that Excel is usefull to present results, not to do the big data processing in itself: your big data &quot;customers&quot; are expecting to have results presented the way they are used to.&#xA;For example, it's not uncommon for my to export a result pandas data frame to excel and then change the layout of the resulting file only ... to make the &quot;customers&quot; happy." CreationDate="2015-04-23T08:25:43.840" UserId="3024" />
  <row Id="5970" PostId="5574" Score="0" Text="I'm just saying the tasks are different. Stock prediction: We need to predict the future of a very complex system that will not care about an equilibrium. Abstract model: They are almost never known payoff-function - welcome to reality. People: They provable do not follow Nash - in a fixed round Prisoners dilemma they do not all default." CreationDate="2015-04-23T11:04:31.383" UserId="723" />
  <row Id="5971" PostId="5588" Score="0" Text="Thanks Henry, suppose if the attributes of items are location name,  company name and components name. In this case how can I model ?" CreationDate="2015-04-23T12:50:38.387" UserId="5091" />
  <row Id="5972" PostId="5595" Score="0" Text="I don't think this is quite right.  I'm a little unclear about where url_data would be coming from.  From the documentation, k.set_contents_from_string() seems to quite literally set the contents of file 'foobar' to whatever is contained in that string.  I want the content at that url to be pushed directly to s3 without needing to be downloaded locally." CreationDate="2015-04-23T15:37:00.280" UserId="9249" />
  <row Id="5975" PostId="1214" Score="1" Text="I recently took a quick look around the market for products that would help with that, and I want to share my findings.&#xA;&#xA;There are two SAAS products that can help a Data Science team to share analyses done in Python and R. They both have an IPython notebook like IDE, and they both build around it a lot of features for running and sharing jobs. I find them both almost identical: [Domino Data Lab][1] and [Sense.io][2]&#xA;&#xA;  [1]: http://www.dominodatalab.com/&#xA;  [2]: http://sense.io/" CreationDate="2015-04-24T02:36:40.490" UserId="3540" />
  <row Id="5976" PostId="1214" Score="0" Text="There is also a Machine Learning environment for Hadoop, which keeps track of Job runs; [h2o.ai][3]. It is not meant for being a tool for streamlining the work of the data team, but with some careful naming conventions it can help a lot. This one fits most with &#xA;&#xA;  [3]: http://h2o.ai" CreationDate="2015-04-24T02:37:27.593" UserId="3540" />
  <row Id="5977" PostId="5601" Score="0" Text="Please, change the topic of your post." CreationDate="2015-04-24T15:15:26.257" UserId="173" />
  <row Id="5978" PostId="5602" Score="0" Text="What type of documents are you intending to look at? Length wise are you looking at Tweets, Tumblr posts, articles, etc?" CreationDate="2015-04-24T15:43:41.600" UserId="9274" />
  <row Id="5979" PostId="5602" Score="0" Text="I am Looking for articles preferably." CreationDate="2015-04-24T19:03:55.397" UserId="9287" />
  <row Id="5981" PostId="5606" Score="0" Text="What are these numbers? What precisely do you do to obtain them?" CreationDate="2015-04-25T15:38:46.580" UserId="173" />
  <row Id="5982" PostId="5606" Score="0" Text="Thanks for your interest. &#xA;I obtain these numbers by entering : mdclassify(['google','France',None,None],tree). The results are the same as the ones obtained by the book. I simply do not understand them.&#xA;mdclassify() is a function used to classify observations when there are missing attributes, but using a weighting process (so the observation is split around different branches in a unequal manner). NB : When an observation fulfill a condition, it goes on the right branch. So here in some way the observation is split on the 3 branches of the right side. But what are these figures?" CreationDate="2015-04-25T17:40:18.930" UserId="9296" />
  <row Id="5984" PostId="5590" Score="0" Text="Depends on what you mean by scale, but that sounds like the answer to your question. What else are you looking for?" CreationDate="2015-04-25T19:57:03.147" UserId="21" />
  <row Id="5985" PostId="5158" Score="0" Text="In a Sage Worksheet (&quot;.sagews&quot;), just enter `%default_mode r` and evaluate the cell. Then you talk to R directly." CreationDate="2015-04-25T20:41:57.987" UserId="9306" />
  <row Id="5986" PostId="5616" Score="0" Text="Wow ! Thanks ! To check my understanding, I tried to apply the same reasoning to the 3 branches at the left but I don't manage to get appropriate results.&#xA;If I enter `mdclassify(['digg', None, None, None], tree)` I'm getting `{'Basic': 2.4615384615384617, 'None': 0.8653846153846154}`. Can you please explain me how it works ? I thought we would have :&#xA;1) Dividing one unit of weight for page&gt;20: None:3 and Basic1 : 0.75 and 0.25&#xA;2) Then counting the weight for for FAQ attribute &#xA;* Yes to FAQ = 4/5&#xA;* No to FAQ =  1/5&#xA;Thus for `None` I would expect to get 0.75/5=0.15. Where am I wrong ?" CreationDate="2015-04-26T09:34:22.187" UserId="9296" />
  <row Id="5988" PostId="5616" Score="0" Text="Try either running the script in debug mode, or add print statements in mdclassify() function and carefully observe what's going on. In short, in this `digg-None-None-None` case we got: `Basic==1`, `None==3`, thus: `Basic==1*0.25` and `None==3*0.75`. Then we go up to attribute &quot;2:yes&quot; and we got: `Basic==4`, and (`Basic==0.25`, `None==2.25`). The `Basic==4` probability is: 4/6.5==0.61, and 4*0.615==2.46 after multiplication. The probability of the second leaf is: `2.5/6.5==0.38`. So, finally we got: Basic==(4*0.615)+(0.25*0.38)=2.55, and None==2.25*0.38=0.85. I get approx. results like these." CreationDate="2015-04-26T13:41:57.590" UserId="173" />
  <row Id="5989" PostId="5616" Score="0" Text="Ok, it's clear now ! You nailed it ! Many thanks !" CreationDate="2015-04-26T20:11:48.590" UserId="9296" />
  <row Id="5990" PostId="5588" Score="0" Text="Hmm, I can't say for certain that I know exactly what you should do.  This falls under the domain of regression with categorical variables [wiki link here](http://en.wikipedia.org/wiki/Categorical_variable#Categorical_variables_and_regression)." CreationDate="2015-04-27T06:12:27.580" UserId="9246" />
  <row Id="5991" PostId="5588" Score="0" Text="--continuation of last comment--&#xA;in which the bulk of the problem lies in translating categorical variables into numerical ones.  &#xA;&#xA;An easy way to do this would just assign numerical representations to each unique categorical entry.  I'm unsure of how this would affect regression. You should be able to train a neural network [(basic introduction video link)](https://www.youtube.com/watch?v=bxe2T-V8XRs) to do this without a hitch" CreationDate="2015-04-27T06:20:30.303" UserId="9246" />
  <row Id="5992" PostId="874" Score="0" Text="The links provided in the answer seem to be dead ;(" CreationDate="2015-04-27T07:27:48.070" UserId="9325" />
  <row Id="5993" PostId="5603" Score="0" Text="Thank you so much for giving a rough scikit-scheme, that was exactly what I needed!" CreationDate="2015-04-27T07:49:26.153" UserId="9281" />
  <row Id="5996" PostId="5485" Score="0" Text="(A comment.) I think this is a classical manifestation of the [curse of dimensionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions)." CreationDate="2015-04-26T11:00:37.970" UserId="6550" />
  <row Id="5997" PostId="5485" Score="0" Text="+1 Looks very much like this" CreationDate="2015-04-27T10:18:03.470" UserId="9085" />
  <row Id="5998" PostId="5574" Score="0" Text="Have a look at Acemoglu (http://economics.mit.edu/files/9789) and Jackson (http://web.stanford.edu/~jacksonm/GamesNetworks.pdf). They write on games on networks, and it may have many practical applications." CreationDate="2015-04-28T17:02:07.333" UserId="5279" />
  <row Id="5999" PostId="5635" Score="0" Text="What are these big problems?" CreationDate="2015-04-28T18:47:30.503" UserId="173" />
  <row Id="6000" PostId="5641" Score="0" Text="Thank you.  I was contemplating something like this, but I wanted to ask around to see if there was an easier way.  Thanks for pointing out all the steps too.  Very helpful" CreationDate="2015-04-29T16:18:32.097" UserId="9249" />
  <row Id="6001" PostId="5639" Score="1" Text="Could you, please, elaborate? How does your data look like? It's hard to imagine unstructured metadata. What programming languages do you consider to use? Have you tried to do anything with these data? Structure it in anyway? Without any details we won't be able to help you." CreationDate="2015-04-29T16:31:12.607" UserId="173" />
  <row Id="6003" PostId="5637" Score="0" Text="Thanks for the Glove paper!" CreationDate="2015-04-30T07:38:10.893" UserId="6523" />
  <row Id="6007" PostId="5651" Score="0" Text="But for $$ #(w_i, c_j) $$ I couldnt figure out what it is because this is loss for a specific pair of words. So why count in there" CreationDate="2015-04-30T17:18:26.663" UserId="6523" />
  <row Id="6008" PostId="155" Score="0" Text="I haven't found any good free comprehensive datasets for typical Business Intelligence applications.  The [Microsoft Contoso BI Demo Dataset for Retail Industry from Official Microsoft Download Center](http://www.microsoft.com/en-us/download/details.aspx?id=18279) download works with some Microsoft products (see [AndyGett on SharePoint and Other Business Software](http://blog.bullseyeconsulting.com/archive/2012/08/14/setting-up-sample-contoso-database-for-performancepoint-and-sharepoint.aspx)), but I don't see any plain sql or csv dumps of it, nor any license info." CreationDate="2015-04-30T17:42:11.410" UserId="9146" />
  <row Id="6009" PostId="5655" Score="0" Text="Thanks for your comments and my apology for not providing more information. The answer to the first three questions of yours is 'yes' and the keyError  is given as KeyError: 0  However, following your remark, I am going to provide you with few lines of the dataset, and what movieGenre is. Also, I will include the traceback but it is kinda large." CreationDate="2015-04-30T18:36:32.797" UserId="3314" />
  <row Id="6010" PostId="5655" Score="1" Text="@user62198 well that's sort of what i'm getting at.  you should be able to minimize traceback length by pulling a small portion of your code out into another script - a script that demonstrates your problem but is standalone/removes most of the code that is not causing the problem." CreationDate="2015-04-30T18:50:48.767" UserId="8953" />
  <row Id="6011" PostId="5655" Score="0" Text="@user62198 i edited to include some potential troubleshooting steps.  can you use that to provide more information?" CreationDate="2015-04-30T19:33:18.420" UserId="8953" />
  <row Id="6012" PostId="5618" Score="0" Text="Do all patients experience the event?" CreationDate="2015-05-01T15:24:03.160" UserId="3347" />
  <row Id="6013" PostId="5534" Score="1" Text="I edited your code a bit but it fails because `rating` isn't defined. If you fix that, you could also add `from BeautifulSoup import BeautifulSoup`, and `import requests`. And why not also show `url=&quot;http://etc&quot;` so we don't have to do that for ourselves?" CreationDate="2015-05-01T15:40:36.713" UserId="471" />
  <row Id="6014" PostId="5669" Score="1" Text="Thanks for the answer, I will take a look there!" CreationDate="2015-05-01T21:45:30.137" UserId="9225" />
  <row Id="6015" PostId="5659" Score="2" Text="@Gred Thatcher, Thanks for the link. This project is part of a learning endeavor on web scraping and hence all these troubles. -:)" CreationDate="2015-05-01T21:47:56.593" UserId="3314" />
  <row Id="6016" PostId="5665" Score="0" Text="Thanks @j.a.gartner" CreationDate="2015-05-01T21:49:35.580" UserId="3314" />
  <row Id="6017" PostId="5655" Score="0" Text="Thanks a lot @aeroNotAuto. I am going to try it out soon and let you know." CreationDate="2015-05-01T21:51:07.253" UserId="3314" />
  <row Id="6018" PostId="5672" Score="0" Text="It's a nice Idea! +1 Thanks!" CreationDate="2015-05-02T10:37:43.103" UserId="9225" />
  <row Id="6020" PostId="5674" Score="0" Text="Thank you for such a clear explanation, and for the terrific links, thinking about the problem as detecting variance vs bias will definely help!" CreationDate="2015-05-03T13:07:09.907" UserId="8344" />
  <row Id="6021" PostId="5679" Score="3" Text="You're asking people to devote their time to explain something to you. Please, before you do so, show these people that you have tried to find the answer by yourself and somehow failed along the way. We do want to help you, but we do not want to be your human googling machines." CreationDate="2015-05-03T15:05:31.360" UserId="173" />
  <row Id="6022" PostId="5680" Score="0" Text="That's not going to help me. If I use an arithmetic mean, the score will be too heavily weighted by b in most cases. I need a change in each to equally be capable of influencing c." CreationDate="2015-05-03T19:20:12.273" UserId="9394" />
  <row Id="6023" PostId="5668" Score="0" Text="My survey has roughly 25 questions, but for any particular classification I only look at two or three specific questions that are designed to indicate a classification. Each of those questions are multiple choice. They could be either select-one or select-any. So I indicate a selected answer with 1 and a non-selected answer with 0. This approach may be incorrect from the get go. I certainly don't have as much data as you indicate." CreationDate="2015-05-04T00:11:28.803" UserId="9410" />
  <row Id="6024" PostId="5668" Score="0" Text="Also, I'm trying to avoid hard coded rules (If question 1 == choice A), because we tend to re-write the questions from time to time, and even change a select-one to a select-any. We add questions and answer choices, and drop questions and answer choices. So I think that any hardcoded rules will change somewhat often.  Thanks for the std err calc, more information, and other options as well!" CreationDate="2015-05-04T00:14:13.937" UserId="9410" />
  <row Id="6025" PostId="5668" Score="0" Text="Without knowing the survey content it's hard to say if the 1-0 scale is appropriate.  You could consider binary classification, i.e. a-&gt;1, b-&gt;2, c-&gt;4, then take their sum, and feed it to a random forest regression model." CreationDate="2015-05-04T04:36:34.547" UserId="8041" />
  <row Id="6026" PostId="5685" Score="0" Text="Thank you for your answer, but now my question is : how does it choose the best model, with which criteria?" CreationDate="2015-05-04T15:03:50.347" UserId="9445" />
  <row Id="6027" PostId="5685" Score="0" Text="The weights that are returned are an expression of the importance of your variable vector.  The training process trains several combinations of different weights, and tests to see which subset gives the best results:&#xA;https://spark.apache.org/docs/latest/mllib-linear-methods.html#mathematical-formulation" CreationDate="2015-05-04T18:45:51.957" UserId="8041" />
  <row Id="6028" PostId="5667" Score="0" Text="I also found my self with a very similar problem, and didn't really find a solution. But what *actually* happens is not clear from this code, because spark has 'lazy evaluation' and is supposedly capable of executing only what it really needs to execute, and also of combining maps, filters and whatever can be done together. So possibly what you describe *may* happen in a single pass. Not familiar enough with the lazy evaluation mechanisms to tell, though. Actually I just noticed the .cache(). Maybe there's a way of only doing one .cache() and getting the full results?" CreationDate="2015-05-04T22:51:41.937" UserId="9114" />
  <row Id="6029" PostId="5689" Score="3" Text="You could give more relevant name to the question... like Models performance or smth..." CreationDate="2015-05-05T08:52:07.107" UserId="97" />
  <row Id="6030" PostId="5689" Score="2" Text="Well, one of them has to be quicker. If it was the other way round, would you still be asking the question? Do you know what these algorithms do?" CreationDate="2015-05-05T09:00:17.697" UserId="471" />
  <row Id="6031" PostId="5686" Score="0" Text="So what you mean is that you create binary features for all these billions of words, web sites, friends etc.? And then the problem becomes bringing this down to a billion or so, so that the problem becomes tractable?" CreationDate="2015-05-05T09:30:37.327" UserId="9385" />
  <row Id="6032" PostId="5689" Score="0" Text="I know what the two algorithms do. I just want to know the reason behind Naive Bayes quicker performance. Thank you." CreationDate="2015-05-05T14:30:45.937" UserId="9436" />
  <row Id="6033" PostId="5676" Score="0" Text="Thanks for taking the time to write such a thorough answer." CreationDate="2015-05-05T16:40:37.650" UserId="3466" />
  <row Id="6034" PostId="5534" Score="1" Text="Just in case: http://opendata.stackexchange.com/questions/1073/where-to-get-imdb-datasets" CreationDate="2015-05-05T19:25:04.780" UserId="5279" />
  <row Id="6035" PostId="5693" Score="1" Text="What do you mean by &quot;unstructured geolocalisation data&quot;? A bunch of lat-long coordinates? A mix of coordinates, postal addresses and IP numbers?" CreationDate="2015-05-05T20:23:40.303" UserId="471" />
  <row Id="6036" PostId="5691" Score="0" Text="Maybe we can calculate the distance of slope." CreationDate="2015-05-06T02:09:09.477" UserId="9457" />
  <row Id="6037" PostId="5694" Score="1" Text="Have you tried Internet search? That should be enough." CreationDate="2015-05-06T04:09:32.220" UserId="2452" />
  <row Id="6038" PostId="5694" Score="1" Text="yes, i had google but that's surely isn't enough, see the updated question." CreationDate="2015-05-06T04:34:01.323" UserId="122" />
  <row Id="6039" PostId="5694" Score="1" Text="I'm not sure that it's such a good idea to seek explanation of complex machine learning concepts &quot;in layman terms&quot;. Also, you should widen your search beyond just Wikipedia." CreationDate="2015-05-06T04:39:13.703" UserId="2452" />
  <row Id="6040" PostId="5676" Score="0" Text="@ sheldonkreger, thanks for the encouraging comment." CreationDate="2015-05-06T06:01:42.837" UserId="8465" />
  <row Id="6041" PostId="5693" Score="0" Text="Sorry I didn't mentioned it, it can be all of those examples." CreationDate="2015-05-06T06:15:23.440" UserId="9461" />
  <row Id="6042" PostId="5607" Score="1" Text="Thank you. I think I have no time to execute these kinds of steps. My solution is to request access to a more powerful machine on my University. I'll mark your answer as the solution anyway." CreationDate="2015-05-06T12:32:01.357" UserId="8152" />
  <row Id="6043" PostId="5693" Score="1" Text="I think you really need to spell out in more detail what you want. Even those four terms you have included could have a zillion possibilities. &quot;Clustering&quot; do you mean grouping location points together for display, for analysis, or is this database clustering (ie a group of database servers)? &quot;Saving/restoring&quot;? Of what? To where? And why?" CreationDate="2015-05-06T14:16:18.547" UserId="471" />
  <row Id="6044" PostId="5699" Score="0" Text="Thanks a lot. But are there any examples of this modification?" CreationDate="2015-05-06T21:16:53.580" UserId="9215" />
  <row Id="6045" PostId="5699" Score="0" Text="I'm not aware of examples of this modification for the reasons I mentioned above - it limits the solution space. Plus, I'm not aware of any benefit to having only non-negative weights. But as I stated, you could always modify the weight update rule of an existing implementation to check if the new value is less than zero and clip it to zero if it is." CreationDate="2015-05-07T03:57:01.463" UserId="964" />
  <row Id="6046" PostId="5704" Score="1" Text="How is dimensionality reduction related to manifold?" CreationDate="2015-05-07T07:00:04.360" UserId="122" />
  <row Id="6047" PostId="5700" Score="0" Text="Are there cases where high dimensionality doesn't uncrinkle to a manifold?" CreationDate="2015-05-07T07:00:56.110" UserId="122" />
  <row Id="6048" PostId="5704" Score="0" Text="It is a way of picking out everything on the manifold and excluding everything else." CreationDate="2015-05-07T14:43:14.963" UserId="9492" />
  <row Id="6049" PostId="5708" Score="2" Text="Need more information.  What is the relationship among the categories? Are the categories mutually exclusive? Is there categorical overlap?" CreationDate="2015-05-07T16:29:29.697" UserId="182" />
  <row Id="6051" PostId="5710" Score="1" Text="I wouldn't call [Limited-memory BFGS](https://en.wikipedia.org/wiki/Limited-memory_BFGS) a &quot;simple logistic regression&quot;.  Except, I guess, in the sense that modern libraries make complex techniques very accessible :)" CreationDate="2015-05-07T17:08:35.237" UserId="9146" />
  <row Id="6052" PostId="5708" Score="2" Text="Welcome to Data Science! Currently your question is of very low quality. You can't expect quality answers without asking well described questions. Please, provide more information (better description of the data, of your background, programming languages, researched approaches etc.)." CreationDate="2015-05-07T17:09:38.163" UserId="173" />
  <row Id="6053" PostId="5703" Score="2" Text="There are too many questions in your post. You should try to split it up." CreationDate="2015-05-07T19:46:16.080" UserId="9508" />
  <row Id="6055" PostId="5700" Score="0" Text="Definitely! Sometimes, data already lies in it's intrinsic space. In that case, trying to reduce dimensionality will probably be deleterious for classification performance. In these cases, you should find that the features in the dataset you are using are largely statistically independent from each other." CreationDate="2015-05-08T00:03:40.407" UserId="9483" />
  <row Id="6056" PostId="5720" Score="1" Text="Credit for second image: http://stackoverflow.com/questions/4629505/svm-hard-or-soft-margins" CreationDate="2015-05-08T04:40:20.313" UserId="182" />
  <row Id="6057" PostId="5702" Score="0" Text="Thank you for your answer. These are indeed very good tips, but I think that some automation here would really help me here. I've stuck with Sumatra (see edit of my question) which forces me to make commits before I run experiments and keeps very detailed records." CreationDate="2015-05-08T12:07:52.260" UserId="9486" />
  <row Id="6058" PostId="5716" Score="0" Text="thank you very much,&#xA;&#xA;sorry for so many questions, is the first time I use stack exchange,&#xA;I am confused by my graduation project, I have no degree advisors or directors :( and I'm lost so about information ontologies, I'm trying to structure my graduation project on wetlands and ontologies and knowledge management, but not It is if I'm doing well.&#xA;&#xA;Thanks again for your help.&#xA;sorry for my English, I'm trying to improve it" CreationDate="2015-05-08T14:59:52.610" UserId="9488" />
  <row Id="6059" PostId="5713" Score="0" Text="thank you very much, sorry for so many questions, is the first time I use stack exchange, I am confused by my graduation project, I have no degree advisors or directors :( and I'm lost so about information ontologies, I'm trying to structure my graduation project on wetlands and ontologies and knowledge management, but not It is if I'm doing well. Thanks again for your help. sorry for my English, I'm trying to improve it." CreationDate="2015-05-08T15:05:14.553" UserId="9488" />
  <row Id="6064" PostId="5731" Score="0" Text="i have made changes in my question , can you please have a re-look and suggest me data that belongs to the changes in question. i am sorry for the inconvenience." CreationDate="2015-05-09T09:27:24.763" UserId="9521" />
  <row Id="6065" PostId="5733" Score="0" Text="Thank you for your input. This is a part of an NLP project(text mining/analytics) to be precise. I have edited the tags to data science." CreationDate="2015-05-09T14:26:30.240" UserId="9544" />
  <row Id="6066" PostId="5716" Score="0" Text="@AntonioEdgarMartinez please, accept one of the answers. I suggest accepting jkbkot's answer as it's more comprehensive than mine." CreationDate="2015-05-09T18:19:54.063" UserId="173" />
  <row Id="6068" PostId="5735" Score="2" Text="Take a look at software, referred to in [this answer](http://datascience.stackexchange.com/a/3723/2452) of mine." CreationDate="2015-05-10T02:02:13.227" UserId="2452" />
  <row Id="6069" PostId="1180" Score="0" Text="this answer doesn't address the question, which is how to &quot;allow for *weighting of certain ranges of values* each vector.&quot;" CreationDate="2015-05-10T05:09:46.130" UserId="609" />
  <row Id="6071" PostId="5730" Score="2" Text="I think that your question is too broad to expect comprehensive enough answers. I would recommend to research yourself major high-level ML techniques (Wikipedia set of relevant articles is a decent starting point) and then formulate more narrow question(s). Since you've mentioned NER, you might find helpful my related answers [here](http://datascience.stackexchange.com/a/5270/2452) and (linked within) [here](http://stats.stackexchange.com/a/136760/31372)." CreationDate="2015-05-10T06:35:58.720" UserId="2452" />
  <row Id="6072" PostId="5701" Score="0" Text="You have to distinguish between _approaches_, _frameworks_ and _tools_. **Reproducible research** approach (paradigm) is IMHO the key. Check my relevant comprehensive answer [here](http://datascience.stackexchange.com/a/759/2452)." CreationDate="2015-05-10T06:43:37.420" UserId="2452" />
  <row Id="6073" PostId="5730" Score="2" Text="Thanks @AleksandrBlekh, I should have been more specific that I am looking for high level direction than low level implementation. Your comment helps a lot with that. I use Wikipedia a lot, but it never crossed my mind to look there on this subject. I'll update the question to better reflect what I am after and follow with new questions in the future." CreationDate="2015-05-10T06:53:15.607" UserId="9534" />
  <row Id="6074" PostId="5730" Score="0" Text="You are welcome. Always glad to help." CreationDate="2015-05-10T06:55:59.187" UserId="2452" />
  <row Id="6075" PostId="5731" Score="1" Text="Given your new question, I suggest you to split it in two different questions: one asking for help on the predicition variables and the other asking for the dataset." CreationDate="2015-05-10T07:39:16.607" UserId="278" />
  <row Id="6078" PostId="5705" Score="0" Text="Could you provide some specific examples of the type of maths @Ashkan (and others young enthusiasts) can learn to better prepare?" CreationDate="2015-05-10T17:35:51.703" UserId="182" />
  <row Id="6079" PostId="5704" Score="0" Text="I think @alvas has a point here.  It's not immediately clear how this relates to the original question regarding an explanation of manifolds and dimensions." CreationDate="2015-05-10T17:44:34.557" UserId="182" />
  <row Id="6080" PostId="5741" Score="3" Text="Welcome to Data Science! Please, make your questions more detailed and specific. You're asking people to devote their time to answer your questions. If you're expecting quality answers, please consider asking quality questions (try some googling, read something on the topic, and update your posts with detailed, self-researched content). We will be glad to help you." CreationDate="2015-05-10T17:46:44.720" UserId="173" />
  <row Id="6081" PostId="5620" Score="0" Text="Could you expand upon this answer or provide an example of its application?" CreationDate="2015-05-10T17:48:47.550" UserId="182" />
  <row Id="6082" PostId="5714" Score="0" Text="Could you provide additional details in the body of the question? While someone might be able to answer this for you, the overall question quality could be improved." CreationDate="2015-05-10T17:51:22.940" UserId="182" />
  <row Id="6083" PostId="5715" Score="1" Text="This question seems very broad.  What are the &quot;immediate suspects&quot; and why aren't they adequate for your needs?" CreationDate="2015-05-10T17:52:23.410" UserId="182" />
  <row Id="6084" PostId="5743" Score="0" Text="Interesting information Jake!" CreationDate="2015-05-11T08:09:56.033" UserId="3550" />
  <row Id="6085" PostId="5712" Score="0" Text="I'm not sure it is possible to use feature scaling in R .. I can not find it..&#xA;Do you know how to perform logistic regression with LBFGS using R to compare the 2 models?" CreationDate="2015-05-11T08:13:59.130" UserId="9445" />
  <row Id="6086" PostId="5710" Score="0" Text="You are right. Do you know a way to implement Limited-memory BFGS with R ?" CreationDate="2015-05-11T08:15:47.930" UserId="9445" />
  <row Id="6087" PostId="5729" Score="1" Text="I appreciate your answer and the references here, even if the question is vague. It's really helpful to me and probably a lot more people who are just getting their feet wet as well. Thanks! :)" CreationDate="2015-05-11T11:16:37.983" UserId="9534" />
  <row Id="6088" PostId="5747" Score="1" Text="I'm voting to close this question as off-topic because its a stats question and would get better treatment on http://stats.stackexchange.com" CreationDate="2015-05-11T13:51:04.833" UserId="471" />
  <row Id="6089" PostId="5715" Score="0" Text="My main issue is that I don't know if I'm over caching or under caching. I would like to know if there is a tool that does spark profiling. I think I covered the usual suspects, but still think it's too slow. My question is beyond the usual suspects, just like a standalone JVM has JProfiler etc, what is the equivalent profiling tool for a distributed system such as Spark?" CreationDate="2015-05-11T15:36:38.027" UserId="9510" />
  <row Id="6090" PostId="5148" Score="0" Text="+1. The Field Guide is a nice high-level overview." CreationDate="2015-05-11T15:42:10.440" UserId="9123" />
  <row Id="6091" PostId="5737" Score="1" Text="You should take a look at the emails before coming up with any decision." CreationDate="2015-05-11T15:43:26.633" UserId="9123" />
  <row Id="6092" PostId="5754" Score="0" Text="This is possible and, of course, always dependent on the data.  However, given that the poster has 10^10 (presumably independent) samples, it seems that 10 dimensions wouldn't be too big a problem here." CreationDate="2015-05-11T18:03:15.220" UserId="182" />
  <row Id="6093" PostId="5754" Score="2" Text="Thanks for your comment @RyanJ.Smith. your comment is exactly in the same direction of mine. I just did not see anything regarding this problem in the post. And about the nr of samples; however he has many sample points he still might get stuck in the problem of dimensionality. I think you are arguing the opposite side of _Low Sample Size Problem_ which I think is not valid. If he has a high dimensional data, low sample size will be a problem but I think a large amount of data **does not necessarily** mean anything." CreationDate="2015-05-11T18:52:44.187" UserId="8878" />
  <row Id="6094" PostId="5747" Score="0" Text="How did I not find this? Thanks!" CreationDate="2015-05-11T20:01:45.507" UserId="8452" />
  <row Id="6095" PostId="5747" Score="0" Text="Probably appropriate in both sites. I'll migrate it." CreationDate="2015-05-11T20:47:36.177" UserId="21" />
  <row Id="6096" PostId="5759" Score="0" Text="you can locate this via wayback machine, its your best friend http://web.archive.org/web/20150402165739/http://konect.uni-koblenz.de/networks/" CreationDate="2015-05-11T21:04:02.217" UserId="1266" />
  <row Id="6097" PostId="5759" Score="2" Text=":))) Thanks @albert" CreationDate="2015-05-11T21:07:26.857" UserId="8878" />
  <row Id="6098" PostId="5760" Score="0" Text="Great info, I will look up Blondel. I'm going to leave the question open for awhile longer to see if others have ideas." CreationDate="2015-05-11T21:09:30.393" UserId="3466" />
  <row Id="6099" PostId="5760" Score="0" Text="Welcome :) Sure! If you have more specific question I'd be glad to answer if I can. I'm a community detection guy! It's been my research direction during last 2.5 years." CreationDate="2015-05-11T21:10:16.870" UserId="8878" />
  <row Id="6100" PostId="5726" Score="1" Text=":Your elaboration is spot on. Right now I am implementing a 3 player problem with around 13 strategies each. That will give me an idea. In future the number of strategies as well as the number of players are going to rise and the question was in that context. I will post my interpretations and conclusions once I get some concrete results." CreationDate="2015-05-12T06:33:33.923" UserId="9227" />
  <row Id="6101" PostId="5726" Score="0" Text="@AdiPiratla: I am glad to help. Good luck with your implementation. It would be interesting to read your insights upon completion." CreationDate="2015-05-12T07:19:49.800" UserId="2452" />
  <row Id="6108" PostId="5737" Score="0" Text="ya you right .. i gone through some of them ... most of them are business related topics and some are contentious like a threaded . this data set contain everything .. i mean inbox,deleted,sent items and so on." CreationDate="2015-05-12T10:03:06.693" UserId="9035" />
  <row Id="6110" PostId="5762" Score="0" Text="Thx @NeilSlater :) Correction done!" CreationDate="2015-05-12T11:54:15.377" UserId="8878" />
  <row Id="6111" PostId="5110" Score="0" Text="Check [this answer](http://datascience.stackexchange.com/a/814/2452) of mine. If you need dynamic visualization (for example, of streaming network data), also take a look at [this answer](http://datascience.stackexchange.com/a/3685/2452)." CreationDate="2015-05-12T12:42:28.347" UserId="2452" />
  <row Id="6112" PostId="5764" Score="0" Text="thanks ... i got some interesting thoughts from your comment . specially titles .. because those are the first thing that decide whether i would like to open it or not . i thought something with the email marketing. what sort of email got more attention, kind of analysis . and also your statistical analysis things also good ." CreationDate="2015-05-12T14:06:45.337" UserId="9035" />
  <row Id="6114" PostId="5703" Score="0" Text="Also, you cross-posted this to opendata. Please read: http://meta.stackexchange.com/questions/64068/is-cross-posting-a-question-on-multiple-stack-exchange-sites-permitted-if-the-qu" CreationDate="2015-05-12T15:44:28.173" UserId="471" />
  <row Id="6115" PostId="339" Score="4" Text="For you &quot;Dealing with Big Data&quot; comment, I would add that python is one of the 3 languages supported by apache spark, which has blazing fast speeds.  Your comment about R having a C back end is true, but so does python the scikitlearn library is very fast as well.&#xA;&#xA;I think your post has nice balance, but I contend that speed is at least a tie, and scalability (i.e. handling big data) is certainly in favor of python." CreationDate="2015-05-12T16:31:21.347" UserId="8041" />
  <row Id="6117" PostId="5761" Score="0" Text="The answer will depend on whether you want to parallelize using approximate versions of algorithms or not (and how you would want to parallelize them, example multicore or multinode). On a single machine, without any parallel processing, the complexity can be fairly well tracked." CreationDate="2015-05-12T22:40:05.890" UserId="847" />
  <row Id="6118" PostId="5764" Score="0" Text="I suggest you should go for it. You aren't doing a Google email filter, try something easy and simple." CreationDate="2015-05-12T23:39:29.810" UserId="9123" />
  <row Id="6120" PostId="5765" Score="0" Text="hmmm... i think you are on to something here. i still need to look at it again. Thanks for the hint though. I will mark it as the correct answer (permanently) since I can't vote up yet. But I might comment again to ask for more clarification (if something comes up)" CreationDate="2015-05-13T02:19:23.160" UserId="4803" />
  <row Id="6121" PostId="5754" Score="0" Text="10 dimensions are not a lot yet." CreationDate="2015-05-13T07:51:24.333" UserId="924" />
  <row Id="6122" PostId="5754" Score="1" Text="How do you determine my friend? what I said was the result of an experiment designed to answer such a question however it CAN NOT be answered in general! What is &quot;a lot&quot; in your comment exactly? it depends on many circumstances as I mentioned in my answer. in some situations 10D could be problematic." CreationDate="2015-05-13T08:21:50.060" UserId="8878" />
  <row Id="6125" PostId="5738" Score="0" Text="Thanks for the suggestions. I had hoped to use NLTK entirely, but these tools look promising as well." CreationDate="2015-05-13T16:15:38.470" UserId="9511" />
  <row Id="6126" PostId="5751" Score="0" Text="For clarification, are you trying to calculate the relative importance of a word quarter to quarter, or are you trying to make a prediction about the the raw frequency in the next quarter?" CreationDate="2015-05-13T18:28:20.150" UserId="8041" />
  <row Id="6127" PostId="5751" Score="0" Text="Relative importance primarily, prediction secondary. Does it make a difference?" CreationDate="2015-05-13T18:33:21.863" UserId="378" />
  <row Id="6130" PostId="5788" Score="0" Text="Thanks, this is an interesting idea. Could you expand on what you mean by 'normalise by the range of all scores'?" CreationDate="2015-05-13T20:54:09.223" UserId="378" />
  <row Id="6131" PostId="5788" Score="0" Text="Sure thing.  Your basic idf score will be of the form:&#xA;&#xA;idf = log_n([Number of documents in corpus]/[number of documents containing word])&#xA;&#xA;This number varies with the number of documents that are produced, and as such you'll want to find the maximal sample IDF so that you aren't favoring words based on the fact that they came from a period when more documents are produced.  There are more complex calculations for IDF that attempt to mitigate such factors, but by normalizing by maximum possible score, you fix idf to a 0-1 scale." CreationDate="2015-05-13T23:45:18.993" UserId="8041" />
  <row Id="6132" PostId="5790" Score="2" Text="I didn't think beautiful soup allowed you to iterate over pages, turns out [you can](http://stackoverflow.com/questions/28597041/scraping-multiple-paginated-links-with-beautifulsoup-and-requests). Thanks" CreationDate="2015-05-14T06:24:01.073" UserId="378" />
  <row Id="6133" PostId="5792" Score="0" Text="This is an obvious approach that I hadn't considered, thanks. My only concern is that it would be sensitive to the pre-cleaning approach used, since stopword removal, or removal based on tf-idf weighting, remove the most common words. Small changes to the stopword dictionary could lead to wild changes in rank. It would also be difficult to communicate the magnitude of a change in rank, especially for lower rank terms with similar frequencies that could change rank significantly with a small change in frequency." CreationDate="2015-05-14T06:36:30.493" UserId="378" />
  <row Id="6136" PostId="5746" Score="1" Text="Several algorithms are described in the book &quot;Mining of Massive Datasets&quot;,&#xA;which you can download for free [here](http://www.mmds.org/). Read Chapter 7 &quot;Clustering&quot;." CreationDate="2015-05-11T12:42:05.487" UserId="9085" />
  <row Id="6138" PostId="5792" Score="0" Text="@polyphant -- agreed.  These are complications that would have to be overcome using the ranked frequency analysis.  Doing both types of analysis would probably be a good idea." CreationDate="2015-05-14T15:43:57.430" UserId="609" />
  <row Id="6139" PostId="5480" Score="1" Text="Actually, I was quite surprised myself when I read the book that you could do so much with Excel. And that it had evolutionary and other non-linear solvers built-in!&#xA;&#xA;A nice benefit of Excel is that your work, especially if you're into reproducible code,  is accessible to more people than R or Python code." CreationDate="2015-05-14T23:17:07.703" UserId="9576" />
  <row Id="6140" PostId="5791" Score="0" Text="You could use a simple binary classifier." CreationDate="2015-05-14T23:40:05.563" UserId="115" />
  <row Id="6141" PostId="5780" Score="0" Text="I didn't know about MacQueen's online algorithm! Does it usually get the same results as &quot;classic&quot; K-means? What about using reservoir sampling instead? That way OP has a sample to re-run K-means on in case multiple values of K should be tested." CreationDate="2015-05-15T00:12:32.950" UserId="9576" />
  <row Id="6142" PostId="5784" Score="0" Text="This is extremely helpful.  Thank you." CreationDate="2015-05-15T08:31:24.463" UserId="9608" />
  <row Id="6143" PostId="5798" Score="0" Text="Quoting : &quot;For test data I have similar structure, just the Y column is missing in File 1.&quot; This means that in the test data, I get File 1 without Y column (we have to predict). The objective is to classify the observations in test data File 2, and using that figure out the classification of X1. For example, a candidate in X1=6 might be appearing in File 2 100 times. If in those 100 instances, we can classify 80 to be 1 and 20 to be 0, we can classify X1=1 with 0.80 probability." CreationDate="2015-05-15T08:53:30.850" UserId="9495" />
  <row Id="6144" PostId="5798" Score="0" Text="Are you assigning a Y value in your training set in file1 for each value of X1?" CreationDate="2015-05-15T08:55:58.200" UserId="2576" />
  <row Id="6145" PostId="5798" Score="0" Text="No. In training data set File 1 has X1 classified already. The observations where X1 (both that are classified as 0 or 1) are participating in File 2. For test data I have X1 only in my File 1, i.e. no classification. The range(X1, training_data) is mutually exclusive to range(X1, test_data)." CreationDate="2015-05-15T08:59:17.523" UserId="9495" />
  <row Id="6146" PostId="5798" Score="0" Text="Ok, so I have one additional question. Are there observations in file2 with the same value of X1 but different values of Y?" CreationDate="2015-05-15T09:57:56.313" UserId="2576" />
  <row Id="6147" PostId="5798" Score="0" Text="file2 does not contain any classification. Let me spill the beans here. This question is an obfuscated version of [facebook](https://www.kaggle.com/c/facebook-recruiting-iv-human-or-bot) problem on kaggle. The X1 is the bidder_id. File1 contains classification info of bidders. File2 contains the bids from the bidders. The bids are not classified for us. Now, I am trying to reformulate this problem as bid-classification leading to bidder-classification. If I can classify bids then i guess classifying bidders is easier." CreationDate="2015-05-15T10:05:00.743" UserId="9495" />
  <row Id="6148" PostId="5761" Score="0" Text="I downvoted your answer, as it doesn't answer your question. Why don't you perform a search on the Internet? I'm sure that a proper comprehensive search will result in significant number (likely, hundreds) of relevant papers." CreationDate="2015-05-15T11:56:46.040" UserId="2452" />
  <row Id="6149" PostId="5729" Score="0" Text="Thanks, I have started working with naive bayes and feature engineering in general.  Any other things apart from naive bayes that I should try?" CreationDate="2015-05-15T14:31:23.923" UserId="9497" />
  <row Id="6150" PostId="5729" Score="0" Text="Well, you still haven't offered very many details about the data itself or the specifics of what you've done, so it's very difficult to give you specific suggestions.  The best I can say is consider incorporating some sequential structure into your model and features either through use of bigrams or markov models / finite state machines." CreationDate="2015-05-15T15:06:13.263" UserId="182" />
  <row Id="6151" PostId="5805" Score="1" Text="Thanks for so many ideas. I have to spend some time on each and will get back to you. :) Highly appreciate you answer." CreationDate="2015-05-16T16:43:45.513" UserId="9003" />
  <row Id="6152" PostId="5812" Score="0" Text="@ ffriend Thanks for your feedback.Product name is important for us because some products have discounts and others have no discounts. Our objective is measure the significance of discounts on product growth. Do you think I can use real commissions/Discounts values in dollars or Zero dollars instead of Yes or NO? How R interpret Date? do I need to convert the data into time series first? Is it Multiple regression is good model for this?" CreationDate="2015-05-16T20:23:54.057" UserId="9663" />
  <row Id="6153" PostId="5812" Score="0" Text="Can you measure significance of discounts _per product_? As for real discount values vs. &quot;yes/no&quot;, try both! It's fairly easy to build several alternative models and estimate how much they fit your data (e.g. using [RSS](http://en.wikipedia.org/wiki/Residual_sum_of_squares)). Regarding date, you probably don't need it per se, but instead convert it into several variables like `month` and `date_of_week` (see 1st paragraph of my answer). And yes, multiple regression looks fine here." CreationDate="2015-05-16T23:38:12.013" UserId="1279" />
  <row Id="6154" PostId="5812" Score="0" Text="Also, please, don't [cross-post](http://stats.stackexchange.com/questions/152628/is-my-sales-growth-dependent-on-commissions-discounts-and-how-do-i-analyze-this) questions on several stackexchange sites. You can always migrate your question if you feel like original site is a bad fit for it, but making cross-posting makes it harder for others to find good answers to the same question, since answers become distributed between several places." CreationDate="2015-05-16T23:42:42.840" UserId="1279" />
  <row Id="6155" PostId="5812" Score="0" Text="@friend Thank you for your help. I am new and I am not aware that all forums are connected. I won't do that next time." CreationDate="2015-05-17T00:41:10.243" UserId="9663" />
  <row Id="6156" PostId="5812" Score="0" Text="No problem. Also note, that on your profile page (e.g. [this](http://datascience.stackexchange.com/users/9663/murali) for current site) you can easily reach all your accounts in StackExchange network." CreationDate="2015-05-17T00:56:46.277" UserId="1279" />
  <row Id="6157" PostId="5818" Score="0" Text="Thank you for your answer @Aleksandr Blekh - really appreciated. Ill digg right into that.&#xA;&#xA;Maybe a stupid question, but please correct me here if I'am wrong here:a correlation analysis, while using one airline as the variable to correlate with. The results were compelling so far, as some airlines espc. those who had codeshare agreements had similar prices. &#xA;&#xA;Would such high correlations e.g.:&#xA;`ColumnUA(LH) 0.90435 &lt;.0001&#xA;ColumnSQ 0.32544 &lt;.0001&#xA;ColumnAF(DL) 0.55336 &lt;.0001`&#xA;&#xA;I assume such results **indicate** similar price patterns.&#xA;With a regression analysis, what would I find out?" CreationDate="2015-05-18T02:55:16.293" UserId="9684" />
  <row Id="6158" PostId="5818" Score="0" Text="@s1x: You're very welcome (feel free to upvote/accept, if you value the answer and when you'll get enough reputation to do so, of course). Now, on to your question. As I said, TS analysis is more sophisticated and comprehensive. In particular TS regression, accounts for so-called [autoregression](http://en.wikipedia.org/wiki/Autoregressive_model) and other TS complexities. Hence, my suggestion to use TS regression analysis instead of simpler traditional one. Also, you should _always_ start with EDA, no matter what data analysis you plan to perform (actually, EDA will often change your plans)." CreationDate="2015-05-18T03:21:38.997" UserId="2452" />
  <row Id="6159" PostId="2258" Score="0" Text="I'm surprised no one has flagged this question as Too Broad" CreationDate="2015-05-18T05:29:36.683" UserId="9670" />
  <row Id="6160" PostId="5810" Score="0" Text="I think this is a very broad question. Can you narrow it down to a particular example problem and how much you know about how control theory applies already?" CreationDate="2015-05-18T07:34:18.473" UserId="21" />
  <row Id="6161" PostId="693" Score="0" Text="You can train n svm's for n classes using a one vs the rest strategy. SciKitLearn has code to do that automatically. Technically you need n-1 classifiers, but i've found having n works better." CreationDate="2015-05-18T19:54:36.913" UserId="1301" />
  <row Id="6163" PostId="5788" Score="0" Text="I have a few issues with this approach. Firstly, as mentioned by @MrMeritology, this approach is only valid when comparing counts for the same term. Secondly, tf-idf would negatively discriminate frequent terms appearing across all documents. If it's these frequent terms that you want to compare between periods then the weighted differences would be very small. It's an interesting idea, with it's own area of application, but I'm looking for something more general purpose, preferably taking in to account the properties of Heap's and Zipf's law that lead to the issue in the first place" CreationDate="2015-05-19T16:47:23.443" UserId="378" />
  <row Id="6164" PostId="5822" Score="0" Text="you need to start by creating some basic stats on individuals do/don't get converted from recipients to actors.  I think you'll have a hard time getting people here to do that for you.  I would suggest tackling the basic stats and see how far you get...if you run into problems then post your code and intent-it'll give folks here something to work with and it will demonstrate that you giving your problem some effort." CreationDate="2015-05-19T17:18:17.507" UserId="3457" />
  <row Id="6165" PostId="1100" Score="0" Text="My initial thought would be to use a random forest of logistic regression on per-server-type models.  Then you have your benchmarks and  you'll find out pretty quick if a neural net will give you more.  Neural nets don't always give the best results." CreationDate="2015-05-19T17:24:41.940" UserId="3457" />
  <row Id="6166" PostId="5788" Score="0" Text="I would contend that if the method can be done for a single word, it's pretty trivial to run it across the entire corpus.   None the less, the method proposed by @MrMeritology does extend more naturally to the entire data set.&#xA;&#xA;For your second point, if you have an idf score that shrinks and grow logrithmically, the linear growth of the tf term will overtake the the weighting for all but the most frequently used words (that would most likely be on a stop word list anyway)." CreationDate="2015-05-19T17:35:03.377" UserId="8041" />
  <row Id="6167" PostId="5836" Score="0" Text="I'm sorry but I can't see how in this case you can even think of trying to train a convnet, it simply can't answer this kind of question for the reasons Neil Slater explained : features vectors do not have in general a &quot;spatial&quot; meaning (except special cases e.g. SIFT). For example, you can permute dimensions. On an image, where you have spatial structure, you scan for patterns and convnets are one of the many possibilities. Anyhow, in this case it just doesn't make any sense and moreover it's wrong. Modulo the fact that the question is a little bit vague." CreationDate="2015-05-19T21:35:43.387" UserId="8040" />
  <row Id="6168" PostId="5836" Score="0" Text="@BertrandR  yes, as I said, I agree with Neil...likely it will not work.  And yes, the question is a bit vague...most likely because nub may not know much about NNs and CNNs.  But (s)he has data and sometimes you learn lessons much better by working with your own data.  Even getting a net to train can be an accomplishment when you're starting out.  I'm not saying to use a NN or CNN because I think it will give the best result...I'm encouraging nub to experiment and break stuff and learn." CreationDate="2015-05-20T06:39:10.653" UserId="3457" />
  <row Id="6169" PostId="5841" Score="0" Text="This comment helped me a bit more to realize how things work. I would vote you up but I need 15 reputation in this stackechange site. I'll do it when I have enough rep. I'll keep reading about this kind of things to see if I can manage what I want to do. Thanks." CreationDate="2015-05-20T10:15:14.173" UserId="9714" />
  <row Id="6170" PostId="5788" Score="0" Text="@ j.a.gartner Even if you run it across all terms, you can't then compare the scores for those terms. Agree with you on the second point though, I ignored the tf part" CreationDate="2015-05-20T10:18:41.960" UserId="378" />
  <row Id="6171" PostId="5809" Score="0" Text="Just because you are doing data science doesn't mean you have to ask your question here.  This question has already been answered in Stackoverflow here and in several other questions (so you could have had your answer imediately): http://stackoverflow.com/questions/27217331/r-subset-dataframe-by-time-only." CreationDate="2015-05-20T13:58:15.283" UserId="9689" />
  <row Id="6172" PostId="5853" Score="0" Text="Thanks for your thorough explanation. I agree with you that such analysis based on prices only are quite limited. This also includes notably fare rules (Refundable tickets, minimum stay etc.) Some of those limitation can be overcome by collecting always same fares to make the comparable. However, a important information - as you mentioned, is missing the amount of seats available (can be != seats in a plane) and the the actually amount of sold tickets." CreationDate="2015-05-21T13:16:38.673" UserId="9684" />
  <row Id="6173" PostId="5838" Score="1" Text="Can you provide more information? what is &quot;categories id of product A&quot; and is &quot;searching keywords id of product A&quot; of the same length for all entries? &quot;the dimensions of the searching keywords id could be more than 10000&quot; why? what are they? How many samples do you have? all questions can be answered if you post a few sample of your data here. Then I could probably suggest you something." CreationDate="2015-05-21T13:17:09.953" UserId="8878" />
  <row Id="6174" PostId="5853" Score="0" Text="Access to such data is very limited and if - outdated (eg. Databank 1B from US DOT). Some research such as Clark R. and Vincent N. (2012) Capacity-contingent pricing [...] [link](http://bit.ly/1HvHViw) includes such data and offer much better insights. &#xA;&#xA;I'am aware of the limitations (hopefully ;-) ) and as you mentioned as there are much more information influencing prices. Still when observing a specific market you can get a _feeling_ of what happens. You can see if there is any compeitive behaviour and different pricing strategy approachs. However, you would never be able to find the cause." CreationDate="2015-05-21T13:19:26.397" UserId="9684" />
  <row Id="6175" PostId="5844" Score="1" Text="Does the data include the amount of funding? Are you trying to compute the probability of going to round N+1 given funding X at round N? Do you have any other info about the company (turnover, size, profit)?" CreationDate="2015-05-21T13:22:37.810" UserId="471" />
  <row Id="6176" PostId="5853" Score="1" Text="@s1x - I agree and I wish I had a solid alternative to offer, but, as you've learned yourself, detailed revenue data is the most jealously guarded secret at any airline. Just wanted to make sure you're aware of that and what goes into the data generation process. Beyond, that, I like what you're trying to do and I think the other answer is a step in the right direction, technique-wise. If I might suggest, you could also take a look at using cross-correlation between your various TS during your data exploration, as it is often valuable for discerning patterns between linked TS." CreationDate="2015-05-21T13:24:43.460" UserId="9448" />
  <row Id="6177" PostId="5838" Score="0" Text="Are product A and product B two products that the user bought?  The wording seems to suggest that products A and B are different for each user, since the keywords can vary.  Is this so? And last comment, do you want to classify or cluster? Those are quite different techniques :)" CreationDate="2015-05-21T14:28:46.533" UserId="1367" />
  <row Id="6178" PostId="2492" Score="0" Text="As pointed above, trying to merge those multiple variables into one becomes more complicated as the number of dummy variables rises. For modelling purposes, it is often useful to keep those variables as dummy variables. If some combinations of those dummies are very common, then you can explore those effect by using interactions in your model. Whether you want to reduce those variables into a composite will depend on the type of analysis you plan on conducting on the resulting data." CreationDate="2015-05-21T14:52:43.857" UserId="9646" />
  <row Id="6179" PostId="5855" Score="0" Text="Regarding your last sentence, it depends on your definition of &quot;really weird&quot;. The [origin of the word](http://en.wikipedia.org/wiki/Gerrymandering#Etymology) refers to the salamander-like shape of the resulting district. So you will have to balance optimization with constraints because the more geometric constraints you impose, the less optimized the result will be." CreationDate="2015-05-21T16:24:39.467" UserId="964" />
  <row Id="6180" PostId="5855" Score="0" Text="You are right @bogatron. But I know that there are certain statistical formulas to measure the compactness of a shape. Ideally this could be involved also in the optimization process." CreationDate="2015-05-21T16:32:13.417" UserId="5211" />
  <row Id="6181" PostId="5838" Score="0" Text="Thank you @kasramsh so much for your replies. I updated the description and also attached a sample data. Hope to get some suggestions from you!" CreationDate="2015-05-21T18:09:33.180" UserId="9724" />
  <row Id="6182" PostId="5838" Score="0" Text="@logc yes, product( i said product A earlier) and website( i said product B earlier) are different from each user. Each product has a few keywords and each website has a few keywords too.  Either clustering or classification is fine, as long as I can make an user profile, such as &quot;male young gamer&quot;; &quot;stay at home mom&quot;. I think clustering is more preferable .  Thank you!!" CreationDate="2015-05-21T18:09:49.113" UserId="9724" />
  <row Id="6183" PostId="5852" Score="0" Text="Thank you @Lennart Kloppenburg for your reply. How to perform feature selection if the attribute (keword_id) are ordered number? I updated a sample data above. Could you please take a look and give me some suggestions? Thank you!" CreationDate="2015-05-21T18:13:42.633" UserId="9724" />
  <row Id="6185" PostId="5857" Score="0" Text="Thank you so so much! It's very helpful. I will start from the mapping. I really appreciate it!" CreationDate="2015-05-21T20:19:48.177" UserId="9724" />
  <row Id="6186" PostId="5855" Score="0" Text="try http://gis.stackexchange.com/ people there may have a better idea or some experience with redistricting." CreationDate="2015-05-21T20:53:28.460" UserId="471" />
  <row Id="6187" PostId="5857" Score="0" Text="Happy to help. :)" CreationDate="2015-05-21T21:44:37.090" UserId="1367" />
  <row Id="6189" PostId="5854" Score="0" Text="Yes, that is what I am aiming at. I have data on funding volume and timing on each round. I'll look into the links you posted and I'll try running the code. Thanks a lot for your help." CreationDate="2015-05-22T14:57:04.613" UserId="10171" />
  <row Id="6191" PostId="5844" Score="0" Text="I am trying to predict how many companies I will have to invest in if I raise a US$ x m fund. This depends on the likelihood of companies advancing through financing stages, and how much I will invest in a Seed, Series A, B, C... round. That way I know that I can invest in less companies if I go from investing 12% to 25% of the total amounted in each Series A a participated in rounds. The stochastic modelling im trying here is to calibrate the survival rates in the model." CreationDate="2015-05-22T15:00:39.960" UserId="10171" />
  <row Id="6192" PostId="5860" Score="3" Text="Not sure if a similar question exists already, but this would be a good question for a community wiki answer." CreationDate="2015-05-22T15:41:27.183" UserId="182" />
  <row Id="6193" PostId="5428" Score="0" Text="for completeness, i edited my answer to include a graphical example." CreationDate="2015-05-22T17:06:33.990" UserId="8953" />
  <row Id="6194" PostId="5541" Score="1" Text="... and it may be a useless result, as the Gini is usually applied to data that has two categorial labelings, while AUROC is applied to numerical ranking data + a binary label. They *may* **coincide *only* if your ranking is binary?** in which case it would not make much sense to use AUROC at all because it is a 3-point curve with only 2 degrees of freedom... (I have not checked that result, too much paper spam on Wikipedia these days.)" CreationDate="2015-05-22T21:34:51.397" UserId="924" />
  <row Id="6195" PostId="3751" Score="0" Text="@zen I would assume Sean meant for you to edit/expand on the question, not post details as a comment.  I think it would be more helpful to future users that way, if they end up here for the first time through an online search." CreationDate="2015-05-23T01:54:55.237" UserId="8953" />
  <row Id="6196" PostId="3751" Score="0" Text="Except of already mentioned great coursera stuff by Geoff Hinton, also this series by Nando de Freitas is worth a look: https://www.youtube.com/playlist?list=PLE6Wd9FR--EfW8dtjAuPoTuPcqmOV53Fu" CreationDate="2015-05-23T10:33:02.757" UserId="173" />
  <row Id="6197" PostId="5864" Score="0" Text="Thanks so much, I'll take a look on these books." CreationDate="2015-05-23T20:02:45.400" UserId="9771" />
  <row Id="6198" PostId="5878" Score="0" Text="Thanks, I'm already watching the first lecture." CreationDate="2015-05-23T20:55:25.817" UserId="9771" />
  <row Id="6199" PostId="5789" Score="2" Text="Web scraping LinkedIn is against their terms of service. See [LinkedIn “DOs” and “DON’Ts”](https://www.linkedin.com/legal/user-agreement)- DON'T:&quot;Use manual or automated software, devices, scripts robots, other means or processes to access, “scrape,” “crawl” or “spider” the Services or any related data or information;&quot;" CreationDate="2015-05-23T21:03:07.677" UserId="1330" />
  <row Id="6200" PostId="5878" Score="0" Text="A good companion to his web series is [Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf). It has a &quot;physical&quot; book also in addition to the free online PDF." CreationDate="2015-05-23T21:08:41.933" UserId="8354" />
  <row Id="6201" PostId="5883" Score="0" Text="Thanks for the links. That looks great. I have plenty of time at the moment and I'll work on this." CreationDate="2015-05-24T01:50:08.913" UserId="9771" />
  <row Id="6203" PostId="5886" Score="1" Text="When you say &quot;divide by the difference between the largest and smallest number&quot; is the smallest number the smallest in the updated array (updated as in the array where every absolute value has been added) or is the smallest number still negative?" CreationDate="2015-05-24T03:56:17.383" UserId="9802" />
  <row Id="6204" PostId="5877" Score="0" Text="Thank you for the advice." CreationDate="2015-05-24T07:15:33.400" UserId="9793" />
  <row Id="6205" PostId="5861" Score="0" Text="where is that image to be found in original size?" CreationDate="2015-05-24T13:41:44.707" UserId="9810" />
  <row Id="6206" PostId="5861" Score="0" Text="@WalterTross, here https://www.google.com/search?q=data+science+road+map" CreationDate="2015-05-24T14:33:45.417" UserId="97" />
  <row Id="6207" PostId="5886" Score="0" Text="@Jonathan: it doesn't matter as long as both values are from the same array, original or updated. Since the same number is added to every value, the difference between the minimum and maximum stays the same." CreationDate="2015-05-24T21:00:55.447" UserId="9819" />
  <row Id="6208" PostId="5859" Score="0" Text="+1 for you! good answer!" CreationDate="2015-05-24T23:39:13.500" UserId="8878" />
  <row Id="6209" PostId="5864" Score="0" Text="Russel is the starting point usually" CreationDate="2015-05-24T23:41:07.907" UserId="8878" />
  <row Id="6210" PostId="5877" Score="3" Text="Also use all of them and reduce dimensionality. e.g. PCA extracts meaningful features according to the variance." CreationDate="2015-05-24T23:42:52.493" UserId="8878" />
  <row Id="6211" PostId="5893" Score="1" Text="Python NLTK module provides stopwords data and if it did not help you better provide more info about your dataset. Why it was not helpful in your case?" CreationDate="2015-05-25T01:25:41.937" UserId="8878" />
  <row Id="6212" PostId="335" Score="0" Text="I agree. The course material is good to get you started but it is mostly entry level." CreationDate="2015-05-25T07:27:09.663" UserId="75" />
  <row Id="6214" PostId="5893" Score="0" Text="@kasramsh: When i filtered for these SWs i had the impression that this did not filter significantly out the spam. I think the reason is, that these list are generated on natural texts (not sure) and therefore are not usable for searchwords and site queries. Like when you cluster ( based on the search string similarity) i had the impression that the spam has a strong effect at entropy level and thereby is mixing up the end result :-/." CreationDate="2015-05-25T09:53:01.813" UserId="4676" />
  <row Id="6215" PostId="5899" Score="1" Text="what do you mean by 'top 5 Id2 matches of each attribute'. Can you modify your output so that match values are correct too" CreationDate="2015-05-25T12:58:05.340" UserId="9801" />
  <row Id="6216" PostId="5899" Score="0" Text="@device_exec output is edited. top 5 attributes are preferences by each attribute." CreationDate="2015-05-25T13:14:42.673" UserId="8657" />
  <row Id="6217" PostId="5893" Score="1" Text="I think @PlagTag don't understand what is `stop words`. Stop-wrods is a list of most common words in some language, for example `I`, `the`, `a` and so on. You will just remove this words from your text before start train your algorithm which try identify which text is spam or not. It didn't help you identify which text is spam or not, it can give your learning algorithm some improve." CreationDate="2015-05-25T14:18:51.500" UserId="9634" />
  <row Id="6218" PostId="5370" Score="0" Text="Some other useful python libraries include : pandas, numpy, scipy etc. Adding this in support of learning python :)" CreationDate="2015-05-25T14:45:49.237" UserId="75" />
  <row Id="6219" PostId="5888" Score="1" Text="This approach is also known as min-max normalization (as we are using min and max values)" CreationDate="2015-05-25T14:50:32.343" UserId="75" />
  <row Id="6220" PostId="5856" Score="0" Text="Probably this question should exist as community wiki." CreationDate="2015-05-25T14:54:08.057" UserId="75" />
  <row Id="6221" PostId="313" Score="1" Text="I think this question should be marked as community wiki." CreationDate="2015-05-25T15:04:39.207" UserId="75" />
  <row Id="6222" PostId="5907" Score="0" Text="Not a bad idea, although in that particular case I think I'd want separation. Things like &quot;software developer&quot; and &quot;software engineer&quot; might be close enough to merge (in our case, maybe not others)." CreationDate="2015-05-25T18:33:22.760" UserId="9807" />
  <row Id="6223" PostId="5897" Score="1" Text="This would be a better question if you could specify for what specific purpose you think you need linear algebra." CreationDate="2015-05-26T06:36:57.857" UserId="21" />
  <row Id="6224" PostId="5137" Score="0" Text="A related discussion on [reddit](https://www.reddit.com/r/datascience/comments/373lpg/data_science_and_automation/)" CreationDate="2015-05-26T08:38:59.127" UserId="75" />
  <row Id="6225" PostId="5902" Score="0" Text="thx a lot, i will try out this one and report here!" CreationDate="2015-05-26T08:46:17.480" UserId="4676" />
  <row Id="6226" PostId="5893" Score="0" Text="@itdxer, thanks for your comment. I used the term stopwords here in an broader extend (as i thought it might be ok for the purpose). Thank you for clearing up the issue ;-)" CreationDate="2015-05-26T08:49:23.637" UserId="4676" />
  <row Id="6227" PostId="5693" Score="0" Text="You are right @Spacedman, I have edited my post. I hope it is now clearer." CreationDate="2015-05-26T09:03:58.997" UserId="9461" />
  <row Id="6228" PostId="5737" Score="2" Text="If you write a paper using the ideas collected here, will you list StackOverflow as one of your collaborators? :)" CreationDate="2015-05-26T09:43:15.897" UserId="1367" />
  <row Id="6230" PostId="5910" Score="1" Text="I think there are no alternatives to your method!" CreationDate="2015-05-26T13:58:05.330" UserId="9766" />
  <row Id="6231" PostId="5806" Score="1" Text="Thanks for the suggestion! I've used dimensionality reduction in the past (PCA), but wasn't able to figure out how to determine how much each variable contributed to the orthogonal components. I'll look through the Burgess tutorial, perhaps that will give me more ideas." CreationDate="2015-05-26T15:00:02.393" UserId="7922" />
  <row Id="6233" PostId="5877" Score="1" Text="Exactly. You can even consider it as a small research project. You assume that there are some features that will work well based on your intuition, and pick those. Or, you assume nothing and input &quot;everything&quot; into a feature selection algorithm. The output for that will be the machine's way of telling you &quot;These N features work the best for me to discriminate between the possible labels&quot;. Good luck!" CreationDate="2015-05-26T15:59:07.330" UserId="8152" />
  <row Id="6237" PostId="5806" Score="0" Text="@NavaneethanSanthanam: You're very welcome!" CreationDate="2015-05-26T17:07:28.127" UserId="2452" />
  <row Id="6239" PostId="5137" Score="0" Text="How can we tell this Q isn't simply advertising for that product?" CreationDate="2015-05-26T21:53:49.950" UserId="471" />
  <row Id="6240" PostId="5913" Score="4" Text="This question is too broad and needs to be framed as a question rather than a seed for a discussion." CreationDate="2015-05-26T23:34:16.273" UserId="847" />
  <row Id="6241" PostId="5916" Score="0" Text="Could you explain the &quot;path&quot; part of data or attach a sample dataset if possible" CreationDate="2015-05-27T05:45:35.840" UserId="75" />
  <row Id="6243" PostId="5916" Score="0" Text="I'm gonna have a csv file like this&#xA;DeviceID,Screen,Time_spent_on_Screen,Transition. &#xA;ABC,Screen1, 3s, 1-&gt;2.&#xA;ABC,Screen2, 2s, 2-&gt;4.&#xA;ABC,Screen4, 1s, 4-&gt;2.&#xA;So the &quot;path&quot; here is 1-&gt;2-&gt;4-&gt;2, 1,2,3,4 are screens" CreationDate="2015-05-27T05:53:12.337" UserId="9852" />
  <row Id="6244" PostId="5916" Score="0" Text="Looking at it. Meanwhile could you add this to question itself?" CreationDate="2015-05-27T05:55:09.507" UserId="75" />
  <row Id="6245" PostId="5916" Score="0" Text="Ohk now how I see it is : screens are basically pages. So you have the data about how much time the a user (identified by device id) spending on each page (identified by screen) and the data about how the user hops from one page to another. Is this right?" CreationDate="2015-05-27T05:58:38.400" UserId="75" />
  <row Id="6246" PostId="5916" Score="0" Text="Yes, I want to cluster users based on the path they use to browse the app." CreationDate="2015-05-27T06:03:05.040" UserId="9852" />
  <row Id="6249" PostId="5918" Score="0" Text="Hey could you add some more details. Also it seems incomplete as of now." CreationDate="2015-05-27T07:55:31.740" UserId="75" />
  <row Id="6252" PostId="5919" Score="0" Text="&gt;how do I go about clustering the users.&#xA;&#xA;It is indeed about clustering the users no?" CreationDate="2015-05-27T08:08:53.417" UserId="75" />
  <row Id="6253" PostId="5917" Score="1" Text="this clusters the pages, not the users." CreationDate="2015-05-27T08:13:41.863" UserId="6550" />
  <row Id="6254" PostId="5919" Score="0" Text="yes yes. actually you use these features that i mentioned to cluster user behaviors (or users as you say :) )" CreationDate="2015-05-27T08:26:29.823" UserId="8878" />
  <row Id="6256" PostId="5917" Score="0" Text="Edited the answer. Thanks for pointing it out." CreationDate="2015-05-27T09:24:12.517" UserId="75" />
  <row Id="6257" PostId="5909" Score="0" Text="Thank you for sharing @Shagun, I will consider this option." CreationDate="2015-05-27T09:54:41.113" UserId="9461" />
  <row Id="6258" PostId="5908" Score="0" Text="Thanks a lot thilak for the explanation" CreationDate="2015-05-27T10:12:22.907" UserId="9793" />
  <row Id="6259" PostId="5909" Score="0" Text="Sure. Do as share your experience." CreationDate="2015-05-27T10:28:02.330" UserId="75" />
  <row Id="6260" PostId="5909" Score="0" Text="What if, instead of using Elastic on top, I use geohashing as an index in Cassandra ?" CreationDate="2015-05-27T13:31:47.127" UserId="9461" />
  <row Id="6261" PostId="5909" Score="0" Text="[Geohash](http://en.wikipedia.org/wiki/Geohash) is not a spatial index. I assume you were referring to some kind of spatial index. So yeah you can do that. My intent of mentioning Elastic (or any external index) was to take care of cases where the indexing offered by the database itself does not take care of spatial datatypes." CreationDate="2015-05-27T13:35:50.253" UserId="75" />
  <row Id="6262" PostId="5909" Score="0" Text="Ok I read too fast, thank you for your explanations @Shagun" CreationDate="2015-05-27T14:02:03.710" UserId="9461" />
  <row Id="6263" PostId="5860" Score="1" Text="This question is clearly more suited for a community wiki answer, since **it doesn't have a unique answer**.  The format of Q&amp;A sites is more suited to questions with a &quot;best&quot; answer." CreationDate="2015-05-27T14:16:13.913" UserId="1367" />
  <row Id="6264" PostId="5925" Score="0" Text="Thanks Shagun, I will try this." CreationDate="2015-05-27T14:42:13.513" UserId="9793" />
  <row Id="6265" PostId="5925" Score="0" Text="any suggestion on how to extra weight to the recent data?" CreationDate="2015-05-27T14:56:53.847" UserId="9793" />
  <row Id="6266" PostId="5916" Score="0" Text="To me, the mixture of Markov chains model calibrated by maximum likelihood methods seems more serious than graph heuristics. For example, this paper: http://link.springer.com/article/10.1023/A:1024992613384." CreationDate="2015-05-27T15:04:30.043" UserId="6550" />
  <row Id="6267" PostId="5925" Score="0" Text="Time based exponential decay is used commonly with data having a temporal aspect. Assuming you have only 2 years data, you can go for something simpler based on your intuition and requirement of data/end product." CreationDate="2015-05-27T16:08:22.920" UserId="75" />
  <row Id="6268" PostId="5857" Score="0" Text="Hi @logc, I applied LDA for selecting the features. I considered each user_id as a &quot;document&quot; and the keywords are the &quot;words&quot; in the &quot;document&quot;, then by applying LDA I got a few topics of keywords. However, i don't know why most of my topics are consist of the same keywords. Does that mean LDA is not the right method for my case or there are some mistakes? Thank you so much!" CreationDate="2015-05-28T00:33:05.980" UserId="9724" />
  <row Id="6270" PostId="5932" Score="1" Text="Flagged the question as it has nothing to do with data science. Probably [Stackoverflow](http://stackoverflow.com/) would be the right place for this." CreationDate="2015-05-28T08:22:04.493" UserId="75" />
  <row Id="6271" PostId="5930" Score="1" Text="You are right! I would like to know what techniques could be used to tackle the problem. Specifically, I was asking whether there is a way to minimize the correlation between X1 and Y without using linear regression. Thank you for the clarification :)" CreationDate="2015-05-28T08:25:50.033" UserId="9838" />
  <row Id="6272" PostId="5857" Score="0" Text="@sylvia : I would suggest that you turn that question into a new question on this site.  Otherwise, we might end up writing a ton of comments, and that is not the best format for Q&amp;A. :)" CreationDate="2015-05-28T10:54:10.410" UserId="1367" />
  <row Id="6273" PostId="5933" Score="0" Text="When I read the question's title, I thought you expected some commercially viable product ideas -- something probably outside the scope of this site.  But you seem to be looking for some interesting analysis.  Why do you call it a product? What is it you expect as an outcome?" CreationDate="2015-05-28T11:04:40.330" UserId="1367" />
  <row Id="6274" PostId="5932" Score="0" Text="A simple Google search of the first error line would take you to a [StackOverflow thread where they discuss this issue](http://stackoverflow.com/questions/14024756/slf4j-class-path-contains-multiple-slf4j-bindings) or [another SO thread where they solve it again](http://stackoverflow.com/questions/22896243/maven-slf4j-class-path-contains-multiple-slf4j-bindings)" CreationDate="2015-05-28T11:08:35.790" UserId="1367" />
  <row Id="6275" PostId="5933" Score="0" Text="I call it a product in the sense that I do not want some simple or trivial analysis. And as you correctly mentioned - I am not looking for any commercially viable idea - just some cool fun idea :)" CreationDate="2015-05-28T12:12:42.793" UserId="75" />
  <row Id="6277" PostId="5937" Score="0" Text="Thank you...I will read the example." CreationDate="2015-05-28T15:56:31.993" UserId="9501" />
  <row Id="6278" PostId="5880" Score="0" Text="Thanks for the recommendation, but the first link seems not working to me." CreationDate="2015-05-28T18:35:27.500" UserId="9771" />
  <row Id="6279" PostId="5857" Score="0" Text="Thanks for the suggestion. Here is the link I posted if you have time to take a look http://datascience.stackexchange.com/questions/5941/need-help-with-lda-for-selecting-features Thanks!" CreationDate="2015-05-28T22:10:02.293" UserId="9724" />
  <row Id="6280" PostId="5943" Score="0" Text="This question does not ask for any form of source code. Please consider including only the relevant portion of code (if at all). Also, mathematically, the logistic loss function (or the cross entropy function) is well defined for binary classification (mapping classes to -1 and 1). What is the loss function mathematically for cross-entropy for multinomial case (in mathematical form)? For example, one way to solve multinomial problems is to solve K-1 binary problems (for K classes) which is not really a loss function for the multinomial case." CreationDate="2015-05-29T03:38:56.487" UserId="847" />
  <row Id="6281" PostId="5946" Score="0" Text="With 2D features you can visualize your data, correct? Everything will be much clearer then. And why  &quot;nor can I use hierarchical clustering&quot;?" CreationDate="2015-05-29T08:08:17.297" UserId="9085" />
  <row Id="6282" PostId="5946" Score="0" Text="Yes I can visualize the data.... Hierarchical clustering is going to give me l clusters and subclusters ..... Oh so are you suggesting that even if I get subclusters , in order to obtain my clusters I should repeatedly subtract a subcluster from its parent cluster?" CreationDate="2015-05-29T08:11:36.060" UserId="8013" />
  <row Id="6283" PostId="5946" Score="0" Text="I just tried to visualize it, but identification of the separate clusters is not possible...." CreationDate="2015-05-29T08:15:11.850" UserId="8013" />
  <row Id="6284" PostId="5946" Score="1" Text="With hierarchical clustering you make a dendogram. Then you cut it at specific level, which you estimate reasonable from the dendogram.  The precedure is very clear and gives you clusters.  No &quot;subclusters&quot;, no substraction.  If you post your visualization (plot of your actual data that you are trying to cluster), it will be easier to tell more." CreationDate="2015-05-29T08:18:26.837" UserId="9085" />
  <row Id="6285" PostId="5944" Score="0" Text="(This question belongs to math.stackexchange..) You can get an approximate solution by simply computing your probability for all n for which L1 distance between x P^n and the stationary distribution x* is more than epsilon. This assumes your chain is also aperiodic." CreationDate="2015-05-29T08:43:39.023" UserId="6550" />
  <row Id="6286" PostId="5910" Score="0" Text="Why don't you just see which input has the least pearson correlation(since the relationship is linear) to the output, and then ignore it ?" CreationDate="2015-05-29T08:49:19.273" UserId="9460" />
  <row Id="6287" PostId="170" Score="0" Text="Is this entropy? I am confused." CreationDate="2015-05-29T12:41:01.670" UserId="6550" />
  <row Id="6288" PostId="455" Score="0" Text="A more appropriate place for this question is http://opendata.stackexchange.com/" CreationDate="2015-05-29T17:38:53.977" UserId="3466" />
  <row Id="6289" PostId="155" Score="0" Text="A great place to find public data sets is http://opendata.stackexchange.com/" CreationDate="2015-05-29T18:00:14.140" UserId="3466" />
  <row Id="6290" PostId="5880" Score="0" Text="In case the link for Intro to Artificial Intelligence doesn't work, try this: go to udacity.com, click &quot;catalog&quot; on the upper right corner, then type &quot;artificial intelligence&quot; in upper right search box. A list of classes will show up. I just tried this. The second class is &quot;Intro to Artificial Intelligence&quot;." CreationDate="2015-05-29T18:51:02.840" UserId="9593" />
  <row Id="6293" PostId="5730" Score="0" Text="Interesting project. The free Udacity class - Intro to Machine Learning (https://www.udacity.com/course/intro-to-machine-learning--ud120) also explores Enron Corpus. To discover which authors might have affinity, maybe one can start with examining the number of emails between two people, assuming the number of emails would indicate that they cared about each other enough so they didn't avoid emailing each other as much as possible. What's your learning goal in this project?" CreationDate="2015-05-29T20:50:07.590" UserId="9593" />
  <row Id="6294" PostId="5730" Score="1" Text="Suppose you find that some people exchanged emails a lot (say 100 email exchanges or above). Can these people be further divided into two groups according to certain features, such as the average time to reply an email (within three days or longer than that)? In addition to clustering, you can do supervised learning - search newspaper stories to find some people who cared about each other and other people who didn't care that much, then check the characteristics of emails from these two groups." CreationDate="2015-05-29T21:30:49.603" UserId="9593" />
  <row Id="6297" PostId="5958" Score="1" Text="Please note the language. Your question is totally unclear. What does &quot;the sample data have 1 observation per case and are not timed&quot; mean?" CreationDate="2015-05-30T13:31:24.033" UserId="8878" />
  <row Id="6298" PostId="5880" Score="0" Text="All right. Thanks again." CreationDate="2015-05-30T19:00:12.117" UserId="9771" />
  <row Id="6299" PostId="5959" Score="1" Text="Thanks for a great answer. My goal, in fact, is to do &quot;segmentation of time series&quot; for each vehicle in my data set." CreationDate="2015-05-30T20:03:43.590" UserId="4933" />
  <row Id="6300" PostId="5958" Score="1" Text="I was talking about the examples I found in online tutorials. The sample data they use have only 1 observation per case / individual (e.g. customer, country, etc). And those data are not time series." CreationDate="2015-05-30T20:07:32.847" UserId="4933" />
  <row Id="6301" PostId="5959" Score="1" Text="I am studying tutorials on time series decomposition. I found that there are ways to decompose them into trend, seasonal and cyclical components. My time series, however, are few seconds of vehicle trajectories. Is it possible to decompose them into different driving behavior components based on the trends in accelerations, speeds, lead vehicle speeds &amp; accelerations within an observed trajectory?" CreationDate="2015-05-30T20:38:09.290" UserId="4933" />
  <row Id="6302" PostId="5959" Score="2" Text="Maybe! for this better to consider both &quot;decomposition&quot; and &quot;segmentation&quot;. For instance if your time series show significant clusters in PC space you can relate them to driving behavior. Segmentation is also to detect different driving behaviors within a time series. The long story short is that you can use segmentation for different driving behavior segments for one vehicle and decomposition techniques for detecting global driving behaviors over all vehicles." CreationDate="2015-05-30T23:52:53.677" UserId="8878" />
  <row Id="6304" PostId="5963" Score="1" Text="btw - a million entries for 50 person sounds like you have many key card doors in this office or somehow the job requires that. Approx 25 entry/exit pairs per person per day for 365 days of year." CreationDate="2015-05-31T11:53:44.037" UserId="9901" />
  <row Id="6305" PostId="5845" Score="0" Text="Turns out that there are various methods for doing this, most of which rely on assumptions about w or the data. I think I've derived a really simple way of converting the problem into a k nearest neighbor search. I'll post about it after I've tested it and found out whether it's actually something well known." CreationDate="2015-05-31T23:37:00.517" UserId="1283" />
  <row Id="6306" PostId="5712" Score="0" Text="Here, check the documentation of function `glmnet` from `glmnet` package and look and parameter `standardize` [http://cran.r-project.org/web/packages/glmnet/glmnet.pdf](http://cran.r-project.org/web/packages/glmnet/glmnet.pdf) - there also is possibleto use regularization" CreationDate="2015-06-01T10:03:17.397" UserId="5224" />
  <row Id="6307" PostId="5966" Score="0" Text="I just read mean shift clustering , and I seem to like it...I will read more and let you know.. thanks a lot!" CreationDate="2015-06-01T11:45:46.800" UserId="8013" />
  <row Id="6308" PostId="5973" Score="0" Text="It seems that least squares estimator and stepwise regression may fail if the variables are almost collinear." CreationDate="2015-06-01T13:23:20.503" UserId="5091" />
  <row Id="6309" PostId="5973" Score="0" Text="You did not include a fundamental piece of information: how many observations do you have? :)" CreationDate="2015-06-01T16:37:38.663" UserId="9766" />
  <row Id="6310" PostId="5969" Score="0" Text="Are your categories being used as inputs to some other model, or are they the output?" CreationDate="2015-06-01T17:01:15.677" UserId="8041" />
  <row Id="6311" PostId="5947" Score="0" Text="Thank you for your reply!" CreationDate="2015-06-01T17:46:27.627" UserId="9724" />
  <row Id="6312" PostId="5982" Score="0" Text="Can you elaborate on your last paragraph (&quot;transfer learning&quot;)? Could you provide any links or names?" CreationDate="2015-06-01T20:49:31.190" UserId="6550" />
  <row Id="6314" PostId="5881" Score="0" Text="With clarity, this question could be made much better. Can you provide more details on what domain would you like to find interesting algorithms and from what time? Some of us here could be able to provide you with a detailed list of breakthrough results/ algorithms developed in a given era." CreationDate="2015-06-01T21:07:30.223" UserId="847" />
  <row Id="6315" PostId="5933" Score="0" Text="@Shagun This is an open ended question that is not suited for this format. What seems trivial to you, might be involved for someone else and high return generating (for a product, app or an organization). The answers to this question only encourage meandering discussions not suitable here (it also seems unlikely that you will ever accept an answer as its going to be hard to judge if this question has been appropriately answered). Please consider moving this question to another forum." CreationDate="2015-06-01T21:19:50.170" UserId="847" />
  <row Id="6316" PostId="5973" Score="1" Text="What is meant by &quot;unknown parameters&quot;? Is your question about how to choose the final set of features using which you would carry out &quot;ordinary least squares regression&quot;? Consider editing your question description with this information as well as updating the title of the question." CreationDate="2015-06-01T21:25:37.343" UserId="847" />
  <row Id="6317" PostId="5981" Score="0" Text="In the set up of your classification problem does the binary feature (the response/ label) that you are trying to predict has categories that roughly mean: &quot;feasible&quot; and &quot;not-feasible&quot;?" CreationDate="2015-06-01T21:49:39.333" UserId="847" />
  <row Id="6318" PostId="5982" Score="1" Text="In conventional machine learning, we have some data that comes from a particular probability distribution. Then we learn some kind of model on that data, hoping that the model will generalize to examples not seen during training. This will only work if these unseen samples come from the same probability distribution, so we assume this is the case. In transfer learning, we don't make that assumption. [Here's](http://www1.i2r.a-star.edu.sg/~jspan/publications/TLsurvey_0822.pdf) a survey paper on the field." CreationDate="2015-06-01T23:24:50.183" UserId="9483" />
  <row Id="6319" PostId="5933" Score="0" Text="@Nitesh You are right about accepting any of the answers as it will be difficult to judge. But as I stated in the question itself, I am not looking for a revenue generating product. I am just looking for what possible stuff can be done with a typical dataset like this. You are correct when you say this is open-ended but then I am not expecting people to explain all of their approach. If the answer mentions a relevant keyword or a pointer towards the answer, I would be upvoting it and would consider it a valid answer. I would move it to another forum if you feel the points are not valid :)" CreationDate="2015-06-02T04:10:01.683" UserId="75" />
  <row Id="6320" PostId="5947" Score="0" Text="You are welcome, Please provide more feedback. Did this way improve the result in your case? What did you get?" CreationDate="2015-06-02T04:15:26.867" UserId="9085" />
  <row Id="6321" PostId="5985" Score="1" Text="There is no &quot;best&quot; here because you haven't clearly defined your criteria, and only you can do that. Why isn't it simply the total number of SEP_11 computers? If a site has 100 SEP_11 why does it matter if it has 0 SEP_12 or 1000 SEP_12? Do you want the ratio of SEP_11/SEP_12? I'm not even sure this is Data Science..." CreationDate="2015-06-02T16:25:10.520" UserId="471" />
  <row Id="6322" PostId="3719" Score="0" Text="Since this was couple of months back, I assume you made some progress. Why not add your own answer (here or elsewhere?)" CreationDate="2015-06-02T16:59:31.707" UserId="9951" />
  <row Id="6323" PostId="5985" Score="0" Text="@Spacedman I wasn't sure if this was the best forum to post, I was looking more for Data Analysis forum .... I think ratio might be a solution ... thinking ......" CreationDate="2015-06-02T17:12:00.467" UserId="9947" />
  <row Id="6324" PostId="5933" Score="0" Text="Unfortunately, this question-answer format does not encourage idea generating discussions (for example, the possible things that can be done with this dataset) even though I personally like to indulge in such discussions :(" CreationDate="2015-06-02T18:42:07.767" UserId="847" />
  <row Id="6325" PostId="5881" Score="0" Text="@Nitesh I made some editing." CreationDate="2015-06-02T21:47:05.827" UserId="9799" />
  <row Id="6326" PostId="5973" Score="0" Text="Sorry- this should be a comment. With prediction we don't need worry about collinearity. Ideally, you should cut out the predictors that measure the same thing. For prediction I would say OLS will do the trick if your response is continuous. As far as what predictors to include in the model, well, you can put them all if you want but you run the risk of overfitting. But generally with prediction, the more predictors the better the prediction." CreationDate="2015-06-01T14:51:56.563" UserId="9698" />
  <row Id="6328" PostId="5981" Score="0" Text="Yes,  indeed, those are exactly the categories I want to &quot;predict&quot;.  I've been furthering my research (will edit OP later): I want to extract rules on the feature space (&quot;minimum&quot; complexity linear combination of features values is enough). Is there something like a tree of linear combinations ?" CreationDate="2015-06-02T23:00:26.640" UserId="9928" />
  <row Id="6329" PostId="1165" Score="0" Text="Hello @EmilyCrutcher. I apologize for the delay, it has been ages since I didn't check this post. I am very excited to start working with you in this project !" CreationDate="2015-06-03T02:47:00.543" UserId="3433" />
  <row Id="6330" PostId="3723" Score="0" Text="@AleksandrBlekh No one up voted my answer :( But your answer is very satisfying. I am going to up vote it!" CreationDate="2015-06-03T03:05:35.083" UserId="847" />
  <row Id="6331" PostId="3723" Score="0" Text="@Nitesh: Thank you for kind words and upvoting. Don't worry too much about your particular answer - it is not bad, just rather limited in scope. Nevertheless, I'll upvote it for the content and to cheer you up a bit :-)." CreationDate="2015-06-03T03:45:13.970" UserId="2452" />
  <row Id="6332" PostId="5990" Score="0" Text="Excellent first question! Can you add some more information about what is your goal to carry out this specific feature transformation? Do you intend to use this transformed feature as an input to a supervised learning problem? If so, please consider adding that information as it may help others answer this question better." CreationDate="2015-06-03T06:52:48.670" UserId="847" />
  <row Id="6333" PostId="5990" Score="1" Text="@Nitesh, Please see update" CreationDate="2015-06-03T06:57:42.743" UserId="8338" />
  <row Id="6334" PostId="5988" Score="0" Text="`R`? This is new to me, seems interesting!" CreationDate="2015-06-03T12:26:03.407" UserId="9947" />
  <row Id="6335" PostId="5933" Score="0" Text="Hmm I get your point. Should I delete it or let it be closed?" CreationDate="2015-06-03T14:01:06.490" UserId="75" />
  <row Id="6336" PostId="5988" Score="0" Text="@SohniMahiwal If you like the answer, please consider up voting/ accepting it. If you'd like, I could send you the script to produce this image." CreationDate="2015-06-03T15:30:09.600" UserId="847" />
  <row Id="6337" PostId="5991" Score="1" Text="I believe, asking here http://opendata.stackexchange.com/ is better. Users on Open Data SE are more familiar with open data sets." CreationDate="2015-06-03T15:37:33.983" UserId="201" />
  <row Id="6338" PostId="5988" Score="0" Text="I need 15 reputation or more to vote up" CreationDate="2015-06-03T15:51:04.437" UserId="9947" />
  <row Id="6339" PostId="5947" Score="0" Text="There are still the same words shown in different topics. Each word only presents once in each document, but may present in different documents. I updated a new graph of the frequency of the words. The few words with the highest frequency each take only 14% of the documents.  I also wonder how to deal with words I removed. Could they be considered as features or too common to exist?" CreationDate="2015-06-03T18:45:27.710" UserId="9724" />
  <row Id="6340" PostId="5990" Score="0" Text="You can find answers here: http://datascience.stackexchange.com/questions/4967/quasi-categorical-variables-any-ideas" CreationDate="2015-06-04T02:13:34.443" UserId="609" />
  <row Id="6341" PostId="5991" Score="0" Text="Thanks a lot @AnastasiosVentouris. Moving my question there." CreationDate="2015-06-04T08:32:14.090" UserId="9958" />
  <row Id="6342" PostId="5984" Score="0" Text="For regression model, if node impurity is variance, what will be node purity?" CreationDate="2015-06-04T16:36:42.223" UserId="9941" />
  <row Id="6343" PostId="6005" Score="1" Text="Welcome to the community :) As there are &quot;r&quot; and &quot;data visualization&quot; tags this question is not irrelevant but maybe stackoverflow is a better place to get a helpful answer" CreationDate="2015-06-04T19:38:04.417" UserId="8878" />
  <row Id="6344" PostId="6007" Score="0" Text="What is cls? What does the acronym and values represent?" CreationDate="2015-06-04T19:55:28.373" UserId="9947" />
  <row Id="6345" PostId="6005" Score="0" Text="@kasramsh: Okay, Thanks!" CreationDate="2015-06-04T20:14:12.180" UserId="9982" />
  <row Id="6346" PostId="6005" Score="0" Text="I found one solution to the problem here: &lt;br/&gt; http://stackoverflow.com/questions/19731187/r-plotting-multiple-groups-of-data-in-a-single-3d-plot &lt;br/&gt; which requires creating a data frame before doing this. I was wondering if there is a simpler, quicker way of doing it." CreationDate="2015-06-04T20:25:06.987" UserId="9982" />
  <row Id="6347" PostId="6008" Score="0" Text="@kylethecreator, yes that is what I exactly need to do - build a classifier based on the data that I have so that it can categorize questions.  [Here](https://drive.google.com/file/d/0B8NiXBOrlRu5cUhscUwxa0tkeHc/view?usp=sharing) is data set I had created manually by categorizing 1000 questions in one of the 10 categories. Please suggest the future course of action." CreationDate="2015-06-05T04:12:48.463" UserId="9966" />
  <row Id="6349" PostId="6014" Score="0" Text="Sorry: ***STANFORD NLP" CreationDate="2015-06-05T05:20:36.170" UserId="9988" />
  <row Id="6351" PostId="6019" Score="0" Text="prior? likelihood?" CreationDate="2015-06-05T12:01:15.473" UserId="9123" />
  <row Id="6352" PostId="5986" Score="0" Text="Sorry for this. My mistake. I expected it to be a comment. But after all, I decided to describe my test with LDA." CreationDate="2015-06-05T12:49:49.887" UserId="9314" />
  <row Id="6353" PostId="6023" Score="0" Text="Neural Networks was one of my options, since I have made a few other projects with them. However, I wanted to extract a formula which can be sent to a third party to use it for predictions based on the input. Since I find the prediction model I will not be able to run it every time to find the probabilities for A, B and C." CreationDate="2015-06-05T15:29:11.960" UserId="201" />
  <row Id="6354" PostId="6023" Score="2" Text="I think at the best case you can provide a third party by probabilities but not a model. If internal functions are random what would a model mean? Maybe better to run many experiment and try to fit a distribution to your empirical distribution (using maximum likelihood for instance). Then provide the third party with that distributions." CreationDate="2015-06-05T15:41:26.837" UserId="8878" />
  <row Id="6355" PostId="6007" Score="0" Text="`cls` is the column with the [climate zones](http://en.wikipedia.org/wiki/K%C3%B6ppen_climate_classification)." CreationDate="2015-06-05T16:08:54.853" UserId="178" />
  <row Id="6356" PostId="6007" Score="0" Text="Now I understand, thanks!" CreationDate="2015-06-05T16:20:22.497" UserId="9947" />
  <row Id="6357" PostId="6019" Score="1" Text="Is this not more of a discussion question than a question with an actual answer?" CreationDate="2015-06-05T19:52:57.123" UserId="8152" />
  <row Id="6358" PostId="6019" Score="0" Text="One cannot do science without a clear definition of terms, so I would be very surprised if there is no official, clear definition of this kind of knowledge" CreationDate="2015-06-05T23:35:55.840" UserId="9960" />
  <row Id="6359" PostId="5986" Score="0" Text="Thank you for your detailed reply. I am going to try TF-IDF then!  Look forward to your update." CreationDate="2015-06-05T23:46:22.013" UserId="9724" />
  <row Id="6361" PostId="5730" Score="0" Text="Thanks @hostjc, those are good ideas. I think you're right that I need to think in terms of features that can be accessed by the clusterer, time to respond is a perfect example of a non-obvious feature that would have a lot of very interesting applications." CreationDate="2015-06-06T09:04:08.643" UserId="9534" />
  <row Id="6362" PostId="6014" Score="0" Text="Hmm, thanks! I've been working with Epic, a tool written in Scala from http://www.scalanlp.org...." CreationDate="2015-06-06T09:06:03.843" UserId="9534" />
  <row Id="6363" PostId="6027" Score="0" Text="Sounds like you have some code, data and visualizations already. Can you share some of them? It would be easier to help then." CreationDate="2015-06-06T11:54:30.327" UserId="7720" />
  <row Id="6364" PostId="6017" Score="0" Text="Is it valid to make such assumptions? From my training set, the distribution is definitely not equal as in your assumption (btw this variable is a response from survey, so a lot of subjectivity would come into the answers). So does it mean I should use the distribution of the training set to create a weighted distance measure?" CreationDate="2015-06-06T15:38:52.847" UserId="1133" />
  <row Id="6365" PostId="6027" Score="3" Text="How about [Dimensionality Reduction with Spherical Constraints](http://www.cs.utah.edu/~suresh/6160/spring09/projects/final/arvind.pdf) or [Spherical Laplacian Information Maps for dimensionality reduction](http://web.eecs.umich.edu/~hero/Preprints/carter_ssp09.pdf)?" CreationDate="2015-06-07T01:09:00.290" UserId="381" />
  <row Id="6367" PostId="3719" Score="0" Text="Hi @Jayan as per our use-case we were initially thinking of storing entire data on Neo4j, but finally we choose MongoDB as the central database and stick with Neo4j  for analyzing relationships only (not as central DB). And also we are exploring with Spark and Graphlab" CreationDate="2015-06-07T11:39:32.940" UserId="5091" />
  <row Id="6369" PostId="5865" Score="0" Text="It is not just that squaring the errors has the effect of treating positive and negative errors equally, because abs( ) would have the same effect. ***Squaring the errors puts more weight on large errors***, compared to linear or log functions. Therefore, minimizing sum-of-squares error serves this decision criteria: &quot;Favor the model that has the least aggregate large errors.&quot;" CreationDate="2015-06-07T20:13:51.580" UserId="609" />
  <row Id="6370" PostId="5882" Score="0" Text="I don't believe that *computational expense* is the main reason why square root is not taken over sum-of-squared errors.  (I don't have any evidence one way or the other.)  After all, you only need to perform the square root operation once for all error values.  Instead, I believe that sum-of-squared errors serves a different decision criterion. ***Squaring the errors puts more weight on large errors***, compared to linear or log functions. Therefore, minimizing sum-of-squares error serves this decision criterion: &quot;Favor the model that has the *least aggregate large errors*.&quot;" CreationDate="2015-06-07T20:28:45.447" UserId="609" />
  <row Id="6373" PostId="6029" Score="0" Text="In most recent benchmarks, PostgreSQL totally owned MongoDB..." CreationDate="2015-06-07T21:38:16.317" UserId="924" />
  <row Id="6374" PostId="6027" Score="0" Text="@Emre The first one looks interesting, I think I will just code up t-sne with the norm constraint and geodesic distance.." CreationDate="2015-06-07T22:44:48.530" UserId="10001" />
  <row Id="6375" PostId="5865" Score="0" Text="Yes, using squared errors puts greater emphasis on large errors but `abs()` has other issues, like the fact that its derivative is discontinuous at 0. And regarding your stated decision criterion, wouldn't that be even better met by $E^4$ or $E^6$? I don't have a citation but I suspect the real reason squared errors are used is related to the quote in the original question: errors *tend* to be Gaussian and minimizing the squared errors provides the maximum likelihood estimate in that case. They also have the benefit of being easily differentiable, which makes them simple to use in practice." CreationDate="2015-06-07T23:14:20.793" UserId="964" />
  <row Id="6376" PostId="5865" Score="0" Text="Good points.  Yes, abs( ) is discontinuous at 0, but I'm not sure what difference that makes in empirical analysis.  Yes, higher ordered error functions would weight large errors even more, and would support the decision criterion mentioned.  If minimizing squared errors leads to max. likelihood estimate (assuming Gaussian), then I agree with that line of reasoning." CreationDate="2015-06-08T01:28:45.933" UserId="609" />
  <row Id="6377" PostId="6015" Score="1" Text="Check [this][1] out. The explanation provided by the guy is very lucid. &#xA;&#xA;&#xA;  [1]: http://stats.stackexchange.com/questions/59124/random-forest-assumptions" CreationDate="2015-06-08T04:10:10.900" UserId="9966" />
  <row Id="6378" PostId="6043" Score="0" Text="please can we talk private ? i have some questions about what you said, can i give you my email please?" CreationDate="2015-06-08T14:34:12.307" UserId="10036" />
  <row Id="6379" PostId="6043" Score="1" Text="to be clear, I do have other data sets like a transation and items. but i thought that is for the next question, the first question is to analysi the client profile," CreationDate="2015-06-08T14:35:03.310" UserId="10036" />
  <row Id="6381" PostId="6043" Score="1" Text="if you have more questions please comment here and I'll modify my answer so that others also can use it. This is the philosophy behind this website :)" CreationDate="2015-06-08T15:20:10.183" UserId="8878" />
  <row Id="6383" PostId="6043" Score="0" Text="Okay, I have updated my question containing a description of the datasets. Could you help me please? Thanks in advance" CreationDate="2015-06-08T16:52:08.327" UserId="10036" />
  <row Id="6384" PostId="6014" Score="0" Text="What language do you use?" CreationDate="2015-06-08T19:44:58.630" UserId="9988" />
  <row Id="6385" PostId="6014" Score="0" Text="I'm using Scala, but I didn't want to make that a predicate on the high level technique that would be good to pursue on a project like this." CreationDate="2015-06-09T06:01:31.427" UserId="9534" />
  <row Id="6386" PostId="6015" Score="0" Text="Thanks, I have got my answers. and seems like my question is a repeated one." CreationDate="2015-06-09T06:29:26.423" UserId="9941" />
  <row Id="6387" PostId="6017" Score="0" Text="Of course. If it's really skewed, I would pick different margins, just make sure they are empirically grounded. Mine were example margins :)" CreationDate="2015-06-09T09:14:21.913" UserId="8152" />
  <row Id="6388" PostId="5882" Score="0" Text="if we are only looking to minimize large aggregate errors, a cubic error is better than a squared error is it not. I know I read it somewhere mentioning the reason as 'optimization technique' but can't recall where" CreationDate="2015-06-09T13:47:44.827" UserId="9801" />
  <row Id="6390" PostId="6051" Score="0" Text="Define AI first." CreationDate="2015-06-09T14:51:48.817" UserId="5316" />
  <row Id="6392" PostId="2651" Score="3" Text="Why is this upvoted. It doesnt show any effort whatsoever and its a duplicate of a dupe" CreationDate="2015-06-09T14:56:12.073" UserId="5316" />
  <row Id="6393" PostId="6056" Score="1" Text="No. the point is that there are terms like &quot;str1 str2&quot; and &quot;str1str2&quot;. When I want to draw histogram of course I need to count the occurrences of each term. So I need to filter the data so that all ambiguous versions of a specific term become the same. Then I start counting." CreationDate="2015-06-09T16:31:59.813" UserId="8878" />
  <row Id="6394" PostId="6054" Score="1" Text="An answer to a similar question (for neural networks) is [here](http://stackoverflow.com/a/10357067/1361822)." CreationDate="2015-06-09T16:39:24.793" UserId="964" />
  <row Id="6395" PostId="6048" Score="0" Text="Can you add more details like the number of rows, number of columns (also how many categorical/ continuous)?" CreationDate="2015-06-09T17:21:02.783" UserId="847" />
  <row Id="6396" PostId="6059" Score="0" Text="Exactly @Victor." CreationDate="2015-06-09T17:22:28.520" UserId="9966" />
  <row Id="6397" PostId="5986" Score="0" Text="I tried TF-IDF and it did work better. Thanks for sharing again. but do u know how to deal with the terms that you removed based on TF-IDF? Are those not the important information for your recommendation system?" CreationDate="2015-06-09T20:46:21.467" UserId="9724" />
  <row Id="6398" PostId="6050" Score="0" Text="Sure: _hashing_ comes to mind." CreationDate="2015-06-10T03:22:54.740" UserId="2452" />
  <row Id="6399" PostId="6050" Score="0" Text="Could you please explain a bit more that how hashing would help me here? Do we make a hash of k-vector?" CreationDate="2015-06-10T07:00:43.013" UserId="8338" />
  <row Id="6400" PostId="6060" Score="0" Text="So we will still use k-vector but this time k is the number of groups and hence less than number of unique users?" CreationDate="2015-06-10T07:01:43.967" UserId="8338" />
  <row Id="6401" PostId="6050" Score="0" Text="What I meant is that [hashing](http://en.wikipedia.org/wiki/Hash_function) allows to _compress_ data of arbitrary size to a smaller data set. However, the side effect of that is the presence of _collisions_. I would research potential options of dealing with collisions, in general and in particular context of your data types, so that you can see, if this approach is _feasible_ (I'm not stating that). Hope this helps." CreationDate="2015-06-10T07:25:04.587" UserId="2452" />
  <row Id="6402" PostId="6050" Score="0" Text="Hmm... thanks I would think about it!" CreationDate="2015-06-10T07:36:01.260" UserId="8338" />
  <row Id="6403" PostId="6064" Score="5" Text="Drug? Is milk a drug where you live?" CreationDate="2015-06-10T09:53:39.213" UserId="471" />
  <row Id="6404" PostId="6060" Score="1" Text="This is a little redundant though.. the information you add from this clustering is just the info contained in the other features, which are already inputs." CreationDate="2015-06-10T11:42:37.627" UserId="21" />
  <row Id="6405" PostId="6050" Score="1" Text="I don't think you literally mean more compact, like, taking fewer bytes right? but instead taking on fewer than 30K distinct values? hashing won't help unless there are lots of collisions then. Although this is a well-known technique for very high-dimensional data I'm not sure it's appropriate here." CreationDate="2015-06-10T11:44:11.767" UserId="21" />
  <row Id="6406" PostId="6050" Score="0" Text="Also are you sure you want to predict based on user ID? you normally predict based on qualities of a person, not their actual identity." CreationDate="2015-06-10T11:44:31.247" UserId="21" />
  <row Id="6408" PostId="6050" Score="0" Text="@SeanOwen You are right, but my problem is little different. There are huge number of operations are done on the users. I have to predict the future performance of these actions per user basis." CreationDate="2015-06-10T12:56:13.683" UserId="8338" />
  <row Id="6409" PostId="6050" Score="0" Text="I think I got your point. These performance totally depend on userIds but in real I could replace these ids with some properties which contribute to the performance impact and make the model agnostic to the user. But the problem here is that sometimes we have limited domain knowledge, so we club all this feature under a group and give them id(say user id)." CreationDate="2015-06-10T13:17:18.703" UserId="8338" />
  <row Id="6410" PostId="6008" Score="0" Text="@kylethecreator: Are you still on this?" CreationDate="2015-06-10T14:58:47.137" UserId="9966" />
  <row Id="6411" PostId="6008" Score="1" Text="@untitledprogrammer: you could definitely apply the methods I described in my answer toward this dataset. Since you have full-text questions in each cell you could tokenize each of these and use them as unigram features for classifiers. Tokenization is fairly straightforward, you could just split on whitespace, or use a more specialized library depending on your programming language. I'm biased toward Python, so the NLTK library is a natural choice for many of these ideas." CreationDate="2015-06-10T15:36:01.220" UserId="4897" />
  <row Id="6412" PostId="6008" Score="0" Text="Thanks for the clarification." CreationDate="2015-06-10T15:43:22.440" UserId="9966" />
  <row Id="6413" PostId="6064" Score="0" Text="Sorry for typo. My real data has Drug name Brand 1, Drug name-Brand 2 etc (Example: Tylenol is brand 1 and Ibrofen is brand 2)...I corrected original post." CreationDate="2015-06-10T16:24:50.287" UserId="9663" />
  <row Id="6414" PostId="6061" Score="1" Text="+1 Will! I'll look at it deeper but a quick look told me it probably doesnt solve my problem. I mean what if &quot;big data&quot; does not happen too often? But thanks after all. Probably a good starting point!" CreationDate="2015-06-10T16:50:05.627" UserId="8878" />
  <row Id="6415" PostId="6067" Score="0" Text="I hope this helps you out with your text. I have worked with text data and I know the pain. Numeric and or factual data is much better to be mined." CreationDate="2015-06-10T17:02:35.200" UserId="9966" />
  <row Id="6416" PostId="6048" Score="0" Text="Hi @Nitesh, I have 32 input variables + 1 target variable. Records are close to 2.5 lakh for training data and say around 1 lakh testing data. Testing data is out of time data." CreationDate="2015-06-10T18:36:26.770" UserId="9793" />
  <row Id="6417" PostId="6058" Score="0" Text="Thanks for the response" CreationDate="2015-06-10T18:39:50.473" UserId="9793" />
  <row Id="6418" PostId="6068" Score="0" Text="Don't be intimidated by the position of the authors of the papers.  The best resources are usually written by people whose goal is to make the topic understandable. The most important thing is to keep at it until you get it and reach out in forums like this when you need help." CreationDate="2015-06-10T18:42:22.807" UserId="178" />
  <row Id="6419" PostId="6059" Score="0" Text="@Victor Thanks a lot for a very detailed explaination." CreationDate="2015-06-10T18:42:52.540" UserId="9793" />
  <row Id="6420" PostId="6054" Score="0" Text="@bogatron - I agree with you completely. But my ones just a **SVM specific** answer." CreationDate="2015-06-10T19:43:54.307" UserId="9966" />
  <row Id="6421" PostId="6054" Score="1" Text="Except it isn't. Your answer is correct but there is nothing about it that is specific to SVMs (nor should there be). $w^{T}x=b$ is simply a vector equation that defines a hyperplane." CreationDate="2015-06-10T22:01:22.693" UserId="964" />
  <row Id="6422" PostId="6061" Score="1" Text="It might be worth trying. It should detect at least the most common examples, and the others may not make a big difference. Also, are all of the examples you are looking for misspellings, like &quot;bigdata&quot;? In that case, you could compare your documents to a list of known words, and then compare them to the list of pairs of adjacent words in the text." CreationDate="2015-06-11T01:44:15.173" UserId="8275" />
  <row Id="6423" PostId="6073" Score="0" Text="Thanks. I belatedly found out also that the answer to the portion &quot;but I have no clue how to view the rpart2 tree.&quot; is to view the `finalModel` element in the output of `rpart2` trained with `caret`." CreationDate="2015-06-11T07:21:15.543" UserId="1133" />
  <row Id="6425" PostId="6074" Score="0" Text="Even with random they will be qualitative. Numbers would be just labels...!" CreationDate="2015-06-11T08:10:42.060" UserId="8338" />
  <row Id="6426" PostId="6067" Score="0" Text="Thanks alot. It helped. :)" CreationDate="2015-06-11T10:10:33.153" UserId="6514" />
  <row Id="6427" PostId="6067" Score="0" Text="Good to know that!" CreationDate="2015-06-11T13:09:20.147" UserId="9966" />
  <row Id="6428" PostId="6061" Score="1" Text="I actually have a huge list of human generated tags and I want to make a ground-truth for them. Some people enter &quot;big data&quot; some enter &quot;bigdata&quot; and currently in my ground-truth they are two different tags but they should be identical." CreationDate="2015-06-11T15:03:25.410" UserId="8878" />
  <row Id="6429" PostId="6061" Score="1" Text="So these are not actual sentences, just phrases without context? Another option you could try would be to use an edit distance/Levenshtein distance to group similar tags: http://en.wikipedia.org/wiki/Levenshtein_distance" CreationDate="2015-06-11T17:01:49.793" UserId="8275" />
  <row Id="6430" PostId="6083" Score="0" Text="Good but How can I make software column?" CreationDate="2015-06-11T17:11:43.873" UserId="3151" />
  <row Id="6432" PostId="6083" Score="0" Text="That is something you are going to have to research yourself. I am not sure what your data set looks like-- the melt function may be of use to you." CreationDate="2015-06-11T17:27:11.387" UserId="9698" />
  <row Id="6433" PostId="6061" Score="1" Text="I was thinking about Levenshtein distance at the beginning but I thought maybe someone here has a better solution. Thanks for answers anyway and I'll keep this question open for a few more days to see if someone has a better idea :)" CreationDate="2015-06-11T17:44:50.090" UserId="8878" />
  <row Id="6434" PostId="6074" Score="0" Text="Obviously they are unordered random numbers and are still categorical data; however, decision trees don't have a linearity presumption. This means that the splits will sort out the different effects of each category. Imagine only one category is different from the rest. A decision tree will take only two splits to identify this category. If there are 30,000 categories, very likely many of them will have similar effects. A decision tree takes advantage of this when partitioning the data." CreationDate="2015-06-11T20:14:00.517" UserId="10086" />
  <row Id="6435" PostId="6084" Score="0" Text="Can you give an example of a dataset that you are trying to visualize? Currently, your question is vague. Providing an example dataset and a corresponding plot you would like to see would help. Also, providing external links (specifically from transient websites like twitter) is discouraged, so try describing it as best as you can in the question itself." CreationDate="2015-06-11T21:05:29.050" UserId="847" />
  <row Id="6437" PostId="5351" Score="1" Text="I found this plugin for R in IntelliJ: https://plugins.jetbrains.com/plugin/6632?pr= ." CreationDate="2015-06-11T23:26:21.860" UserId="5279" />
  <row Id="6438" PostId="6083" Score="0" Text="@LaurenGoodwin My answer to this question uses gridExtra to plot a barplot and boxplot side by side to produce the visualization requested." CreationDate="2015-06-11T23:52:13.277" UserId="847" />
  <row Id="6440" PostId="5351" Score="0" Text="@Anton: Thanks for the information. Either that plug-in info wasn't published as of time of my post, or (more likely) I have simply missed it. However, in general, I would definitely prefer a manufacturer's embedded support, especially, considering the prominence of R in academia, science and industry." CreationDate="2015-06-12T00:48:37.123" UserId="2452" />
  <row Id="6444" PostId="6090" Score="0" Text="My description is not very clear, but you had already got my mind. Really appreciate !" CreationDate="2015-06-12T06:43:46.130" UserId="9728" />
  <row Id="6446" PostId="6070" Score="0" Text="Check [my answer](http://stats.stackexchange.com/a/131284/31372) on _DTW clustering_ of time series." CreationDate="2015-06-12T07:59:24.580" UserId="2452" />
  <row Id="6447" PostId="6070" Score="0" Text="Basically, I have a time series of time series. Let's assume that I use at time $t$ a DTW (but I would rather use $\phi = \arccos \langle p,q \rangle$), how to extend it to the whole time series? This is really my point." CreationDate="2015-06-12T08:36:28.710" UserId="7966" />
  <row Id="6448" PostId="6017" Score="0" Text="what you choose as the margin may even depend on what errors matter more and which matter less as per your requirements. eg say we have an item X for which if label is B or BV, we would be discarding it else keeping it. So in this case the distinction between B and BV is less important then distinction between G and B. So your mismatch score between B and BV should be less than that between B and G" CreationDate="2015-06-12T08:39:12.560" UserId="75" />
  <row Id="6449" PostId="6070" Score="0" Text="You're welcome. My advice does not imply that I think that TDW is universal solution. I just thought that papers, referenced in my linked answer, _potentially_ might contain some _ideas_, useful to your case. I don't have an answer for your &quot;time series of time series&quot; case. As for analyzing distortions, you could consider applying time series _anomaly detection and analysis_ approaches." CreationDate="2015-06-12T09:05:50.133" UserId="2452" />
  <row Id="6450" PostId="6094" Score="0" Text="Thanks for the help. This is some kind of online _online learning_ as I expected, but I am really not clear about the implementation part. Could you throw in some hints for the same. Say I have a model and I have 5 parameters, and corresponding 5 weights, for example the model be f = (0.1*a + 0.8*b + 0.4*c + 0.2*d + 0.5*e) and I use this to recommend initially. Now how do I update the model using the implicit feedback?" CreationDate="2015-06-12T12:17:27.403" UserId="10127" />
  <row Id="6451" PostId="6094" Score="0" Text="If you are planning to apply online learning to update the weights, then you should take a look to the Stochastic Gradient Descent entry in Wikipedia. It includes a very simple example, similar to the one in your comment." CreationDate="2015-06-12T15:49:35.953" UserId="2576" />
  <row Id="6452" PostId="6101" Score="1" Text="I am going to edit the title to be more descriptive. Good question, though." CreationDate="2015-06-12T19:48:17.940" UserId="3466" />
  <row Id="6453" PostId="6100" Score="0" Text="Few clarifications: Are you having problems with the stems that are generated, just asking if their is a lemmatisation module that handles Hungarian, or are you asking which algorithm you should use to classify and build a wordcloud after stem creation? Or am I completely missing your question?" CreationDate="2015-06-12T21:25:01.447" UserId="10019" />
  <row Id="6455" PostId="6100" Score="0" Text="Stems are good to identify different forms of the same word.But stems are not actual words usually just word chunks(at least in Hungarian).  I want to somehow translate stems back to real words that I can use later in my wordcloud. Lemmatisation would be nice,it would handle the problem, but I did not find a Hungarian lemmatisation module (and because of the complexity of the Hungarian language I think it is very difficult to create one). So I want a solution to replace the stems with one of the real word occurrences in the text. Or any other workflow is welcome that would solve my problem." CreationDate="2015-06-12T22:56:32.640" UserId="5211" />
  <row Id="6456" PostId="2464" Score="0" Text="You don't have to represent each word as a vector. You get the new representation for the entire *document* by applying the LDA transformation you learned *to the corpus*. For an example with LSI, see this link: http://radimrehurek.com/gensim/tut2.html The key part is where they apply the learned LSI transformation to the entire corpus with lsi[doc_bow]" CreationDate="2015-06-13T02:29:44.743" UserId="8275" />
  <row Id="6457" PostId="6070" Score="0" Text="Have you tried mutual information?" CreationDate="2015-06-12T06:05:08.283" UserId="10094" />
  <row Id="6458" PostId="6070" Score="0" Text="For measuring dependency between variables, I prefer using copulae, [though mutual information and copula are very much the same]{http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6077935}! Yet, in my case, dependency is not the only information I care about / in this kind of time series. In fact, I wish I could obtain [a result similar to this one]{http://arxiv.org/pdf/1506.00976v1.pdf}." CreationDate="2015-06-12T06:33:56.677" UserId="7966" />
  <row Id="6459" PostId="6098" Score="0" Text="You haven't specified what you are doing or what the problem is." CreationDate="2015-06-13T07:21:18.423" UserId="21" />
  <row Id="6460" PostId="6092" Score="0" Text="What is your problem? do you want to validate your predicted ratings? then you need rating data." CreationDate="2015-06-13T07:22:17.023" UserId="21" />
  <row Id="6462" PostId="6059" Score="0" Text="+1 for this detailed answer. Arun you might want to accept one of the answers." CreationDate="2015-06-13T08:35:03.013" UserId="9123" />
  <row Id="6463" PostId="6092" Score="0" Text="Validation is one problem. Here I am more concerned about building the model itself. I am not very comfortable with unsupervised models." CreationDate="2015-06-13T13:29:26.020" UserId="10127" />
  <row Id="6465" PostId="2432" Score="0" Text="MapReduce is not really a technology for joining. Do you mean Hive? Impala is the closest analog to Redshift and things like Teradata" CreationDate="2015-06-14T09:39:43.943" UserId="21" />
  <row Id="6466" PostId="2432" Score="0" Text="consider [monetdb](http://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html)" CreationDate="2015-06-14T12:44:33.113" UserId="9924" />
  <row Id="6468" PostId="6113" Score="1" Text="The quality of question should be improved to get a proper answer e.g. do all sequences always change at the same point (like how you illustrated in the example)?" CreationDate="2015-06-14T17:42:03.373" UserId="8878" />
  <row Id="6469" PostId="6113" Score="0" Text="My real data is more complicated. It is list of 9-dimension vectors. I will add picture to the main section." CreationDate="2015-06-14T21:30:31.857" UserId="9637" />
  <row Id="6470" PostId="6117" Score="0" Text="Thanks for detailed response. As you can see above, I can not use thresholds for my real data sequence, I think, it is too complicated for it. I am trying to modify k-means algorithm, it will consider the condition of sequence (element can only belong to one of two neighboring clusters). I hope, that I do not reinvent the wheel. :)" CreationDate="2015-06-14T21:42:53.727" UserId="9637" />
  <row Id="6471" PostId="6117" Score="1" Text="I think your data is not so noisy (i.e. complicated) and you can go for threshold stuff. the point is that you have an impression of the data so u can use kinda supervised algorithm i.e. trying to learn thresholds (and hope it generalizes well!) . I also update my answer for a nice solution :)" CreationDate="2015-06-14T22:13:58.960" UserId="8878" />
  <row Id="6472" PostId="6084" Score="1" Text="Excel is the best (visually most beautiful one)! you can find implementations in python or other languages but they are not as great as excel. I tried a month ago!" CreationDate="2015-06-15T01:43:38.460" UserId="8878" />
  <row Id="6473" PostId="6118" Score="0" Text="There's another old question about this same error with lots of other possibilities, in case the Encoding trick doesn't help: http://stackoverflow.com/questions/9637278/r-tm-package-invalid-input-in-utf8towcs" CreationDate="2015-06-15T03:49:48.297" UserId="8275" />
  <row Id="6474" PostId="6116" Score="1" Text="Nice question! But should be a community wiki I suppose" CreationDate="2015-06-15T12:00:39.540" UserId="816" />
  <row Id="6475" PostId="6124" Score="0" Text="In the scikit learn tutorial, to implement this method, they have used the two np.arrays with one having a 2d list and one with 1d list. I tried to replicate the pattern without using numpy and got the error mentioned above. Don't know how else i should approach the implementation" CreationDate="2015-06-15T17:51:48.220" UserId="5043" />
  <row Id="6476" PostId="6124" Score="1" Text="When it says, &quot;Unknown label type&quot; it looks like your &quot;y&quot; is actually a numeric array, not an array of labels. If you are trying to predict &quot;ph&quot; from &quot;fixed acid&quot;, you should use a Regressor, not a Classifier." CreationDate="2015-06-15T18:13:08.803" UserId="8275" />
  <row Id="6477" PostId="6124" Score="0" Text="That's what I thought. I am not 100% clear about what the sgdclassifier example at scikit learn website means. They have 2 lists and using the model and fit to get some output. Could you please explain me what can i do with this data set to work with the sgdclassifier ?" CreationDate="2015-06-15T18:27:19.853" UserId="5043" />
  <row Id="6478" PostId="6124" Score="1" Text="Well, if you really want to use sgd*classifier*, you could try using &quot;quality&quot; as the &quot;y&quot; variable. But that might not be too appropriate, because I assume that quality is really an ordered variable. Why don't you try [SGDRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)?" CreationDate="2015-06-15T18:30:16.023" UserId="8275" />
  <row Id="6479" PostId="6124" Score="0" Text="I have to use both SGDClassifier and SGDRegressor, with this data set, for my project. Not sure what I can do with this dataset to implement SGDClassifier model." CreationDate="2015-06-15T18:32:25.413" UserId="5043" />
  <row Id="6480" PostId="6124" Score="0" Text="Could you tell me a good place to find a tutorial that explains these models properly. I am finding the concept hard to digest." CreationDate="2015-06-15T18:34:30.193" UserId="5043" />
  <row Id="6481" PostId="6124" Score="0" Text="So did you have some trouble understanding the scikit tutorial? I don't know right off the top of my head about a tutorial that is so much better than that one. Did you do the tutorial from the [beginning](http://scikit-learn.org/stable/tutorial/basic/tutorial.html) of the Quick Start? If I were you, I would use the SGDClassifier to predict &quot;quality&quot; and SGDRegressor to predict something else." CreationDate="2015-06-15T18:39:07.217" UserId="8275" />
  <row Id="6483" PostId="6124" Score="0" Text="okay. I did not read the tutorial from the beginning. Probably, should do that now. Thanks for your help Will" CreationDate="2015-06-15T18:48:30.820" UserId="5043" />
  <row Id="6484" PostId="6117" Score="0" Text="Thank you for the interesting links, I think, it can be used for my purpose, but for now, I use k-means with my modifications, which gives me acceptable results (pic. in question)." CreationDate="2015-06-15T20:28:55.453" UserId="9637" />
  <row Id="6485" PostId="6117" Score="1" Text="very nice results! smart move. I'm proud of you :D Good Luck!" CreationDate="2015-06-15T21:58:00.217" UserId="8878" />
  <row Id="6487" PostId="6126" Score="0" Text="My view is that data science is dominated by machine learning and computer scientists and that statisticians take a back seat. One index of this in the States is that the Amer Stats Assoc (ASA) has several blogs on its website about concerns that statistical grant requests are being underfunded, esp relative to CS grants. Not to mention that network analysis is a big thing in data science and obtaining deep, good experience in that would make your instantly employable." CreationDate="2015-06-16T11:03:33.170" UserId="9974" />
  <row Id="6488" PostId="6126" Score="0" Text="I don't want to start a religion war on this subject, but in my opinion machine learning IS statistics in a broader sense: it is statistical learning. Then, of course a statistician who wants to become a data scientist needs to develop some CS skills." CreationDate="2015-06-16T11:07:02.323" UserId="9766" />
  <row Id="6489" PostId="6126" Score="0" Text="There you go! You've confirmed my point. But I do agree that opinions can run high on this issue. Again, by pointing to the ASA's own statements, one can get a sense of what is really happening out there and which side is winning the war right now." CreationDate="2015-06-16T11:42:09.353" UserId="9974" />
  <row Id="6491" PostId="6099" Score="0" Text="I'm voting to close this question as off-topic because this is a stats question, migrate to stats.stackexchange.com" CreationDate="2015-06-16T16:44:49.737" UserId="471" />
  <row Id="6492" PostId="6099" Score="1" Text="@Spacedman Thank you for your opinion. Initially, stats.stackexchange.com wanted to do the same for being off-topic because this was considered a stat-software question." CreationDate="2015-06-16T18:43:59.353" UserId="7966" />
  <row Id="6493" PostId="6088" Score="1" Text="How many samples do you have of each? An alternative could be to try some outlier detection approach and test it against your fraud data." CreationDate="2015-06-16T19:36:54.587" UserId="2621" />
  <row Id="6494" PostId="6132" Score="1" Text="Nicely done. Your answer kind of demarcates expectation from reality. Bravo!" CreationDate="2015-06-16T19:45:37.997" UserId="9966" />
  <row Id="6495" PostId="6132" Score="0" Text="Oddly the upvote here did not register in my points." CreationDate="2015-06-17T06:08:45.433" UserId="7720" />
  <row Id="6496" PostId="6099" Score="0" Text="I think it's close enough for this site since it's more about doing this is in software, and DS is more of the overlap between stats and engineering." CreationDate="2015-06-17T07:33:47.720" UserId="21" />
  <row Id="6497" PostId="6098" Score="0" Text="@sean-owen If you unhold this I can answer." CreationDate="2015-06-17T11:19:09.633" UserId="5303" />
  <row Id="6498" PostId="6134" Score="0" Text="By &quot;Time series can be assigned to clusters based on their fit to the cluster's pdf. &quot; you mean the mixture modelling EM-based clustering? I am not sure to understand your point. Basically, pdfs are my object of study, i.e. my dataset consists in $N \times T$ pdfs. Any of these N series can be viewed as a pdf which is evolving &quot;smoothly&quot; through time, and we have T regularly spaced snapshot of it. So, I want to capture its distorition dynamics AND I want to capture how it relates to other pdfs time series both in pdfs similarity and dynamics." CreationDate="2015-06-17T11:41:45.167" UserId="7966" />
  <row Id="6499" PostId="6133" Score="0" Text="You can try [cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) for fitting your hyper-parameters. It might not help that much in your case." CreationDate="2015-06-17T12:10:54.400" UserId="7966" />
  <row Id="6500" PostId="6133" Score="1" Text="When you say good performance for training data, what do you mean?Predicting the majority class all the time  will get you 95% accuracy. But you may have a cost matrix which makes this  an unattractive choice  ?" CreationDate="2015-06-17T14:00:39.610" UserId="7980" />
  <row Id="6501" PostId="6068" Score="0" Text="Try to pick something manageable. Decide what a minimum viable model would be for the data you are interested in. Go at it hammer and tongs until done. Start writing your paper based on what you learned. If you get to this stage and still have time left, add more sophistication if you want to. Version control is your friend with experimental software." CreationDate="2015-06-17T16:09:44.760" UserId="5303" />
  <row Id="6502" PostId="6133" Score="1" Text="@image_doctor, For training data. i get close to 75% accuracy for both majority class and minortiy class. However when i bring in some new data to test for the performance, there it is not giving good results. I get accuracy close to 40% in the new data. Simply put, my model is not robust or it is over-fitting." CreationDate="2015-06-17T18:21:49.197" UserId="9793" />
  <row Id="6503" PostId="6133" Score="0" Text="@mic , thanks for the suggestion. I tried k fold cross validation for rpart. However i dont know how to use the same in C5.0 algorithm in R." CreationDate="2015-06-17T18:23:22.850" UserId="9793" />
  <row Id="6504" PostId="6133" Score="0" Text="@image_doctor, what you are referring as cost matrix? is that a technique ?" CreationDate="2015-06-17T18:24:15.480" UserId="9793" />
  <row Id="6505" PostId="6137" Score="0" Text="It appears you are mis-applying PCA with regard to this data. It is not as simple as making the association that `PC2--&gt;stage`. When you look at the data for your known stages, they do not align with the y (PC2) axis. And your samples appear to be extreme outliers with respect to at least one of your input variables. What is the dimensionality of your inputs? You might consider generating some scatter plots to figure out which input feature is blowing up your variance in the first PC." CreationDate="2015-06-17T20:39:00.477" UserId="964" />
  <row Id="6508" PostId="6140" Score="1" Text="Needs to go on StackOverflow, not here." CreationDate="2015-06-18T10:25:44.007" UserId="7720" />
  <row Id="6509" PostId="5352" Score="0" Text="I did not see anything like this there. Am I blind or did it get taken down?" CreationDate="2015-06-18T11:39:02.277" UserId="7720" />
  <row Id="6510" PostId="6141" Score="1" Text="Why don't you use Python? That would be so easy. If you want Python recipe I can answer." CreationDate="2015-06-18T12:12:22.873" UserId="8878" />
  <row Id="6511" PostId="5352" Score="0" Text="Second to last paragraph mentioned it. Or do you mean in Visual Studio itself?" CreationDate="2015-06-18T14:04:45.950" UserId="587" />
  <row Id="6513" PostId="6134" Score="0" Text="@mic think my reading of your post oversimplified the question. Do we have any simplifying assumptions, e.g. could the distribution at time $t$ have a parametric form?" CreationDate="2015-06-18T14:53:52.793" UserId="5303" />
  <row Id="6514" PostId="6142" Score="0" Text="Thanks, kasramsh. I update my questions, adding more info about the input dataset. You are right. It is possible there are other factors leading to the difference on PC1. Generally, that difference is due to two sources of data, not limiting to protocol itself." CreationDate="2015-06-18T14:57:21.120" UserId="10201" />
  <row Id="6515" PostId="6142" Score="0" Text="What I am interested in is the difference on PC2. Based on the public datasets (points on the right), the difference on PC2 is due to time. What upsets me is whether I can project my samples (points on the left) to public data (points on the right) to conclude my samples are on stages between D12 and D19. I tried to find the answer by learning PCA algorithm but I'm still not sure. Thanks!" CreationDate="2015-06-18T14:57:34.183" UserId="10201" />
  <row Id="6516" PostId="6137" Score="0" Text="Thanks, bogatron. I updated the question with more details about the data. The differences between my 18 samples and 24 published samples are expected. Some points overlap on the PCA plot. On the left, there are 18 points. Sorry, this might mislead you." CreationDate="2015-06-18T15:07:11.690" UserId="10201" />
  <row Id="6517" PostId="6134" Score="0" Text="No. But, approximation is allowed if it can lead to a reasonable solution." CreationDate="2015-06-18T15:40:25.380" UserId="7966" />
  <row Id="6518" PostId="6098" Score="0" Text="@ahmad-tay it would help future users if you used math notation, check out: http://meta.math.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference" CreationDate="2015-06-18T15:50:20.863" UserId="5303" />
  <row Id="6519" PostId="30" Score="1" Text="Others describe 4 V's of big data [IBM](http://www.ibmbigdatahub.com/infographic/four-vs-big-data) or even 5 V's [DAVE BEULKE 2011](http://davebeulke.com/big-data-impacts-data-management-the-five-vs-of-big-data/)" CreationDate="2015-06-18T15:57:30.660" UserId="10220" />
  <row Id="6520" PostId="6134" Score="1" Text="@mic what are your samples? Do you actually have the distributions at all times for each series or do you have a sample(s) from the distribution at time $t$ in series $n$?" CreationDate="2015-06-18T15:59:59.043" UserId="5303" />
  <row Id="6521" PostId="30" Score="1" Text="The original 3 V's were set out in 2001 by Doug Laney  [3D Data Management: Controlling Data Volume, Velocity, and Variety](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)." CreationDate="2015-06-18T16:18:20.830" UserId="10220" />
  <row Id="6522" PostId="6068" Score="0" Text="@ChristopherLouden: When it comes to deep learning, sadly, there hardly any people who have been working to make the topic understandable." CreationDate="2015-06-18T17:08:17.960" UserId="10085" />
  <row Id="6523" PostId="6142" Score="1" Text="Regarding your update I'd say be careful about interpretation! you do not know the procedure of getting those data and it would be problematic for interpretation. The point is that those datasets are from different stages but also from different sources! so we can not make decision here. The other point is about using prcomp function. I've never used it but usually in machine learning tools the setting is like samples in rows and features in columns so I recommend to have a quick look at it again." CreationDate="2015-06-18T18:34:10.583" UserId="8878" />
  <row Id="6524" PostId="6137" Score="0" Text="If I understand your update to the question, you have a data set with 17436 variables (gene expressions) and 42 samples. So I'm curious how you're even doing PCA because your covariance matrix is definitely singular. I wonder if you have your data matrix transposed (inadvertently treating it as 42 variables with 17436 samples). Someone could help you better if you post the relevant portion of your code." CreationDate="2015-06-18T20:16:22.547" UserId="964" />
  <row Id="6525" PostId="6139" Score="1" Text="Can you please clarify what your problem is exactly? Is it poor utilization of server resources? Are you unable to process the data?" CreationDate="2015-06-18T20:48:32.777" UserId="3466" />
  <row Id="6526" PostId="6145" Score="0" Text="That is exactly what you wanna do. Cheers!" CreationDate="2015-06-18T21:07:45.607" UserId="9966" />
  <row Id="6528" PostId="2360" Score="0" Text="@xtian x_t is in the transform scale while x_i is in the current scale. Both x_{max} and x_{min} are also in the current scale." CreationDate="2015-06-18T21:19:26.000" UserId="847" />
  <row Id="6530" PostId="6141" Score="0" Text="Just let the search function be a conditional in an if...then statement." CreationDate="2015-06-18T23:30:48.443" UserId="8005" />
  <row Id="6531" PostId="6137" Score="0" Text="I transposed the matrix. The code for PCA is pasted in the update. Thanks." CreationDate="2015-06-19T02:22:31.723" UserId="10201" />
  <row Id="6532" PostId="6142" Score="0" Text="I transposed the matrix, thus rows are for samples and columns for genes. The code I use is in the 2nd update. Thanks!" CreationDate="2015-06-19T02:28:08.547" UserId="10201" />
  <row Id="6533" PostId="6088" Score="0" Text="bias–variance tradeoff should be there in training data." CreationDate="2015-06-19T07:19:03.470" UserId="5091" />
  <row Id="6534" PostId="6141" Score="0" Text="I wrote a command to do it in bash but for exploration excel is something far better." CreationDate="2015-06-19T07:33:30.907" UserId="3151" />
  <row Id="6535" PostId="6150" Score="0" Text="Could you edit your question and include what command have you tried in Matlab, and what is the specific error that Matlab is throwing? Otherwise, it's difficult to say more than &quot;`tdfread` is the command listed in the documentation for this kind of task&quot;" CreationDate="2015-06-19T07:37:32.037" UserId="1367" />
  <row Id="6536" PostId="6145" Score="0" Text="It worked. Thanks. The first match would suffice for me because the text is short (an abstract of a paper) and a word with a specific root will most probably happen once in the text." CreationDate="2015-06-19T07:45:47.720" UserId="3151" />
  <row Id="6537" PostId="6149" Score="0" Text="subset did not solve the problem." CreationDate="2015-06-19T08:13:25.170" UserId="3151" />
  <row Id="6538" PostId="6148" Score="1" Text="yes it might be possible in programming, but for R, the way it process data  is quite differently. I am using the library twitteR from R and the min it an retrieve is one day. I am not sure how am I going to continue from the point I stop tweet if I ran the searchTweets func again." CreationDate="2015-06-19T08:34:56.430" UserId="10225" />
  <row Id="6539" PostId="6133" Score="0" Text="@Arun It's a way of expressing the different values placed on correctly and incorrectly classifying instances of your data. Perhaps misclassifying the minority class has a great cost, perhaps it doesn't , using a cost matrix accounts for this." CreationDate="2015-06-19T12:21:02.233" UserId="7980" />
  <row Id="6543" PostId="6149" Score="0" Text="Well, it worked for me! so try `Sys.setlocale` and see if it helps." CreationDate="2015-06-19T15:56:02.300" UserId="5055" />
  <row Id="6545" PostId="6148" Score="0" Text="Wish I knew more about R to help you out. Sorry!" CreationDate="2015-06-19T22:07:12.613" UserId="3466" />
  <row Id="6548" PostId="6154" Score="12" Text="Is it possible to summarise the content of any one of those links, in a short paragraph? The links might be useful for further research, but ideally a stack exchange answer should have enough text to address the basic question without needing to go off site." CreationDate="2015-06-20T07:01:29.390" UserId="836" />
  <row Id="6549" PostId="6154" Score="0" Text="I am sorry but the content of these pages is too large to be summarized in a short paragraph." CreationDate="2015-06-20T09:11:39.483" UserId="9943" />
  <row Id="6551" PostId="6154" Score="6" Text="A full summary is not required, just a headline - e.g. &quot;A deconvolutional neural network is similar to a CNN, but is trained so that features in any hidden layer can be used to reconstruct the previous layer (and by repetition across layers, eventually the input could be reconstructed from the output). This allows it to be trained unsupervised in order to learn generic high-level features in a problem domain - usually image processing&quot; (note I am not even sure if that is correct, hence not writing my own answer)." CreationDate="2015-06-20T11:08:49.030" UserId="836" />
  <row Id="6552" PostId="6162" Score="0" Text="yes preparing model with CRF is ok. But I would like to know is there any other packages/libraries available? like Stanford NER for normal text message." CreationDate="2015-06-20T16:57:30.433" UserId="5091" />
  <row Id="6553" PostId="6162" Score="0" Text="Well the thing is that for open texts the standard CRF models might work poorly. The advantage of having your own is that you can make corrections and re-train." CreationDate="2015-06-20T16:58:55.340" UserId="7848" />
  <row Id="6554" PostId="6166" Score="0" Text="You may be correct it looks like naive Bayes only has the imbalance problem with text classification. Can you explain what smoothing is?" CreationDate="2015-06-21T15:34:14.737" UserId="10255" />
  <row Id="6555" PostId="6166" Score="0" Text="Have you had a look at the link? It's explained there. In short, when estimating $P(x \mid y)$ sometimes $x$ is never seen during training and  smoothing ensures that it doesn't ruin the classifier performance by adding some extra count (in case of laplace or +1 smoothing) to all the features." CreationDate="2015-06-21T17:35:16.417" UserId="816" />
  <row Id="6556" PostId="6146" Score="1" Text="Why don't you use Python or just command line tools to retrieve Twitter data, save it into chunks (even `gzipped`), and then get back to R for analysis?" CreationDate="2015-06-21T18:56:10.850" UserId="5279" />
  <row Id="6557" PostId="5371" Score="0" Text="Include in your comparison [Revolution R](http://www.revolutionanalytics.com/get-revolution-r), which is an optimized proprietary version of R with some libraries released for free." CreationDate="2015-06-21T18:58:08.923" UserId="5279" />
  <row Id="6558" PostId="155" Score="1" Text="Have you joined the Open Data Stack Exchange? http://opendata.stackexchange.com" CreationDate="2015-06-21T21:00:42.177" UserId="10133" />
  <row Id="6559" PostId="6139" Score="0" Text="Poor utilization of resource so cloud solution is needed. &#xA;But I don't know if I can use Big Data Analytics I can use in this case or not ." CreationDate="2015-06-22T04:11:22.663" UserId="10210" />
  <row Id="6560" PostId="6143" Score="0" Text="Sir, thank you for your reply. But my question is how to write this  \alpha which depends on Z and W." CreationDate="2015-06-22T07:57:15.493" UserId="10131" />
  <row Id="6561" PostId="6143" Score="0" Text="@AhmadTay, unfortunately your question suggested you had misread the book as some of your matrices had incorrect dimensions and your distribution for $f$ was off. In the model above, $\alpha$ is given as a parameter." CreationDate="2015-06-22T08:52:56.483" UserId="5303" />
  <row Id="6562" PostId="6173" Score="0" Text="and I think@mike-wise is right about the correct place for this question." CreationDate="2015-06-22T13:01:22.803" UserId="10270" />
  <row Id="6563" PostId="6175" Score="0" Text="I suspect the answer to this question will depend somewhat on the analysis method you're hoping to use; so more detail there would be helpful." CreationDate="2015-06-22T15:07:42.637" UserId="5303" />
  <row Id="6564" PostId="6175" Score="0" Text="Ok I gave some examples, thank you for your interest @conjectures" CreationDate="2015-06-22T15:27:25.147" UserId="10275" />
  <row Id="6565" PostId="6175" Score="0" Text="out of interest, with mongodb did you try using ensureIndex: http://www.tutorialspoint.com/mongodb/mongodb_indexing.htm (the indexing process itself takes a while, but speeds up queries afterwards)" CreationDate="2015-06-22T15:43:28.980" UserId="5303" />
  <row Id="6566" PostId="6175" Score="0" Text="also the second bullet is totally possible via 1 line at a time reading of file and adding a number to whichever groups the line belongs to and corresponding group counts." CreationDate="2015-06-22T15:45:31.873" UserId="5303" />
  <row Id="6567" PostId="6175" Score="0" Text="no I didn't try ensureIndex, to be honnest it was my almost first experience with mongoDB so it may be possible to optimize. It's probable that I can do every request I want with mongo but I am worried about the time it will take and if it is not the proper solution for my purpose, I am ready to change. Thank you for the link !" CreationDate="2015-06-22T16:03:33.887" UserId="10275" />
  <row Id="6568" PostId="6159" Score="0" Text="Thank you very much. Will try this and let you know." CreationDate="2015-06-23T02:29:21.323" UserId="9039" />
  <row Id="6569" PostId="6159" Score="0" Text="@logc Thank you for replying. I am going to work on it again and will let you know and if it does not work well I will go ahead and edit the question with more detail" CreationDate="2015-06-23T02:30:06.517" UserId="9039" />
  <row Id="6570" PostId="6178" Score="0" Text="sixth, [you could use sql](http://www.asdfree.com/2013/03/column-store-r-or-how-i-learned-to-stop.html)" CreationDate="2015-06-23T02:35:29.223" UserId="9924" />
  <row Id="6571" PostId="6174" Score="0" Text="How much time is &quot;too much time&quot;? 10 seconds? 10 minutes? 10 hours? What sort of &quot;data file&quot;? CSV? How many rows, columns? What is the rest of the spec of your machine? Are you sharing this server with other people? Does your server sysadmin limit your resources? What version of R? Post the sessionInfo() output, because if its a 32 bit R you need to switch. What are you going to do with it when you have got it?" CreationDate="2015-06-23T07:31:13.677" UserId="471" />
  <row Id="6572" PostId="6178" Score="1" Text="I'm not sure how any of this addresses the questioners extremely vague question. This Q should be voted down and clarification asked for in comments." CreationDate="2015-06-23T07:35:06.810" UserId="471" />
  <row Id="6574" PostId="6179" Score="0" Text="Thank you for your answer, I will check that. I hope I am not that silly, I am a statistician and still have a lot to learn about computers on a hardware point of view (it is not clear for me why RAM is necessary when you don't need to have access to all the data at the same time, but maybe speed of writing and reading is essentially based on RAM..). I have limited ressources at work and at home, so I am wondering : how do people who participate to challenges like kaggle as a hobby cope with these issues ? Are they necessarily very well equipped ?" CreationDate="2015-06-23T09:10:16.143" UserId="10275" />
  <row Id="6576" PostId="6183" Score="1" Text="In the context of text mining I've seen &quot;annotated data&quot; vs. &quot;unannotated data&quot; or &quot;raw data&quot;." CreationDate="2015-06-23T13:55:09.790" UserId="2673" />
  <row Id="6577" PostId="6183" Score="0" Text="@Suzana_K, that already seems a bit better. Thanks for the suggestion! More suggestions are very welcome." CreationDate="2015-06-23T13:56:38.377" UserId="9486" />
  <row Id="6578" PostId="432" Score="0" Text="The first two links are dead." CreationDate="2015-06-23T14:30:02.550" UserId="2673" />
  <row Id="6579" PostId="2651" Score="1" Text="You should at least accept an answer." CreationDate="2015-06-23T14:51:23.123" UserId="2673" />
  <row Id="6580" PostId="432" Score="0" Text="@Suzana_K Thanks for reporting -- updated." CreationDate="2015-06-23T15:12:43.107" UserId="241" />
  <row Id="6581" PostId="6181" Score="0" Text="Let me know if there's a better StackExchange for this..." CreationDate="2015-06-23T16:41:07.827" UserId="10293" />
  <row Id="6582" PostId="6190" Score="0" Text="Are the y values of the baseline and the elevated parts known beforehand? Because then its just a case of looking for runs in over- or under-threshold values." CreationDate="2015-06-23T21:40:47.407" UserId="471" />
  <row Id="6583" PostId="6184" Score="1" Text="In `r`, `data.table` and `dplyr` packages will do that." CreationDate="2015-06-23T23:36:32.363" UserId="10307" />
  <row Id="6584" PostId="6189" Score="1" Text="[Using SQL, pandas, and Python for data analysis](http://johnbeieler.org/blog/2013/06/06/using-sql/) &amp; [Reading Files in HDFS with Pandas framework](http://stackoverflow.com/questions/16598043/reading-files-in-hdfs-hadoop-filesystem-directories-into-a-pandas-dataframe) &amp; [Large Data Work with pandas](http://stackoverflow.com/questions/14262433/large-data-work-flows-using-pandas)" CreationDate="2015-06-24T03:14:57.380" UserId="10019" />
  <row Id="6585" PostId="6191" Score="0" Text="Actually to be as clear as possibleI  the 1st paragrpah is an intro and the 2nd specifically describes the problem I'd like to solve through machine learning . Anyway, what do you mean &quot;highest overlap&quot; please ?" CreationDate="2015-06-24T07:09:34.467" UserId="10102" />
  <row Id="6586" PostId="6175" Score="1" Text="Why not buy more RAM?" CreationDate="2015-06-24T09:37:21.043" UserId="7925" />
  <row Id="6588" PostId="6175" Score="0" Text="I tried Monetdb which failed to load my big dataset but I managed to load it with Rsqlite in 2 hours." CreationDate="2015-06-24T09:30:11.707" UserId="10275" />
  <row Id="6589" PostId="6191" Score="0" Text="It's not clear what the assumptions are.  For example, it's not clear where the tags come from, where you are drawing recommendations from, or whether your set that you want to draw recommendations from is tagged with the same tagset." CreationDate="2015-06-24T15:14:53.687" UserId="10303" />
  <row Id="6590" PostId="6191" Score="0" Text="&quot;Overlap&quot; means &quot;in common&quot;,  There are many similarity metrics based on overlap, for example [overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient)" CreationDate="2015-06-24T15:15:54.553" UserId="10303" />
  <row Id="6591" PostId="6175" Score="0" Text="@reinierpost : I will not buy extra RAM for my professionnal computer obviously but I am thinking about improving my personnal hardware. Actually before I struggled with these data, I had no idea that 4GB RAM was very little. It is not clear for beginners what you can do or not do depending on softwares or hardware..." CreationDate="2015-06-24T16:19:37.247" UserId="10275" />
  <row Id="6592" PostId="6179" Score="0" Text="I tried Monetdb with no success, but I succeeded in loading at least with rSQLite, thank you for answering though." CreationDate="2015-06-24T16:21:48.517" UserId="10275" />
  <row Id="6594" PostId="6150" Score="0" Text="@logc I edited the question and I have included the command and the error message I get. Would you please be able to help." CreationDate="2015-06-24T17:47:01.017" UserId="9039" />
  <row Id="6595" PostId="6209" Score="0" Text="Do you need to join texts that are separated by new lines?" CreationDate="2015-06-24T20:41:27.160" UserId="9966" />
  <row Id="6596" PostId="6209" Score="0" Text="Yes but only in certain cases. Some lines are perfectly ok" CreationDate="2015-06-24T20:46:04.903" UserId="10322" />
  <row Id="6597" PostId="6209" Score="0" Text="Sounds good. My solution should work then. Could you please show us a glimpse of your data?" CreationDate="2015-06-24T20:48:06.330" UserId="9966" />
  <row Id="6598" PostId="6209" Score="0" Text="Sure, here you go:" CreationDate="2015-06-24T20:58:46.333" UserId="10322" />
  <row Id="6599" PostId="6209" Score="0" Text="Employers' Liability Assurance&#xA;Corporation (Limited) of London, England, Lawfford &amp; McK3m, General&#xA;Agents, 19 and 21 Chamber of Commerce&#xA;Fidelity and Casualty Company of&#xA;New York , BLrekhead &amp; Son,&#xA;Agents-,. 306 Water  &#xA;Fidelity and Casualty Oo of New York&#xA;,. Robt Schaefer,, res mngr, 22 s&#xA;Holliday&#xA;Frankfort Marine Accident and  Pl ate-Glass Insurance Company  of&#xA;Frankfort-on-the-Main, Germany, Spear&#xA;&amp; Burbank, General Agents, 10-12 s Holliday	.&#xA;General Accident Assurance Corporation&#xA;of Scotland S03 Merchants* Nat’l Bank Bldg&#xA;GREAT EASTERN CASUALTY and Indemnity Company of NewYork," CreationDate="2015-06-24T20:59:59.260" UserId="10322" />
  <row Id="6600" PostId="6209" Score="0" Text="If you want it in another format let me know" CreationDate="2015-06-24T21:00:17.147" UserId="10322" />
  <row Id="6601" PostId="6209" Score="0" Text="This won't help much cause the formatting goes away when you comment. Attach screen shots of the data in hand and how you want it to be processed." CreationDate="2015-06-24T21:02:48.830" UserId="9966" />
  <row Id="6602" PostId="6211" Score="0" Text="Firstly, thanks for the answer and explanation (+1), @Azrael. So, according to you, shouldn't I care about the number of observation? Could you provide some reference about that?" CreationDate="2015-06-24T21:05:13.830" UserId="9225" />
  <row Id="6603" PostId="6209" Score="0" Text="http://imgur.com/ztYCaDU,BoZYHo6#0    The first pic is what it looks like now, the second is what I'd like it to look like" CreationDate="2015-06-24T21:11:42.230" UserId="10322" />
  <row Id="6604" PostId="6211" Score="1" Text="The data points mean the observations." CreationDate="2015-06-24T21:17:52.873" UserId="9943" />
  <row Id="6605" PostId="6211" Score="0" Text="Ok, thanks for the explanation again!" CreationDate="2015-06-24T21:22:12.917" UserId="9225" />
  <row Id="6606" PostId="6211" Score="0" Text="No problem. Happy to help." CreationDate="2015-06-24T21:22:47.287" UserId="9943" />
  <row Id="6607" PostId="6211" Score="0" Text="Maybe someone else knows better, but I&quot;m not entirely sure this is true. You aren't trying to solve the equations of the neural network in a classical sense. You could quite easily have a complex network that was trained to recognise the difference between only two data points." CreationDate="2015-06-24T21:33:52.300" UserId="7980" />
  <row Id="6608" PostId="6211" Score="0" Text="Yes, you are right. For example, you can keep giving a neural network 0 as an input, and 1 as an output multiple times and it will start learning to become a NOT gate. But what the question has demanded is a rule of thumb. The number of parameters that are required can not be correctly predicted without knowing more details." CreationDate="2015-06-24T21:38:27.727" UserId="9943" />
  <row Id="6610" PostId="677" Score="0" Text="Cross-posted with http://stackoverflow.com/q/24583249/2954547" CreationDate="2015-06-24T22:59:00.503" UserId="1156" />
  <row Id="6611" PostId="6211" Score="0" Text="I think that demonstrates that your opening paragraph is not true ?" CreationDate="2015-06-24T23:40:55.280" UserId="7980" />
  <row Id="6612" PostId="6211" Score="0" Text="Again, the question asks for a thumb rule." CreationDate="2015-06-24T23:43:07.173" UserId="9943" />
  <row Id="6613" PostId="6211" Score="0" Text="And your first para presents a rule, not a rule of thumb ?" CreationDate="2015-06-25T08:04:17.000" UserId="7980" />
  <row Id="6614" PostId="6211" Score="0" Text="This edit should address your concerns." CreationDate="2015-06-25T10:17:30.757" UserId="9943" />
  <row Id="6615" PostId="6175" Score="0" Text="@Stéphanie C: I don't know who is paying for your computer, but if they are also paying for your time, getting them to add more RAM may be actually be the most cost-effective solution for them - unless you plan to work with much larger data sets later on." CreationDate="2015-06-25T15:37:55.773" UserId="7925" />
  <row Id="6616" PostId="6179" Score="0" Text="You are correct: the reason to add RAM is access speed, primarily for writing." CreationDate="2015-06-25T15:39:33.640" UserId="7925" />
  <row Id="6617" PostId="6201" Score="1" Text="Check http://opendata.stackexchange.com/ and https://www.reddit.com/r/datasets" CreationDate="2015-06-25T16:11:14.307" UserId="816" />
  <row Id="6618" PostId="6213" Score="0" Text="Have you read about item-based collaborative filtering or matrix factorization for recommender systems? Your problem is very much like Amazon's &quot;customers who bought this also bought that.&quot;" CreationDate="2015-06-25T16:34:09.020" UserId="9956" />
  <row Id="6622" PostId="1165" Score="0" Text="teaches me not to check my stackoverflow email :-).  If you are still interested my direct email emily at equitieslab dot com" CreationDate="2015-06-25T18:51:39.887" UserId="8344" />
  <row Id="6624" PostId="6216" Score="0" Text="Can you explain why it doesn't seem like the right way? If you provide clear information on what's wrong, its easier for people to help you." CreationDate="2015-06-25T20:51:29.997" UserId="41" />
  <row Id="6625" PostId="6217" Score="2" Text="If I understand correctly, the concept you are looking for is _embedding_. Look up _kernel methods_, and _kernel PCA_ in particular." CreationDate="2015-06-26T00:22:35.050" UserId="381" />
  <row Id="6626" PostId="6146" Score="0" Text="Can python retrieve old tweets using the method you mentioned?" CreationDate="2015-06-26T01:44:51.267" UserId="10225" />
  <row Id="6627" PostId="6192" Score="0" Text="I think  it is possible to consider 'click only data' same as purchase data. From 'purchase data' you know keywords for each link. right ? can't you try using this data for 'click only data' also? I mean group 'purchase data' keywords with 'click data' will that help ?" CreationDate="2015-06-26T04:08:54.240" UserId="5091" />
  <row Id="6628" PostId="6200" Score="0" Text="cross validation" CreationDate="2015-06-26T04:16:13.223" UserId="5091" />
  <row Id="6629" PostId="6204" Score="0" Text="What do you mean by swm?" CreationDate="2015-06-26T05:00:48.350" UserId="2643" />
  <row Id="6630" PostId="6217" Score="0" Text="I am not sure about this, so I am not posting it as an answer. In a neural network type of model, you can keep the hidden layer dimensionality &gt; input layer dimensionality. Then you can use the hidden layer as input to another network/model. But doing so requires lots of data." CreationDate="2015-06-26T09:01:10.423" UserId="9943" />
  <row Id="6631" PostId="6216" Score="0" Text="So I initialised all the values in the first iteration.. computed the V(S)t and then I used all the V(S)t already computed to computed alpha trial-by-trial.. first, I really don't know if it is the right way of proceeding ..second I've got a alpha vector of all 1 and NaN values..." CreationDate="2015-06-26T09:29:57.800" UserId="10344" />
  <row Id="6632" PostId="6225" Score="0" Text="How would you &quot;cluster&quot; them into more generic groups? By means of a linguistic distance?" CreationDate="2015-06-26T12:31:31.587" UserId="982" />
  <row Id="6633" PostId="6146" Score="0" Text="It can, but you'll need to buy the API access from Twitter or another seller of Twitter history." CreationDate="2015-06-26T13:15:10.223" UserId="5279" />
  <row Id="6635" PostId="6225" Score="0" Text="In this case since number of FB categories are less, you can manually cluster them or go for similarity matching algorithms such as BOW/cosine similarity/ wordvec etc" CreationDate="2015-06-26T16:45:34.130" UserId="5091" />
  <row Id="6637" PostId="6225" Score="0" Text="@Sreejithc321 Your observation about the number of categories simplified the problem greatly." CreationDate="2015-06-27T08:26:21.873" UserId="75" />
  <row Id="6638" PostId="6234" Score="0" Text="Thanks for the answer. Would you mind giving a R code to do it  with the example I gave in order to have the answer to be perfect?" CreationDate="2015-06-27T14:24:14.200" UserId="6527" />
  <row Id="6639" PostId="5252" Score="0" Text="Thanks M.Dax a lot ! Yes you are right, thank you very much for the advice." CreationDate="2015-06-27T15:13:37.987" UserId="3433" />
  <row Id="6640" PostId="6175" Score="0" Text="A friend of mine is being hired as a datascientist in private company and get a 16 GB ram computer, is it a benchmark of the acceptable minimum for datascience do you think ? @reinierpost" CreationDate="2015-06-27T15:31:18.117" UserId="10275" />
  <row Id="6641" PostId="2258" Score="0" Text="@Sean Owen, Why are there so many questions on how to get started on neural networks? Shouldn't they be marked duplicate?" CreationDate="2015-06-27T16:20:01.383" UserId="9943" />
  <row Id="6642" PostId="6175" Score="0" Text="@Stéphanie C: I don't know, it entirely depends on the amounts of data you need to process and the complexity of the processing (e.g. selecting is usually cheap, joining expensive)." CreationDate="2015-06-27T19:04:05.067" UserId="7925" />
  <row Id="6643" PostId="2258" Score="0" Text="Heh don't ask me, but please flag duplicates as you see them." CreationDate="2015-06-27T21:19:36.917" UserId="21" />
  <row Id="6644" PostId="6240" Score="0" Text="The problem I have with the Pearson is that I am not sure how to join my data and take care of categorical variable (maybe could be done with dummy variable like bey suggested but not sure it would work with it)" CreationDate="2015-06-27T22:00:47.417" UserId="6527" />
  <row Id="6645" PostId="6208" Score="0" Text="you asked this [on Stack Overflow](http://stackoverflow.com/questions/30996952/working-with-inaccurate-incorrect-dataset/31003163#31003163) and @MaximHaytovich gave you a good answer" CreationDate="2015-06-28T03:12:17.343" UserId="10019" />
  <row Id="6646" PostId="6234" Score="1" Text="@zipp here you go. I had it calculate the normalized vectors, I also created a normalized vectors for each person, by normalizing the vector sum of the rows that correspond to that person. Finally, I take the dot product of each person against every other person using the $XX^T$ formula" CreationDate="2015-06-28T04:47:08.893" UserDisplayName="user9424" />
  <row Id="6647" PostId="6224" Score="0" Text="Is the network being used for classification or regression ?" CreationDate="2015-06-28T11:04:41.577" UserId="7980" />
  <row Id="6648" PostId="6235" Score="1" Text="You might want to consider asking this on OpenData StackExchange http://opendata.stackexchange.com/" CreationDate="2015-06-28T11:06:27.840" UserId="7980" />
  <row Id="6649" PostId="6217" Score="0" Text="When you say 2 dimensional data, defined by at least three variables, in what sense do you use the term 'variable'? Would classes be a suitable substitution ? It's worth noting that PCA extracts maximally variant dimensions from data, this is not necessarily the most discriminative transform to apply. Have you looked at clustering ?" CreationDate="2015-06-28T11:20:08.813" UserId="7980" />
  <row Id="6650" PostId="6234" Score="0" Text="Thank you very much" CreationDate="2015-06-28T13:16:33.127" UserId="6527" />
  <row Id="6651" PostId="6204" Score="0" Text="Support vector machine. Just as an example, this algorithm offers an ability to assign different weights to classes manually" CreationDate="2015-06-28T15:49:45.353" UserId="7969" />
  <row Id="6653" PostId="6242" Score="0" Text="Are you talking about [that GATE](https://gate.ac.uk/family/process.html)? The answer depends on what level of integration you mean. For a basic direction, I suggest you to search for Java-Python interoperability software modules." CreationDate="2015-06-28T22:51:15.643" UserId="2452" />
  <row Id="6654" PostId="5248" Score="1" Text="I've found a spare.cor functions for sparse matrixes in R that calculates correlation for any dimension for sparse matrix. @Hack-R think about this bias. By the way sampling coefficients should have also something-like bootstrap confidence intervals: see this: http://stats.stackexchange.com/questions/126176/whats-the-bias-of-calculating-the-kendall-coefficient-of-correlation-on-a-sampl&#xA;&#xA;And the sparse.cor function for sparse matrixes is here: &#xA;http://stackoverflow.com/questions/5888287/running-cor-or-any-variant-over-a-sparse-matrix-in-r" CreationDate="2015-06-29T09:44:16.767" UserId="5224" />
  <row Id="6655" PostId="6224" Score="0" Text="It is being used for regression!" CreationDate="2015-06-29T12:23:32.627" UserId="10353" />
  <row Id="6656" PostId="6224" Score="0" Text="Are you happy with both those levels of error for estimating your target function ?" CreationDate="2015-06-29T12:32:50.320" UserId="7980" />
  <row Id="6657" PostId="6224" Score="0" Text="Thats the problem! I do not know the significance of the term 'relative sum of squares error'. Are these errors out of 1? Meaning, does 0.9 mean 90% error? Or is it some other way? Please tell me the significance of the term 'relative sum of squares error'." CreationDate="2015-06-29T13:12:42.593" UserId="10353" />
  <row Id="6658" PostId="6224" Score="0" Text="Also, is there some way to calculate RMSE in SPSS Neural Network analysis? Like instead of relative sum of squares error i want to calculate the RMSE of the network." CreationDate="2015-06-29T13:15:48.470" UserId="10353" />
  <row Id="6659" PostId="6243" Score="0" Text="Does anyone have a clue?" CreationDate="2015-06-29T15:01:07.867" UserId="10381" />
  <row Id="6660" PostId="5982" Score="0" Text="And how do you come up with `d` variable? Is it a fixed number that is chosen by a scientist?" CreationDate="2015-06-29T15:29:27.583" UserId="10404" />
  <row Id="6661" PostId="6255" Score="0" Text="Is this what you are looking for? http://math.stackexchange.com/questions/647395/why-root-mean-square-error" CreationDate="2015-06-29T15:45:25.667" UserId="10404" />
  <row Id="6662" PostId="6257" Score="0" Text="Is there a real-life problem behind your problem? Maybe you could use some algorithm that solves &quot;Market Basket Analysis&quot; (explained here for example: http://www.albionresearch.com/data_mining/market_basket.php)" CreationDate="2015-06-29T15:53:57.800" UserId="10404" />
  <row Id="6663" PostId="6257" Score="0" Text="I am trying to induce tags in blog post articles. This is exactly what I was looking for thank you https://en.wikipedia.org/wiki/Association_rule_learning. Could you post both links as an answer so I can accept it?" CreationDate="2015-06-29T15:57:33.363" UserId="9202" />
  <row Id="6664" PostId="6257" Score="0" Text="I'm glad I could help. :)" CreationDate="2015-06-29T16:02:54.327" UserId="10404" />
  <row Id="6665" PostId="6214" Score="0" Text="I've followed this before. Not working" CreationDate="2015-06-29T16:25:09.573" UserId="5043" />
  <row Id="6666" PostId="5982" Score="0" Text="A common approach is to have $d$ words, and each of the $d$ elements in the vector represent the frequency with which that word occurs in the text. There are only so many unique words used in all of the samples you're considering, so there's a definite upper-bound on $d$. Researchers usually also remove certain kinds of words that they don't think will be useful for classification, like &quot;the,&quot; &quot;and,&quot; &quot;it,&quot; etc." CreationDate="2015-06-29T16:28:22.523" UserId="9483" />
  <row Id="6667" PostId="6243" Score="0" Text="This is too vague...are you interested in the Shannon Entropy specifically?" CreationDate="2015-06-29T16:34:39.893" UserDisplayName="user9424" />
  <row Id="6668" PostId="6229" Score="0" Text="This has worked. Thanks a lot" CreationDate="2015-06-29T16:56:35.813" UserId="9793" />
  <row Id="6669" PostId="6192" Score="0" Text="@Sreejithc321 If for Kmeans, there isn't the numeric value of the click data keywords, as there isn't any purchasing event occurred, so there isn't any sales amount that i could enter for the value of the keywords. I am trying the topic modeling for grouping the purchase data and click data right now, but the purchase data should have more weights than the click data and i don't know how to allocate the weights for them is appropriate.  I am trying purely 2:1 right now." CreationDate="2015-06-29T17:00:37.733" UserId="9724" />
  <row Id="6670" PostId="6186" Score="1" Text="Interesting, I haven't read the Bishop book, but I've heard online learning and sequential learning treated as different concepts. I understand online learning to mean a learning problem in which a model is able to use new incoming data as a way to &quot;grow&quot; the training set and update the model dynamically. I've heard sequential learning applied to problems where the goal is to predict a sequence instead of individual instances, as in predicting part-of-speech sequences in POS tagging." CreationDate="2015-06-29T19:22:13.487" UserId="4897" />
  <row Id="6671" PostId="5233" Score="0" Text="I second @PabloSuau's thought. Many packages have the capability to train multi-class classifiers, but often what's happening under the hood is that n binary classifiers are trained, and these are used individually to make predictions for each class. The SO question here also seems relevant: http://stackoverflow.com/questions/22009871/how-to-perform-multi-class-classification-using-svm-of-e1071-package-in-r" CreationDate="2015-06-29T19:32:29.603" UserId="4897" />
  <row Id="6675" PostId="6243" Score="0" Text="@Bey Yes, let's say Shannon Entropy." CreationDate="2015-06-29T20:56:02.983" UserId="10381" />
  <row Id="6676" PostId="6224" Score="0" Text="Have you seen ..http://stats.stackexchange.com/questions/71315/whats-relative-error-in-a-neural-network-model and this http://www-01.ibm.com/support/knowledgecenter/SSLVMB_20.0.0/com.ibm.spss.statistics.help/idh_idd_mlp_output.htm" CreationDate="2015-06-29T21:23:59.767" UserId="7980" />
  <row Id="6677" PostId="6243" Score="0" Text="Ok, well unlike least squares, the Shannon Entropy requires that you be able to assign a probability or density value to each error. Do you want to assume a gaussian error with mean 0 and a pre-specified stdev? Also, do you really want to maximize entropy or minimize entropy?" CreationDate="2015-06-30T03:23:40.490" UserDisplayName="user9424" />
  <row Id="6679" PostId="6270" Score="0" Text="Plain text file, Word document, PDF, RTF, HTML, LaTeX source, PostScript file, document scan in image format?" CreationDate="2015-06-30T08:25:33.737" UserId="471" />
  <row Id="6681" PostId="6259" Score="0" Text="Have you read the &quot;Web Technologies&quot; task view on CRAN? Tells you all about web scraping in R." CreationDate="2015-06-30T08:30:04.243" UserId="471" />
  <row Id="6682" PostId="6274" Score="0" Text="Is time really a categorical attribute ?  Perhaps day of the week or month might be?" CreationDate="2015-06-30T10:46:02.823" UserId="7980" />
  <row Id="6683" PostId="6274" Score="0" Text="Value under time attribute appearing as  1/11/2011  11.54  and showing as Factor/W 20823 labels" CreationDate="2015-06-30T12:03:47.883" UserId="10416" />
  <row Id="6684" PostId="6275" Score="0" Text="yeah...got it. Thanks." CreationDate="2015-06-30T12:04:21.970" UserId="10416" />
  <row Id="6685" PostId="6274" Score="0" Text="My meaning was to suggest that interpreting time as a category, is probably not useful in a machine learning context." CreationDate="2015-06-30T12:37:42.687" UserId="7980" />
  <row Id="6686" PostId="6243" Score="0" Text="@Bey You are right a pdf is needed. But since we get a sequence of error, i.e. {epsilon_i}, can we use emperical distribution? I think it is the simplist and roughest way but please tell me if I am wrong (like using a Gaussian actually help). And I want to maximize entropy." CreationDate="2015-06-30T13:28:23.157" UserId="10381" />
  <row Id="6687" PostId="6243" Score="0" Text="@Bey And I am also interested in general if there is any other theory or tool to support self-desinged objective, other than least-square, for multiple linear regression. If a general method or tool is available, I believe I can apply my particular case (just make the objective Shannon entropy)." CreationDate="2015-06-30T13:34:13.610" UserId="10381" />
  <row Id="6689" PostId="6243" Score="0" Text="@Bey Actually, the underlying reason of why I am trying this is because I try to use a linear-regression kind of &quot;filter&quot; to process the sequences {x_i} and try to make the &quot;filtered&quot; sequence have maximum entropy (i.e. In fact I try to use x_i as y_i). If you have any other comments about how I can do this or whether this makes sense, your comments are welcome. I hope I am not confusing you." CreationDate="2015-06-30T13:34:29.787" UserId="10381" />
  <row Id="6691" PostId="6279" Score="1" Text="Machine learning classification algorithms are designed to seek features that usefully predict the  instances class, as you note a highly predictive feature is a very attractive target for them.  You might make more progress if you had training and test data which had a significant percentage of language learner decisions which differed from the true class of the sentence, forcing the classifier to look elsewhere for discriminative information.  The logical conclusion of this process is to remove the  learner decision from the features, which places you back at the start. Maybe combine both?" CreationDate="2015-06-30T14:08:56.323" UserId="7980" />
  <row Id="6692" PostId="6279" Score="0" Text="Unfortunately, the learner data is not annotated so I have no knowledge of their errors as of now. I could manually add a bias towards the existing class with probabilty (so if initial guess + 0.5 &gt;0.95 or something, then assign the error), but it is more elegant if the classifier does all this implicitly." CreationDate="2015-06-30T14:50:00.540" UserId="8152" />
  <row Id="6693" PostId="6279" Score="0" Text="I may be a little confused, I thought you had the learner decision and had included it, but I may have misunderstood ?" CreationDate="2015-06-30T15:13:38.797" UserId="7980" />
  <row Id="6694" PostId="6272" Score="0" Text="Hey, could you please explain to me what this block does? I ran my script with this code added and I got  [6, 1, 4, 3, 5, 7, 8, 0, 2] as the output. I want to substitute numerical values to the work class content using the values in the dictionary." CreationDate="2015-06-30T15:43:15.680" UserId="5043" />
  <row Id="6695" PostId="6279" Score="0" Text="I do, but the 'true class' is not known, so the differences between those cannot be included unfortunately." CreationDate="2015-06-30T16:08:32.220" UserId="8152" />
  <row Id="6696" PostId="6279" Score="0" Text="So you have two sets of data, labelled sentences, not from language learners and unlabelled data from language learners ?" CreationDate="2015-06-30T16:20:36.663" UserId="7980" />
  <row Id="6697" PostId="6272" Score="0" Text="Hi, The mapr function will return numerical value associated with the category value. eg : 6 for 'Self-emp-not-inc', python dictionaries are unordered. If you want an ordered dictionary, try collections.OrderedDict." CreationDate="2015-06-30T16:35:19.633" UserId="5091" />
  <row Id="6698" PostId="6280" Score="0" Text="You've had two hours to look at this and see how badly formatted it is. Please try and improve the formatting of your questions. There is a preview that shows you what it will look like. Learn a bit about marking up questions." CreationDate="2015-06-30T17:03:59.993" UserId="471" />
  <row Id="6699" PostId="6203" Score="0" Text="Mahout in Action is a great book if you are comfortable with Java. I would highly suggest it." CreationDate="2015-06-30T17:09:24.360" UserId="3466" />
  <row Id="6700" PostId="6280" Score="0" Text="@Spacedman, I am sorry for this. Its my bad. Thanks for pointing." CreationDate="2015-06-30T17:38:50.230" UserId="9793" />
  <row Id="6701" PostId="6259" Score="0" Text="Use https://www.kimonolabs.com/ for scraping. Then just use its `csv` output." CreationDate="2015-06-30T18:20:37.223" UserId="5279" />
  <row Id="6702" PostId="6272" Score="0" Text="Okay, now I understand the function. The thing is, I have a CSV with several thousand rows and there is a column named Workclass which contains any one of the value mentioned in the dictionary. So, for each row, I need to change the text in that column to a number by comparing the text with the dictionary and substitute the corresponding number. How do I use a function to parse the column by rows and compare the values with the dictionary?" CreationDate="2015-06-30T21:16:01.960" UserId="5043" />
  <row Id="6703" PostId="6279" Score="0" Text="That is correct! I train on the labelled ones, and I evaluate shortly on those as well, but the actual data it is created for, is the unlabelled data." CreationDate="2015-06-30T23:05:08.647" UserId="8152" />
  <row Id="6704" PostId="6243" Score="0" Text="I really don't follow what you are trying to do. Perhaps a small example? In theory, any increasing function of $\epsilon$ can be used, but not all of these generate sensible fits. The problem with entropy is that if you want to maximize it, you will be generating horrible fits." CreationDate="2015-07-01T01:33:14.853" UserDisplayName="user9424" />
  <row Id="6706" PostId="6287" Score="0" Text="thank you. This has worked." CreationDate="2015-07-01T04:53:41.733" UserId="9793" />
  <row Id="6713" PostId="6243" Score="0" Text="@Bey If to maximize entropy, I think you are probably right about getting horrible fits. But my purpose is not fitting. I am sorry I confuse you but the following might be clear. I just want to get a sequence (1) having linear relationship (like a transformation) with the original one (2) having maximum entropy." CreationDate="2015-07-01T13:29:42.680" UserId="10381" />
  <row Id="6714" PostId="6243" Score="0" Text="Here's the problem: the empirical distribution puts $\frac{1}{n}$ probability  on each residual, so it will always result in the minimum entropy (uniform)" CreationDate="2015-07-01T13:43:34.627" UserDisplayName="user9424" />
  <row Id="6715" PostId="6294" Score="0" Text="Look up Markov models and sequential machine learning." CreationDate="2015-07-01T14:18:46.747" UserId="381" />
  <row Id="6716" PostId="6287" Score="0" Text="Always happy to help! If it works for you, please accept the answer! Thank You!" CreationDate="2015-07-01T14:43:44.873" UserId="10299" />
  <row Id="6720" PostId="6287" Score="0" Text="Hi Shiva, I have accepted the answer. Thank you" CreationDate="2015-07-01T19:58:56.327" UserId="9793" />
  <row Id="6721" PostId="6305" Score="0" Text="The fields I'm extracting will be the same for a certain type of document, of which many types come in. I'm now thinking something along the lines of using a classifier to detect the document type and then some kind of business logic per document class to extract the information. Thanks for your answer and making me aware of regularization, leaving this open for now for more feedback." CreationDate="2015-07-01T21:22:09.747" UserId="10429" />
  <row Id="6722" PostId="6294" Score="0" Text="I have a hunch that Bayesian Analysis might be able to address this problem. But couldn't find reliable literature for it.&#xA;@Emre, can you provide references where Markov models are used for this type of Prediction/Modelling?" CreationDate="2015-07-02T07:18:27.340" UserId="10125" />
  <row Id="6731" PostId="6325" Score="0" Text="Have you thought about forming cluster centres using sets comprised of a minimum number of points from each dataset and then force the clusters only to grow ?" CreationDate="2015-07-03T06:36:40.873" UserId="7980" />
  <row Id="6732" PostId="6293" Score="0" Text="The analysis aIgos are in batch. I mostly use R for the the modelling where as in live the code is mostly C++. I am exploring  parallelization as the problem seems embarrisingly parallel. Thanks" CreationDate="2015-07-03T08:51:53.410" UserId="10409" />
  <row Id="6735" PostId="6325" Score="0" Text="Thanks! I think the problem with this would lie in defining the initial clusters centers which I would like to be determined by some variation minimising scheme." CreationDate="2015-07-03T12:26:22.910" UserId="10463" />
  <row Id="6736" PostId="6291" Score="0" Text="The max and min of the training dataset are not necessary the same for the predictive dataset. Neither the average and the ST. Dev. So, the assumption to use the same statistics is not right." CreationDate="2015-07-03T13:51:55.823" UserId="201" />
  <row Id="6737" PostId="6294" Score="0" Text="Markov models are the simplest correlated sequence models, so they are widely used. A good example is in modeling language. You can predict the next word in a sequence given the past few words. Many spelling checkers work this way." CreationDate="2015-07-03T14:17:26.657" UserId="381" />
  <row Id="6738" PostId="6291" Score="1" Text="Like I said, I agree that the min and max are not good scale factors. They are too sensitive to outliers. However, if the robust statistics of mean and standard deviation the two datasets are that much different than why are you training with that dataset? It would seem that you should not be using a training set that is so far away from the prediction set." CreationDate="2015-07-03T17:31:59.777" UserId="2728" />
  <row Id="6740" PostId="6325" Score="0" Text="The implication seems to be that whatever distance metric you use, should not be derived solely from the feature space distance, but have a component dependent on the class of the data, or some data dependent variable weighting of feature distance and class. Without sight of representative data this may prove challenging ?" CreationDate="2015-07-03T19:58:34.043" UserId="7980" />
  <row Id="6741" PostId="6068" Score="0" Text="possible duplicate of [Deep learning basics](http://datascience.stackexchange.com/questions/2651/deep-learning-basics)" CreationDate="2015-07-03T21:40:24.750" UserId="9943" />
  <row Id="6743" PostId="6272" Score="0" Text="You can create an additional column, say 'workclass_num'  which store numerical values corresponding to the categorical value. Check Python Pandas library." CreationDate="2015-07-05T08:32:26.057" UserId="5091" />
  <row Id="6744" PostId="6335" Score="0" Text="Brilliant way to handle it. It would give 0 and 23 hrs similar scores but won't it make am/pm time similar too? Which is in fact separated by 12hr window." CreationDate="2015-07-05T12:34:18.123" UserId="8338" />
  <row Id="6745" PostId="6343" Score="1" Text="All these clusters are *convex*, and this doesn't answer the question." CreationDate="2015-07-05T13:21:12.920" UserId="924" />
  <row Id="6746" PostId="6335" Score="0" Text="12 hour (AM/PM) time doesn't work, just convert it to 24 hour time." CreationDate="2015-07-05T16:41:36.560" UserId="9420" />
  <row Id="6747" PostId="6335" Score="0" Text="I just noticed that you are dividing by 24. When you gave analogy to clock, I thought it is a standard 12-hour clock. However you are taking 24-hr clock. It seems to be the best way for me to transform. Thank You!" CreationDate="2015-07-06T07:28:18.883" UserId="8338" />
  <row Id="6748" PostId="6348" Score="0" Text="Thanks @twalbaum! Exactly what I needed!" CreationDate="2015-07-06T07:42:01.357" UserId="9225" />
  <row Id="6750" PostId="6360" Score="2" Text="Are you affiliated with this company @henry.oswald?" CreationDate="2015-07-06T21:28:24.317" UserId="21" />
  <row Id="6752" PostId="6369" Score="0" Text="SE discourages link-only answers -- summarize the content that is relevant to the answer?" CreationDate="2015-07-07T08:23:10.430" UserId="21" />
  <row Id="6753" PostId="6360" Score="0" Text="Yes, sorry I should have made that clear. Have now." CreationDate="2015-07-07T10:33:04.977" UserId="10509" />
  <row Id="6755" PostId="6374" Score="0" Text="What's not available in Orange?&#xA;I actually tried around other libs with Apriori and FP-Growth and indeed you're right, FPG is way faster but the results look a bit different (maybe just because of the lib I used) but still good. I'd go that way I guess." CreationDate="2015-07-07T15:29:02.187" UserId="10513" />
  <row Id="6756" PostId="6133" Score="0" Text="@image_doctor, thanks for the explanation. Could you please advice me whether cost matrix possible in random forest? I use R. Thanks in advance." CreationDate="2015-07-07T17:07:22.577" UserId="9793" />
  <row Id="6758" PostId="694" Score="3" Text="And now there's a new contender - [Scikit Neuralnetwork](http://scikit-neuralnetwork.readthedocs.org/en/latest/): Has anyone had experience with this yet? How does it compare with Pylearn2 or Theano?" CreationDate="2015-07-07T12:20:57.617" UserId="10519" />
  <row Id="6759" PostId="6144" Score="0" Text="Why would you want to work with data that hasn't been standardized?  I'm not familiar enough with hidden markov, but there are so many pitfalls to not scaling, it is so easy to do, and it is reversible, so why would you not just scale the data and move on?" CreationDate="2015-07-07T21:30:21.050" UserId="9420" />
  <row Id="6760" PostId="6377" Score="0" Text="Thanks for the tip, I'll check it out! It looks like it'll help. The difficulty is going to be in defining a good metric for &quot;similarity of response to inputs&quot; in any case." CreationDate="2015-07-07T23:02:37.607" UserId="10427" />
  <row Id="6761" PostId="6363" Score="0" Text="Thanks, what you have said makes sense. The questions are looking for me to fake a data set by taking random numbers off relevant distributions with real/related data from real data sets. I had a hard time figuring out what the end result should be, but a dataset makes sense." CreationDate="2015-07-07T23:11:57.310" UserId="10510" />
  <row Id="6762" PostId="6379" Score="0" Text="This has been asked before in StackOverflow. Your answer is here: http://stackoverflow.com/questions/11053899/how-to-get-a-reversed-log10-scale-in-ggplot2" CreationDate="2015-07-08T05:07:40.883" UserId="609" />
  <row Id="6763" PostId="6374" Score="0" Text="for all I know FP-Growth is not available in Orange. The results should be the same for same input data when you use same min-support / min-confidence, but like you said, different libs are using different variants of apriori / FP-growth for better performance." CreationDate="2015-07-08T08:31:47.937" UserId="10521" />
  <row Id="6764" PostId="6361" Score="0" Text="I suggest that you read books and tutorials on process modeling (deterministic and stochastic).  Too many people who are new to data science assume that they can produce good/working models without knowing anything about the underlying process.  Sometimes this is so, but many times it is not. The *last* thing you should think about his how to implement in R." CreationDate="2015-07-08T08:55:17.250" UserId="609" />
  <row Id="6765" PostId="6361" Score="0" Text="By analogy, it is though you have learned optics and now you assume that you can do astronomy using optics alone. No, you can't. *PLEASE* learn how to model processes: queues (FIFO), stacks (LIFO), serial, parallel, branching, critical path, and associated decision processes." CreationDate="2015-07-08T09:00:19.807" UserId="609" />
  <row Id="6767" PostId="6370" Score="1" Text="You are confusing two completely different types of systems.  A REST API (or API of any sort) applies to a software library that you call inside your code (Java, C++, Python, or whatever).  Separately, there are systems that perform Map-Reduce on data stored in HDFS (e.g. Hadoop).  Typically, there are intermediary systems (e.g. Hive) that interpret your query and execute it on the designated infrastructure (e.g. Hadoop).  I suggest that you read/watch tutorials on Hadoop/Hive (or similar), and don't think about REST API in that context." CreationDate="2015-07-08T09:10:40.023" UserId="609" />
  <row Id="6769" PostId="6361" Score="0" Text="Thanks. What you are saying makes sense. R is designed to be used by data analysts/scientists, exclusively, and therefore it is important to know these topics before considering learning r." CreationDate="2015-07-08T13:31:48.927" UserId="10510" />
  <row Id="6770" PostId="6361" Score="0" Text="Thanks for the process modeling tip. I agree with your sentiment that learning data science is quite involved." CreationDate="2015-07-08T13:46:42.840" UserId="10510" />
  <row Id="6772" PostId="6385" Score="0" Text="Thanks for the specific references.  I learned a lot from them." CreationDate="2015-07-08T15:06:40.703" UserId="10531" />
  <row Id="6773" PostId="6390" Score="0" Text="Thank you for addressing the &quot;output&quot; question.  One thing though:  *&quot;You could decide to project the observations into the first two principal components&quot;*  Is that the same thing as step 5 in your quoted section?  The eigenvectors of the covariance matrix and the principal components are the same, are they not?" CreationDate="2015-07-08T15:08:57.303" UserId="10531" />
  <row Id="6774" PostId="6390" Score="0" Text="Also, that step by step example article you linked is *really* excellent." CreationDate="2015-07-08T15:10:23.720" UserId="10531" />
  <row Id="6775" PostId="6144" Score="1" Text="Upon further reflection of my question, scaling the data seemed like the right thing to do.  Thank you for confirming my thought process." CreationDate="2015-07-08T15:42:52.877" UserId="10187" />
  <row Id="6776" PostId="6390" Score="1" Text="Thanks for pointing that out, I failed to realize my suggestion did overlap with the given example. I've edited my answer to include further steps or alternatives to the example." CreationDate="2015-07-08T16:23:59.703" UserId="10536" />
  <row Id="6779" PostId="253" Score="0" Text="It is not absolutely necessary. It is just one of the tools. What is necessary is an understanding of statistics and linear algebra. The choice of tool is secondary." CreationDate="2015-07-09T02:01:40.770" UserId="10522" />
  <row Id="6780" PostId="6379" Score="0" Text="And so it has.  Thanks." CreationDate="2015-07-09T05:49:38.633" UserId="916" />
  <row Id="6781" PostId="694" Score="0" Text="There is also Keras - https://github.com/fchollet/keras - which is relatively recent. The problems with tracking &quot;best&quot; by any measure, and keeping the Q&amp;A valid over time is why this sort of question is usually off topic in other Stack Exchange networks." CreationDate="2015-07-09T08:37:25.873" UserId="836" />
  <row Id="6782" PostId="6408" Score="0" Text="I edited my findings into the question, I'm not sure if it notifies you of that or not." CreationDate="2015-07-09T12:50:07.247" UserId="10560" />
  <row Id="6783" PostId="6408" Score="1" Text="Maybe use some &quot;optimism bias&quot;. Initialize all Q-values above any reasonable estimate of their long run value. Then states that haven't been tried will always have Q-values above those that have and this encourages exploration." CreationDate="2015-07-09T13:11:34.297" UserId="10568" />
  <row Id="6784" PostId="6375" Score="0" Text="what are your feature vectors right now?" CreationDate="2015-07-09T17:38:14.180" UserId="10587" />
  <row Id="6785" PostId="6401" Score="0" Text="mrjob seems to be the one I'm looking for.Thanks!" CreationDate="2015-07-09T20:22:24.340" UserId="10327" />
  <row Id="6786" PostId="6408" Score="1" Text="I tried initializing the Q-values a couple different ways with no success. Let me see if I'm understanding the update process correctly: ∀ $i≤ t ∈ T$, ∀ $x_i$, $a_i$ Update all Q-Values according to their eligibility traces&#xA;&#xA;$Q_t^{k+1}(x_i, a_i) ← Q_i^k(x_i, a_i) + α(x_i^k,a_i^k)δ_t^ke_t^k(x_i,a_i)$" CreationDate="2015-07-10T01:06:29.527" UserId="10560" />
  <row Id="6787" PostId="6408" Score="0" Text="So essentially what this is saying is that the Q value for the time period that I'm in during the next episode will be equal to the current Q value + learning rate * TD error * eligibility trace for all state-action pairs that I've visited during this episode, correct?" CreationDate="2015-07-10T01:13:11.537" UserId="10560" />
  <row Id="6788" PostId="6375" Score="0" Text="My feature vectors are composed of numerical values acquired by dense optical flow (from OpenCV). Basically what happens is that there is a 17 frame video, and each frame is composed of 136 points used in dense optical flow. For each frame, the 136 point vector constantly concatenates with each other until the 17th frame to form a single 2312 sized vector. If there were to be a 2nd video, there will be another 2312 sized vector in the 2nd row." CreationDate="2015-07-10T02:56:48.723" UserId="10523" />
  <row Id="6789" PostId="6391" Score="2" Text="Here is an interesting article that relates to your question. http://labs.eeb.utoronto.ca/jackson/ecol.%20modelling%20ANN.pdf" CreationDate="2015-07-10T04:06:19.367" UserId="609" />
  <row Id="6790" PostId="6412" Score="0" Text="Thanks for your attention. I will try what you suggested and will get back to you." CreationDate="2015-07-10T06:11:04.710" UserId="9323" />
  <row Id="6791" PostId="694" Score="0" Text="Does any of this packages scale like h2o deep learning?&#xA;As far as I know lasagne doesn't.Theano does support GPU so as any library basing on it,but does any of them support mapreduce or spark." CreationDate="2015-07-09T21:20:08.200" UserId="10327" />
  <row Id="6792" PostId="6225" Score="0" Text="By the way, I am finding more than a thousand categories from Facebook." CreationDate="2015-07-10T09:36:09.710" UserId="982" />
  <row Id="6793" PostId="6366" Score="0" Text="I need an suggestion, if we use this logs processing as web service for end user like my employee wants to give 100-200 gb for processing so Can I provide web service through which he/she can access this ??" CreationDate="2015-07-10T10:13:55.187" UserId="10210" />
  <row Id="6794" PostId="6391" Score="0" Text="Thanks for the comment, @MrMeritology! I found that really useful!" CreationDate="2015-07-10T11:25:26.440" UserId="9225" />
  <row Id="6795" PostId="6421" Score="0" Text="I'm mainly interested in the area of deep learning.My company can't put their data on EC2 due to security reason hence GPU isn't an option for me.I use h2o deep learning on clusters,however it lacks some feature I think,such as setting learning rate for different layers.Since it is written in Java and my java is bit rusty  I was looking for any open source scalable deep learning written in python.Yes pyspark gives access to clusters,but even if I build something on it'll work on top of scala layer and not on native python code.Hence I am on a search for an ongoing python project." CreationDate="2015-07-10T11:36:24.000" UserId="10327" />
  <row Id="6796" PostId="6374" Score="0" Text="Yes, I see. I tried several libs now, FP-Growth as well and it's indeed faster. Still playing around with different libs - Orange seems not that awesome to me after I wrapped my head more around it." CreationDate="2015-07-10T13:23:54.443" UserId="10513" />
  <row Id="6797" PostId="6375" Score="0" Text="each &quot;point&quot; is and (x, y), yes? so your dimensionality is 2312*2 = 4624? what classification algos are you using now?" CreationDate="2015-07-10T15:09:00.657" UserId="10587" />
  <row Id="6798" PostId="6412" Score="0" Text="Great. If you think that has answered your question, please mark at as such. In any case, best of luck." CreationDate="2015-07-10T15:09:50.777" UserId="10587" />
  <row Id="6799" PostId="6424" Score="0" Text="Looks like what I'm trying to do. Thanks for the tip!" CreationDate="2015-07-10T15:53:44.027" UserId="10596" />
  <row Id="6800" PostId="6412" Score="0" Text="I think, I should ensemble SMO with CART according to your suggestion. I edited my question." CreationDate="2015-07-10T19:48:35.977" UserId="9323" />
  <row Id="6801" PostId="6412" Score="0" Text="Few things: 1) Why not try each combination of three algorithms you have?, 2) for that matter, why aren't you adding other algorithms? Random Forest, KNN come to mind as strong for difficult problems, 3) At some point stacking will not help - you'll need more examples, better features, and hyperparameter tuning. That's going beyond scope of this post and more than someone can answer in a comment. Stacking, unfortunately, is not an solution in itself - merely an augmentation to the basics." CreationDate="2015-07-10T22:29:31.323" UserId="10587" />
  <row Id="6802" PostId="6412" Score="0" Text="Actually these are the algorithms which I had the best results and I tried each combination of them. I didn't want to just say I tried the everything and these are the best results. I wanted to add some logical explanations, like I select Logistic Regression because it have ... properties, which SMO doesn't have, to improve SMO. Thanks for your answer." CreationDate="2015-07-10T22:42:03.117" UserId="9323" />
  <row Id="6803" PostId="6414" Score="0" Text="Define scalable. What exact requirements do you have? Do you only care about feeding data into models to get predictions or is training them a concern as well? I also wouldn't make the assertion that sci-kit isn't scalable, it most certainly is for quite a few (if not most) cases." CreationDate="2015-07-10T23:52:41.367" UserId="947" />
  <row Id="6804" PostId="6391" Score="0" Text="While I'm sure you you'll be able to understand this (pretty simple) neural network, if interpretability is a relatively large concern then you probably shouldn't be using a neural network in the first place. Is there a specific reason you selected one over other algorithms?" CreationDate="2015-07-11T00:06:31.173" UserId="947" />
  <row Id="6805" PostId="6398" Score="0" Text="I suggest taking a MOOC to learn a little about model tuning and assessment. Two options are Andrew Ng's ML Coursera course or the tutorials on Kaggle.  But this question is too green and vague to answer with anything other than a tutorial on the basics of ML." CreationDate="2015-07-11T00:33:34.890" UserId="9420" />
  <row Id="6806" PostId="6414" Score="0" Text="@David well, some of the scikit learn models such as random forest can run in parallel and can use all the cores in a node,but can't extend beyond that.To me scalability means the capacity to run on multiple nodes.Also I think it's the training that matters as that consumes most memory(specially in the case of text mining)." CreationDate="2015-07-11T02:10:29.980" UserId="10327" />
  <row Id="6807" PostId="694" Score="0" Text="h2o doesn't even use the GPU yet so it's hardly scalable." CreationDate="2015-07-11T06:51:22.540" UserId="381" />
  <row Id="6808" PostId="6391" Score="0" Text="Yes, @David! I would like to learn using this kind of model. I never used them in my job and I'm studying that just for fun. Do you have any idea about interpreting the plot?" CreationDate="2015-07-11T09:27:26.707" UserId="9225" />
  <row Id="6809" PostId="6433" Score="0" Text="Please be a little more specific about what exactly you mean by &quot;connect&quot;.  Do you mean that you want to train all 3 NNs separately and then combine their result on any input data to get the result ? If so take a look at [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning)." CreationDate="2015-07-11T14:38:32.473" UserId="10446" />
  <row Id="6810" PostId="6381" Score="0" Text="Thank you for this considered reply - especially the 'Stepping back...' paragraph.  I'm aware that various operations on time series constitute groups. The rub is &quot;finding other time series&quot;. Rather than rely upon serendipity, I'd like to establish how to find these &quot;other time series&quot;  by constructing candidates systematically.  I expect the effective strategies to be domain-specific.  I am focused on time domain over frequency domain approaches because the time series which interest me most, right now, are finite length and non-cyclical." CreationDate="2015-07-11T15:47:36.983" UserId="3328" />
  <row Id="6811" PostId="6414" Score="1" Text="Ok, that's helpful information. Scalable typically refers to being able to handle a large amount of input data for making predictions, since training is generally a one time cost and not performed on that much data (relatively speaking). It seems like you're really interested in using a natively distributed system, which spark makes a lot of sense for." CreationDate="2015-07-11T16:04:29.477" UserId="947" />
  <row Id="6813" PostId="6431" Score="0" Text="Thanks for your reply and suggestions. I will try to provide a good sample in near future." CreationDate="2015-07-11T19:36:54.190" UserId="10318" />
  <row Id="6814" PostId="6433" Score="0" Text="yes, that is what I mean" CreationDate="2015-07-11T19:48:24.643" UserId="10584" />
  <row Id="6815" PostId="6434" Score="0" Text="Neither GD or direct solve is feasible as it would be take too many computational resources when training a convolutional neural network (CNN). While it therefore did not solve my problem, your answer provided a nice insight to the issue." CreationDate="2015-07-11T22:18:16.937" UserId="3044" />
  <row Id="6816" PostId="6434" Score="0" Text="Wouldn't GD also be affected by outliers, although to a lesser degree?" CreationDate="2015-07-11T22:22:06.347" UserId="3044" />
  <row Id="6817" PostId="6366" Score="0" Text="It sounds like you need to develop some sort of web service by yourself. Otherwise if you have budget, try some SaaS based data platform like [Databricks](https://www.databricks.com/), they are the new kid in town but I have heard great things about them." CreationDate="2015-07-11T22:27:26.303" UserId="708" />
  <row Id="6818" PostId="6434" Score="0" Text="Affected by, yes, but to a lesser degree i.e. less stochastic.  In addition to just turning down the learning coefficient, you can prime lots of descenders and use the most common minima." CreationDate="2015-07-12T00:32:55.410" UserId="9420" />
  <row Id="6819" PostId="6434" Score="0" Text="How can you simultaneously have a sparse dataset for which removing outliers isn't feasible and a problem with so much data that resources are tight and SGD is needed? Is it that you have a non normal dataset? Then try some normalizing transformations like log, square root, or inverse hyperbolic tangent." CreationDate="2015-07-12T02:15:30.597" UserId="9420" />
  <row Id="6820" PostId="6432" Score="0" Text="thanks that explanation makes sense. Yes I am aware that machine learning algorithms do not always produce output which is in line with actual probabilities which is why I use a calibration plot on an independent data set to assess the quality of the output &quot;probabilities&quot; versus observed events." CreationDate="2015-07-12T06:09:06.797" UserId="2817" />
  <row Id="6821" PostId="6434" Score="0" Text="The ML course doesn't cover mini-batches, which would help here as a compromised between fully online SGD and full batch learning. Also using dropout for regularisation may help." CreationDate="2015-07-12T19:35:12.177" UserId="836" />
  <row Id="6822" PostId="5375" Score="0" Text="Related: [How hard is debugging with Theano?](https://www.quora.com/How-hard-is-debugging-with-Theano)" CreationDate="2015-07-12T19:49:09.137" UserId="843" />
  <row Id="6823" PostId="6378" Score="0" Text="you need to give input on how did you do it and what are the number of data elements to help others offer help." CreationDate="2015-07-13T01:48:34.840" UserId="10638" />
  <row Id="6824" PostId="253" Score="0" Text="Look at this free e-book and it tries to answer your question.http://www.oreilly.com/data/free/files/analyzing-the-analyzers.pdf" CreationDate="2015-07-13T01:49:48.203" UserId="10638" />
  <row Id="6825" PostId="6446" Score="1" Text="does this not belong on stack overflow?" CreationDate="2015-07-13T13:37:51.517" UserId="8953" />
  <row Id="6826" PostId="694" Score="1" Text="@Emre: Scalable is different to high performance. It typically means you can solve larger problems by adding more resources of the same type you have already. Scalability still wins out, when you have 100 machines available, even if your software is 20 times slower on each of them . . . (although I'd rather pay the price for 5 machines and have benefits of both GPU and multi-machine scale)." CreationDate="2015-07-13T15:25:45.183" UserId="836" />
  <row Id="6827" PostId="6095" Score="0" Text="This is true for your test set, but not training. Oversampling is necessary for problems like these since the vast majority of instances will not be fraud." CreationDate="2015-07-13T15:32:04.297" UserId="947" />
  <row Id="6829" PostId="694" Score="1" Text="So use multiple GPUs...nobody uses CPUs for serious work in neural networks. If you can get Google-level performance out of a good GPU or two, just what are you going to do with a thousand CPUs?" CreationDate="2015-07-13T16:11:09.433" UserId="381" />
  <row Id="6830" PostId="6217" Score="0" Text="Do you know anything about the nonlinearity of the model?  Though it may be too complex to simulate, knowing that it is at most made up of degree 3 polynomials restricts the feature engineering significantly e.g. you could add all 3rd degree polys and then PCA it back down to 3D." CreationDate="2015-07-13T16:20:07.693" UserId="9420" />
  <row Id="6831" PostId="6438" Score="0" Text="I think the question needs a bit of clarification. Do I understand correctly, that you give the user a set of polygons that can be transformed with a simple transformation. Now, is for each of the polygons only a set of outputs possible, or is there a global space defined as legal transformation outcomes?" CreationDate="2015-07-13T17:25:00.237" UserId="10655" />
  <row Id="6832" PostId="6447" Score="0" Text="That should work! are you sure there's no typo, say in `newdf`?" CreationDate="2015-07-13T17:43:10.307" UserId="5055" />
  <row Id="6833" PostId="6447" Score="0" Text="@EhsanM.Kermani - Thanks for the reply, My code is similar one, as i could not copy that from my office network, i pasted this one. I did similar to this." CreationDate="2015-07-13T17:50:56.920" UserId="9793" />
  <row Id="6834" PostId="6447" Score="0" Text="This is strange, b/c when you have the coefficients in this particular case then prediction is a simple algebra, but returning 'NA's should indicate a problem with new data not the loaded model!" CreationDate="2015-07-13T18:04:10.193" UserId="5055" />
  <row Id="6835" PostId="6447" Score="0" Text="@EhsanM.Kermani - Thanks again. The model works if i use directly in the same programme. (means, build model and predict immediately for a new data). Where as I get into this problem only when i save the model, load again and then use it for prediction." CreationDate="2015-07-13T18:48:20.063" UserId="9793" />
  <row Id="6836" PostId="6217" Score="0" Text="I have discussed with a statistician friend of mine who suggested using kernel PCA on the **derivative** of my data, since I'm looking for slopes. Would taking the derivative count as &quot;feature engineering&quot;?" CreationDate="2015-07-13T18:49:26.167" UserId="10346" />
  <row Id="6837" PostId="6446" Score="0" Text="@aeroNotAuto - I apologize if i had posted in wrong place. I thought confusionMatrix is something related to data science and hence posted here." CreationDate="2015-07-13T19:21:52.870" UserId="9793" />
  <row Id="6838" PostId="6414" Score="0" Text="While checking on python xgboost I found the existence of this open source project https://github.com/dmlc/rabit that helps create scalable machine learning program.Should be worth exploring." CreationDate="2015-07-14T06:12:25.237" UserId="10327" />
  <row Id="6840" PostId="6447" Score="0" Text="@EhsanM.Kermani - as you said, this has worked. The issue which was preventing from working is, one of the old models which was still in the R's Cache memory. I have removed that using rm=list(ls)) which has solved the problem. Thanks you very much for your help." CreationDate="2015-07-14T08:41:55.277" UserId="9793" />
  <row Id="6841" PostId="6088" Score="0" Text="Have you seen this question:https://datascience.stackexchange.com/questions/6200/what-are-the-basic-approaches-for-balancing-a-dataset-for-machine-learning/6249#6249" CreationDate="2015-07-14T08:44:52.227" UserId="7980" />
  <row Id="6842" PostId="6088" Score="0" Text="You might also consider layering a cost-matrix over your classification algorithm, as there is an imbalance in the penalty for making different errors in classification." CreationDate="2015-07-14T08:47:24.423" UserId="7980" />
  <row Id="6843" PostId="6088" Score="0" Text="Is there a particular reason you want to use Adaboost in this context ?" CreationDate="2015-07-14T08:48:42.773" UserId="7980" />
  <row Id="6845" PostId="6453" Score="0" Text="Have you considered using machine learning to calculate the variable directly from the observables ?" CreationDate="2015-07-14T14:12:24.287" UserId="7980" />
  <row Id="6846" PostId="6359" Score="0" Text="Outliers would be points that in deed don't have correspondence in all datasets. Still I was looking for a way to constrain the clustering for enforcing memberships from all datasets, which in case of the outliers would be too costly. I agree that using a fuzzy clustering might be a good idea, because defining cluster membership could then be assigned using some trade off criterium which on this postprocessing level might be easier to formulate. Thanks!" CreationDate="2015-07-14T14:17:08.993" UserId="10463" />
  <row Id="6847" PostId="6447" Score="0" Text="Glad you solved the issue :) cheers" CreationDate="2015-07-14T14:17:26.613" UserId="5055" />
  <row Id="6848" PostId="6453" Score="0" Text="I haven't but it is also not possible as I don't have the possibility to change the calculation." CreationDate="2015-07-14T14:19:57.587" UserId="10669" />
  <row Id="6849" PostId="6453" Score="0" Text="So is it the case you have , observables, true output and error available? You could play around and see if you can get a better result with smaller errors  than the existing  &quot;calculation&quot;. Failing that you can use machine learning to build a mapping directly between observables and error. Without any idea of the form of the data and errors it's a challenge to be more precise." CreationDate="2015-07-14T15:02:20.557" UserId="7980" />
  <row Id="6851" PostId="6208" Score="0" Text="Add a noise term, and model it to account for the types of errors you observe." CreationDate="2015-07-14T21:19:03.333" UserId="381" />
  <row Id="6852" PostId="6459" Score="3" Text="I think you are *grossly* underestimating the challenge of AI music composition, and you are trying to take on music recognition at the same time.  I also think you (and many other people) have an inflated opinion of what ML (including recurrent neural networks) can do." CreationDate="2015-07-15T05:27:45.877" UserId="609" />
  <row Id="6854" PostId="6459" Score="0" Text="Have you considered how you will deal with instruments that produce cords, such as piano, guitar and other stringed instruments?" CreationDate="2015-07-15T10:10:26.687" UserId="7980" />
  <row Id="6856" PostId="6456" Score="0" Text="I note that your probabilities do not sum to one, is that the case ?" CreationDate="2015-07-15T10:21:55.870" UserId="7980" />
  <row Id="6857" PostId="6462" Score="0" Text="create a regression where a, b, and c predict x for every state, then apply it to every zip code.  but your results will surely be biased by whatever differences there are in the relationship between these variables at the state vs local levels." CreationDate="2015-07-15T11:31:48.000" UserId="9924" />
  <row Id="6858" PostId="6453" Score="0" Text="I have the observables. Then the output is calculated in a &quot;black box&quot;. The output is not exact. It has an error. I have training data which has true error data. Now I want to find the relation between the input observables and the error of the output. It's really not about the &quot;output&quot;, it's about the error." CreationDate="2015-07-15T12:30:52.273" UserId="10669" />
  <row Id="6859" PostId="6459" Score="0" Text="[Deep Dreaming](https://photos.google.com/share/AF1QipPX0SCl7OzWilt9LnuQliattX4OUCj_8EP65_cTVnBmS1jnYgsGQAieQUc1VQWdgQ?key=aVBxWjhwSzg2RjJWLWRuVFBBZEN1d205bUdEMnhB) for audio?" CreationDate="2015-07-15T12:39:43.553" UserId="836" />
  <row Id="6860" PostId="6464" Score="0" Text="Can you clarify what you're asking? what have you tried and what is the result? what's the problem?" CreationDate="2015-07-15T14:57:37.437" UserId="21" />
  <row Id="6861" PostId="6464" Score="0" Text="There is no problem except this is my first task in machine learning. I just want to ensure that I have choosen right algorithm and, for example, should I normalize features? Also I am still confused whether it is correct to combine categorical and continuous features." CreationDate="2015-07-15T15:12:14.460" UserId="10694" />
  <row Id="6862" PostId="6453" Score="0" Text="Just to be clear, if you could make another black box, without error, that wouldn't be of use ?" CreationDate="2015-07-15T15:42:32.453" UserId="7980" />
  <row Id="6863" PostId="6456" Score="0" Text="yes, maybe probabilities isn't the word I should use, frequencies is a better description of the numbers." CreationDate="2015-07-15T16:09:57.470" UserId="10584" />
  <row Id="6864" PostId="6456" Score="0" Text="I wasn't certain if you had automated the generation of the training data by sampling from a distribution based on the relative frequencies of your data. That approach is nicely general in that it will work with any machine learning algorithm , some of which may perform better than a neural network. You could build your own neural network code which used relative frequencies, but that seems like hard work when resampling will do the job for you. Of course the number of training epochs affects how many times samples are seen, but normally they apply equally to all samples." CreationDate="2015-07-15T16:34:34.737" UserId="7980" />
  <row Id="6866" PostId="6459" Score="1" Text="This is such a tough problem that you'll have to do a lot of paper sighting. I don't think it's possible to sum everything up that you need to know in a few paragraphs (since there are no established best practices)" CreationDate="2015-07-15T17:22:22.137" UserId="5316" />
  <row Id="6870" PostId="6468" Score="0" Text="What you say is true: In the second case I am more confident that the classification is C1 or C2 however I am less confident that is C1 (because it is almost equally probable that it is C2).  I have added another example into the main question to elaborate this situation more clearly." CreationDate="2015-07-15T18:55:35.497" UserId="10701" />
  <row Id="6874" PostId="6472" Score="0" Text="Try shift+enter." CreationDate="2015-07-15T22:17:11.193" UserId="381" />
  <row Id="6875" PostId="6467" Score="0" Text="[Confidence intervals for cross-validated statistics](http://stats.stackexchange.com/questions/69831/confidence-intervals-for-cross-validated-statistics)" CreationDate="2015-07-15T22:19:45.947" UserId="381" />
  <row Id="6876" PostId="6257" Score="1" Text="This is a studied problem: [tag recommendation](https://scholar.google.com/scholar?q=tag+recommendation)." CreationDate="2015-07-15T22:31:28.813" UserId="381" />
  <row Id="6878" PostId="6473" Score="0" Text="That is great , tnks!" CreationDate="2015-07-16T07:26:45.977" UserId="10094" />
  <row Id="6879" PostId="6479" Score="0" Text="I looked in the tag-list for something a little more descriptive - surely part of data-science includes finding and making use of publically (or commercially available) data sources - sharing and collaborating on how/where to find such sources, at least to me, would seem to be a common type of question - befitting a tag of its own!" CreationDate="2015-07-16T12:36:24.750" UserId="10399" />
  <row Id="6880" PostId="6479" Score="0" Text="Your question should probably be migrated to  http://opendata.stackexchange.com/ which is designed for exactly  what you need." CreationDate="2015-07-16T12:48:29.543" UserId="7980" />
  <row Id="6886" PostId="6476" Score="1" Text="I'd link to the paper and elaborate what you are asking for here." CreationDate="2015-07-16T20:56:37.950" UserId="21" />
  <row Id="6887" PostId="6462" Score="0" Text="The method you have suggested seems basically right, but you are likely loosing a lot of information during the aggregation and gaining a lot of noise as you scale back down to the zip level.  Since projecting from state down to zip is a big jump, maybe you should try simply leaving one zip out.  e.g. For `n` zip codes per state regress on `n-1` zip codes and on `n` zip codes. Then the difference in the `n` and `n-1` target variable will be the target variable for the zip code that was left out.  This way the regression will be closer to the region it was trained on." CreationDate="2015-07-16T21:45:25.930" UserId="9420" />
  <row Id="6888" PostId="6462" Score="0" Text="This will also ensure that the aggregate totals are equal to the state totals, so effectively constrains the problem." CreationDate="2015-07-16T21:55:43.623" UserId="9420" />
  <row Id="6889" PostId="6471" Score="0" Text="Yep, I think this the correct direction.  The information gain is a nice measure of &quot;purity&quot; of the choice, which is what I was looking for in &quot;confidence&quot;.  Thanks." CreationDate="2015-07-16T23:03:18.977" UserId="10701" />
  <row Id="6890" PostId="266" Score="0" Text="In addition to learning Machine Learning by Andrew Ng you can try with some courses in data science signature track in kaggle.Also a quick way to learn practical machine learning is to take part in following machine learning competition at kaggle,as that has nice guide material on how to do feature selection,data munging and building final model in R and in Python.https://www.kaggle.com/c/titanic/details/getting-started-with-python" CreationDate="2015-07-17T00:49:46.610" UserId="10327" />
  <row Id="6891" PostId="6479" Score="0" Text="Thanks, yes looking at it, opendata is probably the place to go - what's the protocol for doing this, do I take this one down and post it back up in the other place, or wait for a mod to do it?" CreationDate="2015-07-17T07:54:45.103" UserId="10399" />
  <row Id="6892" PostId="6479" Score="0" Text="Linked to [here](http://opendata.stackexchange.com/questions/5640/where-can-i-get-a-list-of-atm-identifiers-that-i-can-map-to-geographic-location)" CreationDate="2015-07-17T08:00:23.097" UserId="10399" />
  <row Id="6893" PostId="6479" Score="0" Text="If you are lucky a nice moderator will migrate it :)" CreationDate="2015-07-17T10:01:54.850" UserId="7980" />
  <row Id="6894" PostId="6484" Score="0" Text="I see that they are trying to get rules out of minimal episodes but if given a event stream like 'G,E,A,A,B,D,H,G,F,D,G,F,C,F,D,H,H,H,G,F,D,G,E,B,D,H,H,H,G,F,C'  where each event occurs with 1 time unit gap then what are the possible episodes in this stream?" CreationDate="2015-07-17T10:55:36.107" UserId="10717" />
  <row Id="6896" PostId="6484" Score="0" Text="Then this doesn't fit within the definition of finding episodes as the events in an episode do not occur at regular frequency e.g. that is the definition of an episode.  The paper you referenced specifically has inhomogeneous event frequency.  It looks like a sequencing problem and you should employ sequencing methods." CreationDate="2015-07-17T17:12:00.150" UserId="9420" />
  <row Id="6897" PostId="6493" Score="0" Text="this is such a great answer. thank you" CreationDate="2015-07-17T19:48:53.047" UserId="10761" />
  <row Id="6900" PostId="337" Score="1" Text="&quot;R has a better community for [...] learning&quot; - I guess this highly depends on the type of learning. How much is going on with neural networks (arbitrary feed-forward architectures, CNNs, RNNs) in R?" CreationDate="2015-07-19T14:41:19.090" UserId="8820" />
  <row Id="6901" PostId="6498" Score="0" Text="Can you give an example showing how using XML would affect the annotations?" CreationDate="2015-07-20T05:22:12.170" UserId="843" />
  <row Id="6904" PostId="6508" Score="1" Text="To answer your question about the knee in the variance case, it looks like it's around 6 or 7, you can imagine it as the break point between two linear approximating segments to the curve . The shape of the graph is not unusual, % variance will often asymptotically approach 100%. I'd put k in your BIC graph as a little lower , around 5." CreationDate="2015-07-20T15:12:26.903" UserId="7980" />
  <row Id="6905" PostId="6510" Score="0" Text="Questions explicitly seeking opinions are generally discouraged on stack exchange sites, http://datascience.stackexchange.com/help/dont-ask , perhaps you could rephrase the question to require exemplars in support of users experience ? Or seek a theoretical basis for one position or the other." CreationDate="2015-07-20T15:18:49.560" UserId="7980" />
  <row Id="6906" PostId="6510" Score="1" Text="Random Forests are less likely to overfit the other ML algorithms, but cross-validation (or some alternatively hold-out form of evaluation) should still be recommended." CreationDate="2015-07-20T15:53:07.157" UserId="947" />
  <row Id="6907" PostId="6510" Score="0" Text="I think you sholud ask that question on statistician SO: http://stats.stackexchange.com/" CreationDate="2015-07-20T16:01:12.860" UserId="5224" />
  <row Id="6908" PostId="6511" Score="0" Text="What is cool or boring for you ?" CreationDate="2015-07-20T16:02:12.503" UserId="7980" />
  <row Id="6909" PostId="6518" Score="0" Text="Thanks. I was thinking of R. Any reason for python over R?" CreationDate="2015-07-20T17:42:12.913" UserId="10522" />
  <row Id="6910" PostId="6518" Score="1" Text="No specific reason for python. R vs. Python is a hotly debated topic.  R has been around for a long time so has tons of packages.  Python is more user friendly and is the future of data science.  Python will make you happier and will help you get things done faster.  If you really need R then you can call R from within Python.  Someone will probably respond and tell you why R is best, but I like python.  Take a look at all of the following: pandas, scikit-learn, numpy, matplotlib, ipython notebook.  There's a great book &quot;Python for Data Analysis&quot; and the scikit learn user guide is fantastic." CreationDate="2015-07-20T17:50:59.883" UserId="9420" />
  <row Id="6911" PostId="799" Score="0" Text="Took me an year, but I decided to accept this answer. I still don't know what that bitstream is, but I probably won't find out. It does have a nice pattern, though!" CreationDate="2015-07-20T20:12:03.117" UserId="2604" />
  <row Id="6912" PostId="6510" Score="0" Text="I would like to second @David...one way or another, you're going to be doing cross validation." CreationDate="2015-07-20T20:37:23.120" UserDisplayName="user9424" />
  <row Id="6914" PostId="6522" Score="0" Text="Thank you. That's exactly what I was looking for." CreationDate="2015-07-20T21:18:11.153" UserId="10799" />
  <row Id="6915" PostId="6508" Score="0" Text="but I should have (more or less) the same results in all the methods, right?" CreationDate="2015-07-21T08:38:30.263" UserId="989" />
  <row Id="6916" PostId="6508" Score="0" Text="I don't think I know enough to say. I doubt very much that  the three methods are mathematically equivalent with all data, otherwise they wouldn't exist as distinct techniques, so the comparative results are data dependent. Two of the methods give numbers of clusters that are close, the third is higher but not enormously so. Do you have a priori information about the true number of clusters ?" CreationDate="2015-07-21T08:59:10.210" UserId="7980" />
  <row Id="6917" PostId="6529" Score="0" Text="Thanks image_doctor! This is quite a help." CreationDate="2015-07-21T10:02:12.903" UserId="10810" />
  <row Id="6918" PostId="6508" Score="0" Text="I'm not 100% sure but I expect to have from 8 to 10 clusters" CreationDate="2015-07-21T10:29:22.837" UserId="989" />
  <row Id="6919" PostId="6518" Score="0" Text="And I thought Julia was the future of data science ;)" CreationDate="2015-07-21T12:44:16.980" UserId="7980" />
  <row Id="6920" PostId="4876" Score="0" Text="This is a good way to show your results.&#xA;But sometimes, if you want to calculate many things it's better to store the table in &quot;long format&quot;.  Google it, long and wide format.  You can use cast, melt, reshape..." CreationDate="2015-07-21T15:55:01.177" UserId="10815" />
  <row Id="6921" PostId="6531" Score="0" Text="Please add some additional information to your question in order to elicit a better response and avoid being down voted.  In particular 1) how big is your dataset? 2) How many features do you have? 3) How many test cases do you have?  Have you searched for other sources for this information?  Simply googling &quot;k-means clustering python&quot; returns the result that most people will steer you towards initially: `mini-batch k-means` using `scikit-learn`.  If you need better parallel scalability then we can steer you toward another package based on the answer to 1,2, and 3." CreationDate="2015-07-21T17:44:29.127" UserId="9420" />
  <row Id="6922" PostId="6529" Score="0" Text="Implementation of newton's method also exists in [scipy](http://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.optimize.newton.html) and the source is [here](https://github.com/scipy/scipy/blob/v0.14.0/scipy/optimize/zeros.py#L45)" CreationDate="2015-07-21T17:50:39.420" UserId="10446" />
  <row Id="6923" PostId="6528" Score="0" Text="Newton's method can apply in a lot of contexts, and EM is really a whole class of algorithms. I think you need to narrow down your purpose." CreationDate="2015-07-21T18:05:22.000" UserId="21" />
  <row Id="6924" PostId="6536" Score="0" Text="but how would you store intermediate calculations in long format? (such as averages, cumsums or lagged differencies)" CreationDate="2015-07-21T19:34:33.463" UserId="10815" />
  <row Id="6925" PostId="6536" Score="0" Text="Indeed. You have to provide some more information with what data you want to do which calculations and how many individuals you excpect as well as how sparse you except an individual's features to be." CreationDate="2015-07-21T20:52:01.573" UserId="10314" />
  <row Id="6926" PostId="6533" Score="0" Text="Lists, as inspired by Lisp,  are a very powerful data structure. They can store arbitrarily deep and ragged data structures of mixed types. Take a look at Mathematica or  perhaps JSON for some inspiration." CreationDate="2015-07-21T22:23:46.713" UserId="7980" />
  <row Id="6927" PostId="6528" Score="0" Text="Hi Sean, I got the answer I was looking for. I know it might be too broad but this is exactly the thing I was looking for." CreationDate="2015-07-22T05:37:58.867" UserId="10810" />
  <row Id="6931" PostId="6536" Score="0" Text="This week my data is a table with 2400 medical variables (such as diastolic blood pressure, results from biochemical analysis...)  most of them real numbers, some missings, subdived in groups (and repeated several times through time), and almost 30000 individuals." CreationDate="2015-07-22T09:20:43.470" UserId="10815" />
  <row Id="6932" PostId="6546" Score="0" Text="Have you checked the [scaladoc](https://spark.apache.org/docs/1.4.1/api/scala/index.html#org.apache.spark.sql.DataFrame)? It has an example for average and max: `.agg(avg(people(&quot;salary&quot;)), max(people(&quot;age&quot;)))`. With sorting you can probably find (using `skip` and `take`) the percentiles, but there might be faster options." CreationDate="2015-07-22T16:47:07.370" UserId="1359" />
  <row Id="6933" PostId="6546" Score="0" Text="I had seen this previously in the scaladocs.  When I try to use them like the example I receive and error `not found: value avg` and `not found: value max`" CreationDate="2015-07-22T18:46:14.193" UserId="10832" />
  <row Id="6934" PostId="6546" Score="0" Text="What are your imports? It might be easier to help if there is an example and you describe what were the problem." CreationDate="2015-07-22T18:48:26.920" UserId="1359" />
  <row Id="6935" PostId="6536" Score="0" Text="It sounds like space shouldn't be an issue - even in long form, we're talking max, 72 MM rows which a DB on a reasonably beefy server should handle. You probably want to draw timeseries for repeated measurements which should definitely be &quot;long&quot; form - a field for the timestamp and a field that tells you &quot;what&quot;, e.g., &quot;diastolic BP&quot;, and a field for the value. Not sure what other analysis you're going to want to do. If you want to do some kind of regression, you'll want to transform your data such that one row represents one patient." CreationDate="2015-07-22T18:56:55.960" UserId="5010" />
  <row Id="6936" PostId="6546" Score="0" Text="`import org.apache.spark.rdd.RDD`  &#xA;`import org.apache.spark.sql.SQLContext`  &#xA;`import org.apache.spark.{SparkConf, SparkContext}`  &#xA;`import org.joda.time.format.DateTimeFormat`" CreationDate="2015-07-22T19:02:33.493" UserId="10832" />
  <row Id="6937" PostId="6546" Score="0" Text="The following [test](https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/DataFrameAggregateSuite.scala) might help start using DataFrame functions. It seems you have to import the `org.apache.spark.sql.functions._` too. (BTW.: I think the additional information is better added to the question itself and it is enough to add a comment after edit.)" CreationDate="2015-07-22T19:11:27.673" UserId="1359" />
  <row Id="6938" PostId="6555" Score="0" Text="Could it be, that the following would work? `$ ipython --profile=pyspark notebook`? It might be that the problem was only with the order of the arguments." CreationDate="2015-07-23T06:17:06.870" UserId="1359" />
  <row Id="6939" PostId="6553" Score="1" Text="Are the books you are using solely on the topic of democracy, if not, might not your distance metric get swamped by larger differences between the books contents? This is a side effect of your problem being in a very high dimensional space and being touched by the hand of the curse of dimensionality.  Perhaps taking only a small region of text around the word of interest would help, but it is still a problem with significant dimension." CreationDate="2015-07-23T06:41:46.460" UserId="7980" />
  <row Id="6940" PostId="6553" Score="0" Text="@image_doctor That's a excellent question. Technically, I expected &quot;democracy&quot; to be mentioned in different contexts across books. But you mean that the other words will &quot;move away&quot; the mentions of democracy because they themselves are not well-placed relative to the content of other books?" CreationDate="2015-07-23T08:08:00.993" UserId="5279" />
  <row Id="6941" PostId="6553" Score="1" Text="Yes that's the essence of that. here goes with a probably ill thought out metaphor. Imagine chapters of books being represented by colours. And a book a a whole represented as the mixture of all the colours of the chapters. A book on democracy in western europe would likely end up with an overall reddish hue as the sum of it's chapters. If we represent tourism by blue, a book on Tourism in Cuba, with a sole chapter on democracy and it's influence on economic development, would have a strong blue hue. So the two books would appear very different when viewed as a whole." CreationDate="2015-07-23T08:38:22.250" UserId="7980" />
  <row Id="6942" PostId="6553" Score="1" Text="That's the more accessible way of saying  what a data scientist would phrase as the  vectors for the two books will be a long way apart in feature space and so will appear quite dissimilar. It's really hard to quantify beforehand how many examples you will need without playing with the data, but language is subtle and layered so you will probably want as many as you can get .... and maybe more. Ultimately you won't know until you try. It's not a concrete answer, but unless someone direct experience of doing a similar thing, it's probably the best you will get." CreationDate="2015-07-23T08:43:59.097" UserId="7980" />
  <row Id="6943" PostId="6551" Score="0" Text="Thanks for your time, but please see my first bullet of &quot;Resources&quot;; I have reviewed this option, and looking for something that meets my &quot;Constraints&quot;." CreationDate="2015-07-23T13:41:22.267" UserId="1406" />
  <row Id="6944" PostId="6549" Score="0" Text="Thank you for your efforts on the deep discussion. Though programming this doesn't seem too bad (quite interesting, I may say, to deep dive into the algorithms), I am curious of packages that already are available. Do you know of anything that exists that is simple to install? Note this is not the same as simple to implement, which I understand cannot be guaranteed. If I can get my environment functional, I believe I can finesse it based on examples for my task." CreationDate="2015-07-23T13:47:01.377" UserId="1406" />
  <row Id="6945" PostId="6552" Score="0" Text="Thanks! I haven't considered this package yet - I will add it to the list of candidates. To clarify, when you say &quot;beyond version 3 it has similar module available in python as well&quot;, do you know if h2o's anomaly detection module (beyond ver 3) is available in Python, or some other module?" CreationDate="2015-07-23T13:52:16.183" UserId="1406" />
  <row Id="6946" PostId="6552" Score="1" Text="@ximik Well,I revisited the python documentation of their latest version 3.0.0.26(http://h2o-release.s3.amazonaws.com/h2o/rel-shannon/26/docs-website/h2o-py/docs/index.html) and it seems like h2o.anomaly is not yet available unlike its R api.I've raised the question in their google group(https://groups.google.com/forum/#!topic/h2ostream/uma3UdpanEI) and you can follow that." CreationDate="2015-07-23T15:48:47.820" UserId="10327" />
  <row Id="6947" PostId="6552" Score="1" Text="Well,h2o support group has answered the question and anomaly is available in python as well.An example is available here.&#xA;https://github.com/h2oai/h2o-3/blob/master/h2o-py/tests/testdir_algos/deeplearning/pyunit_anomaly_largeDeepLearning.py" CreationDate="2015-07-23T16:07:13.250" UserId="10327" />
  <row Id="6948" PostId="6552" Score="0" Text="Perfect! thank you for investigating. i'll update this post with results." CreationDate="2015-07-23T16:58:11.390" UserId="1406" />
  <row Id="6949" PostId="5257" Score="1" Text="Might be useful: [Deriving the Reddit Formula](http://www.evanmiller.org/deriving-the-reddit-formula.html)" CreationDate="2015-07-23T17:36:26.543" UserId="381" />
  <row Id="6951" PostId="6491" Score="0" Text="I totally agree with this answer. I just wanted to add that another advantage of random hyperparameter optimization is the possibility of exploring interesting areas of the hyperparameter space that may be ignored by the grid search (take a look to Fig. 1 in the paper linked by @AN6U5)." CreationDate="2015-07-24T07:43:49.900" UserId="2576" />
  <row Id="6952" PostId="6567" Score="0" Text="In some months I'll have to apply survival and logistic regression analysis on a file with 12000 rows (people) and 45000 variables. before that I want to practice and plan a general strategy." CreationDate="2015-07-24T09:41:25.450" UserId="10815" />
  <row Id="6953" PostId="6572" Score="1" Text="Great answer @AN6U5. Appreciate your comments. Thanks..." CreationDate="2015-07-24T15:25:35.157" UserId="3314" />
  <row Id="6954" PostId="6574" Score="0" Text="Thanks. I haven't decided if I'm going to do the larger project with R, Python or both so this is helpful" CreationDate="2015-07-24T17:19:12.903" UserId="10799" />
  <row Id="6955" PostId="6027" Score="1" Text="@Pratik, what was the disposition of your investigation of dimensionality reduction with spherical constraints?  Can you now answer your own question based on Emre's suggestions?  It would be great if you could post an update since this is a pretty fascinating question." CreationDate="2015-07-24T17:31:59.110" UserId="9420" />
  <row Id="6956" PostId="6574" Score="0" Text="I was thinking you were asking how to actually reconcile the inhomogeneity (e.g. fill in the missing values) rather than how to concatenate dataframes with missing values, but it looks like you just needed the append and/or smartbind methods?" CreationDate="2015-07-24T18:32:18.947" UserId="9420" />
  <row Id="6957" PostId="6558" Score="0" Text="This is a tough problem.  The closest thing I have personally heard of is the IBM Watson project.  They digested Wikipedia and created such semantic relationships in order to answer Jeaopardy questions.  Torsten Bittner gave a great talk on this in Seattle.  It looks like [another version is on YouTube](https://www.youtube.com/watch?v=tlontoyWX70).  One can imagine a method that consists of 1) finding the wikipedia heading for each subject, 2) cross searching the subject for the other word, 3) downselecting cases with a clear object-subject pairing, 4) choosing the most common case." CreationDate="2015-07-24T18:46:07.663" UserId="9420" />
  <row Id="6959" PostId="6545" Score="0" Text="Nice, good explanation." CreationDate="2015-07-24T22:09:49.037" UserId="10826" />
  <row Id="6960" PostId="6578" Score="0" Text="If you're looking for a search term, the problem is called &quot;online document classification&quot;. The biggest challenge in your scenario is that the classes are dynamic." CreationDate="2015-07-25T03:07:38.777" UserId="381" />
  <row Id="6961" PostId="253" Score="0" Text="I have a similar question on IBM Watson Analytics,Google's Bigquery and other cloud based analytics are this technologies better then Hadoop and spark .....I am just starting to learn Hadoop and spark and do I really need to learn Hadoop and spark to do big data analytics" CreationDate="2015-07-24T21:29:47.357" UserDisplayName="user10883" />
  <row Id="6962" PostId="6582" Score="0" Text="Thanks for your answer. How did you learn?" CreationDate="2015-07-25T07:19:22.530" UserId="10879" />
  <row Id="6963" PostId="6582" Score="1" Text="Books, tutorials online and a lot of hands on code related to play with data. Try the kaggle.com and try thru competitions. Is great in starting to learn ML." CreationDate="2015-07-25T07:30:58.487" UserId="8752" />
  <row Id="6964" PostId="6582" Score="0" Text="and ultimately try to find a community of data scientists  and participate in the projects, you will gain so much experience shared in the projects what no books can teach." CreationDate="2015-07-25T07:42:53.463" UserId="8752" />
  <row Id="6965" PostId="6577" Score="0" Text="Yes a worked example would be useful, from you, to tell us how much a consumption of 625 would cost. You could do this very easily." CreationDate="2015-07-25T07:47:41.743" UserId="471" />
  <row Id="6966" PostId="6582" Score="0" Text="But I am not good at theory like stats, Maths etc. I did study them in Uni days" CreationDate="2015-07-25T10:28:52.427" UserId="10879" />
  <row Id="6967" PostId="6582" Score="0" Text="I'n my particular case I did considered returning back to school and move to Ph.D program in Analytics and Data Science ... requiring calculus 1,2, Linear algebra, numerical linear algebra, SAS, R, math for big data, graph theory and much more ..." CreationDate="2015-07-25T15:17:41.173" UserId="8752" />
  <row Id="6968" PostId="6585" Score="0" Text="I am not exactly sure what you want, but [ODASE](http://www.missioncriticalit.com/odase.html) looks something similar. (You might find publications if you search for `mercury lang ontology`.)" CreationDate="2015-07-25T19:47:52.993" UserId="1359" />
  <row Id="6969" PostId="6584" Score="0" Text="This answer is the same as the previous answer.  It also doesn't do a very good job of explaining the solution, uses poor formatting, and lots of abbreviations, emoticons, and text message style vernacular.  If you have content not contained in the previous answer, then please edit your answer with a proper explanation and proper copy editing.  If not, then consider deleting this answer." CreationDate="2015-07-25T20:30:11.260" UserId="9420" />
  <row Id="6970" PostId="6355" Score="1" Text="This is a great question.  My initial thought was R-squared, which tells you how much of the variation is explained by the regression for a given set of features.  Since the Bayes error rate gives a statistical lower bound on the error achievable for a given classification problem **AND** associated choice of features.  Though the Bayes Error Rate is difficult to calculate (estimate), it has great universal utility for any classifier as you point out.  So I started thinking about Bayesian Regression and it almost seems like you are looking for the Bayes Loss." CreationDate="2015-07-25T21:29:49.480" UserId="9420" />
  <row Id="6971" PostId="759" Score="0" Text="A link about [literate programming here](http://infohost.nmt.edu/~shipman/soft/litprog/): basically, it's about commenting the code enough so that the code becomes a standalone documentation." CreationDate="2015-07-26T01:07:59.163" UserId="2544" />
  <row Id="6972" PostId="759" Score="0" Text="@gaborous: I am aware about the literate programming's meaning and have not included any links to the paradigm, as there are many sources for that and they are very easy to find. Nevertheless, thank you for your comment." CreationDate="2015-07-26T01:16:20.780" UserId="2452" />
  <row Id="6973" PostId="759" Score="1" Text="I guessed it, that's why I added this info as a comment for the interested reader :)" CreationDate="2015-07-26T01:28:38.933" UserId="2544" />
  <row Id="6974" PostId="759" Score="0" Text="@gaborous: Good, thanks again :-)." CreationDate="2015-07-26T01:30:07.517" UserId="2452" />
  <row Id="6975" PostId="6583" Score="0" Text="Is there some specific regarding large number of cases?" CreationDate="2015-07-26T13:56:39.463" UserId="10882" />
  <row Id="6977" PostId="6349" Score="0" Text="In addition to answer of @lollercoaster. I found the paper of LUDMILA I. KUNCHEVA and CHRISTOPHER J. WHITAKE which title is &quot;Measures of Diversity in Classiﬁer Ensembles and Their Relationship with the Ensemble Accuracy&quot;. I found it very explanatory about diversity." CreationDate="2015-07-26T15:37:31.013" UserId="9323" />
  <row Id="6981" PostId="6582" Score="0" Text="good starting point for statistics will be: Statistics 1: Introduction to ANOVA, Regression, and Logistic Regression. https://support.sas.com/edu/schedules.html?ctry=us&amp;id=1979 next for learning ML https://www.dataquest.io/course/kaggle-competitions &amp; https://www.kaggle.com/c/titanic/details/getting-started-with-python all this ones will help you get started with data science. (try to do all tutorials to can get used to data) I prefer more python versus Excel and R but hence you try to be a Data scientist than will need broad knowledge in more tools/language. R is easy to learn too." CreationDate="2015-07-26T19:30:17.013" UserId="8752" />
  <row Id="6982" PostId="6590" Score="1" Text="Feature selection is always going to help unless your initial features happen to super high quality to begin with. Sklearn offers a lot of different feature selection libraries (http://scikit-learn.org/stable/modules/feature_selection.html) I'm partial to RFE myself." CreationDate="2015-07-26T19:58:06.537" UserId="947" />
  <row Id="6987" PostId="6590" Score="0" Text="Thanks David, shall let you know how it goes!" CreationDate="2015-07-27T05:41:10.303" UserId="9061" />
  <row Id="6988" PostId="6591" Score="0" Text="&quot;...the training data I have is just for the move that was made, and whether that had good or bad results.&quot;  So you don't have the state of the game?  You can't train your network with just the move and its effect on the error.  You have to connect those moves to the game." CreationDate="2015-07-27T05:58:03.803" UserId="9420" />
  <row Id="6992" PostId="6355" Score="1" Text="Thank you for your answer. The computation of R-squared requires predictions, so I am wondering whether a theoretical bound of R-squared can be estimated. I read a paper on the estimation of the Bayes error rate by  means of an ensemble of classifiers; maybe something similar can be applied to R-squared (just a random thought here).&#xA;&#xA;I am not familiar with Bayesian regression. I will check that out." CreationDate="2015-07-27T09:04:25.463" UserId="2576" />
  <row Id="6993" PostId="6227" Score="2" Text="Can you share the table structure? What database are you using? Have you looked into recursive CTEs (common table expressions)?" CreationDate="2015-07-27T10:44:03.677" UserId="4766" />
  <row Id="6994" PostId="2448" Score="0" Text="ETL tools have fuzzy logic built into them that can match like terms. Also [postgresql 9.0 (and greater) has Levenshtein algorithm](http://www.postgresql.org/docs/9.0/static/fuzzystrmatch.html) in the `fuzzystrmatch` function" CreationDate="2015-07-27T10:52:35.230" UserId="4766" />
  <row Id="6995" PostId="6558" Score="0" Text="As @AN6U5 said, it is a really tough problem. From my experience, it is even more difficult to find an ontology that can cover everything. At the moment, ontologies are focused on a specific field. For example, you can find an ontology for Pathology or Algebra in Mathematics. Imagine that an ontology for Medicine is too broad based on the Pathology one." CreationDate="2015-07-27T12:28:05.563" UserId="201" />
  <row Id="6996" PostId="6591" Score="0" Text="Ow yeah, sorry, of course I have that. But I don't have results for any other possible moves." CreationDate="2015-07-27T19:25:52.617" UserId="10907" />
  <row Id="6997" PostId="6591" Score="2" Text="&quot;Reinforcement Learning&quot; might be a good topic to Google whilst waiting for an answer, although there are a lot of variants that won't apply to your specific game. Also, for a small network, you could look into combining neural networks with genetic algorithms to search for good NN weights as opposed to backpropagation of error terms (where precise error values may not be known, or are significantly delayed)" CreationDate="2015-07-27T19:49:11.860" UserId="836" />
  <row Id="6998" PostId="6591" Score="0" Text="Good that you have the state.  2048 has 1. initial game state, 2. move then resulting game state, 3. resulting game state + stochastic addition of extra tiles.  If you are just connecting 1 to 2, then this piece is deterministic.  Why use a learning algorithm at all?  Really you want to use the learning algo for what is the best state before adding random 2 tiles.  This seems better because there is invarriance in the system under rotation, so left, right, up, down loose meaning for different game states.  e.g. don't have your ANN choose between 4 moves, have it choose between 4 final states." CreationDate="2015-07-27T21:52:20.007" UserId="9420" />
  <row Id="6999" PostId="6602" Score="0" Text="Oh, believe me the RAM is by far not the slowest component in the system!" CreationDate="2015-07-28T01:32:34.867" UserId="7848" />
  <row Id="7000" PostId="6600" Score="0" Text="No mathematical model unfortunately. The data is from a survey, so a lot have to do with subjective perspective.&#xA;&#xA;I am considering regression-based feature eliminations for the numeric models. But I'm not sure how well it will work combined with the other pure categorical predictors. I believe in a regression, each level in a category will be split into its own yes/no variable, so lasso may eliminate one level but retain others. My aim is to identify the category as a whole to retain or not.&#xA;&#xA;Hence my thought of doing those I can represent numerically separately from the pure categories." CreationDate="2015-07-28T01:49:07.600" UserId="1133" />
  <row Id="7004" PostId="6605" Score="0" Text="What does `Ip` refer to?" CreationDate="2015-07-28T09:44:24.463" UserId="122" />
  <row Id="7005" PostId="6605" Score="0" Text="I believe it's the priors, but not exactly sure what the formulation using `Ip` refers to." CreationDate="2015-07-28T09:45:37.917" UserId="7980" />
  <row Id="7006" PostId="6602" Score="0" Text="Are you saying the Xeon processor might be the slowest component ? ;)" CreationDate="2015-07-28T09:48:56.237" UserId="7980" />
  <row Id="7007" PostId="6602" Score="0" Text="This is a hardware related question - you should ask it in http://superuser.com instead." CreationDate="2015-07-28T12:42:20.190" UserId="10937" />
  <row Id="7008" PostId="5966" Score="0" Text="Be aware that in the case of Mean Shift Clustering you are replacing one parameter (number of clusters) by another one (kernel radius) that in fact will indirectly determine the final number of clusters. Therefore, model selection is still required." CreationDate="2015-07-28T13:05:41.813" UserId="2576" />
  <row Id="7009" PostId="6610" Score="0" Text="How much data do you expect now, in 1 year and in 5 years? What kind of analysis do you plan to apply to your data?" CreationDate="2015-07-28T15:31:41.003" UserId="1279" />
  <row Id="7010" PostId="6608" Score="0" Text="Most Hadoop tools, including standard MapReduce and Spark, treat single file and directory with files the same way. E.g. if on HDFS you have files`/data/jobresult/part-00001`, `/data/jobresult/part-00002`, `/data/jobresult/part-00003`, etc., you can read them all from Spark using `sc.textFile(&quot;/data/jobresult&quot;)`. Note, that nested directories are not supported - only plain files or flat directories with such files." CreationDate="2015-07-28T15:43:41.173" UserId="1279" />
  <row Id="7011" PostId="6609" Score="0" Text="Thanks @kpb. My problem is classification. And you are right, randomForest broke because I had too many possible values over the range of the categorical predictors.&#xA;&#xA;Would converting the categorical predictors to binary sparse matrix myself, rather than letting caret do the grunt work behind the scene on a data frame, allow randomForest to handle more possible levels? If so, I can give it a try.&#xA;&#xA;And I agree about issue with centering; I may resort to normalizing the numerics into a 0-1 range instead." CreationDate="2015-07-28T15:47:53.537" UserId="1133" />
  <row Id="7012" PostId="6612" Score="0" Text="what method of word embedding are you using?" CreationDate="2015-07-28T20:23:26.923" UserId="10587" />
  <row Id="7013" PostId="6612" Score="0" Text="@lollercoaster word2vec and GloVe." CreationDate="2015-07-28T20:26:37.597" UserId="843" />
  <row Id="7014" PostId="6506" Score="0" Text="Use the cosine similarity because the Euclidean distance behaves counter-intuitively due to the [concentration of distance](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions) in high-dimensional spaces." CreationDate="2015-07-28T20:51:17.687" UserId="381" />
  <row Id="7015" PostId="6591" Score="1" Text="@Mark Please, read this answer on StackOverFlow http://stackoverflow.com/a/22498940/2309097...You will love it :)" CreationDate="2015-07-28T21:22:27.090" UserId="201" />
  <row Id="7016" PostId="6576" Score="0" Text="Please be specific about what you want to &quot;get into&quot;.  Not only the field, but also at what level.  For example-- &quot;professional medical text miner&quot; or &quot;amateur astrophysical universe examiner&quot;" CreationDate="2015-07-28T22:01:28.717" UserId="1077" />
  <row Id="7017" PostId="6617" Score="0" Text="Can you elaborate on these terms?" CreationDate="2015-07-29T01:06:35.087" UserId="3466" />
  <row Id="7018" PostId="6617" Score="1" Text="@sheldonkreger the operations that the algorithm does need to be independent of how you order or group your data...this minimizes the need for cross-talk in the algorithm and leads to more efficiency." CreationDate="2015-07-29T02:05:27.273" UserDisplayName="user9424" />
  <row Id="7019" PostId="6576" Score="0" Text="I am willing to become something that could work as a consultant or an employee that could be contact for companies to dug into their data and get insights of it." CreationDate="2015-07-29T04:26:30.353" UserId="10879" />
  <row Id="7020" PostId="6610" Score="0" Text="Hard to say as we are more building to cater for possible growth. Right now we have around 100 gigs of data for 30 entities over 3 years.&#xA;&#xA;So that is approximately 10 gigs per entity per year. We would like to be able to support thousands as we foresee major growth in the coming year or two. e.g. could break the 1T mark per year. &#xA;&#xA;The analysis currently is to predict the ETAs of these coordinates coming in. The final algorithm hasn't been decided on, but it will be a combination of machine learning from historical data and path matching." CreationDate="2015-07-29T08:03:18.347" UserId="10940" />
  <row Id="7022" PostId="6615" Score="0" Text="Thanks for the response. Originally thought the &quot;Big Data&quot; eco-system was an all or nothing. Good to know that we can utilise some of the frameworks such as Storm without committing to the full stack.&#xA;&#xA;Want to incrementally implement this as oppose to building a full-fledged system straight away. The reason for this, is that initially we won't have any demand and only building this system to offer a service and scale it when the demand warrants it. &#xA;&#xA;Don't doubt that, should that service becomes popular, we will need Hadoop but wondering if the first implementation could do without it." CreationDate="2015-07-29T08:29:13.810" UserId="10940" />
  <row Id="7023" PostId="6615" Score="0" Text="I may very well be over-estimating the investment needed for Hadoop and the reason ATS would be an easier route is because we have implemented it before so it is a familiar technology. &#xA;&#xA;Also, being extra cautious as we do have an impending deadline on this service, so I want to make sure we aren't over-engineering our first release, while still ensuring that it is future proof.&#xA;&#xA;Last question, is it relatively easy to migrate from ATS to Hadoop?" CreationDate="2015-07-29T08:38:16.797" UserId="10940" />
  <row Id="7025" PostId="6615" Score="1" Text="Yes the ecosystem is getting as broad as the umbrella term &quot;Linux&quot; -- hundreds of relevant projects. You'd never use more than a fraction. I don't think demand for your service determines how many things you put in your architecture; it determines how big your cluster is. You may add architecture as you add features. If by &quot;Hadoop&quot; you mean &quot;more than one machine&quot; I think you clearly need that from the start. There is no such thing as migrating from ATS to Hadoop; there are analogs (like HBase) but totally different API. Architecture is reusable; code is not." CreationDate="2015-07-29T09:05:28.870" UserId="21" />
  <row Id="7029" PostId="6609" Score="0" Text="1. converting to sparse format (yourself or caret, doesn't matter really) will help, but randomForest is not super efficient - you want tree-based methods, i would suggest xgboost&#xA;2. normalizing to 0-1 (i am guessing by min/max scaling and subtraction) has exact same problem. if you insist on normalizing, just divide by standard deviation. BTW, this is only important for logistic regression, svm and the like - trees are invariant under monotone transforms, so for xgboost / randomForest normalization is not necessary" CreationDate="2015-07-29T09:07:57.250" UserId="10936" />
  <row Id="7032" PostId="6618" Score="0" Text="Can you clarify a little, if you have 50 test scores for each individual, the  test scores do not differentiate in any way whether an individual has {disA} , {disC},..,{disA,disB}, {disB,disC},...,{disA,disB,disC} ?" CreationDate="2015-07-29T11:24:52.883" UserId="7980" />
  <row Id="7034" PostId="6618" Score="0" Text="yes. each individual has only one specific test score. But can have {disA} or {disB} or {disC} or {disA,disB} or {disB,disC} or {disA,disB,disC}. Actually, test scores are according to individuals instead of diseases." CreationDate="2015-07-29T11:42:22.393" UserId="10951" />
  <row Id="7035" PostId="6618" Score="0" Text="But having one test score per individual  is not quite the same thing  as it not differentiating which disease(s) they have ?" CreationDate="2015-07-29T11:45:16.150" UserId="7980" />
  <row Id="7036" PostId="6618" Score="0" Text="These diseases are neurodevelopmental disorders which are closely linked with each other. Here, I am trying to find out phenotypic difference among them. Imagine Communication is common problem in all diseases and I haves Test for communication skills. Now, I want to check in which disease or disorder higher values of communication test scores are associated or vice versa." CreationDate="2015-07-29T12:30:41.660" UserId="10951" />
  <row Id="7037" PostId="6618" Score="0" Text="Thank you for the background, I'm not sure that helped clarify how f(dis{A,B,C}) -&gt; g(testscore). If the test score   is not different for different combinations of diseases then you cannot separate them with machine learning, but perhaps that is not the case here ?" CreationDate="2015-07-29T12:38:31.243" UserId="7980" />
  <row Id="7038" PostId="6620" Score="0" Text="It appears the migration to &quot;cross validated&quot; was rejected by their users (http://stats.stackexchange.com/questions/163548/which-language-best-to-use-for-machine-learning-library).  Would you like me to attempt to migrate it to &quot;data science&quot;?" CreationDate="2015-07-29T03:52:12.783" UserDisplayName="Lev Reyzin" />
  <row Id="7039" PostId="6620" Score="0" Text="Weird -- seemed like a relevant question for TCS also; people certainly ask mush &quot;softer&quot; ones on a regular basis. But yeah, sure -- feel free to migrate it to whatever forum won't ban it..." CreationDate="2015-07-29T13:00:57.150" UserDisplayName="Aryeh" />
  <row Id="7043" PostId="6618" Score="0" Text="Unfortunately, I have this f(dis{A,B,C}) -&gt; g(testscore) case." CreationDate="2015-07-29T14:07:40.823" UserId="10951" />
  <row Id="7044" PostId="6618" Score="0" Text="It's not clear from your answer if you have a usable discriminative feature set, but if you want a mapping between  f(dis{A,B,C}) -&gt; g(testscore) and its inverse, try merging your three disease features into one and run it through a random forest and see how that performs and let us know how it goes." CreationDate="2015-07-29T14:24:28.980" UserId="7980" />
  <row Id="7045" PostId="6618" Score="0" Text="I will follow your suggestion.Thanks you for your answer." CreationDate="2015-07-29T14:58:34.943" UserId="10951" />
  <row Id="7046" PostId="6620" Score="2" Text="You will get a range of opinions on this. Several languages have good ML libraries. Stack Exchange sites Q&amp;A format doesn't really do polls or popularity contests well - votes are mainly for finding answers useful, they don't work for garnering support, at least not on the main sites. As an opinion I would propose you create a reference implementation in C, with a documented library/API. Many languages have support for creating bindings to a C library, so if the idea takes off you may find collaborators willing to make those bindings in e.g. Python" CreationDate="2015-07-29T15:21:42.853" UserId="836" />
  <row Id="7047" PostId="5637" Score="0" Text="And the GloVe vectors are significantly faster to compute than word2vec" CreationDate="2015-07-29T15:25:17.123" UserId="7848" />
  <row Id="7048" PostId="6591" Score="0" Text="@TaVen: A heuristic-based search will often beat NNs, and that was a fun read. I believe that the OP's question isn't about finding best 2048 player though, but gaining skills in teaching NNs to play games." CreationDate="2015-07-29T15:35:25.343" UserId="836" />
  <row Id="7049" PostId="6551" Score="0" Text="To reiterate, and perhaps be more blunt, using Twitter's AnomalyDetection package is NOT an option here: Please read the &quot;Constraints&quot; section more carefully. I do not mean to denounce any sincere attempts to help on this, but the question is strictly for Python-based packages. Therefore, future voters, PLEASE do not upvote this answer because it is not usable option. I would recommending clearing the current 2 votes for this via downvoting but perhaps this is unethical within the Stackexchange community and do not want to catch any flack." CreationDate="2015-07-29T16:30:41.523" UserId="1406" />
  <row Id="7050" PostId="6551" Score="0" Text="Again, I apologize to harp on this, but I am simply trying to make this question very clear and usable for others encountering a similar problem, and don't want them to go on a wild goose chase." CreationDate="2015-07-29T16:31:24.387" UserId="1406" />
  <row Id="7051" PostId="6620" Score="1" Text="I agree with Neil, ultimately most people would probably use something in Python but that something is probably just a binding to optimized C." CreationDate="2015-07-29T17:12:39.180" UserId="947" />
  <row Id="7052" PostId="6591" Score="0" Text="@Neil It's true. I found a YouTube video about 2048 and evolving neural networks, but couldn't find a source for the code." CreationDate="2015-07-29T18:30:23.300" UserId="201" />
  <row Id="7053" PostId="6627" Score="1" Text="Yeah, we are in a very initial phase of project so sorry for the very broad overview.&#xA;&#xA;We are also converting non-Iot devices to IoT along with user profiling. So these non-IoT devices will have ordinal values (sometimes probably on or off only) but the new actual IoT devices will run on continuous values. It will be a mix of both, probably thresholding continuous values can determine states for converted devices.&#xA;&#xA;Last section was an interesting look at the problem, but i am skeptical if we can determine that based on others preferences in same area/conditions.&#xA;&#xA;Thanks for the idea and links." CreationDate="2015-07-30T04:10:23.053" UserId="10967" />
  <row Id="7054" PostId="6626" Score="0" Text="None of the libraries will auto-select a network architecture for you. At the least, you will be wanting to test with variations in number of layers, types of layers (convolutional, pooling, dropout etc), hidden layer sizes, and choosing between several other hyper-params and/or variations on training. The easiest way to &quot;auto-pick&quot; those kinds of things is to follow a tutorial on a related problem to the one you want to solve, and keep things as similar as possible so you don't need to think about those things for now . . ." CreationDate="2015-07-30T06:53:46.407" UserId="836" />
  <row Id="7055" PostId="6626" Score="0" Text=". . . so it might be worth giving a brief summary of what kind of problem you are trying to solve (e.g. image classifying). A library with good tutorials and sample code related to your problem may be the best choice." CreationDate="2015-07-30T06:56:32.497" UserId="836" />
  <row Id="7056" PostId="6591" Score="0" Text="@TaVen Yeah that answer is great for both AI and CS, and it made me realize that looking 6 turns into the future is probably enough, so now the score of a move is based on the points gained in the next 6 turns. I found two Youtube videos, here's the code for one https://github.com/anubisthejackle/2048-Deep-Learning" CreationDate="2015-07-30T09:34:06.360" UserId="10907" />
  <row Id="7058" PostId="6591" Score="0" Text="@AN6U5 Taking the symmetry into account is a good idea. But using transitions would make the output encoding more difficult right? Seems easier to align the boards, maybe using image moments. Does that seem okay?" CreationDate="2015-07-30T09:52:03.297" UserId="10907" />
  <row Id="7059" PostId="6626" Score="0" Text="@NeilSlater I am exploring the possibility of using DL on futures contracts in the financial markets. I will be using minute by minute price data, ratios, etc. All the tutorials I can find are for text recognition or image classifying, as you mentioned. I haven't been able to find one related to financial &quot;prediction&quot;..." CreationDate="2015-07-30T13:17:16.670" UserId="10949" />
  <row Id="7060" PostId="6626" Score="0" Text="CNN is not synonymous with Deep Learning, it is just one of the more successful designs. Do you have a plan for the convolution parts? If not, an RNN (deep or not) may be better suited. Not that this changes your question much, just you are more likely to find what you want looking up RNN and time-series than CNN with anything" CreationDate="2015-07-30T13:25:55.227" UserId="836" />
  <row Id="7061" PostId="6626" Score="0" Text="@NeilSlater Sorry if my question was poorly formulated. I have been planning to use RNN, specifically a variant with LTSM. Thanks for all the insights." CreationDate="2015-07-30T13:30:27.270" UserId="10949" />
  <row Id="7062" PostId="6591" Score="0" Text="@AN6U5 I made a new question about that https://datascience.stackexchange.com/questions/6631/alignment-of-square-nonorientable-images-data" CreationDate="2015-07-30T14:07:59.307" UserId="10907" />
  <row Id="7065" PostId="6627" Score="1" Text="As a beginner, would you recommend me to use libraries directly or should I stick with naive but my own simple implementations which i understand thoroughly. Before your advice I had been working on simple online reinforcement learning for adaptive weight adjustment of features.&#xA;&#xA;And I did upvote, but stackexchange says it will accept my vote only after I have earned certain amount of 'reputation'." CreationDate="2015-07-30T17:19:15.357" UserId="10967" />
  <row Id="7066" PostId="6627" Score="0" Text="As a learning tool, I think its really useful to code up your own implementation of the basics like (stochastic) gradient decent, linear regression, k-means, SVMs, decision trees.  But you can probably do this as part of a MOOC separate from your actual work.  I absolutely suggest using a well known library for your problem.  Scikit-Learn is great for the reasons mentioned above and for prototyping, but doesn't scale particularly well.  [H20](http://h2o.ai/) and [Mahout](http://mahout.apache.org/) are high quality scalable libraries that I would recommend for a big data production system." CreationDate="2015-07-30T17:26:45.673" UserId="9420" />
  <row Id="7069" PostId="6291" Score="0" Text="This is good advice...  @dpmcmlxxvi is right that 1) standard deviation tends to be more robust than min and max and 2) the validity of your model depends on the mean and standard deviation of your training data set being similar to the mean and standard deviation of your test data set." CreationDate="2015-07-31T02:01:23.080" UserId="9420" />
  <row Id="7070" PostId="6611" Score="1" Text="Are you planning on using distributed computation? If so, have you considered Apache Spark...it's much faster in many cases than map reduce." CreationDate="2015-07-31T02:07:37.277" UserDisplayName="user9424" />
  <row Id="7072" PostId="6635" Score="0" Text="Also, see the _&quot;Embedding Fine-Tuning&quot;_ section on the page 5 of [this paper](http://lebret.ch/wp-content/uploads/2013/12/nips2013.pdf) - I know, it's not software, but I thought it might give you some useful ideas." CreationDate="2015-07-31T05:23:35.197" UserId="2452" />
  <row Id="7073" PostId="6545" Score="0" Text="The one part about this that is unclear to me, is - how does this account for the variance in the denominator?  It was  my understanding that NCC is normalized by the variance of the function.  Is that true?  I'm not seeing this in your explanation." CreationDate="2015-07-31T09:33:36.263" UserId="10826" />
  <row Id="7074" PostId="6638" Score="1" Text="Yeah it's helpful, thanks! But I still need a way to map all theses states to one in a consistent way... But now I know exactly which states and operations there are :-)" CreationDate="2015-07-31T11:35:06.033" UserId="10907" />
  <row Id="7075" PostId="6545" Score="0" Text="The distinction between standardizing the cohort and normalizing the individual vectors is confusing when NCC is put in functional form.  I'll try to edit my answer over the weekend to hash this out." CreationDate="2015-07-31T15:54:01.107" UserId="9420" />
  <row Id="7076" PostId="6638" Score="0" Text="I tried to answer this with the edit I just made.  Hope it helps!" CreationDate="2015-07-31T16:20:00.683" UserId="9420" />
  <row Id="7077" PostId="6643" Score="0" Text="Can you add some additional information to help clarify your question?  Is the algorithm you are running written by you in CUDA or are you using a package written by someone else?  Does that package or code only run on OSX or are you wondering about connectivity between AWS and OSX?  You can certainly connect a Mac to AWS and run your code/package on AWS in a Linux or Windows environment.  Further, OSX is built on a Unix environment, so most small code packages that run in OSX will run in Linux." CreationDate="2015-07-31T16:38:54.557" UserId="9420" />
  <row Id="7078" PostId="6645" Score="0" Text="It would be very useful if you would provide the R package that you are using  and a sample of the data.  My guess is that there are some sort of hidden differences in the features.  Often R data comes in as factors or character strings instead of numbers, so even though they look like numbers, they aren't.  There are several ways to convert to numeric data, but `as.numeric()` is the most popular.  Check out [this stack overflow post](http://stackoverflow.com/questions/2288485/how-to-convert-a-data-frame-column-to-numeric-type)" CreationDate="2015-07-31T16:54:22.150" UserId="9420" />
  <row Id="7079" PostId="6645" Score="0" Text="@AN6U5- Thanks for the reply. I am using randomForest package. The error i got during the predict statmenet.  I will try to provide some sample data by tomorrow. However I could see that both the training data and the test data contains same factors for categorical data and the few numerical variables." CreationDate="2015-07-31T18:42:48.107" UserId="9793" />
  <row Id="7081" PostId="6611" Score="0" Text="I'm using spark. columnSimilarity is implemented in scala for spark, I'm using python so I implemented it myself in the python bindings. Either way, doing all pairs similarity brute force is infeasible even on huge spark clusters. DIMSUM offers a smart trick for sampling only column similarities that have a high probability of being over a certain threshold, the issue is that DIMSUM works only when you have more rows than columns. I have many more columns than rows. see the linked paper for details" CreationDate="2015-07-31T20:44:15.750" UserId="10943" />
  <row Id="7083" PostId="6645" Score="0" Text="I'm still skeptical that either the types are different, the number of features are different, or you are giving it the transpose of your data so the shape is different.  Those are the things I would investigate with that error." CreationDate="2015-07-31T21:40:55.240" UserId="9420" />
  <row Id="7084" PostId="6645" Score="0" Text="@AN6U5 - Thank you again. So do you advice to try all the numeric fie lds to be declared as.numeric explicitly ( and the factor fields as as.factor() respectively) and give a try?" CreationDate="2015-07-31T21:52:19.893" UserId="9793" />
  <row Id="7085" PostId="6645" Score="0" Text="I just want you to run `sapply(data, mode)` and `sapply(data, class)` and `dim(data)` on your training data and testing data to see if everything matches." CreationDate="2015-07-31T21:58:13.987" UserId="9420" />
  <row Id="7086" PostId="6645" Score="0" Text="@AN6U5 - Thank you. I will try this and get back to you." CreationDate="2015-07-31T22:54:08.683" UserId="9793" />
  <row Id="7087" PostId="6645" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/26481/discussion-between-arun-and-an6u5)." CreationDate="2015-07-31T23:17:18.823" UserId="9793" />
  <row Id="7088" PostId="6645" Score="0" Text="@AN6U5 - As per your suggestion. I tried with sapply functions mentioned above. Everything looks ok for me.                                          But one clue I got from your earlier reply - &quot;or you are giving it the transpose of your data so the shape is different&quot;- Could you please let me know what you mean by this? Actually i started getting this problem only after applying a filter with in the program.   I apply something like this testing_data &lt;- subset(testing_data, var1 != &quot;xyz&quot;).. I will explain you a little bit more in the next comment." CreationDate="2015-08-01T07:08:41.753" UserId="9793" />
  <row Id="7089" PostId="6645" Score="0" Text="@AN6U5 - I have two R programmes. Let us call it as A &amp; B. Program A does the preprocessing, puts the data into a .csv and saves the file in a location. Program B picks the .csv, builds the model and does the prediction job. Everything was working fine for me, until i applied the above said filter to the data, that is, testing_data &lt;- subset(testing_data, var1 != &quot;xyz&quot;). As you said that &quot;transposing  could have reshaped the data&quot;, I suspect that this subset filter could be causing some issue. Could you please advice me how can i find what is making this problem please?" CreationDate="2015-08-01T07:20:10.400" UserId="9793" />
  <row Id="7091" PostId="6629" Score="0" Text="Not really clear what you want since if you make it generally reversible, it has not been anonymized . You probably have a user/role model and a consumption model in mind, but you need to explicitly describe this in the question. As well as taking Franck's answer into consideration." CreationDate="2015-08-01T20:14:55.703" UserId="7720" />
  <row Id="7092" PostId="6545" Score="0" Text="Okay.  I'll wait.  Thanks." CreationDate="2015-08-02T01:18:39.470" UserId="10826" />
  <row Id="7093" PostId="6643" Score="0" Text="Thank you AN6U5! I want to use lasagne to train my neural network. And yes, I wrote that code, and will try convolutional NN later. I want to use the AWS instances to accelerate the computation. My doubt came from the AWS website saying that new users have 750h / month of Linux /Windows micro instances usage for free, Mac OS is not included." CreationDate="2015-08-02T06:47:53.220" UserId="10994" />
  <row Id="7094" PostId="6650" Score="0" Text="Hi Emre, thank you for your reply! Actually I am thinking about buying a new laptop since my Macpro is really old. I likeMac, but it seems they don't come with any NVIDIA card. Could you recommand me some laptops suitable for machine learning tasks? (Sorry I know almost nothing about hardware)" CreationDate="2015-08-02T07:09:31.780" UserId="10994" />
  <row Id="7096" PostId="6650" Score="0" Text="The [Macbook Pro comes with a GeForce GT 750M](https://www.apple.com/macbook-pro/performance-retina/) with [384 cores](http://www.gpuzoo.com/GPU-NVIDIA/GeForce_GT_750M.html). Outside of Apple, you will probably want a gaming laptop, [such as this Alienware with an Nvidia GTX 980](http://www.laptopmag.com/gaming-laptops), [which has 1536 cores](http://www.geforce.com/hardware/notebook-gpus/geforce-gtx-980m/specifications). [Here are some others](http://www.ultrabookreview.com/5729-gaming-laptops-nvidia-970m-980m/). Note that machine learning algorithms outside of deep learning don't need GPUs." CreationDate="2015-08-02T07:41:57.957" UserId="381" />
  <row Id="7097" PostId="6650" Score="0" Text="Ok! Thank you very much for the answer, it's more clear for me now!" CreationDate="2015-08-02T08:21:53.440" UserId="10994" />
  <row Id="7099" PostId="6660" Score="2" Text="you are most of the way there,  nearest neighbour is not too hard to implement yourself. As you observe, you just need to compute the distance between your candidate who doesn't have a score for your potential recommendation and the other users who have a score for that item and then select the smallest distance using whatever metric empirically works best. Testing will show whether that might be euclidean, manhattan, jaccard or something else." CreationDate="2015-08-02T12:09:05.290" UserId="7980" />
  <row Id="7100" PostId="6644" Score="0" Text="Thank you. I couldn't quite follow this part, would you mind elaborating further please? - &#xA;&#xA;&quot;I might have weights going from activation of old output i=3, AOld3 to logit of new outputs ZNewj, where ZNewj=Σi=9i=0Wij∗AOldi as follows:&#xA;&#xA;W3,0=−10&#xA;W3,1=−10&#xA;W3,2=+10&#xA;W3,3=+10&quot;" CreationDate="2015-08-02T12:10:38.450" UserId="10990" />
  <row Id="7102" PostId="6660" Score="0" Text="Thanks image_doctor, is there any resource you could direct me to where I can learn about implementing custom functions in R? I am familiar with the theory for Jaccard and I have some experience implementing this in Excel but I don't know how to &quot;tell&quot; R to loop through each pair of users to compute similarity or how to store the results?" CreationDate="2015-08-02T13:03:09.710" UserId="11029" />
  <row Id="7103" PostId="6660" Score="1" Text="You'll probably find functional programming useful here, it will be faster than loops, here is a reference: http://adv-r.had.co.nz/Functionals.html" CreationDate="2015-08-02T14:23:39.143" UserId="7980" />
  <row Id="7104" PostId="6644" Score="0" Text="@VictorYip: The equation is just the normal feed-forward network equation, but to use it I had to define my terms carefully (since you have no reference maths in your question). The &quot;logit&quot; Z value is the value calculated at the neuron *before* activation functions have been applied (and generally $A_i = f( Z_i )$ where $f$ is e.g. sigmoid function). The example weights are the values I would use for connecting new output layer neurons to old ones, but just the ones that connect the 4 neurons in the new output layer to one of the neurons in  old output layer (the one for output &quot;3&quot;)" CreationDate="2015-08-02T15:20:18.047" UserId="836" />
  <row Id="7109" PostId="6645" Score="0" Text="@AN6U5 - Do you have any other thoughts on this please?" CreationDate="2015-08-03T05:18:55.950" UserId="9793" />
  <row Id="7110" PostId="6666" Score="1" Text="Interesting comments in your second paragraph.  Are there any general rules of thumb in this regard?  Or guidelines in how certain parameter choices or data cleaning will affect your results?" CreationDate="2015-08-03T14:28:20.290" UserId="1097" />
  <row Id="7112" PostId="6666" Score="0" Text="@Matt: I'd like to know that, too. I just took some data I understand already pretty well and experimented with them, with the results above. I found that adjusting the stopwords to the actual corpus helps a lot in clearer topic definition." CreationDate="2015-08-03T14:36:35.620" UserId="10169" />
  <row Id="7113" PostId="6645" Score="0" Text="You haven't provided enough information yet for people to help you. Please provide some data and your code." CreationDate="2015-08-03T18:05:43.870" UserId="9420" />
  <row Id="7114" PostId="6645" Score="0" Text="@AN6U5 - Sorry for the delay. I have now the added the required infoarmtion." CreationDate="2015-08-03T18:47:27.477" UserId="9793" />
  <row Id="7115" PostId="6676" Score="2" Text="A question and an observation: how stable is your accuracy of SGD on repeated runs?   the two algorithms are not equivalent and will not necessarily produce the same accuracy given the same data. Practically you could try changing the epochs and or the learning rate for SGD. Beyond that you could try normalising the features for SGD." CreationDate="2015-08-04T10:29:42.717" UserId="7980" />
  <row Id="7116" PostId="6672" Score="0" Text="In my case I think the cohort split makes sense. If for instance after a special communication a lot of new customers are acquired I can at least better predict now the impact it will have in the coming months, especially if they don't spend immediatly." CreationDate="2015-08-04T14:17:37.427" UserId="10983" />
  <row Id="7120" PostId="6676" Score="0" Text="So, I didn't test the SGD on repeated runs because the above uses 10 fold cross validation; for me this sufficed." CreationDate="2015-08-04T16:21:46.450" UserId="8774" />
  <row Id="7121" PostId="6676" Score="0" Text="Can you explain to me how come these algorithms are not equivalent? If I look at the SGDClassifier here, it mentions &quot;The ‘log’ loss gives logistic regression, a probabilistic classifier.&quot; I believe there is a gap in my machine learning knowledge." CreationDate="2015-08-04T16:23:21.147" UserId="8774" />
  <row Id="7122" PostId="6638" Score="0" Text="Ignore my previous comment, there was a code mistake. Operations can be applied in any order even though they don't commute. It seems to work well now!" CreationDate="2015-08-04T16:27:55.207" UserId="10907" />
  <row Id="7123" PostId="6638" Score="1" Text="For future users: I used image moments instead of highest value tile, which generalizes to things other than 2048 boards. For \ mirroring, I check the sum of the upper triangle vs that of the lower." CreationDate="2015-08-04T16:29:23.577" UserId="10907" />
  <row Id="7124" PostId="6638" Score="1" Text="Note that there is a loss of information in summing the diagonal region since multiple different sums and orientations can result in the same sum.  I try to stay away from sums or averages when trying to determine absolute differences.  Image moments also have a loss of information.  For large images, you wouldn't expect much conflict, but the 2048 board would have a significant number of conflicts." CreationDate="2015-08-04T16:36:39.037" UserId="9420" />
  <row Id="7125" PostId="695" Score="0" Text="Note that nolearn is a wrapper that makes other libraries easier to use and compatible with sklearn. It's not of itself a neural network library, but nonetheless recommended. At the time of writing it's mostly for Lasagne but there's some Caffe code and maybe others." CreationDate="2015-08-04T16:36:51.790" UserId="10907" />
  <row Id="7126" PostId="6673" Score="1" Text="Not sure I understand what you're saying. I never said that knowing &quot;applied statistics&quot; isn't important - I simply made the distinction that gaining experience applying methods is more important than gaining theoretical knowledge about the methods itself." CreationDate="2015-08-04T16:57:07.137" UserId="947" />
  <row Id="7128" PostId="6684" Score="0" Text="Thanks David. Any insight on how to choose the threshold above which features are useful? (put aside from removing the least useful feature, running the RF again and see how it impacts the prediction performance)" CreationDate="2015-08-04T18:02:24.060" UserId="843" />
  <row Id="7129" PostId="6684" Score="1" Text="As with most automated feature selection I'd say most people use a tuning grid. But using domain expertise when selecting (and engineering) features is probably the most valuable -- but isn't really automatable." CreationDate="2015-08-04T18:05:32.373" UserId="947" />
  <row Id="7130" PostId="6673" Score="0" Text="David, that was exactly my point of disagreement. Without having theoretical knowledge of the methods themselves we are simply just script kiddies. Experience is important, but it is a by-product of theoretical knowledge, not the other way around." CreationDate="2015-08-04T18:31:36.300" UserId="11054" />
  <row Id="7132" PostId="6673" Score="0" Text="I see, well then we are in extreme disagreement. I think knowing how to correctly apply something (when to, how to, assumptions, etc...) is far more important than knowing its underlying proof." CreationDate="2015-08-04T18:37:00.090" UserId="947" />
  <row Id="7133" PostId="6673" Score="0" Text="Not really, what you call &quot;experience&quot; is actually theoretical knowledge - the two are inseparable. What you advocate is a person who knows how to drive a car, but has no clue how it runs. Companies hiring data scientists would prefer to have a person who can do both. The script kiddie reading kaggle tutorials without any background in statistics won't be in the game for very long." CreationDate="2015-08-04T18:43:56.830" UserId="11054" />
  <row Id="7134" PostId="6673" Score="1" Text="No, it isn't. There is a vast difference between applied experience and theoretical knowledge, it is frequently the difference between what is gained in industry vs in the classroom. For example, it's more valuable to know how to effectively verify that a model has not overfit using an applied method like cross validation than it is to know the theoretical underpinnings of regularization. Also, please stop mentioning &quot;script kidies&quot; -- no one is advocating using kaggle's new and horrible one-click-to-submit functionality." CreationDate="2015-08-04T18:47:42.433" UserId="947" />
  <row Id="7135" PostId="6673" Score="1" Text="If what you are saying is true, then why do companies prefer PhD's and people with Masters degrees over people with simply Bachelors? It is because they have theoretical knowledge of the techniques which drive the algorithms. They are the engine builders per se. Theoretical knowledge is deeper knowledge. Kaggle is a holding tank for script kiddies." CreationDate="2015-08-04T18:53:00.963" UserId="11054" />
  <row Id="7136" PostId="6673" Score="0" Text="If the trade-off is between someone with a BA and 5 years of successful industry experience or a PhD with no industry experience then they wouldn't. If you think that someone needs a PhD to succeed in the field then you are wrong. Masters degrees are nice -- and provide an applied, not theoretical, education -- but are not required. Speaking as one myself, I'm pretty sure you don't understand what employers are looking for but I know that I cannot say is anything to convince you otherwise so I'll stop here." CreationDate="2015-08-04T19:04:12.603" UserId="947" />
  <row Id="7137" PostId="6673" Score="0" Text="Your position is a false dichotomy - one doesn't study for 1 year or 5 years and then suddenly start getting experience. Experience is learned along the way. Data Science &gt; Kaggle Competitions.&#xA;&#xA;Kaggle is a holding tank for script kiddies, whose models are either useless, overtly complex, or overfitted. This why they had to fire 1/3 of their staff." CreationDate="2015-08-04T19:05:10.387" UserId="11054" />
  <row Id="7138" PostId="6673" Score="0" Text="David, generalizations about Master's degrees are never good. Some are theoretical and some are applied. Some are both. To be successful, a PhD is not necessary, but a Masters degree most certainly is. Most of the people on Kaggle only have a Bachelors degree. &#xA;&#xA;&quot;.. insiders say that they haven’t been lucrative enough for Kaggle. And the algorithms that win them aren’t always general enough to be useful to the company sponsoring the competition.&quot;&#xA;&#xA;Source: http://www.wired.com/2015/02/data-science-darling-kaggle-cuts-one-third-staff/" CreationDate="2015-08-04T19:05:23.117" UserId="11054" />
  <row Id="7139" PostId="6545" Score="0" Text="Have you figured this out?  Thanks." CreationDate="2015-08-04T19:57:43.857" UserId="10826" />
  <row Id="7140" PostId="6675" Score="2" Text="The Neural Networks (and other ML methods) are designed to use *when* you don't know the hidden logic. You want the machine to learn what output to provide given the input. In other words, you know that there is some hidden process which given your input get that specific output." CreationDate="2015-08-04T22:38:25.267" UserId="7848" />
  <row Id="7141" PostId="6680" Score="0" Text="The format and data sample shown is likely the LIBSVM format. The first is the label (+1 or -1), followed by dimension/value pairs. This format is convenient for sparse vector storage. From the example, there might be a problem with format (I see the commas at the end). You can use LIBSVM software package on this data." CreationDate="2015-08-04T23:26:04.747" UserId="7848" />
  <row Id="7145" PostId="6676" Score="0" Text="Without a detailed study of the implementations I don't think I can be specific about why they are not equivalent, but a good clue that they are not equivalent is that the results for each method are significantly different. My guess would be that it has to do with the convergence properties of the estimation methods used in each." CreationDate="2015-08-05T06:51:20.053" UserId="7980" />
  <row Id="7146" PostId="6696" Score="0" Text="Worth mentioning that a grid world problem is presented as part of that course." CreationDate="2015-08-05T07:26:37.860" UserId="836" />
  <row Id="7148" PostId="6696" Score="0" Text="yes, have seen that, but not enough to code the same" CreationDate="2015-08-05T07:51:40.553" UserId="8013" />
  <row Id="7149" PostId="6689" Score="0" Text="Dirk thank you for the answer. The thing that is giving me (lots of) problems is the definition of a churn event, because this is not a subscription-based service. Isn't survival analysis only useful for model where I have a &quot;death event&quot; (a user who cancels his subscription, a patient who dies, etc.)?" CreationDate="2015-08-05T07:54:22.430" UserId="11066" />
  <row Id="7150" PostId="6295" Score="0" Text="You could use the sqldf package." CreationDate="2015-08-04T01:08:09.050" UserId="11054" />
  <row Id="7151" PostId="6685" Score="0" Text="They're the same length. Thanks!" CreationDate="2015-08-05T06:08:10.840" UserId="11064" />
  <row Id="7152" PostId="6698" Score="0" Text="SimHash and MinHash do not use these similarity functions. I think a better way to say it would be that they create digests which approximate these functions." CreationDate="2015-08-05T09:15:04.923" UserId="816" />
  <row Id="7153" PostId="6430" Score="0" Text="Thank you very much for the reply and advice. Just want to know how different is using a caret package for feature selection when compared to selecting important variables using the randomForest algorithm?" CreationDate="2015-08-05T10:39:40.660" UserId="9793" />
  <row Id="7154" PostId="6430" Score="0" Text="That sounds like a different question to post -- the differences are vast given that you run things like recursive feature selection using any algorithm of your choice in caret." CreationDate="2015-08-05T14:38:33.157" UserId="947" />
  <row Id="7155" PostId="6676" Score="1" Text="These algorithms are different because logistic regression uses gradient descent where as stochastic gradient descent uses stochastic gradient descent.  The convergence of the former will be more efficient and will yield better results. However, as the size of the data set increases, SGDC should approach the accuracy of logistic regression.  The parameters for GD mean different things than the parameters for SGD, so you should try adjusting them slightly.  I would suggest playing with (decreasing) learning rates of SGD a bit to try to get better convergence as it may be thrashing around a bit." CreationDate="2015-08-05T16:17:22.207" UserId="9420" />
  <row Id="7156" PostId="6545" Score="0" Text="I think I get it @an6u5." CreationDate="2015-08-05T17:30:41.507" UserId="10826" />
  <row Id="7157" PostId="6698" Score="0" Text="@AlexeyGrigorev I am a little confused. I looked into the following implementation for minHash 'computeSimilarityFromSignatures' @ [link](https://mymagnadata.wordpress.com/2011/01/04/minhash-java-implementation/). It uses a |HashedArray(A) &amp; HashedArray(B)|/ (total number of entries)" CreationDate="2015-08-05T19:15:24.313" UserId="5179" />
  <row Id="7160" PostId="6707" Score="0" Text="how do I set a threshold... What i am doing id the update the value of each grid with respect to the grids that the control can go to from the present grid.. What do you mean by saying V is a function" CreationDate="2015-08-06T04:00:21.690" UserId="8013" />
  <row Id="7161" PostId="6705" Score="0" Text="actually i wanted to see a grid world problem being solved by calculating on pen and paper, because that would help me understand the concept, unless i can understand the concept I cannot code(specially value iteration)" CreationDate="2015-08-06T04:01:29.510" UserId="8013" />
  <row Id="7163" PostId="6707" Score="0" Text="$V(s)$ is a function that returns the utility of that state. In a computer program, where you have enumerated the states, you may well end up modelling $V$ as a simple array and treat it as an array lookup" CreationDate="2015-08-06T08:53:37.033" UserId="836" />
  <row Id="7164" PostId="6689" Score="0" Text="Survival doesn't have to model death. You can define it, make sure it's definition is accepted in the business." CreationDate="2015-08-06T10:07:55.240" UserId="11080" />
  <row Id="7167" PostId="6399" Score="0" Text="Yeah, I would currently recommend blocks today over pylearn2 if you're ok putting in a bit of time to understand Theano." CreationDate="2015-08-06T19:50:16.547" UserId="684" />
  <row Id="7168" PostId="6399" Score="0" Text="Great library built by great people." CreationDate="2015-08-06T19:50:22.947" UserId="684" />
  <row Id="7170" PostId="6720" Score="0" Text="another question, in most of the grid world problem one state has a high valued number written on it,like +100. Is that the reward or the value of that state" CreationDate="2015-08-07T08:48:39.850" UserId="8013" />
  <row Id="7171" PostId="6710" Score="0" Text="Thanks for your answer. But I have to further ask that, in the case $p&gt;&gt;n$, the complexity is $O(n^2p+n^3)$?" CreationDate="2015-08-07T09:24:58.777" UserId="9893" />
  <row Id="7172" PostId="6714" Score="0" Text="Thanks. Could you please kindly give some link of the related publications?" CreationDate="2015-08-07T09:26:10.847" UserId="9893" />
  <row Id="7173" PostId="6710" Score="0" Text="@aaronyxt, Yes, in that case you can additionally take into consideration all fast growing terms with p." CreationDate="2015-08-07T11:03:42.057" UserId="10989" />
  <row Id="7175" PostId="6720" Score="0" Text="It is the reward, which is fixed for the problem. The value is learned, it is an estimate of accumulated future rewards going from that state." CreationDate="2015-08-07T14:48:13.530" UserId="9814" />
  <row Id="7177" PostId="6715" Score="0" Text="Thank you for the detailed responses!                                                           As a follow-up to all of your answers:  I understand that if the features are on different scales, this could present a problem.  However, if the distance metric is normalized to the variance, does this achieve the same result as standard scaling before clustering?  i.e. I usually use a normalized euclidean distance [related](http://stackoverflow.com/questions/31869799/how-to-implement-callable-distance-metric-in-scikit-learn) - does this also mitigate scaling effects?" CreationDate="2015-08-07T16:38:04.743" UserId="11124" />
  <row Id="7178" PostId="6722" Score="0" Text="But, then couldn't you just include that as an additional feature that you cluster on?  i.e. the price becomes a feature to cluster on?" CreationDate="2015-08-07T16:47:28.703" UserId="11124" />
  <row Id="7181" PostId="6710" Score="0" Text="Thanks for your help and reply." CreationDate="2015-08-08T01:44:28.203" UserId="9893" />
  <row Id="7182" PostId="6714" Score="0" Text="Thanks for your reply and  references." CreationDate="2015-08-08T01:45:50.533" UserId="9893" />
  <row Id="7183" PostId="6673" Score="1" Text="While I can see points both of you are trying to make, I think it's perhaps out of context. The original question was 'how can a programmer transition into a job in data science ?'&#xA;&#xA;If the response is 'drop everything, spend some years getting a PH.D in statistics, then do some projects on your own and then start applying', that's a pretty onerous obstacle and you may as well tell them not to bother in a practical sense.  Conversely, given the number of Stats PHD (or even Masters) and the number of people looking, employers may consider people who can demonstrate experience without a degree." CreationDate="2015-08-08T07:49:38.170" UserId="11146" />
  <row Id="7184" PostId="6716" Score="0" Text="Please see my comment to my question.  Thanks" CreationDate="2015-08-08T09:17:20.130" UserId="11124" />
  <row Id="7185" PostId="6726" Score="2" Text="Context matters. I would suspect a bug or misunderstanding in your script (e.g. a mistake when constructing features of test data, or using the wrong/untrained model when predicting). If it's a 101 competition,  you may be better off asking in the Kaggle forum for that competition, or in the Getting Started forum there. To get answers here, you may need to show some of your code and explain it before someone could offer advice." CreationDate="2015-08-08T09:33:50.633" UserId="836" />
  <row Id="7186" PostId="6693" Score="0" Text="There may be better implementations, and your support may be set too high. Also, there are plenty of alternative algorithms..." CreationDate="2015-08-08T13:19:08.697" UserId="924" />
  <row Id="7187" PostId="6729" Score="0" Text="thank you Tasos , i'm still a newbie for python . if you can please support me with the code ." CreationDate="2015-08-08T13:27:55.673" UserId="9035" />
  <row Id="7188" PostId="6729" Score="0" Text="@Miller Check my edited answer." CreationDate="2015-08-08T14:24:14.633" UserId="201" />
  <row Id="7189" PostId="6728" Score="0" Text="I'd suggest you plot the clusters at least to get an idea of how good they are. [This](http://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering) is a good one for hierarchical clustering, with code examples of nice plots in the links, although unfortunately is in python. Hope it helps anyway." CreationDate="2015-08-08T19:02:26.327" UserId="9584" />
  <row Id="7190" PostId="6726" Score="0" Text="Thank you. I have some things to look at as a result of posting there and I may come back here if I have more questions, The code itself is very simple. I think It's either the formatting of the data or the options I am using that are at issue here" CreationDate="2015-08-08T20:41:09.967" UserId="11146" />
  <row Id="7191" PostId="6728" Score="0" Text="Thank you for your answer Irnzcig. I have already plotted dendrograms for all the agglomeration methods. I'm looking for a method like Silhouette, where I can get a measure for the goodness" CreationDate="2015-08-08T22:51:20.533" UserId="11063" />
  <row Id="7192" PostId="6728" Score="0" Text="You're welcome. I was trying to say that you have plots similar to Silhouette in the links coming from the link I posted, and in particular [this one](http://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_clustering_metrics.html#example-cluster-plot-agglomerative-clustering-metrics-py). Maybe it was not so easy to find, and it is in python, but if you take a look to the `for loop` that plots the distances you might get an idea. Hope it is more helpful this time." CreationDate="2015-08-09T11:44:17.933" UserId="9584" />
  <row Id="7193" PostId="5302" Score="0" Text="What software can you use to solve this? Are you limited to SAS?" CreationDate="2015-08-09T13:30:35.230" UserId="8021" />
  <row Id="7194" PostId="1128" Score="0" Text="It is interesting that time series form an abelian group. I am looking for an example of binary time series that form a group. Let A be the set of all binary time series. (for example, all spike trains between time t_1 and t_2) I am looking for an operation *, such that (A,*) form a group. Is the set of all spike trains form an abelian group?" CreationDate="2015-08-09T10:44:16.053" UserDisplayName="user12157" />
  <row Id="7196" PostId="6704" Score="0" Text="Thank you very much for the answer. I implemented this with 2 methods WavDistance and diss.DWT but I got different values (I assume both these methods does the same thong where diss.DWT in TSclust is a wrapper for waveDistance in TSDist). The issue was my data was in columns. Once I took the transpose it was fine. Sorry about the delayed reply." CreationDate="2015-08-10T01:23:55.903" UserId="11063" />
  <row Id="7197" PostId="6726" Score="1" Text="What happens when you use `predict_proba()`? If &quot;1&quot; is a rare outcome then it's pretty likely that your predictions are different but all less than 0.5. If that's the case then you can do a number of things, such as downsampling your training set or using the `class_weights` argument when building your RF." CreationDate="2015-08-10T01:39:32.667" UserId="947" />
  <row Id="7199" PostId="6735" Score="1" Text="I suggest you to read about [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)." CreationDate="2015-08-10T04:50:12.297" UserId="2452" />
  <row Id="7200" PostId="6707" Score="0" Text="how do I set the threshold" CreationDate="2015-08-10T05:08:08.020" UserId="8013" />
  <row Id="7201" PostId="6735" Score="0" Text="What type of analysis do you intend to apply to the variables, this will affect the answer, as will the size of your data set. In some circumstance highly correlated variables may contain useful discriminative information. Ultimately run your analysis with the variable removed and with it left in and see how the results differ." CreationDate="2015-08-10T08:24:15.340" UserId="7980" />
  <row Id="7204" PostId="6735" Score="0" Text="@AleksandrBlekh Thanks for your suggestion" CreationDate="2015-08-10T10:33:59.843" UserId="9793" />
  <row Id="7205" PostId="6735" Score="0" Text="@image_doctor - Thank you very for your advice. My intention of doing chisquare test is to check for the collinearity between the variables. I got confused between the purposes of collinearity and chisquare test. Hence i raised the question. Thanks again." CreationDate="2015-08-10T10:36:31.123" UserId="9793" />
  <row Id="7207" PostId="6735" Score="0" Text="@Arun: You're welcome." CreationDate="2015-08-10T10:49:34.543" UserId="2452" />
  <row Id="7208" PostId="6748" Score="0" Text="I tried to plot it as heat map, but it looks different. Could you please give me any hits to start? Thank you." CreationDate="2015-08-10T18:46:09.120" UserId="12177" />
  <row Id="7209" PostId="6707" Score="0" Text="Make some test to what is best for you. Typically 0 is the optimal solution. That means that there is no better solution than this one. Since it's an hyperparam, you can learn it via a neural network." CreationDate="2015-08-10T20:55:57.640" UserId="11113" />
  <row Id="7210" PostId="6749" Score="0" Text="Yes, it is quite straightforward. What have you tried so far? Do you have code to work with?" CreationDate="2015-08-10T22:03:51.397" UserId="947" />
  <row Id="7211" PostId="6748" Score="0" Text="IMHO, this question fits the _Cross Validated_ SE site the best. I suggest to consider migrating it there or the _DSP_ SE site, as @maj noted." CreationDate="2015-08-11T03:10:14.830" UserId="2452" />
  <row Id="7212" PostId="6749" Score="0" Text="I suggest migrating this question to _StackOverflow_, as this is a purely R programming question (_data visualization_)." CreationDate="2015-08-11T03:28:42.997" UserId="2452" />
  <row Id="7213" PostId="6707" Score="0" Text="@Dref360 i want to learn it via dynamic programming , I dont want to learn it via neural," CreationDate="2015-08-11T03:53:12.397" UserId="8013" />
  <row Id="7214" PostId="6707" Score="0" Text="@Dref360 what is hyperparam, i googled, i got the term hyperparameter, i that the short form of hyperparam ?" CreationDate="2015-08-11T03:54:34.170" UserId="8013" />
  <row Id="7215" PostId="6707" Score="0" Text="@Dref360 can I stop learning when I notice no new updation in any of the states ??" CreationDate="2015-08-11T04:19:40.790" UserId="8013" />
  <row Id="7216" PostId="6673" Score="0" Text="Chrisfs, this is actually a straw man argument.. Re-read what I wrote, &quot;To be successful, a PhD is not necessary, but a Masters degree most certainly is&quot; and compare it to what you posited. They are quite different." CreationDate="2015-08-11T06:32:47.547" UserId="11054" />
  <row Id="7217" PostId="6748" Score="0" Text="ok. I will do that, thank you" CreationDate="2015-08-11T12:57:16.717" UserId="12177" />
  <row Id="7218" PostId="6760" Score="0" Text="What kind of topic model are you using? Have you tried varying the number of topics?" CreationDate="2015-08-11T15:03:03.807" UserId="11136" />
  <row Id="7220" PostId="6760" Score="0" Text="What sort of analysis are you doing? Is it supervised? Is it exploratory? The issue may not even be the topics themselves but the data you are feeding into forming topics." CreationDate="2015-08-11T17:11:06.830" UserId="947" />
  <row Id="7221" PostId="6638" Score="0" Text="That's true, though on the other hand I think it's good to use &quot;meaningful&quot; indicators of orientation. Calculating a hash would probably be unique, but two very similar states would be mapped completely differently." CreationDate="2015-08-11T19:39:57.133" UserId="10907" />
  <row Id="7222" PostId="6707" Score="0" Text="@Rishika HyperParam == HyperParameter for exemple in neural network : number of layer, number of hidden neuron. Yes you can stop learning when there is not update in the state. That mean there is no better solution." CreationDate="2015-08-11T21:19:23.810" UserId="11113" />
  <row Id="7224" PostId="6756" Score="0" Text="Interesting answer! That's an excellent idea.Thanks" CreationDate="2015-08-11T22:39:03.580" UserId="8037" />
  <row Id="7226" PostId="6767" Score="0" Text="Thank you! So if I have this right, my probability for a white male, age 45-64, for 2010-2012 would be &#xA;$$ &#xA;P = {7.0 * 12.0 * 6.7 \over 7.3^2}/1000 = 0.01056108087 &#xA;$$&#xA;Meaning there is about a 1.056% chance of diagnosis for an individual with that profile." CreationDate="2015-08-11T23:47:55.327" UserId="12203" />
  <row Id="7228" PostId="6767" Score="0" Text="Almost. You forgot the normalization step. You need to also compute the value for not getting diabetes:&#xA;$$ \frac{993.0 * 988.0 * 993.3}{992.7^2}/1000 = 0.98889591936 $$&#xA;And then normalize so that these two numbers add to one (as probabilities should):&#xA;$$ P = \frac{0.01056108087}{0.01056108087 + 0.9888959194} = 0.01056681865 $$&#xA;For a **1.057%** chance. With these particular sets of numbers, the normalization step doesn't make much of a difference. With other numbers, though, leaving off the normalization step can lead to rather nonsensical results." CreationDate="2015-08-12T00:33:20.407" UserId="11142" />
  <row Id="7229" PostId="6707" Score="0" Text="okay, got it :)" CreationDate="2015-08-12T03:45:20.677" UserId="8013" />
  <row Id="7230" PostId="6768" Score="1" Text="What did the data look like in 2 or 3 dimensions after you applied PCA, were there noticeable clusters ? What examples are being misclassified, is there a pattern ?" CreationDate="2015-08-12T07:04:40.567" UserId="7980" />
  <row Id="7231" PostId="6768" Score="0" Text="What do the power spectra of the traces look like?  if you plot the mean spectra for each class, do they look different, if so how and can you optimise a classifier to capture that difference ?" CreationDate="2015-08-12T07:12:40.623" UserId="7980" />
  <row Id="7232" PostId="6748" Score="0" Text="Tough one -- I'd consider 'how to plot in R' on-topic for this SE, although then the question should contain more detail about what is being plotted and what's been tried so far." CreationDate="2015-08-12T09:32:36.350" UserId="21" />
  <row Id="7236" PostId="6775" Score="0" Text="The most likely answer is yes, though if can you clarify what you mean as &quot;just as an image&quot; and &quot;object recognition&quot; ( segmentation + classification ? ) the differences will become clearer." CreationDate="2015-08-12T15:15:48.667" UserId="7980" />
  <row Id="7237" PostId="6775" Score="0" Text="Like counting people in a cam video and counting people in image. Is there anything thing I should consider for video?" CreationDate="2015-08-12T15:25:54.070" UserId="11141" />
  <row Id="7238" PostId="6775" Score="0" Text="For video you have a time component, so methods that involve tracking as a component would become relevant. You might develop a model of human movement which would help increase accuracy with video sequences. Depending on frame rate and object velocity, deformation or blurring of the object in the video frame may be an issue or not." CreationDate="2015-08-12T16:07:36.460" UserId="7980" />
  <row Id="7239" PostId="6768" Score="0" Text="1) Can you show us the PCA cluster plot?, 2) Have you tried decision trees? If the original features are somewhat human-scrutinizable, you might be able to make sense of where it is going wrong. Otherwise (barring some silly bug on your part) it would seem your features are simply not discriminative enough." CreationDate="2015-08-12T16:20:58.547" UserId="10587" />
  <row Id="7240" PostId="6773" Score="0" Text="Just use `table(Data$ID)` or `as.data.frame(table(Data$ID))` if you want a `data.frame` back." CreationDate="2015-08-13T06:12:13.583" UserId="8479" />
  <row Id="7241" PostId="6783" Score="0" Text="Interestingly combining the binning and the bearing/distance has increased my overall accuracy to 0.82." CreationDate="2015-08-13T07:54:57.957" UserId="10998" />
  <row Id="7242" PostId="6783" Score="0" Text="Is the price your target variable or a feature ?" CreationDate="2015-08-13T09:14:27.173" UserId="7980" />
  <row Id="7243" PostId="6783" Score="0" Text="Price is my label I am attempting to classify. In my training data I have binned it into $50 buckets" CreationDate="2015-08-13T09:17:52.380" UserId="10998" />
  <row Id="7244" PostId="6783" Score="0" Text="The geospatial element definitely has an impact on the prices that are returned - so I can't ignore it at all." CreationDate="2015-08-13T10:29:14.940" UserId="10998" />
  <row Id="7246" PostId="6788" Score="0" Text="Well there is a valid use as a visualisation/introspection tool to find patterns that your network has learned. In this link http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html see the discussion about the dumbbell classifier. Not sure if that counts as a true purpose, since it is self-referential" CreationDate="2015-08-13T15:48:05.083" UserId="836" />
  <row Id="7247" PostId="6788" Score="0" Text="I think that's what makes the question difficult to answer, there are things behind deep dream that are widely applicable depending on how far you go, but is it still considered &quot;an application of deep dream&quot;? To me, it seems like deep dream is using an application of *those* techniques - which is what is applicable elsewhere. But I can't view that link at the moment so maybe I am incorrect." CreationDate="2015-08-13T16:02:10.147" UserId="12222" />
  <row Id="7248" PostId="6760" Score="0" Text="@David,@NBratley I updated my question please see above" CreationDate="2015-08-13T19:35:47.453" UserId="12240" />
  <row Id="7249" PostId="6786" Score="2" Text="Assuming you do have a set of weights for each component, why not use a metric like $d(x,y)=\sqrt{\sum_{i=1}^nw_i(x_i-y_i)^2}$ to figure out the closest neighbors?" CreationDate="2015-08-13T20:27:14.670" UserId="12241" />
  <row Id="7250" PostId="6786" Score="2" Text="Have you considered scaling your data before applying K-nerarest neighbours ?" CreationDate="2015-08-14T00:40:32.040" UserId="7980" />
  <row Id="7251" PostId="6786" Score="0" Text="@AlexR. Will using a custom metric as you suggested still work for knn search using kd-trees?" CreationDate="2015-08-14T06:57:35.167" UserId="226" />
  <row Id="7252" PostId="6789" Score="0" Text="11% accuracy is same as random guessing, or simply guessing same value each time. I cannot see any obvious bug. What is size of training set, and have you looked at your learning curve (the `J` values over time)? It is definitely worth checking your expansion of `y` into `Y` is correct - it looks over-complex, although could well be correct, I cannot tell. I might have instead just do something like `Y[y[i],i] =1;` for simplicity." CreationDate="2015-08-14T07:46:38.750" UserId="836" />
  <row Id="7253" PostId="6789" Score="0" Text="Have you verified that back propagation is working as you might expect on a small network and simple problem like XOR ?" CreationDate="2015-08-14T08:13:22.453" UserId="7980" />
  <row Id="7254" PostId="6787" Score="0" Text="Wonders in what context they meant that, regression, linearly separable data ?" CreationDate="2015-08-14T10:52:51.787" UserId="7980" />
  <row Id="7256" PostId="6760" Score="0" Text="How are you presenting the data to the model? Are you aggregating it at all? Traditional topic modeling on extremely short documents is difficult to get information out of." CreationDate="2015-08-14T14:40:09.230" UserId="11136" />
  <row Id="7257" PostId="6789" Score="1" Text="I've just ran your code with 10 iterations and got 70.58% of correct results on the training set. Can you repeat your experiment? Also, tracking error after each iteration (essentially, filling up your `J` array) may help to debug the issue if any." CreationDate="2015-08-14T14:55:18.163" UserId="1279" />
  <row Id="7261" PostId="6787" Score="0" Text="They probably meant the boundary between classes; is it composed of hyperplanes or not." CreationDate="2015-08-14T22:53:54.833" UserId="381" />
  <row Id="7262" PostId="6786" Score="3" Text="If you **want** to weight one dimension higher than others then I suggest you standardize all of your data so that the mean is zero and the standard deviation is one.  Then you can multiply the less important dimensions by a factor (2-10) so that they appear farther away to the KNN distance metric and leave the most important dimension un-scaled. Note that both standardizing and scaling are completely reversible processes, so there is very little reason not to use this simple solution." CreationDate="2015-08-14T23:29:10.197" UserId="9420" />
  <row Id="7264" PostId="6789" Score="0" Text="@friend You are right. It must be an after effects of late night coding that I mentioned 11% error. After running, 20 iterations the accuracy is 61.133333333333326 and the cost array looks like:&#xA;`20x1 Array{Float64,2}:&#xA; 4.14202&#xA; 4.65382&#xA; 4.02049&#xA; 4.57622&#xA; 4.55148&#xA; 5.61416&#xA; 5.51633&#xA; 4.70868&#xA; 4.68755&#xA; 5.10752&#xA; 4.79347&#xA; 5.51952&#xA; 5.05628&#xA; 5.04076&#xA; 5.07781&#xA; 4.9929 &#xA; 5.01385&#xA; 4.80254&#xA; 5.20314&#xA; 4.9887 `" CreationDate="2015-08-15T07:04:22.187" UserId="12250" />
  <row Id="7265" PostId="6789" Score="0" Text="@friend I am using cross entropy cost function:&#xA;`lambda = 1; &#xA;function costFunction(truth, prediction)&#xA;    cost = (-truth.*log(prediction)) - ((1-truth).*log(1-prediction));&#xA;    regularization = (lambda/(2*m))*(sum(sum(Theta1[2:end,:].^2)) + sum(sum(Theta2[2:end,:].^2)));&#xA;&#xA;    return (1/m)*sum(sum(cost)) + regularization; # regularized cost&#xA;end`" CreationDate="2015-08-15T07:16:45.673" UserId="12250" />
  <row Id="7266" PostId="6798" Score="1" Text="I don't know of a comprehensive list, but for now Kaggle has an open NLP competition. They tend to appear regularly on their website." CreationDate="2015-08-15T08:36:16.970" UserId="10517" />
  <row Id="7267" PostId="6789" Score="0" Text="Just to clarify, in every iteration I am using all the training inputs to modify the weight parameters. There is something I am missing which is not letting the cost to decrease with every iteration." CreationDate="2015-08-15T16:27:01.000" UserId="12250" />
  <row Id="7268" PostId="6796" Score="0" Text="But would this tactic actually work? If humans were manually checking stuff then yeah I can imagine. However a data combing algorithm which can predict pregnancy based on browsing history (or that image parsing neural network at Google) probably has a very different view on the world. Frankly they seem magic to me. :)&#xA;&#xA;(&quot;Little brother&quot; looks interesting. Now that I visited that page I'm on some kind of an agency list right?)" CreationDate="2015-08-16T07:57:00.067" UserId="12258" />
  <row Id="7269" PostId="6805" Score="1" Text="Have you overlapped test set data somehow, so the RF is being leaked the test data via one of the lower-level models? Please explain more about your data splitting strategy." CreationDate="2015-08-16T20:43:52.487" UserId="836" />
  <row Id="7270" PostId="6747" Score="1" Text="I'd say this is the most useful way because it requires little programming effort and it's practical." CreationDate="2015-08-16T21:37:34.590" UserId="8152" />
  <row Id="7271" PostId="6789" Score="1" Text="Without analyzing the _whole_ code I'd say you don't use specified cost function for optimization, but only for error calculation. If this is the case, gradient descent does a fair job optimizing non-regularized function and allowing coefficients in `Theta` (and thus error given by `costFunction`) to grow. I just used simpler error function (`J[i] += sum(delta3 .^ 2)`) and error seems to constantly decrease (though computation is not finished yet on my machine)." CreationDate="2015-08-16T23:36:19.617" UserId="1279" />
  <row Id="7272" PostId="6789" Score="0" Text="Seems like you have a bad luck - without any modifications in your code I've got 75.9% :)" CreationDate="2015-08-17T07:12:25.177" UserId="1279" />
  <row Id="7273" PostId="6807" Score="0" Text="Thanks, just what i wanted, nice intuition....i'll see all the references." CreationDate="2015-08-17T11:21:52.617" UserId="11141" />
  <row Id="7275" PostId="6806" Score="0" Text="if he's using a randomly selected test set there is no reason why using the same methodology within CV should cure a massive difference in results." CreationDate="2015-08-17T12:49:53.740" UserId="947" />
  <row Id="7276" PostId="6786" Score="1" Text="@AN6U5 Thanks. That certainly makes sense. However, my k-d tree is not constant. It needs to support both adding nodes (which is less frequent) and the k-neighbor search query(which is very frequent).  In that case standardizing the data won't be a good option correct." CreationDate="2015-08-17T13:05:33.690" UserId="226" />
  <row Id="7277" PostId="6786" Score="1" Text="The definition of &quot;nearest&quot; becomes meaningless in multiple dimensions with un-standardized data.  If Alice has 2 dogs and 10 apples and Bobby has 4 dogs and 5 apples, the distance between them without standardization is measured as some fractional power law of dog-apples, which changes as the distance vector changes orientation.  Its absolute garbage!  Once you standardize, the distance metric is measured in units of standard deviation of the population.  I understand the you have some sort of online learning algo, but the math only makes sense if you can define a mean and std." CreationDate="2015-08-17T15:35:42.190" UserId="9420" />
  <row Id="7278" PostId="6786" Score="0" Text="Thanks for clarifying @AN6U5." CreationDate="2015-08-17T17:33:47.727" UserId="226" />
  <row Id="7279" PostId="4971" Score="3" Text="The &quot;creative use&quot; depends on your application.  The key is in the details: &quot;Creative use of a combination of other resources and various access levels can satisfy nearly every application use case&quot;.  What's your application?  Do you want geo-encoded tweets, a statistical representation of all historical tweets, a longitudinal representation over 1 year, a representative sample from a specific region, a representative sample in a particular subject area?" CreationDate="2015-08-17T18:42:59.463" UserId="9420" />
  <row Id="7280" PostId="6789" Score="0" Text="@ffriend: I think `prediction - truth` *is* the correct gradient of $\frac{\partial J}{\partial z}$ for cross-entropy with sigmoid activation in output layer. So calculation of `delta3` is correct in the given code, **except** no regularisation is applied, so OP must set `lambda` to zero for correct values." CreationDate="2015-08-17T19:07:02.590" UserId="836" />
  <row Id="7281" PostId="6816" Score="0" Text="You could look at the fourier transform or perhaps count peaks and troughs, maybe after applying varying degrees of low pass filtering." CreationDate="2015-08-18T14:02:16.930" UserId="7980" />
  <row Id="7282" PostId="6816" Score="1" Text="Have a look at Wavelets because they provide a very powerful framework for comparing signals: https://en.wikipedia.org/wiki/Wavelet" CreationDate="2015-08-18T19:27:00.330" UserId="12241" />
  <row Id="7283" PostId="6791" Score="0" Text="Can you create a feature which is effectively your distance from some mean value ?" CreationDate="2015-08-18T19:46:28.650" UserId="7980" />
  <row Id="7284" PostId="5370" Score="0" Text="This is spot on.  I would note that if you don't have a CS background, the chance that you write code more efficiently than the underlying functions for python or packages for R is quite remote.  I programmed in C++ for 13 years, and still think there are aspects of memory management and performance optimization that I did not do well.&#xA;&#xA;Additionally, python &amp; R have very smart computer scientists optimizing distribution issues, so C languages will really be relegated to extreme low latency systems." CreationDate="2015-08-18T22:31:54.493" UserId="8041" />
  <row Id="7285" PostId="6817" Score="0" Text="This post is a little challenging to understand.  It seems like what you're saying is that you have gone through the data and you believe that in actuality there are more 'condition negative' than 'condition positive' instances?  If that is the case, then you simply have an error in the way you are tagging data as positive or negative (might you have inverted them?)" CreationDate="2015-08-18T22:45:09.963" UserId="8041" />
  <row Id="7286" PostId="6791" Score="0" Text="That is a good idea, however the more I think about it, the more I see that my problem isn't really measuring and ranking a single point in time, but more like a change over time and amplifying the impact of moving away from the mean more than moving away.  But in this case then perhaps I can use the rate of change as that amplification.  Does anyone have any experience with this type of measurement?" CreationDate="2015-08-18T23:08:58.797" UserId="12256" />
  <row Id="7287" PostId="6822" Score="1" Text="That's exactly what had happened. Really helpful post, thanks :)" CreationDate="2015-08-19T08:05:33.123" UserId="12314" />
  <row Id="7288" PostId="6818" Score="1" Text="You mean simply copy pasting others comments, right?" CreationDate="2015-08-19T11:02:07.217" UserId="8479" />
  <row Id="7289" PostId="6829" Score="0" Text="I'm using JNI in my projects and lets say I'm (not much but) comfortable in that By now. But if we are doing real processing of data via Some DLL methods only then is it wise to use Apache Spark  and how do we use it optimally ??" CreationDate="2015-08-19T11:52:13.877" UserId="10210" />
  <row Id="7290" PostId="6829" Score="0" Text="I know it is application specific, still some guideline or some documentation I'm looking for. Apart from SO questions there is no definitve Guideline is availabe as far as I saw." CreationDate="2015-08-19T11:53:00.727" UserId="10210" />
  <row Id="7291" PostId="6829" Score="0" Text="@spt025 Could you edit your question to add some specific concerns? I'm not sure if you asking about concurrency, performance, or technical implementation." CreationDate="2015-08-19T11:54:50.957" UserId="10189" />
  <row Id="7292" PostId="6829" Score="0" Text="Yea sure , Will do that .." CreationDate="2015-08-19T11:55:49.233" UserId="10210" />
  <row Id="7293" PostId="6831" Score="0" Text="Are the temperatures related to any temporal events ? The mean is least squares optimal, but perhaps that  does not capture some aspect of the sequences which you wish to account for ? WHy are you particularly interested in the extrema ?" CreationDate="2015-08-19T15:50:46.683" UserId="7980" />
  <row Id="7294" PostId="6827" Score="0" Text="Human behaviour is often related to the day of the week so allocating entire days to either training or test doesn't not seem initially to be an optimal choice." CreationDate="2015-08-19T16:07:48.427" UserId="7980" />
  <row Id="7295" PostId="6831" Score="0" Text="So the environment is inside of a coffee roaster, and I have temperature measurements in different areas inside of the enclosure of the system. The picture shows just one of these areas. I also have measurements for Watts, Volts and Amps.&#xA;&#xA;Given certain conditions (say, Power draw), I want to predict what the temperature would be in each of these areas that I've tested." CreationDate="2015-08-19T16:17:29.560" UserId="12202" />
  <row Id="7296" PostId="6834" Score="1" Text="Find strongly connected components (using DFS) and contract each component into a single vertex. https://en.wikipedia.org/wiki/Strongly_connected_component." CreationDate="2015-08-19T17:32:24.933" UserId="6550" />
  <row Id="7297" PostId="6827" Score="0" Text="However would you suggest to make classifier for each app ? In order to predict the possibility of an app to be used at a location combined with time of day and also day of week ? For example logistic regression trained only on app1, logistic regression trained only for app2 etc...and then after collecting data for 2 weeks. Train these classifiers with  the data from the first week and test my predictions on the second week? Is logistic regression for each app a good approach ? that is what i am actually askiing.." CreationDate="2015-08-19T17:34:12.863" UserId="12333" />
  <row Id="7298" PostId="6827" Score="0" Text="1 v All classification schemes, such as you suggest, can be effective, or you could just use a classifier that can deal with multiple classes such as RandomForest  or MultiLayerPerceptrons. Two weeks doesn't sound like a lot of data, but if that's all you've got, then it sounds reasonable. Though weeks in August won't be like those in December. I&quot;d look at a range of classifiers not just one, that's the best way to explore the function solution space." CreationDate="2015-08-19T18:18:36.607" UserId="7980" />
  <row Id="7299" PostId="6828" Score="0" Text="@Sean Owen Kindly can you explain the reason why you put this question on hold as it is sort of subjective question and I can quote such question from SO only where it is still not answered." CreationDate="2015-08-19T19:14:54.587" UserId="10210" />
  <row Id="7300" PostId="6827" Score="0" Text="&quot;Spatio Temporal Statistics&quot;" CreationDate="2015-08-19T22:25:18.487" UserId="7848" />
  <row Id="7302" PostId="4929" Score="1" Text="@AleksandrBlekh I was able to finally get docker to work on my Windows 7 machine by regenerating the certificates&#xA;`docker-machine regenerate-certs`&#xA;I hope that helps :)" CreationDate="2015-08-20T06:12:54.847" UserId="643" />
  <row Id="7303" PostId="6838" Score="1" Text="The accuracy score is the only true measure of their relative performance, though there may be prior art that indicates which type of classifier from the many available might be better on  your type of data if you are lucky. You may be more constrained by how long it takes to tune your parameters, in which case the Random Forest will probably prove an easier proposition of the two." CreationDate="2015-08-20T06:44:44.977" UserId="7980" />
  <row Id="7304" PostId="6828" Score="0" Text="I migrated it to StackOverflow, since it's about developing a software application." CreationDate="2015-08-20T07:09:37.123" UserId="21" />
  <row Id="7305" PostId="6828" Score="0" Text="@SeanOwen Thank you for your help.. :)" CreationDate="2015-08-20T07:55:07.583" UserId="10210" />
  <row Id="7306" PostId="4929" Score="0" Text="@R.K.: Thank you for letting me know. I will give it a try when I'll get a chance (it might take a while, though, as there are some higher priority matters waiting to be taken care of)." CreationDate="2015-08-20T09:11:07.180" UserId="2452" />
  <row Id="7307" PostId="6840" Score="1" Text="What features do you have on the apps and on the conditions of the download?" CreationDate="2015-08-20T09:33:36.993" UserId="10814" />
  <row Id="7308" PostId="6840" Score="1" Text="@Edmund number of downloads per category and percentage of per app downloads based on gender" CreationDate="2015-08-20T09:36:51.150" UserId="12355" />
  <row Id="7309" PostId="6840" Score="1" Text="I suspect Google Play and App Store already have a very good estimator of the user's gender based on their account details and email address." CreationDate="2015-08-20T10:22:03.933" UserId="7980" />
  <row Id="7310" PostId="6818" Score="1" Text="Yeah, i guess it would help if I had seen the comment. Sorry about that! :)" CreationDate="2015-08-20T11:31:51.077" UserId="12316" />
  <row Id="7311" PostId="6839" Score="0" Text="This is a great response!  When I try to use the Holt-Winters function, an error is shown telling me that my time series has no or less than 2 periods.  How do I correct this issue?" CreationDate="2015-08-20T12:20:50.683" UserId="12202" />
  <row Id="7312" PostId="6839" Score="1" Text="I answered my own question:&#xA;`HoltWinters(watts, gamma=false)`&#xA;I had to set gamma=false to apply a non-seasonal model." CreationDate="2015-08-20T13:02:17.223" UserId="12202" />
  <row Id="7313" PostId="6839" Score="0" Text="I can see some distinct trends. [Trends](http://imgur.com/zmX1yfH).&#xA;Now that I've seen these trends, how can I create a &quot;model&quot; based off of these trends?" CreationDate="2015-08-20T14:41:54.660" UserId="12202" />
  <row Id="7314" PostId="6840" Score="1" Text="But what features are they looking for in order to better the estimate of gender" CreationDate="2015-08-20T15:47:56.860" UserId="12355" />
  <row Id="7315" PostId="6843" Score="1" Text="But what features should I choose in order to better train the dataset and estimate the gender?" CreationDate="2015-08-20T15:51:17.210" UserId="12355" />
  <row Id="7316" PostId="6841" Score="3" Text="Please fix your formatting and use `dput()` to created a minimal reproducable example. This is really a question for [Stack Overflow](http://stackoverflow.com/) though" CreationDate="2015-08-20T17:37:33.590" UserId="11021" />
  <row Id="7317" PostId="6839" Score="0" Text="That's a whole lot of smoothing.  If you think the real temperature varies that slowly then you probably have a bad temperature probe.  It not, then I would dial the smoothing way back.  Try a linear regression to get things started.  But after shaking out the bugs, it seems like you probably want to capture the non-linearity with a nonlinear model like a [Support Vector Regression (SVR)](http://alex.smola.org/papers/2003/SmoSch03b.pdf)" CreationDate="2015-08-20T20:07:02.157" UserId="9420" />
  <row Id="7319" PostId="6852" Score="0" Text="thanks for the comments. I will check out that." CreationDate="2015-08-21T04:55:35.303" UserId="3465" />
  <row Id="7320" PostId="6548" Score="0" Text="@user10456 was your problem solved ?" CreationDate="2015-08-21T09:49:25.057" UserId="10834" />
  <row Id="7321" PostId="6841" Score="1" Text="Tough one. I think this would be fine on SO, but as R is a key data science tool, I don't find it off-topic for this site either. It's in the overlap, IMHO." CreationDate="2015-08-21T09:52:35.357" UserId="21" />
  <row Id="7322" PostId="6848" Score="1" Text="I&quot;m not familiar with High Dimensional statistical methods, might you have some examples of techniques which would be of this type ?" CreationDate="2015-08-21T11:08:58.610" UserId="7980" />
  <row Id="7324" PostId="6857" Score="1" Text="Neat, I hadn't heard of InfluxDB. It looks like it's designed for web analytics but could probably handle the data I'm collecting -- I sample at ~100Hz on ~200 channels. Thanks for the pointer!" CreationDate="2015-08-21T14:24:56.377" UserId="4647" />
  <row Id="7325" PostId="6866" Score="0" Text="How many bins of sizes are there  ?" CreationDate="2015-08-21T15:37:33.110" UserId="7980" />
  <row Id="7326" PostId="6857" Score="0" Text="It's actually designed for IoT sensor data :-)" CreationDate="2015-08-21T15:57:11.987" UserId="12234" />
  <row Id="7327" PostId="6853" Score="2" Text="Thanks! Levenshtein is causing some issues because there are merchants like 'Google Marketplace' and 'Goodwill Marketplace' which have a lower score than even 'Wal-mart' and 'walgreens'.  I think I can probably whitelist the outliers though.  And double thanks for the book recommendation." CreationDate="2015-08-21T15:57:13.977" UserId="12364" />
  <row Id="7328" PostId="6866" Score="0" Text="Potentially limitless." CreationDate="2015-08-21T16:03:07.447" UserId="12381" />
  <row Id="7329" PostId="6853" Score="1" Text="That's a tough one as normalized Levenshtein is less than $.16$ which is a cutoff I often use.  I guess you could also try extracting the words through space delineation and then comparing all words to all words in some smart way.  This is a tough problem if you are going for high confidence!  Another option is see where you get with exact match and then use a mechanical turk (or yourself) for the last couple million." CreationDate="2015-08-21T16:07:18.047" UserId="9420" />
  <row Id="7330" PostId="6866" Score="0" Text="You *might* be able to re-frame your problem using an RNN or some variant, then use a small window working sequentially. But it is not a subject area I know much about. I searched for uses of RNNs as signal denoisers, and found this: http://www1.icsi.berkeley.edu/~vinyals/Files/rnn_denoise_2012.pdf" CreationDate="2015-08-21T18:17:34.927" UserId="836" />
  <row Id="7331" PostId="6841" Score="0" Text="@SeanOwen fair enough." CreationDate="2015-08-21T21:22:56.167" UserId="11021" />
  <row Id="7332" PostId="6868" Score="1" Text="for 2013: https://www.quora.com/What-were-the-scientific-highlights-of-NIPS-2013" CreationDate="2015-08-21T21:33:10.130" UserId="843" />
  <row Id="7334" PostId="6866" Score="0" Text="I think we might need more information on the data, maybe you could normalise time if that was appropriate, maybe you could bin sequences into a variety of fixed lengths  , maybe there are features of the signal that  are discriminative and relatively independent of time?" CreationDate="2015-08-22T11:05:00.337" UserId="7980" />
  <row Id="7335" PostId="6870" Score="0" Text="Info gain from what to what? are you looking for feature's predictiveness?" CreationDate="2015-08-23T06:49:18.340" UserId="21" />
  <row Id="7336" PostId="6868" Score="0" Text="These questions are generally considered too broad for SE, being mostly opinion-based" CreationDate="2015-08-23T06:50:03.160" UserId="21" />
  <row Id="7337" PostId="6858" Score="0" Text="Thanks Tom for your reply!" CreationDate="2015-08-23T19:39:22.803" UserId="10810" />
  <row Id="7338" PostId="6878" Score="0" Text="This is among the things I've been doing manually, however with two sets of 10k observations each it's very time consuming. Do you have any idea on how this could automated?" CreationDate="2015-08-23T19:45:53.663" UserId="12403" />
  <row Id="7339" PostId="6875" Score="0" Text="Thanks, I'll give that method a try" CreationDate="2015-08-23T20:00:05.287" UserId="12403" />
  <row Id="7340" PostId="6874" Score="0" Text="Why does the combination of individually non-unique not help you? Does it become unique or not?" CreationDate="2015-08-23T20:11:01.390" UserId="12402" />
  <row Id="7341" PostId="6879" Score="0" Text="I think you may need to elaborate on &quot;There is some intuition about what results are good, however.&quot; The most obvious combination to me would simply be $J = - \prod_{\forall i} \frac{v_i}{100}$. Other than that, there are very many possible cost functions here, and you don't give any real indication of why you might choose one over another" CreationDate="2015-08-23T20:12:16.880" UserId="836" />
  <row Id="7342" PostId="6879" Score="0" Text="@NeilSlater: You are correct.  That's why I want a tunable family of cost functions :)  An example based on your suggestion: product( (v_i/100) ** p ) ** (1/q) for some p, q which can be adjusted until they match intuition.  Intuition can/should also be studied quite rigorously, by giving pairs of scenarios to experts to rank relative to each other." CreationDate="2015-08-23T20:30:21.233" UserId="26" />
  <row Id="7343" PostId="6879" Score="0" Text="Alternatively the log form $J = -\sum_{\forall i} log( \frac{v_i + C}{100} )$ would also be typical. Ultimately that's the same stat, but whether it is useful depends on which algorithm you want to use to optimise the value." CreationDate="2015-08-23T20:31:00.517" UserId="836" />
  <row Id="7346" PostId="6878" Score="1" Text="Sorry, I hadn't considered anyone would try to do this manually," CreationDate="2015-08-24T00:16:47.570" UserId="7980" />
  <row Id="7348" PostId="6867" Score="0" Text="So, by default, I think 'glm' function models for 1?" CreationDate="2015-08-24T07:21:43.540" UserId="10050" />
  <row Id="7349" PostId="6819" Score="0" Text="Thats a good idea - unfortunately I am in Australia, but I will check the Australian Bureau of Statistics to see if they have something similar (from memory a Census Collection Area might be workable)." CreationDate="2015-08-24T07:49:36.123" UserId="10998" />
  <row Id="7350" PostId="6655" Score="0" Text="Thanks for laying out like this. Yeah.. what I was looking for was de-identification." CreationDate="2015-08-24T09:22:49.330" UserId="10979" />
  <row Id="7351" PostId="6874" Score="0" Text="I haven't gotten any combinations to work well enough: Either they are not unique or they are but filter out a lot of matches that they shouldn't due to different notations in the two datasets" CreationDate="2015-08-24T09:27:09.403" UserId="12403" />
  <row Id="7352" PostId="6874" Score="0" Text="Are you really tied to using Excel? Are you also going to use Excel in your ultimate analysis? I think if you want to avoid using something else, @Stereo's answer, or maybe a slightly modified version is what you'd have to go for" CreationDate="2015-08-24T09:30:54.163" UserId="12402" />
  <row Id="7353" PostId="6878" Score="0" Text="Yes I've added to the answer a few tools I would consider to automate this, if you have data we could experiment with  maybe  solutions would be tractable." CreationDate="2015-08-24T12:36:34.870" UserId="7980" />
  <row Id="7354" PostId="6866" Score="0" Text="I've tried working with time-independent features with mixed results, and I wanted to see if there were any techniques for handling the irregular periodicity so I could work in the time domain." CreationDate="2015-08-24T13:11:08.677" UserId="12381" />
  <row Id="7355" PostId="6884" Score="0" Text="What do you mean by R/SAS? Do you refer to SAS IML or how H2O compares to R and/or SAS?" CreationDate="2015-08-24T16:20:33.863" UserId="12384" />
  <row Id="7356" PostId="6887" Score="0" Text="By the way, I think there have been a lot of questions of this kind over at StackOverflow.com." CreationDate="2015-08-24T16:58:49.897" UserId="10025" />
  <row Id="7357" PostId="6874" Score="0" Text="Well, Excel is the only tool I'm familiar with. What tool would you recommend for a beginner?" CreationDate="2015-08-24T17:52:50.807" UserId="12403" />
  <row Id="7358" PostId="6878" Score="0" Text="Thanks. The data stems from commercial data providers so I'm not sure I'd be allowed to share it, unfortunately. I haven't worked with any of the tools you've mentioned - which would you recommend for a beginner?" CreationDate="2015-08-24T18:00:12.633" UserId="12403" />
  <row Id="7359" PostId="6878" Score="0" Text="import.io - it's designed to require little or no programming" CreationDate="2015-08-24T18:01:09.373" UserId="7980" />
  <row Id="7360" PostId="5705" Score="0" Text="I think I get what you are aiming for, with the &quot;travel the world&quot; bit, but I don't think that specific wording is good advice for at least some people. &quot;*A very boring person with little in the way of creativity to do anything new*&quot; is an uninspired person, and you are saying that not only the skills are important, but that being inspired and creative is very important as well. Traveling the world is one way to gain inspiration, but everybody gains inspiration in different ways. (*It would not give me any help towards Computer Science, Engineering, or Science - at all*)" CreationDate="2015-08-24T18:54:25.553" UserId="12222" />
  <row Id="7361" PostId="6889" Score="0" Text="Works perfectly! I should have been able to think of that. Thanks for this simple and elegant solution." CreationDate="2015-08-24T20:05:21.027" UserId="10345" />
  <row Id="7362" PostId="6885" Score="0" Text="ECG data perhaps" CreationDate="2015-08-24T21:51:19.010" UserId="7980" />
  <row Id="7363" PostId="6885" Score="0" Text="Do you have data?  What data sets do you have available?  Or are you asking for data set sources also? There are zillions of great neural network problems in medicine, but much less data and much much less public data.  Moderators have flagged this due to vagueness and I'm trying to keep it alive, but please add some additional context.  Thanks!" CreationDate="2015-08-24T23:35:02.990" UserId="9420" />
  <row Id="7364" PostId="6827" Score="0" Text="image_doctor. Do you believe Kohenes neural network is a good approach to use in my case?" CreationDate="2015-08-25T03:00:41.597" UserId="12333" />
  <row Id="7366" PostId="6885" Score="0" Text="If language processing is your thing, then try analyzing topics in the medical literature using some brand-spanking-new neural network topic models: http://nlp.cs.rpi.edu/paper/AAAI15.pdf (for example)" CreationDate="2015-08-25T04:21:27.753" UserDisplayName="user9424" />
  <row Id="7367" PostId="6881" Score="0" Text="That worked perfectly. Thanks a lot Stereo." CreationDate="2015-08-25T05:28:25.057" UserId="12351" />
  <row Id="7369" PostId="6890" Score="1" Text="This option is okay in a sequential way i think, but i forgot to say that i try to do the trick in a distributed way..." CreationDate="2015-08-25T09:18:04.690" UserId="9257" />
  <row Id="7373" PostId="6895" Score="0" Text="Thanks a lot @Stereo" CreationDate="2015-08-25T13:25:08.587" UserId="10050" />
  <row Id="7375" PostId="6885" Score="0" Text="@AN6U5 - I do not have any data and what I can get is only open data that is publicly available on internet. As of now I am more concerned on the finding a good idea and this project can last long at least for an year. I want use machine learning techniques in my project. I am just a starter in this topic so i do not have much info on this. As for the vagueness in the question i will try to reduce it soon. i just forgot to mention that i have a hardware limitation, i cannot use high end systems or some systems with special architecture as i cannot afford them - i am just a college student." CreationDate="2015-08-25T13:52:26.400" UserId="11104" />
  <row Id="7376" PostId="6890" Score="1" Text="This makes no difference if you loop through all of your parallel ranks and have a map of where the clusters sit on the computer cluster.  Further, you hash can sort your clusters so that most communication is local.  Even if you do it the way that you specified in the question.  Just include some sort of `if(cluster A exists and cluster B exists): fusecluster(A,B) else: findNearestCluster(A)`" CreationDate="2015-08-25T14:34:44.207" UserId="9420" />
  <row Id="7377" PostId="6900" Score="0" Text="I appreciate your reply, I have used this metic however, the drawback here is that the vectors magnitude are not taken into account. For example, similarity of {1,1,1} and {2,2,2} is same is {1,1,1) and {1,1,1}" CreationDate="2015-08-25T17:48:16.560" UserId="12433" />
  <row Id="7378" PostId="6900" Score="0" Text="Check out the Sorensen metric that I added then.  I think 1-sorensen will work for you." CreationDate="2015-08-26T04:02:56.063" UserId="9420" />
  <row Id="7379" PostId="6899" Score="0" Text="Are you classifying each input case to be either true or false ?" CreationDate="2015-08-26T07:05:34.047" UserId="7980" />
  <row Id="7381" PostId="5357" Score="0" Text="May I ask whether there is any good book of machine learning in C?" CreationDate="2015-08-26T03:05:34.000" UserId="12446" />
  <row Id="7382" PostId="6904" Score="0" Text="With 230 features (that you describe as likely actually being 1,000 features) the best machine learning method and cross validation method depends quite a bit on whether &quot;many thousands of entries&quot; means 4,000 or 900,000.  Please consider editing your post to add a bit more specificity here.  Also, is the application the generates the guess and the &quot;success&quot; and &quot;fail&quot; an ML algorithm or some sort of deterministic system?  Why not just use ML for the whole thing?  Some additional information will help generate the best possible answer..." CreationDate="2015-08-26T15:04:10.837" UserId="9420" />
  <row Id="7383" PostId="6904" Score="0" Text="Thanks for your comment. The user enters the success / fail.... it needs outside (human) evaluation." CreationDate="2015-08-26T15:27:15.510" UserId="12454" />
  <row Id="7385" PostId="6904" Score="1" Text="A physical measurement in a lab" CreationDate="2015-08-26T15:33:06.597" UserId="12454" />
  <row Id="7386" PostId="6907" Score="2" Text="You could set a threshold in terms of the number of standard deviations from the mean." CreationDate="2015-08-26T15:37:58.060" UserId="7980" />
  <row Id="7387" PostId="6906" Score="0" Text="Why particularly do you feel that decision trees would be appropriate for the OP's data?" CreationDate="2015-08-26T15:40:07.393" UserId="7980" />
  <row Id="7390" PostId="6903" Score="0" Text="This solution has two issues I can see. 1, since the diff is not squared as for sum of square errors, so errors with opposite direction  will cancel each other. In addition,  correct me if wrong, this solution does not take into account unique positons of vector elements in case this important" CreationDate="2015-08-26T19:24:03.013" UserId="12433" />
  <row Id="7391" PostId="6906" Score="0" Text="These combinations are used in the pharmaceutical industry to see which combinations molecules are working. after the test they want to know which combinations of molecules have an effect on each other and which don't. So they know where not to look and that saves money and time." CreationDate="2015-08-26T19:30:11.590" UserId="10517" />
  <row Id="7392" PostId="6903" Score="0" Text="Good point about forgetting to add Abs ! What is unique about the positions of the vector elements ? If you wanted a weighted sum of similarities something would have to determine that weighting. Is there any any information on that ? Or would you want to apply a machine learning approach to the weighting scheme ?" CreationDate="2015-08-26T19:54:42.790" UserId="7980" />
  <row Id="7393" PostId="6907" Score="1" Text="This stretches the definition of &quot;outlier&quot;, maybe to the point that you will get bad results in your work. Is there some knowledge or expectation in general about the numbers you are seeing in your project, that you could somehow include (if so, maybe explain)? Or is there some common theme amongst repeated sets of numbers that could maybe used to learn a rule (as you have tagged this &quot;machine-learning&quot;)?" CreationDate="2015-08-26T19:58:43.753" UserId="836" />
  <row Id="7395" PostId="6904" Score="0" Text="Good question: about 20,000, I updated the question." CreationDate="2015-08-27T00:38:57.320" UserId="12454" />
  <row Id="7396" PostId="6835" Score="0" Text="The lowest RMSE I've found so far is contained here:&#xA;http://users.cecs.anu.edu.au/~akmenon/papers/autorec/paper.pdf" CreationDate="2015-08-27T03:21:00.773" UserId="12306" />
  <row Id="7397" PostId="6908" Score="1" Text="WEKA has a range of filters that can be used for preprocessing data http://weka.sourceforge.net/doc.dev/weka/filters/Filter.html" CreationDate="2015-08-27T08:33:33.990" UserId="7980" />
  <row Id="7399" PostId="5370" Score="0" Text="Agreed - you would want to learn C for the performance critical sections because it is very 'close to the metal' - you are in total control of the low level memory allocations, pointer manipulation etc. which makes it comparatively much more difficult to write and debug (so you want to do as little of it as possible), but also blazing fast, and you can even tailor your code more specifically to the exact hardware you have, squeezing every ounce of power out of it." CreationDate="2015-08-27T14:53:37.090" UserId="9587" />
  <row Id="7400" PostId="6350" Score="0" Text="Although it is a bit offtopic, but it looks like OP is trying to solve outlier detection problem. I would suggest Isolation Trees, the method is pretty mature and is used in production systems." CreationDate="2015-08-27T15:54:48.870" UserId="7848" />
  <row Id="7402" PostId="6904" Score="0" Text="Take a look here https://www.kaggle.com/c/higgs-boson, in the forum are showed the solutions of the winners." CreationDate="2015-08-27T19:44:40.480" UserId="10938" />
  <row Id="7403" PostId="6899" Score="0" Text="@image_doctor : yes" CreationDate="2015-08-27T20:27:15.677" UserId="12438" />
  <row Id="7404" PostId="6921" Score="0" Text="I am not sure where you found a reference showing dropout used to mask *weights*? I think that is incorrect." CreationDate="2015-08-27T20:51:02.060" UserId="836" />
  <row Id="7405" PostId="6921" Score="0" Text="you right, I used the wrong concept." CreationDate="2015-08-27T21:05:01.353" UserId="10938" />
  <row Id="7406" PostId="6870" Score="0" Text="I am applying ID3 algorithm (Classification tree). So, right from root node, i have to calculate entropy and IG. IG for feature predictiveness. [I have referred this link for ID3 implementation ](https://www.youtube.com/watch?v=wL9aogTuZw8)" CreationDate="2015-08-26T13:31:42.913" UserDisplayName="user12456" />
  <row Id="7407" PostId="6925" Score="0" Text="This sounds too broad for StackExchange by a fair margin. Can you narrow it down to something more specific?" CreationDate="2015-08-28T08:30:06.767" UserId="21" />
  <row Id="7408" PostId="6920" Score="1" Text="Thank you for the detailed explanation. To summarise your points, my understanding is as follows. First, we need to know the underlying distribution of a group of numbers to define the outliers. I guess what this implies is data points not lying on the distribution called outliers? Second, mean and stdev can be used to detect outliers only for a large sample. They won't work on small sets of numbers (as for the example that I have given)." CreationDate="2015-08-28T09:55:42.180" UserId="12457" />
  <row Id="7409" PostId="6900" Score="1" Text="Seems like a great solution!. Tried this solution over the following vectors {0.88,0.60} and {1,0.33} and got similarity of 0.961. By itself this similarity seems to be quite high isn't it?  Compared to the similarity obtained by the solution provided by image_doctor the similarity is 0.805. It seems like it is more appropriate, however I am still not sure which method to use." CreationDate="2015-08-28T11:19:40.380" UserId="12433" />
  <row Id="7410" PostId="6900" Score="0" Text="@image_doctor solution is a good one... its basically Manhattan distance with a percentile based standardization.  For that matter, you could also use Euclidean with a percentile or range based standardization.  The only drawback of both is that the normalization is very sensitive to outliers.  So you have to decide whether you want to prioritize immunity to outliers (i.e. std based standardization) or the enforcement of [0-1].  Sorensen might give the best of both worlds.  Note that comparing the raw value between metrics doesn't really make sense." CreationDate="2015-08-28T11:35:57.593" UserId="9420" />
  <row Id="7411" PostId="6920" Score="1" Text="Kind of.. I'm saying you at least must have some assumption about the distribution; you may not know it for sure. Typically for a continuous distribution like the normal distribution all values are &quot;possible&quot; in the distribution, just some are unlikely. What I mean is that the sample mean/stdev tends to the real population mean/stdev as the sample size increases, which is a little different. For a small sample, it by itself is 'not enough information' even given an assumption of normality." CreationDate="2015-08-28T12:22:19.447" UserId="21" />
  <row Id="7412" PostId="6916" Score="1" Text="Thanks. GLMS cover linear regression, so it means linear regression cannot be parallelized?" CreationDate="2015-08-28T13:28:45.413" UserId="10522" />
  <row Id="7413" PostId="6915" Score="1" Text="Ok, so the data needs to be moved from sql db to HDFS before Hadoop can work its magic?" CreationDate="2015-08-28T13:54:08.897" UserId="10522" />
  <row Id="7414" PostId="6915" Score="1" Text="Yes, if you were to switch to Hadoop you would be moving to a completely new architecture, with a SQL-like interface/wrapper running on top of Hadoop. It would not be an insignificant undertaking. Given the importance of your data it would be incredibly wise to bring in a vendor it consultant who specializes in creating new architectures from the ground up. Without intimate access to you current database structure, the data contained, and a look at the processes currently running on your system anything that me or anyone else could tell you online is merely speculation/best guess." CreationDate="2015-08-28T14:05:23.203" UserId="11021" />
  <row Id="7415" PostId="6937" Score="1" Text="This isn't a &quot;Meta&quot; question right? It's a question for the regular site itself. Although career-oriented questions and open-ended &quot;what's important&quot; questions are usually off-topic on StackExchange, I find it passable for DS." CreationDate="2015-08-29T08:52:43.640" UserId="21" />
  <row Id="7416" PostId="6900" Score="0" Text="what do you mean by enforcement of [0-1]. Also what is range based standardization. Anyway I will apply both sorensen and  @image_doctor and see if I need to outliers are important or not" CreationDate="2015-08-29T09:18:12.693" UserId="12433" />
  <row Id="7417" PostId="6903" Score="0" Text="disregard the issue with unique position. Regarding the weighting, so far I dont see reason to apply that, anyway that is a great point to keep in mind" CreationDate="2015-08-29T09:19:52.250" UserId="12433" />
  <row Id="7418" PostId="6932" Score="0" Text="Thanks for the answer. That worked!" CreationDate="2015-08-29T11:58:57.310" UserId="8338" />
  <row Id="7419" PostId="6900" Score="0" Text="just that you want the metric to be bounded i.e. to be a value between 0 and 1." CreationDate="2015-08-29T16:11:09.863" UserId="9420" />
  <row Id="7420" PostId="6937" Score="1" Text="potential duplicate: http://datascience.stackexchange.com/questions/6576/i-am-a-programmer-how-do-i-get-into-field-of-data-science" CreationDate="2015-08-29T16:43:34.693" UserId="947" />
  <row Id="7421" PostId="1185" Score="1" Text="I'm voting to close this question as off-topic because it is a general request for off-site resources, with no problem statement or other way to scope recommendations." CreationDate="2015-08-29T20:04:12.283" UserId="836" />
  <row Id="7422" PostId="6939" Score="0" Text="For many classifiers up-weighting can be used to counter imbalanced data. Only 20 samples of one class would still only draw a rough picture of what that class looks like or what the other(out of 2) class not looks like." CreationDate="2015-08-30T06:58:58.393" UserId="8987" />
  <row Id="7423" PostId="6947" Score="0" Text="I don't think there is such a wide-ranging framework, at least not on the data-connecting side, but most languages will have libraries supporting the data sources you suggest. You need to define &quot;appropriate language&quot; - probably best to simply state which languages you would accept, or at least what specific qualities count as &quot;appropriate&quot;. You might instead try http://softwarerecs.stackexchange.com/ (but do read their help, and do fix the vague parts of your question first - such as which data sources are actually important to you, and which OS/language to support)." CreationDate="2015-08-30T06:59:55.793" UserId="836" />
  <row Id="7424" PostId="6947" Score="0" Text="Or are you looking for the middle-ware here for co-ordinating data movement, something like Microsoft's SQL Server Integration Services (was called Data Translation Services)?" CreationDate="2015-08-30T07:14:04.513" UserId="836" />
  <row Id="7425" PostId="6947" Score="0" Text="It's for IoT / Data Science project.  The sources of data are specified as web scraping, web apis (rest request replying with json, blue tooth sensor data, and legacy text files).   It's likely there will be more, but they haven't been nailed down yet.   Language would probably be python.   The assumption of the team is that this has happened many times before and that some framework would already exist.   We don't mind writing one and open sourcing it, but it'd be a shame to re-invent the wheel unnecessarily." CreationDate="2015-08-30T10:44:54.703" UserId="12536" />
  <row Id="7426" PostId="6947" Score="0" Text="I think you need to define better what this imagined framework would do. Pretty much *all* software that handles data will &quot;wrangle&quot; or &quot;munge&quot; it, so you have not given enough information. For instance, is the goal to get all the data into a format suitable for specific ML algorithms (usually a matrix or sparse matrix)? Do you need support for data sets larger than RAM allocation for a single algorithm?" CreationDate="2015-08-30T11:16:57.297" UserId="836" />
  <row Id="7427" PostId="6942" Score="0" Text="What I mean is this: if I have ensembles 1 and 2, with (sensitivity, positive predictive value) = (.90, .80) and (.97, .93) respectively, then 1 is not Pareto optimal, because there is another ensemble, namely 2, that beats it in every way.&#xA;&#xA;Regarding your proposed algorithm : there is a tradeoff between sensitivity and PPV, so &quot;the ensemble get the best performance improvement&quot; is not well defined." CreationDate="2015-08-30T14:49:12.710" UserId="12438" />
  <row Id="7428" PostId="6937" Score="0" Text="I suggest taking a &quot;beginners&quot; course such as Coursera's [Machine Learning presented by Andrew Ng](https://www.coursera.org/learn/machine-learning). Studying a basic course like that will give *you* the introduction you need to make your own wishlist on what to pick up in more detail." CreationDate="2015-08-30T15:07:00.383" UserId="836" />
  <row Id="7429" PostId="6946" Score="0" Text="I agree, but the problem I have to even begin doing Kaggle competitions is that I don't feel I have the necessary tools/knowledge. I definitely am tired of the standard study/quiz/test/hw methodology though." CreationDate="2015-08-30T17:57:09.873" UserId="12516" />
  <row Id="7430" PostId="6947" Score="0" Text="Well, we don't want to get too specific as it will eliminate potential solutions.  As long as the data can be processed into some internal format that is fine.  We can deal with it from there." CreationDate="2015-08-30T18:57:40.487" UserId="12536" />
  <row Id="7431" PostId="6947" Score="0" Text="I think you are being too generic, and need to get into specifics. The problem IMO, is your request matches most if not all open-source computer languages (including their libraries), but no frameworks would be so general-purpose. But perhaps someone else will agree with you and suggest an answer . . ." CreationDate="2015-08-30T19:02:02.183" UserId="836" />
  <row Id="7433" PostId="6916" Score="1" Text="@Victor: not directly, but you can do one of two things: either use a stochastic gradient descent (which should get you close to a similar solution if you split your data carefully) or try to parallelize the calculation of the matrix operations - essentially by replacing a single inner product (which is problematic to parallelize) by a sum of outer products." CreationDate="2015-08-31T10:27:46.507" UserId="10936" />
  <row Id="7434" PostId="6924" Score="0" Text="From the top of my head:&#xA;- for Random Forest I would go for a proximity matrix, although that can be tricky for very large datasets (essentially for N observations you need  a NxN matrix to represent all similarities)&#xA;- for DNN just google &quot;deep learning visualisations&quot; for some approach to representing what the intermediate features are learning&#xA;- for methods that assign feature relevance (like gbm or rf) try plotting the points in coordinates represented by the most relevant variables. if you're familiar with ggplot2 in R, i would definitely recommend that" CreationDate="2015-08-31T11:03:28.857" UserId="10936" />
  <row Id="7435" PostId="6952" Score="0" Text="Thanks @kpb. That worked like a charm. However, my plot looks fine now with observed, trend, seasonal and random component. I get a error when I try to subtract seasonal component from the decomposed series. It appears that I can perform a math operation on the series as it is not numeric and needs conversion.                                                                                 Error looks like this: `code` Error in `-.default`(salestsDec, salestsDec$seasonal) : &#xA;  non-numeric argument to binary operator `code`." CreationDate="2015-08-31T16:56:55.197" UserId="12533" />
  <row Id="7436" PostId="6956" Score="0" Text="Thanks for the answer. I will try this. In the meanwhile, I have a doubt. Lets say that i want to build the above classification model now, and reuse that later to classify the documents later, how can i do that? Given that we have to perform the TDM as a single step for both training and test dataset, will saving a model and reusing it will work?" CreationDate="2015-08-31T18:18:41.357" UserId="9793" />
  <row Id="7439" PostId="6956" Score="0" Text="@kpb - This has worked. Thanks a lot. - Arun" CreationDate="2015-09-01T09:28:22.557" UserId="12554" />
  <row Id="7440" PostId="6960" Score="0" Text="So the input is a body of a question and output is a list of suggested tags, correct?" CreationDate="2015-09-01T10:52:07.883" UserId="1279" />
  <row Id="7441" PostId="6974" Score="0" Text="I have the tags with the mined data.&#xA;1. is given question mentioned by you, the question from which tags are to be predicted or from the training set..please clear out what is target set?&#xA;2. What are TF-IDF Vectors?" CreationDate="2015-09-01T16:38:26.470" UserId="12557" />
  <row Id="7442" PostId="6960" Score="0" Text="input= Training Data Set Questions&#xA;output= Predicted Tags from Testing Set" CreationDate="2015-09-01T17:17:08.810" UserId="12557" />
  <row Id="7443" PostId="6974" Score="0" Text="by the target set, I simply mean the values your output could take on.  &#xA;&#xA;If you have N possible tags, then your output could be a N-dimensional vector with 1's for the index of the relevant tags and zeros elsewhere. &#xA;Let's say there were only 4 tags you cared about predicting: &quot;machine-learning,&quot; &quot;networks,&quot; &quot;classification,&quot; and &quot;hardware.&quot;  Your output vector for this question would look like: &#xA;&#xA;`var   | machine-learning  networks  classification  hardware&#xA; q      |            1                       0                1                  0&#xA;`" CreationDate="2015-09-01T17:24:11.747" UserId="12306" />
  <row Id="7444" PostId="6974" Score="0" Text="When you use tf-idf (term frequency, inverse document frequency), you have a 'vocabulary' of terms.  Each document is represented by a vector.  Each index of the vector is a term and its value is the term's frequency divided by the number of documents that term shows up in.&#xA;&#xA;There are other ways to calculate tf-idf vectors as well [shown here](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)" CreationDate="2015-09-01T17:31:25.483" UserId="12306" />
  <row Id="7445" PostId="6972" Score="0" Text="It is perhaps hard to get a good grip on what the things/patterns you want to create regex's for might look like with only one example. Maybe more information will help resolve this ? Is it, for instance, only blank lines that define elements to search for ?" CreationDate="2015-09-01T17:35:50.447" UserId="7980" />
  <row Id="7447" PostId="6972" Score="0" Text="That is the problem @image_doctor. It is not something specific, but it could be anything that have been given as an input." CreationDate="2015-09-01T17:52:40.390" UserId="201" />
  <row Id="7448" PostId="6972" Score="0" Text="Are there constraints on what constitutes well formatted input ? How do you know what constitutes something you have to create a regex for  ? :)" CreationDate="2015-09-01T17:58:20.860" UserId="7980" />
  <row Id="7449" PostId="6972" Score="0" Text="I was thinking about it as: A text contains 100.000 times the same structure of a specific data. A user starts to check a few of these times. The algorithm creates a Regex expression that can find all the other 99.999 times. Each one a user check, the Regex will be improved." CreationDate="2015-09-01T18:19:22.823" UserId="201" />
  <row Id="7450" PostId="6972" Score="0" Text="Sequence analysis maybe relevant, if what the user selects can be broken down into a sensible set tokens." CreationDate="2015-09-01T18:27:24.157" UserId="7980" />
  <row Id="7453" PostId="6904" Score="1" Text="How about first conducting a sensitivity study on your 230 input parameters?  PCA? Expert knowledge? Something?" CreationDate="2015-09-02T05:01:03.087" UserId="1077" />
  <row Id="7454" PostId="6904" Score="0" Text="Yes I am starting to think and old-school expert system might be the way to go." CreationDate="2015-09-02T05:27:19.637" UserId="12454" />
  <row Id="7455" PostId="6547" Score="0" Text="I recommend these videos if you are just beginning Scikit: https://github.com/justmarkham/scikit-learn-videos" CreationDate="2015-07-23T21:30:47.970" UserId="8986" />
  <row Id="7456" PostId="6952" Score="0" Text="@keval: again, tricky without replication, but if  I understand correctly you are trying to subtract the seasonality from the original series. If that is the case, either add the remaining components (trend + random) or subtract salestsDec$seasonal from salestsDec[,2] and not from salestsDec. Better still, can you post the output of str(salestsDec, max.level = 1) or somesuch? That should have info on object classes, dimensions etc." CreationDate="2015-09-02T08:36:49.150" UserId="10936" />
  <row Id="7457" PostId="6956" Score="0" Text="@Arun: In Python, you can fit the transformation and store it as a pickle for later use. Closest counterpart in R I can think of is dumping the fitted model (TDM) in an .RData file and then filtering a new test set (matching the column names) prior to use. Crude, but it might get the job done." CreationDate="2015-09-02T08:40:36.890" UserId="10936" />
  <row Id="7458" PostId="6984" Score="0" Text="I guess MAP@10 is mean average precision with test set of one user with from 1 to 10 items to rate, and scored by comparing ranking of those items? So your goals is to correctly rank ratings for a user for some new/unknown items?" CreationDate="2015-09-02T12:46:58.543" UserId="836" />
  <row Id="7459" PostId="6984" Score="0" Text="@Neil it's not rating but come up with a list of 10 coupons a user is likely to buy in next one week. To be precise I am working on this problem https://www.kaggle.com/c/coupon-purchase-prediction" CreationDate="2015-09-02T13:08:08.790" UserId="1151" />
  <row Id="7460" PostId="6985" Score="0" Text="What is map? I have heard about ndcg. Will prediction accuracy be the same between scikit learn gbm and xgboost?" CreationDate="2015-09-02T13:09:34.653" UserId="1151" />
  <row Id="7461" PostId="6985" Score="0" Text="map = mean average precision, i.e. your MAP@10. I think xgboost accuracy might be higher, because it optimizes MAP directly and has more parameters to customize." CreationDate="2015-09-02T13:16:34.283" UserId="10936" />
  <row Id="7462" PostId="6984" Score="0" Text="OK, not ratings but probability then. Metrics are not the same as training objectives (which may be constrained by what the model can optimise directly). If you don't have the metric available already in your library of choice, you could just code it. Often they are only a few lines to implement. In addition, worth checking the forums, one of the first threads or scripts to be created is often an implementation of the metric in R/Python/Matlab etc" CreationDate="2015-09-02T13:17:01.450" UserId="836" />
  <row Id="7464" PostId="6984" Score="0" Text="I know how map@K is implemented but I am using a ML classifier which will give probability of a user - item pair to be purchased, then I want to sort the items by this probability for each user and then use ap@k to evaluate the recommender system. So my question is about the metric to evaluate the ml classifier given imbalance of class. I cannot use ap@k to tune parameters of classifier which gives probability  for a single user - item right?" CreationDate="2015-09-02T13:27:18.187" UserId="1151" />
  <row Id="7465" PostId="6985" Score="0" Text="Oh you are saying xgboost gives option of using map@k for evaluating a binary classification?  Can you add link or some example to the answer please ?" CreationDate="2015-09-02T13:29:41.793" UserId="1151" />
  <row Id="7466" PostId="6985" Score="0" Text="Here you go:&#xA;&#xA;https://github.com/dmlc/xgboost/blob/master/doc/parameter.md&#xA;&#xA;and then check the section &quot;Learning Task Parameters&quot;." CreationDate="2015-09-02T13:51:40.750" UserId="10936" />
  <row Id="7467" PostId="6984" Score="0" Text="Yes you can use the metric to tune hyper-parameters, to trigger early stopping etc, but you cannot use it as an objective function during learning phase. The question is then only slightly different - which objective function is going to be most compatible with the given metric. Which I don't know, unfortunately" CreationDate="2015-09-02T14:32:55.160" UserId="836" />
  <row Id="7468" PostId="6987" Score="0" Text="I assume the scikit-learn documentation does not highlight the difference? Did you check?" CreationDate="2015-09-02T15:06:04.667" UserId="12350" />
  <row Id="7469" PostId="6987" Score="0" Text="1. What kernel did you use in SVC? default settings = &quot;rbf&quot;?&#xA;2. One-against-one and one-against-all are different approaches" CreationDate="2015-09-02T15:10:46.633" UserId="10936" />
  <row Id="7470" PostId="6987" Score="0" Text="the documentation is kinda sparse/vague on the topic. It mentions the difference between one-against-one and one-against-rest, and that the linear SVS is `Similar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better (to large numbers of samples).`" CreationDate="2015-09-02T15:11:09.577" UserId="12580" />
  <row Id="7471" PostId="6987" Score="0" Text="for regular SVC, I used the default kernel.&#xA;&#xA;I know 1v1 and 1vR are different approaches, but I guess that's what I want to know - why do they produce such different results? Is it the kernel choice or the different approach to multiple category classification?" CreationDate="2015-09-02T15:12:24.280" UserId="12580" />
  <row Id="7472" PostId="6988" Score="2" Text="You want to minimize RSS + reg.term because you (1) want to fit the model to the training data which is likely from the distribution of the process you want to model, (2) and not overfit." CreationDate="2015-09-02T21:04:27.813" UserId="7848" />
  <row Id="7475" PostId="6997" Score="0" Text="thanks so much for your answer. I don't see how having a topic per document would help to predict rating... but I will have a look on LDA later.&#xA;&#xA;I will try the SVM with a non-linear kernel. thanks" CreationDate="2015-09-03T15:57:22.980" UserId="12597" />
  <row Id="7477" PostId="6997" Score="0" Text="LDA seeks to cluster tokens (either words or n-grams) that co-occur in documents.  This allows you to learn a lower-dimensional input than, say, word/n-gram frequency.  &#xA;If your initial intuition holds true (that the same n-grams will occur in reviews of the same sentiment), then these n-grams will cluster to topics that reflect that sentiment.&#xA;Though I haven't seen this implemented in literature, I believe that [labeling your documents](https://frickj.wordpress.com/2015/09/03/a-simple-thought-on-sentiment-analysis/) with a &quot;sentiment token&quot; would help your topics converge." CreationDate="2015-09-03T16:36:29.090" UserId="12306" />
  <row Id="7478" PostId="6924" Score="0" Text="Thanks for the comment @kpb. RF with proximity matrix looks great! I'll look into detecting the most relevant features and working from there." CreationDate="2015-09-03T19:49:48.823" UserId="12485" />
  <row Id="7479" PostId="6681" Score="1" Text="Are you trying to predict when someone is likely to churn or to detect when someone has churned? Survival analysis is a reasonable choice for the first but for the latter I would recommend something else -- potentially change point analysis." CreationDate="2015-09-03T22:22:06.627" UserId="947" />
  <row Id="7482" PostId="6978" Score="0" Text="Unfortunately, it's not enough to convince someone to hire you if you need to be trained. The knowledge need to be understood before the first day of work." CreationDate="2015-09-04T06:00:20.067" UserId="9123" />
  <row Id="7484" PostId="6300" Score="0" Text="This will only find the `sum`. And it is better to say &quot;You *could* use&quot;" CreationDate="2015-09-04T07:10:09.713" UserId="8479" />
  <row Id="7485" PostId="6295" Score="0" Text="Please read [this](https://github.com/Rdatatable/data.table/wiki/Getting-started). Your syntax is incorrect." CreationDate="2015-09-04T07:11:58.577" UserId="8479" />
  <row Id="7486" PostId="6978" Score="0" Text="@StudentT I don't understand why it's not an enough background for getting hired.  The question mentions **What courses / subjects are most important to the field of Data Science?**&#xA;&#xA;And these are exactly those courses which I have done, and now I am leading a  data science team too.   Of course, you have to complete the projects done in the courses too." CreationDate="2015-09-04T07:19:24.867" UserId="11097" />
  <row Id="7489" PostId="6978" Score="0" Text="You're lucky or you're just too good : - ). Normally, a data scientist is expected to be like a phd and be real master in everything. That's because the field is very competitive, only the best out of the best should stay. Someone who just come out from uni is expected to work on like data entry or like software engineer." CreationDate="2015-09-04T07:28:09.530" UserId="9123" />
  <row Id="7492" PostId="6978" Score="0" Text="@StudentT I definitely don't think this discussion would fit the answer.  But, as per industry requirements and the OP's question, I guess what I mentioned above would give a solid foundation of data science." CreationDate="2015-09-04T07:34:39.333" UserId="11097" />
  <row Id="7493" PostId="6997" Score="0" Text="With SVM with a non-linear kernel (so far I tried the default 'rbf') i only get 20% accuracy. I will try now with kernel &quot;poly&quot;" CreationDate="2015-09-04T13:21:56.670" UserId="12597" />
  <row Id="7494" PostId="8005" Score="0" Text="A Google search yields many papers and presentations that can help you get started, e.g.: http://www.stat.fsu.edu/~jfrade/HOMEWORKS/STA5168/FRADE_STA5168_paper.pdf , https://smartdrill.com/pdf/Credit%20Risk%20Analysis.pdf , http://www.pomsmeetings.org/confpapers/007/007-0210.pdf, https://bib.irb.hr/datoteka/466476.sarlija_soric_vlah_vojvodic.pdf. Here are several books: https://books.google.com/books?id=bbaTr3bb110C , https://books.google.com/books?id=7LlGfPvOJLoC" CreationDate="2015-09-04T18:48:46.587" UserId="609" />
  <row Id="7497" PostId="8018" Score="0" Text="Have you tried to identify mathematically what it is you do when you manually classify the lane change manoeuvres? Are you broadly looking for a change from a period of  stable close to zero gradient of the lane position function followed by a large increase in the magnitude of gradient, leading to either another period of close to zero gradient or the end of the data ?" CreationDate="2015-09-06T09:08:44.647" UserId="7980" />
  <row Id="7498" PostId="8018" Score="0" Text="Do you have a number of the original images for us to experiment with ?" CreationDate="2015-09-06T09:26:22.287" UserId="7980" />
  <row Id="7499" PostId="8017" Score="0" Text="Thanks for the informative answer. I will wait a few more days to see if anyone else has anything to add or comment." CreationDate="2015-09-06T09:42:41.440" UserId="10181" />
  <row Id="7500" PostId="8018" Score="0" Text="The axis and scales are legends not particularly consistent across the example images, is there a way to standardise the plots, or do you have no control over the image creation ?" CreationDate="2015-09-06T13:06:47.250" UserId="7980" />
  <row Id="7501" PostId="8018" Score="0" Text="Maybe I misunderstood, do you have the x coordinate data used to plot the graphs , as in the example data `a` you've given?" CreationDate="2015-09-06T13:23:56.643" UserId="7980" />
  <row Id="7502" PostId="8018" Score="0" Text="@image_doctor Mathematically, I subtract the time at the start of lane change from the time at the end of lane change. I have the lateral position and time frame number for each vehicle as shown in the plots and example data. Each vehicle is identified by a unique `Vehicle.ID` not shown here. Because the data are available, I can standardize the plots." CreationDate="2015-09-06T15:45:30.090" UserId="4933" />
  <row Id="7503" PostId="8018" Score="0" Text="And I do have thousands of vehicles' data." CreationDate="2015-09-06T15:59:18.507" UserId="4933" />
  <row Id="7504" PostId="6997" Score="0" Text="with poly got the same accuracy: around 20%" CreationDate="2015-09-07T08:36:46.417" UserId="12597" />
  <row Id="7505" PostId="6792" Score="0" Text="The methods you are sketching are known under the name *steganography*. Read something about this topic to learn more." CreationDate="2015-09-07T08:53:08.063" UserId="10169" />
  <row Id="7506" PostId="8029" Score="0" Text="Notabene: This is not a recommendation problem because we would like to incorporate data from very different action types and it is very hard to model this as classical recommendation problem and use existing algorithms. But you could prove me wrong on this one:)" CreationDate="2015-09-07T16:16:28.667" UserId="12663" />
  <row Id="7507" PostId="8026" Score="1" Text="Perhaps you could explain a bit more what your data looks like? How many dimensions, etc? Do you have (x,y) data and you are trying to fit a curve through the points, or is it a density you are trying to match?" CreationDate="2015-09-07T16:29:11.193" UserId="471" />
  <row Id="7508" PostId="8030" Score="0" Text="So to form a policy we need another machine learning tool? But any machine learning tool will find the one policy which is the best, how can multiple policies be obtained in such a case?" CreationDate="2015-09-08T03:47:47.083" UserId="8013" />
  <row Id="7510" PostId="8027" Score="0" Text="This is a function fitting problem. Do you have data to share ? This might suggest which form of functional approximation might be most suitable. If not, why not start with a linear regression and move on from there to add terms in $x^n$." CreationDate="2015-09-08T07:51:58.410" UserId="7980" />
  <row Id="7511" PostId="8027" Score="0" Text="@image_doctor thanks, actually that's what i decided to use, Linear Regression, while i observe how that work out." CreationDate="2015-09-08T09:06:30.240" UserId="5027" />
  <row Id="7514" PostId="8027" Score="0" Text="Excellent, let us know how it goes, visual inspection of the fit you get can give a good indication of how well your regression model is working." CreationDate="2015-09-08T10:04:28.360" UserId="7980" />
  <row Id="7516" PostId="8027" Score="0" Text="@image_doctor thanks will do that." CreationDate="2015-09-08T11:14:04.163" UserId="5027" />
  <row Id="7518" PostId="8030" Score="0" Text="@Rishika: If the reinforcement learning is working correctly, the result will be the best policy that can expressed in the chosen architecture (by varying the parameters usually called weights). So changing architecture could result in a better policy, if it can express a policy the first one couldn't or it makes it easier for the RL algorithm to work." CreationDate="2015-09-08T12:15:41.540" UserId="10568" />
  <row Id="7519" PostId="6997" Score="0" Text="I have used all the dataset, the 48000 reviews, 90% training - 10% testing, with LinearSVC, looking at bigrams and adjectives only and get an accuracy of 93%, is it normal? also verified with 10 fold cross validation and mean accuracy is 99.6%..." CreationDate="2015-09-08T15:10:01.560" UserId="12597" />
  <row Id="7520" PostId="6997" Score="0" Text="how can i show the most informative features within classes?" CreationDate="2015-09-08T15:10:18.500" UserId="12597" />
  <row Id="7524" PostId="8043" Score="1" Text="You have a lot of options. Could you clarify what the variable types are here (I suspect just a vector of real  numbers for input and single real number for output - but this is critical, if any values are discrete integers, it changes what may work)? Could you also clarify what it costs you to call the black box - i.e. how many calls per second it could cope with? Also, how much you value your own time understanding more sophisticated optimisation approaches compared with simpler more brute-force approaches." CreationDate="2015-09-08T18:08:13.860" UserId="836" />
  <row Id="7525" PostId="8043" Score="0" Text="I've amended the post to address your questions. If things are still ambiguous I can try to clarify them." CreationDate="2015-09-08T18:35:15.180" UserId="5103" />
  <row Id="7526" PostId="8043" Score="0" Text="Hmm 1 call per 2 seconds may not be fast enough for simple generic solvers. Your premise in the question that there is a single optimal way *how* to find an answer is not necessarily correct. Random guessing is surprisingly efficient with the amount of unknowns you present so far. Can we make some assumptions such as function is continuous and differentiable everywhere, and has some general global shape (so we worry about local minima *close* to global minimum, not trying to find deepest valley in the Alps)?" CreationDate="2015-09-08T18:39:43.137" UserId="836" />
  <row Id="7527" PostId="8043" Score="0" Text="Sure. Let's assume that the function is continuous and differentiable. I suspect the global shape is Gaussian -- or at least unimodal." CreationDate="2015-09-08T18:52:14.587" UserId="5103" />
  <row Id="7528" PostId="8027" Score="0" Text="Consider [Poisson regression](https://en.wikipedia.org/wiki/Poisson_regressionWikipedia) since you have count data." CreationDate="2015-09-08T19:47:58.220" UserId="381" />
  <row Id="7530" PostId="8040" Score="0" Text="Generic answer: Insert them all into a max priority queue of length three. Not direct, but efficient enough." CreationDate="2015-09-09T01:52:47.163" UserId="381" />
  <row Id="7531" PostId="8030" Score="0" Text="a neural net would also find the best policy. But RL alone cannot. Then why we need RL" CreationDate="2015-09-09T03:33:53.317" UserId="8013" />
  <row Id="7532" PostId="8028" Score="0" Text="Can you expand your answer a bit?  What resources have you found, and what do you mean by: **Is there any work on data science in computer science/software engineering education (adding data science courses to a traditional software engineering curriculum) ,management and other disciplines**  ?" CreationDate="2015-09-09T06:33:16.023" UserId="11097" />
  <row Id="7533" PostId="6899" Score="0" Text="I&quot;m not clear on , &quot;... that are disjoint in the sense that no two will return true on the same input...&quot; and you are classifying to a binary output, how you can have more than two classifiers in your ensemble, I'm probably missing something?" CreationDate="2015-09-09T08:11:13.463" UserId="7980" />
  <row Id="7534" PostId="8045" Score="0" Text="Looks a bit like tokenised words with each row being a sentence. Of course it may not be, but perhaps techniques that worked with that data would work well with this?" CreationDate="2015-09-09T12:51:18.637" UserId="836" />
  <row Id="7535" PostId="8045" Score="0" Text="@NeilSlater: That's what I thought. I've converted those values to binary features. Will update the post to indicate it." CreationDate="2015-09-09T12:53:20.080" UserId="12699" />
  <row Id="7536" PostId="8045" Score="0" Text="Sine the cross-posts are being processed (my apologies again), I would like to keep track on a comment from @cel :&#xA;&#xA;_What is a `formal way`? A fixed recipe? There's no such thing in machine learning and even general principles how to tackle such a problem will be highly opinion based._" CreationDate="2015-09-09T13:07:16.473" UserId="12699" />
  <row Id="7537" PostId="8045" Score="0" Text="My reply:&#xA;I totally agree, just wondering if there's something I don't know. In fact I dislike this task in the first place since it doesn't feel like a properly defined one." CreationDate="2015-09-09T13:11:00.060" UserId="12699" />
  <row Id="7538" PostId="8045" Score="0" Text="As well as binary features for categories, the sequence might be important. It may be worth checking if you have a range of n-grams that occur repeatedly, as that would be a strong indicator that approaches used in NLP could help - even a simple approach such as creating new feature categories for most common n-grams might give you something." CreationDate="2015-09-09T13:16:13.060" UserId="836" />
  <row Id="7539" PostId="8045" Score="0" Text="@NeilSlater : Indeed. I have done some quick frequent pattern analysis and found certain n-grams. The problem was that they seemed associated with both labels 0 and 1. I will run some feature templates of bi-gram and tri-gram, and then get back to you. One thing that is also intriguing to me is, in NLP, sometimes labels also got dependencies, namely some 1 may follow by certain patterns of 0 and vice versa." CreationDate="2015-09-09T13:22:25.283" UserId="12699" />
  <row Id="7540" PostId="8028" Score="0" Text="I mean how computer science curriculum have been enriched by data science skill set. For Example, adding statistics and visualization courses." CreationDate="2015-09-09T13:45:57.550" UserId="3151" />
  <row Id="7541" PostId="8045" Score="0" Text="Have you looked at the distribution of numerical values for each X, Y, and Z?" CreationDate="2015-09-09T14:12:07.843" UserId="12306" />
  <row Id="7542" PostId="8045" Score="0" Text="@jamesmf : Not thoroughly but it did cross my mind. Will do it, too. At first glance X, Y, Z seem usually being with &gt;6 figures, 4-6 figures, and 1-3 figures, respectively." CreationDate="2015-09-09T14:27:53.220" UserId="12699" />
  <row Id="7543" PostId="8045" Score="0" Text="I strongly agree that NLP-based methods could work here, but if the data seem more continuous, you might try constructing features based on the X,Y,Z values.  It would be interesting to plot the positive/negative examples vs. mean(X), mean(Y), mean(Z) or vs max(X), max(Y), max(Z).  If any of those seem somewhat separable, it could give hints to how the data were generated.&#xA;&#xA;Another route to go if the data takes on a large space numerically would be to bin it.  If you were to use NLP methods after binning, it would be similar to stemming" CreationDate="2015-09-09T14:58:44.280" UserId="12306" />
  <row Id="7544" PostId="6553" Score="1" Text="word2vec already only uses &quot;a small region of text around the word of interest.&quot;  The `window` parameter sets how many words in the context are used to train the model for your word _w_" CreationDate="2015-09-09T15:03:29.003" UserId="12306" />
  <row Id="7546" PostId="8028" Score="0" Text="Data Science is a huge domain in itself. So, it cannot be added as a curriculum in any curriculum.&#xA;&#xA;However, computer science can include distributed computing which would help the students understand and appreciate the idea of distributed systems and Big Data." CreationDate="2015-09-09T17:28:13.240" UserId="11097" />
  <row Id="7548" PostId="8046" Score="0" Text="What do you mean by &quot;When I create a gains table, the predicted response rate of the first decile comes out at 50%, but it should be around 2%&quot;?" CreationDate="2015-09-09T20:51:32.973" UserId="10587" />
  <row Id="7549" PostId="6508" Score="0" Text="Such plots arise often when the data has not been preprocessed well enough. Make sure to validate your results to not depend on a single variable only or to have degenerated in any other way (e.g. due to outliers)" CreationDate="2015-09-10T04:23:03.787" UserId="924" />
  <row Id="7550" PostId="6925" Score="0" Text="The main challenge is that **nobody has *interesting* big data**. Everybody just pretends to have so." CreationDate="2015-09-10T04:28:31.553" UserId="924" />
  <row Id="7551" PostId="8055" Score="0" Text="Thanks for this, I found it really helpful. In regards to Andrew Ng's course, do you really feel it's helpful? Have you taken it yourself? I'm almost done with the course, but still don't feel it's as good as if I were to have gone through a more rigorous textbook. It's very superficial, and it's done in Matlab, which is rarely used in DS." CreationDate="2015-09-10T04:33:16.843" UserId="12516" />
  <row Id="7553" PostId="8045" Score="0" Text="Re: cross-posting. I'm not sure what is the best way to handle this question, considering existing discussion on both CV and this site. IMHO this question definitely doesn't belong to StackOverflow, but between CV and Data Science, I think it better fits this site. So, from the moderation perspective, perhaps, my fellow moderators could chime in and share their opinion on the best course of action. CC: @SeanOwen" CreationDate="2015-09-10T05:13:51.193" UserId="2452" />
  <row Id="7554" PostId="8056" Score="0" Text="Yes, I have seen it. You link does not work by the way." CreationDate="2015-09-10T06:18:25.383" UserId="690" />
  <row Id="7555" PostId="8055" Score="0" Text="@tofu_bacon I would say yes, if and only if you supplement with [his real course notes](http://cs229.stanford.edu/materials.html) which are better. [Tom Mitchell's](http://www.cs.cmu.edu/~tom/mlbook.html) book was the book my school's ML class used and i found Ng's notes rather helpful to supplement. I'll edit my answer to mention this. If you are following along in the ML coursera course, I'd just do everything in Python/R as well." CreationDate="2015-09-10T06:41:24.433" UserId="11098" />
  <row Id="7556" PostId="8056" Score="0" Text="This is the author of the C library that is being used by the arules package btw." CreationDate="2015-09-10T07:42:04.500" UserId="690" />
  <row Id="7557" PostId="6964" Score="0" Text="Example link of such results: http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (not sure if this is one you have seen)" CreationDate="2015-09-10T09:08:43.057" UserId="836" />
  <row Id="7558" PostId="6964" Score="0" Text="@NeilSlater Yes, that's one nice implementation I saw but unfortunately not for R." CreationDate="2015-09-10T09:13:10.503" UserId="5211" />
  <row Id="7559" PostId="8058" Score="2" Text="It seems promising. According to the author he will publish it on CRAN when finished." CreationDate="2015-09-10T09:15:39.510" UserId="5211" />
  <row Id="7560" PostId="8045" Score="0" Text="@AleksandrBlekh : thank you very much and sorry for the mess. Either way, I will do my best to keep track on comments by manually posting them." CreationDate="2015-09-10T10:08:50.963" UserId="12699" />
  <row Id="7561" PostId="8045" Score="0" Text="@jamesmf : Yes, and that's why I also took a strange (?) choice to try conditional random fields with only unigram, and yet it's the best so far (about 79%). I will continue trying bigram/trigram and also several polynomial ways with those numerical values." CreationDate="2015-09-10T10:11:40.660" UserId="12699" />
  <row Id="7562" PostId="8050" Score="0" Text="I know conceptual relationships. I am looking for papers or reports of curriculum redesign to put data science skills in computer science or management. An example of doing this job for statistics is this:  https://www.researchgate.net/publication/266856309_Data_Science_in_Statistics_Curricula_Preparing_Students_to_Think_with_Data" CreationDate="2015-09-10T10:21:37.163" UserId="3151" />
  <row Id="7563" PostId="8028" Score="0" Text="I agree. this was the reason I mentioned data science skills not the whole data science concepts." CreationDate="2015-09-10T10:24:49.590" UserId="3151" />
  <row Id="7564" PostId="8045" Score="0" Text="@jamesmf : Speaking of binning, one of my usual tactic is to convert numerical numbers by log2, but not done yet, will get back to you once I completed it." CreationDate="2015-09-10T10:27:35.377" UserId="12699" />
  <row Id="7565" PostId="8050" Score="0" Text="I couldn't find any nice papers on that. But, by observing the curricula of the top programs around the world, where data science is successfully blended with main-stream business and Computer Science, I highlighted three such curricula which I found very impressive and nicely done." CreationDate="2015-09-10T10:37:57.037" UserId="11097" />
  <row Id="7566" PostId="8050" Score="0" Text="The point is statisticians explicitly mentioned their curriculum drawbacks in different papers and suggested new courses and skills to add to a traditional statistics curriculum . Moreover, they raised new viewpoints for teaching well-established courses. But such phenomenon is not seen for computer science curriculum." CreationDate="2015-09-10T14:04:08.907" UserId="3151" />
  <row Id="7567" PostId="8026" Score="0" Text="I heavily edited my question. Please see if now it can be acceptable." CreationDate="2015-09-10T14:53:20.863" UserId="12665" />
  <row Id="7568" PostId="8032" Score="0" Text="I don't see any obvious issues. (Although, I'm more familiar with the python wrapper). Have you tried adding `outputmargin=F` to the `predict` function? If somehow the `outputmargin` is set to `T`, it will return the value before the logistic transformation." CreationDate="2015-09-10T17:24:38.880" UserId="10816" />
  <row Id="7569" PostId="8056" Score="0" Text="My bad, not sure how [1] got appended at the end. The idea with sharing this implementation( as implemented in the arules pkg) was the explanation stated along the side. Using that one can write the same in R. What do you think ? I read more into your question. Let me see if i can help you out more." CreationDate="2015-09-10T18:10:28.643" UserId="5179" />
  <row Id="7570" PostId="8045" Score="0" Text="@Barabbas: You are welcome and no problem." CreationDate="2015-09-10T18:48:19.690" UserId="2452" />
  <row Id="7571" PostId="8055" Score="0" Text="I just wanted to double check that the link you provided was correct, because those notes you linked don't exactly correspond with the coursera course" CreationDate="2015-09-10T19:31:04.460" UserId="12516" />
  <row Id="7572" PostId="8056" Score="0" Text="Would you be able to write it in R? It seems pretty involved..." CreationDate="2015-09-10T19:58:43.150" UserId="690" />
  <row Id="7573" PostId="8055" Score="0" Text="@tofu_bacon i apologize if my edit isn't clear - those notes are from the &quot;real&quot; stanford class, not the coursera class. There shouldn't be a 1:1 correspondence - they provide a great deal more depth. I can make that more clear in my answer if need be." CreationDate="2015-09-10T20:52:39.207" UserId="11098" />
  <row Id="7574" PostId="8062" Score="0" Text="Fundamentally, it's best when cross validation indicates it's best amongst the other classifiers you might have opted to choose for the problem. You might regard SVMs as a special class of nearest neighbour classifiers, where the neighbours are restricted to the support vectors." CreationDate="2015-09-10T23:40:57.537" UserId="7980" />
  <row Id="7575" PostId="8026" Score="2" Text="You don't have histograms nor do you have random samples from a mixture of normal distributions.  (If so, you certainly wouldn't be getting negative numbers for the counts or for a probably density.)  It appears that you have samples from a fixed set of x's where the expected values for the vertical axis are from a linear combination of curves with a Gaussian shape and you've added random noise from a separate normal distribution.  I don't mean to be so negative and I'll be more constructive in my next comment." CreationDate="2015-09-11T01:29:52.650" UserId="12729" />
  <row Id="7576" PostId="8062" Score="0" Text="There is no best algorithm, even per class. You can craft a dataset which the best for a particular algorithm, e.g. nearest neighbor." CreationDate="2015-09-11T04:58:38.043" UserId="7848" />
  <row Id="7577" PostId="8026" Score="1" Text="But N=3 does have the smallest AIC value.  -446.6 &lt; -443.5 &lt; -442.7.  I also wonder if you've restricted the values of c to be non-negative." CreationDate="2015-09-11T05:22:05.227" UserId="12729" />
  <row Id="7578" PostId="8066" Score="0" Text="Thank you..!!! Is solr is only related to query?" CreationDate="2015-09-11T06:21:46.497" UserId="12732" />
  <row Id="7579" PostId="8066" Score="0" Text="Yes, Solr is only for search and navigation." CreationDate="2015-09-11T06:23:12.007" UserId="11097" />
  <row Id="7581" PostId="8066" Score="0" Text="What do you mean by **current Trend**?  Can you explain or elaborate?" CreationDate="2015-09-11T06:29:49.490" UserId="11097" />
  <row Id="7582" PostId="8066" Score="0" Text="Is Solr used in current Technologies?" CreationDate="2015-09-11T06:48:07.873" UserId="12732" />
  <row Id="7583" PostId="8066" Score="0" Text="Yes, definitely. Here are some links which would prove it: http://wiki.apache.org/solr/PublicServers,  http://www.quora.com/Which-major-companies-are-using-Solr-for-search&#xA;&#xA;And there are a lot of wrappers and libraries for using Solr with some of the top programming languages out there (like Python, etc)." CreationDate="2015-09-11T06:51:30.613" UserId="11097" />
  <row Id="7584" PostId="8066" Score="0" Text="Thank you sir!!" CreationDate="2015-09-11T07:06:12.940" UserId="12732" />
  <row Id="7585" PostId="8066" Score="0" Text="Glad that it helped.  Do accept the answer, so that future viewers would get benefitted." CreationDate="2015-09-11T07:07:03.237" UserId="11097" />
  <row Id="7586" PostId="6114" Score="0" Text="I think this is a quite non-standard definition of degrees of freedom!" CreationDate="2015-09-11T11:04:17.680" UserId="12736" />
  <row Id="7587" PostId="8046" Score="0" Text="I am creating a gains table, using the &quot;gains&quot; package in R. The predicted response rate should be coming through around 2%, but instead is at 50% predicted response within the top decile." CreationDate="2015-09-11T12:01:59.493" UserId="12235" />
  <row Id="7590" PostId="8026" Score="0" Text="yes, of course, N=3 does have the smallest AIC value... sorry. No, I haven't restricted the values of c to be not negative." CreationDate="2015-09-11T16:27:02.493" UserId="12665" />
  <row Id="7591" PostId="8026" Score="0" Text="About the negative numbers in the histogram: there is a random uniform noise in all data. I have obtained the average from other data where there are no peaks and subtracted to all the histograms (I have reasons to believe that the two process are independent)." CreationDate="2015-09-11T16:29:57.393" UserId="12665" />
  <row Id="7592" PostId="8063" Score="0" Text="thanks. Also, as noted by @Jim_Baldwin, indeed the AIC is indeed minimum at N=3, so it already gives a good criterion to fix its value." CreationDate="2015-09-11T16:37:01.870" UserId="12665" />
  <row Id="7594" PostId="8026" Score="0" Text="I'm curious as to the rationale for wanting a linear combination of gaussians.  (Obviously for this data that seems to work.)  But are you interesting in identifying peaks or valleys?  Do you need a parsimonious description of your data so that the curves can be reconstructed more simply in Excel or some other application?  If you just need a description/summary of the observed relationship, why not use a nonparametric regression estimate (loess, for example)?  That would fit any unforeseen bumps in this and future data." CreationDate="2015-09-11T17:33:28.887" UserId="12729" />
  <row Id="7595" PostId="8069" Score="0" Text="For binary classification I don't understand why you do this instead of simply counting elements from each of the classes. If you have many classes, it might make sense - it is easy to prove that algorithm R in https://en.wikipedia.org/wiki/Reservoir_sampling gives you a uniformly random sample." CreationDate="2015-09-11T20:29:19.527" UserId="6550" />
  <row Id="7596" PostId="8065" Score="0" Text="@SeanOwen I have edited the question details.  Seems like it appears clear now." CreationDate="2015-09-12T04:26:21.817" UserId="11097" />
  <row Id="7599" PostId="8026" Score="1" Text="You may have a look at a similar question I answered at Mathematica Stack Exchange: http://bit.ly/1XW2Iob." CreationDate="2015-09-12T07:54:01.280" UserId="12301" />
  <row Id="7600" PostId="5865" Score="1" Text="@MrMeritology discontinuity at 0 wouldn't bother me either; but there is an other thing that makes (y-f(x))^2 a much better choice than abs(y-f(x)):&#xA;Imagine you had two data points both at the same x (x,y1) and (x,y2). Then sum of squares error is minimal at the mean of y1 and y2 while the absolute error is the same anywhere between y1 and y2. And the mean feels like a good choice for me." CreationDate="2015-09-12T12:01:10.493" UserId="12760" />
  <row Id="7602" PostId="6899" Score="0" Text="@image_doctor : You might be thinking that I am saying that no two classifiers return the same output on the same input.  I am saying no two will return true.  They can both return false." CreationDate="2015-09-12T14:35:04.063" UserId="12438" />
  <row Id="7603" PostId="8063" Score="1" Text="That's right! It's basically the same approach!&#xA;&#xA;Good work @psmith!" CreationDate="2015-09-12T14:38:22.747" UserId="12730" />
  <row Id="7604" PostId="4980" Score="0" Text="Cross-posted on https://www.quora.com/Help-on-training-NLTKs-NER" CreationDate="2015-09-13T00:01:09.837" UserId="843" />
  <row Id="7605" PostId="8026" Score="0" Text="@JimBaldwin I'm interested in the peaks: I need to know the number of peaks and their corresponding position and width. This because each peak is associated with a process that is completely described by the position and the width. Other features and peculiarities of the data are not relevant to me for this analysis. For what I understand, an analysis based on a nonparametric regression would not be optimal in this case, since it could emphasize other features of the data (due to some noise) that I'm not interested here." CreationDate="2015-09-13T10:26:07.460" UserId="12665" />
  <row Id="7606" PostId="8077" Score="0" Text="I really appreciate the detailed answer, but I think I might not have phrased my question clearly. Only 1 value (X) will ever be shown for a time value (Y). There's no need to bucket as what's being graphed are an individual's test results." CreationDate="2015-09-13T14:16:23.257" UserId="12747" />
  <row Id="7607" PostId="8026" Score="1" Text="Just a small caution:  when fitting with linear combinations of gaussian curves, the number of peaks can be less than the number of gaussian curves.  There are a variety of automated methods to find peaks (although none are perfect in part because a peak depends on the scale that you define what a peak is).  You might google for &quot;bump hunting&quot; and &quot;peaks&quot; in this forum and the related Cross Validation and Mathematica forums.  An example with Mathematica is at http://mathematica.stackexchange.com/questions/23828/finding-local-minima-maxima-in-noisy-data/23893#23893." CreationDate="2015-09-13T18:09:03.107" UserId="12729" />
  <row Id="7608" PostId="8084" Score="1" Text="The param to be optimised, `x0` is a `ndarray` . . . so your &quot;one variable&quot; is an array - i.e. any number of scalar or concatenated array variables. In what way is this not working for multi-variable functions?" CreationDate="2015-09-14T06:24:16.097" UserId="836" />
  <row Id="7609" PostId="8084" Score="0" Text="I upvoted @NeilSlater's comment, but I shouldn't have. The question is about multivariate (rectangular) constraints, not about multivariate argument." CreationDate="2015-09-14T06:55:55.870" UserId="6550" />
  <row Id="7610" PostId="8084" Score="0" Text="@NeilSlater when you say, &quot;in what way is this not working,&quot; what is the &quot;this&quot; you're referring to?  i.e., which scipy function are you referring to?  the different functions aren't suitable to my case for different reasons." CreationDate="2015-09-14T07:24:50.067" UserId="12777" />
  <row Id="7611" PostId="8084" Score="0" Text="Sorry, I missed the constraints as being the main blocker to using `fsolve`, and thought you had in turn missed the first param being an array. Perhaps making the question more self-contained would have helped me spot the difference." CreationDate="2015-09-14T07:55:26.750" UserId="836" />
  <row Id="7612" PostId="8056" Score="0" Text="I don't think Apriori can be efficiently/elegantly implemented in R *at all*. The language is just not designed for such operations. Compare the fastest R implementation you can find to a C implementation such that of Borgelt. I bet there is a factor of 100x or more in difference; and probably growing with data set size. And this will just *kill* you on any realistic data set when attempting to do this in plain R." CreationDate="2015-09-14T09:41:35.033" UserId="924" />
  <row Id="7613" PostId="8051" Score="0" Text="It's walled, requires registration. How does it benchmark, compared to e.g. arules, on some classic benchmark sets?" CreationDate="2015-09-14T09:42:18.767" UserId="924" />
  <row Id="7614" PostId="6532" Score="0" Text="Do *not* use synthetic data sets. You are avoiding all the challenges. If you generate the data using Gaussians, the methods will converge easily, and find good solutions; but that is not what real data is like." CreationDate="2015-09-14T09:44:00.723" UserId="924" />
  <row Id="7615" PostId="8090" Score="0" Text="This is probably better asked at SO?" CreationDate="2015-09-13T18:27:48.567" UserId="12736" />
  <row Id="7616" PostId="8090" Score="2" Text="@kjetilbhalvorsen I think it would be better suited for the Data Science SE" CreationDate="2015-09-13T18:42:54.103" UserId="11097" />
  <row Id="7617" PostId="8089" Score="0" Text="Interesting. Thanks for sharing this. I wonder why they didn't test this approach on english corpora." CreationDate="2015-09-14T11:33:15.923" UserId="2750" />
  <row Id="7618" PostId="8051" Score="0" Text="@Anony-Mousse: I haven't benchmarked it but I expect it to be considerably slower because arules calls, as I wrote in my question, a compiled C implementation." CreationDate="2015-09-14T13:11:27.623" UserId="690" />
  <row Id="7619" PostId="8051" Score="0" Text="Yes, but it would be good to be able to put a number on that. Two orders of magnitude, or four? Because in my experience, plain R code is *incredibly* slow (so it does not make sense to even start implementing such algorithms in R in my opinion)." CreationDate="2015-09-14T13:16:25.060" UserId="924" />
  <row Id="7620" PostId="8051" Score="0" Text="@Anony-Mousse: It can make sense for pedagogical reasons, to understand the underlying concepts and algorithms." CreationDate="2015-09-14T13:18:25.793" UserId="690" />
  <row Id="7621" PostId="8056" Score="1" Text="I think @vonjd just needs a simple implementation to demonstrate the algorithm, just a prototype." CreationDate="2015-09-14T16:21:06.987" UserId="5179" />
  <row Id="7622" PostId="8051" Score="2" Text="I disagree. It only teaches them to use the wrong tools the wrong way. You are better off using pseudocode that doesn't bother with irrelevant syntax hacks necessary to make it work in R. Your students will be fighting with R code they'll never use, and miss the subtle details of Apriori. I just had a look at this Apriori implementation, and it already lacks some of these simple yet efficient optimizations that simply exploit the sortedness and drastically reduce the apriori-gen cost. Focus on the key ideas of generating as few candidates, and clever pruning instead." CreationDate="2015-09-14T17:17:25.037" UserId="924" />
  <row Id="7623" PostId="8056" Score="0" Text="Note that the description of &quot;prune&quot; in this pdf is incomplete. It never prunes anything, and never uses s_min (it's fairly obvious what is missing, though). In candidates, it does not explicitly exploit that the frequent itemset are always sorted, which improves runtime substantially (one may argue this to be hidden in the &quot;with&quot; condition). But **do use Christian Borgelt's implementations** (via the `arules` package)!" CreationDate="2015-09-14T17:29:04.573" UserId="924" />
  <row Id="7624" PostId="8051" Score="0" Text="Which is why I'd really like to see a benchmark comparison... because it is not just R, there are also algorithmic aspects that tend to get lost when people try to do a &quot;simple implementation&quot;. It's no longer Apriori then, if all the good parts are left away for simplicity." CreationDate="2015-09-14T17:32:33.457" UserId="924" />
  <row Id="7625" PostId="8056" Score="0" Text="(I'm *not* sure if the `arules` package has his latest version, though!)" CreationDate="2015-09-14T17:34:55.067" UserId="924" />
  <row Id="7626" PostId="8059" Score="0" Text="Thank you for your input :) VW is an interesting recommendation and I like the python online learner. To extend your answer can you please provide sample code how to feed VW with profiles (I'm assuming dimensionality reduction will be handled in VW)" CreationDate="2015-09-14T18:09:59.900" UserId="12663" />
  <row Id="7627" PostId="8085" Score="0" Text="How does the `geom_smooth()` continue to work? I mean, for a line to fit, I was of the understanding that there needs t be two set of numbers. In this case, I don't understand how a factor and a number can come to give a linear regression model line. Or may be I just don't understand it to begin with." CreationDate="2015-09-15T02:12:37.443" UserId="10345" />
  <row Id="7628" PostId="8095" Score="0" Text="Thanks for trying to understand. I have edited the question in the hope it is clearer. Note that 0.06 is not a percentage, I am not building a classifier but rather a regression since the network generates images. Also, I don't think the initial config has overfitting, since when I plot the learning curve the validation error comes close to the training error." CreationDate="2015-09-15T12:39:49.063" UserId="12776" />
  <row Id="7629" PostId="8087" Score="1" Text="Have you looked at the PCA-reduced output examples? Any values above 1.0 or below 0.0 in any &quot;pixels&quot;? (Oh, hang on, networks 5 should have coped with that . . .)" CreationDate="2015-09-15T13:07:43.963" UserId="836" />
  <row Id="7630" PostId="8101" Score="0" Text="This is essentially a link-only answer and tends to be discouraged on SE. Maybe you can elaborate what it is and why it's helpful." CreationDate="2015-09-15T15:14:01.840" UserId="21" />
  <row Id="7631" PostId="8087" Score="0" Text="That is a good point, values are not in [0,1] anymore. Somehow I thought of the input that is not normalized anymore (which doesn't prevent it from working in network 3), but did not think of this... But yeah you're right, 5 should have coped with that. I double checked by making the output linear in network 2, same result." CreationDate="2015-09-15T15:40:08.857" UserId="12776" />
  <row Id="7632" PostId="8018" Score="0" Text="@umairdurrani Are you sure you can't infer change of lanes from the data? `xcoord` is relative to an origin? `Frame.ID` represents a time interval (e.g. 1 frame = 0.2 seconds)?" CreationDate="2015-09-15T23:36:57.663" UserId="4621" />
  <row Id="7633" PostId="8018" Score="0" Text="@RobertSmith Inferring lane change is not the problem. Finding the start and end `Frame.ID`s of lane change is." CreationDate="2015-09-16T00:57:50.470" UserId="4933" />
  <row Id="7634" PostId="8101" Score="0" Text="@AlbertoD The archaic language of the vignette you shared is not helpful for someone new to the concept of changepoint analysis." CreationDate="2015-09-16T01:00:30.837" UserId="4933" />
  <row Id="7635" PostId="8098" Score="0" Text="This is actually what I did in the first place. The problem is that the difference threshold and &quot;high&quot; values are very subjective because each vehicle's journey is different." CreationDate="2015-09-16T01:02:28.460" UserId="4933" />
  <row Id="7636" PostId="8018" Score="0" Text="I think I don't understand the issue here. If you are able to know the lanes at each point in time, wouldn't that be enough to identify the start and end of lane change? (e.g. `Frame.ID: 452, Lane: 3, Frame.ID: 453, Lane: 3, ..., Frame.ID: 549, Lane: 3, Frame.ID.: 550, Lane: 4`)" CreationDate="2015-09-16T04:13:01.030" UserId="4621" />
  <row Id="7637" PostId="8095" Score="0" Text="Rather than just accepting @Djizeus answer and editing your question, you need to do a detailed `cross validation`, or better yet, a ['bias-variance decomposition'](http://datascience.stackexchange.com/questions/5268/how-to-detect-overfitting-of-a-stock-screener).  Then you will know whether the first case is really overfit or not.  You should always cross validate! Please let us know what the the results show.  I suspect that the solution is more nuanced and is more closely related to fitting a different PCA transformation to every image rather than doing PCA on the entire corpus of data." CreationDate="2015-09-16T04:33:25.310" UserId="9420" />
  <row Id="7642" PostId="8064" Score="1" Text="Fork sklearn and add support yourself; it's not that hard!" CreationDate="2015-09-16T07:31:15.873" UserId="381" />
  <row Id="7643" PostId="8064" Score="0" Text="K. Are you saying modify the sklearn source file I think griddearchcv.py file? Any directions will be great." CreationDate="2015-09-16T07:42:41.057" UserId="1151" />
  <row Id="7645" PostId="8059" Score="0" Text="Assuming each record is a description of an &quot;event&quot; (a user-ad interaction, parametrized by user / ad info, and recording 0/1 target), the best description on VW input is:&#xA;&#xA;http://mlwave.com/predicting-click-through-rates-with-online-machine-learning/&#xA;&#xA;As for dimensionality reduction, you can control via matrix factorization options (rank of the projection) and (implicitly) via the hashing table size. I think LDA can be abused if you care about interpretation, but I am not sure. In general, have a look at:&#xA;&#xA;https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments" CreationDate="2015-09-16T08:50:21.683" UserId="10936" />
  <row Id="7649" PostId="8105" Score="1" Text="What problems are you unable to solve with the tools your group currently uses? There is no reason to unnecessarily invest in commercial software unless it meets a need of the business. Personally, I don't think that BI tools are useful for &quot;Data Scientists&quot; -- their target audience is generally less technical." CreationDate="2015-09-16T14:20:20.830" UserId="947" />
  <row Id="7650" PostId="8095" Score="1" Text="Yes, I had done the decomposition and it looked fine, and I did not want to make the question even larger. But I added it now. As for the not working cases, it looks really strange..." CreationDate="2015-09-16T15:18:07.503" UserId="12776" />
  <row Id="7652" PostId="8059" Score="0" Text="Thanks for the update. However the problem with mlwave and Criteo  dataset based tutorials is the fact that categorical features come pre-hashed (some of them might be a &quot;topic&quot; based on user behavior) this does not solve the problem of creating features from user profiles. But I have almost no experience with VW so there might be something obvious I'm missing. Can I feed it directly with something like 1 | [1,2,3]; 2 | [3,6,8,11] ?" CreationDate="2015-09-16T16:39:38.680" UserId="12663" />
  <row Id="7654" PostId="8064" Score="0" Text="Look at the definition of [sklearn.metrics.f1_score](https://github.com/scikit-learn/scikit-learn/blob/a95203b/sklearn/metrics/classification.py#L479)" CreationDate="2015-09-16T21:18:00.753" UserId="381" />
  <row Id="7655" PostId="8105" Score="0" Text="Tableau is free if you're fine with public sharing." CreationDate="2015-09-16T21:29:14.327" UserId="12241" />
  <row Id="7656" PostId="8105" Score="1" Text="I'd invest in hardware instead; nice monitors, SSDs, RAM, etc. I've never encountered a situation where I wished I had commercial software (though technical support might have been nice)." CreationDate="2015-09-17T07:08:42.590" UserId="381" />
  <row Id="7657" PostId="8108" Score="0" Text="A cool open source CAS is Sage...you might like it if you're into Mathematica." CreationDate="2015-09-17T07:31:42.187" UserDisplayName="user9424" />
  <row Id="7658" PostId="8107" Score="0" Text="I agree! Our company has been abandoning Tableau because of their ridiculous fees...clients would rather pay us to develop customized software that is more integrated with their data...and then they actually own the software too!" CreationDate="2015-09-17T07:35:25.013" UserDisplayName="user9424" />
  <row Id="7659" PostId="8105" Score="0" Text="@Emre totally agree...invest in enabling tech not limiting licenses" CreationDate="2015-09-17T07:36:56.223" UserDisplayName="user9424" />
  <row Id="7660" PostId="8059" Score="0" Text="I don't have a vowpal installation on the computer I'm typing from, so I can only share a few &quot;offline&quot; thoughts:&#xA;1. if you replace {1,2,3} with letters, drop the semicolons and add variable names, this could work&#xA;2. you can verify/experiment yourself  via http://hunch.net/~vw/validate.html&#xA;3. would it be possible to paste a line or two of your input? i am quite certain that hashing is done internally in VW anyway, so [1,2,3] as a feature value could work." CreationDate="2015-09-17T08:04:55.163" UserId="10936" />
  <row Id="7663" PostId="8105" Score="0" Text="This is probably too &quot;opinion based&quot; for SE, if you're not looking to solve a particular problem. Might be better for the Software Recommendations site." CreationDate="2015-09-17T13:40:27.087" UserId="21" />
  <row Id="7664" PostId="8111" Score="0" Text="Can you augment this by explaining what you mean with this example? otherwise it's just links." CreationDate="2015-09-17T13:41:28.777" UserId="21" />
  <row Id="7665" PostId="8079" Score="0" Text="Run R with `sudo`. Regular user is not able to write into `/usr/local`" CreationDate="2015-09-17T13:55:48.610" UserId="755" />
  <row Id="7666" PostId="6805" Score="0" Text="So it turns out that Neil was right - in my code to split the data sets I had an error (or instead of an and) which meant that half my test data was included in the training set and thus over-fitting." CreationDate="2015-09-17T14:15:25.297" UserId="12294" />
  <row Id="7668" PostId="8111" Score="0" Text="Thanks, I've added more detail." CreationDate="2015-09-17T16:17:59.473" UserId="12844" />
  <row Id="7669" PostId="8111" Score="1" Text="I think the problem with this idea is it doesn't really align with how Deep Dreaming works. You would need to train a network to recognise &quot;suitable clothing&quot;, but it would not then appropriately dress unclothed figures -  instead it would output drapery-looking stuff over places that already looked a bit like pieces of suitable attire. I.e. it is more likely to make a tree trunk into a trouser leg than to put a dress on a girl in a bikini. Deep Dreaming doesn't pick targets for replacement like an image regular-expression engine, it hallucinates matches in like-for-like way." CreationDate="2015-09-17T16:56:20.457" UserId="836" />
  <row Id="7670" PostId="8114" Score="0" Text="maybe I just need to add another vector in addition to the words...? but that seems odd." CreationDate="2015-09-17T18:31:19.187" UserId="9373" />
  <row Id="7671" PostId="8111" Score="0" Text="See http://cs.stackexchange.com/questions/47262/google-deep-dream-has-these-understandings I can't demonstrate or prove much, as it's mostly personal experience and observation and I havn't be giving it many nude people, but I think it is smarter than you give it credit for, although I do appreciate that my examples do indeed look quite cobbled together :-P" CreationDate="2015-09-17T19:16:07.037" UserId="12844" />
  <row Id="7672" PostId="8115" Score="0" Text="can you give examples of how one would perform regression on a bag of words?" CreationDate="2015-09-17T19:25:35.700" UserId="9373" />
  <row Id="7673" PostId="8115" Score="1" Text="If you used LDA for example, each document would be represented by a topic vector.  You could then transform your labels (&quot;big&quot;, &quot;small&quot;, etc) into real-values (big=100,small=10,tiny=1) and perform any kind of regression, even simple linear regression, to predict the real-valued targets.&#xA;&#xA;There are a number of places you could get implementations of LDA.  What language are you using?" CreationDate="2015-09-17T19:29:44.790" UserId="12306" />
  <row Id="7674" PostId="8115" Score="0" Text="We're using python and scikit-learn. I've used LDA and LSA for topic modelling, but I'm not sure how that fits onto my labels. And what about just using a tfidf vector of the document words?" CreationDate="2015-09-17T19:58:01.133" UserId="9373" />
  <row Id="7675" PostId="8115" Score="1" Text="Sure, tf-idf vectors might be sufficient.  If you need your output labels to be ordered, you need to either assign real values to the targets (as I said above). In order to go from tf-idf vectors or topic vectors to your labels, you would either use any kind of regression (if you map labels to numbers) or a classifier (which it sounds like you've already tried)." CreationDate="2015-09-17T20:17:29.080" UserId="12306" />
  <row Id="7676" PostId="8111" Score="1" Text="The pictures are good - amongst some of the most interesting I've seen from Deep Dreaming. However, I think the search for deeper meaning and structure beyond clever pattern matching is like looking for general intelligence in a dissected retina . . . there is a level that bigger/faster/deeper networks just trained on images will not take us to - something more is needed." CreationDate="2015-09-17T21:09:07.990" UserId="836" />
  <row Id="7677" PostId="8111" Score="0" Text="Thanks for your input :) It's interesting." CreationDate="2015-09-17T21:41:30.637" UserId="12844" />
  <row Id="7678" PostId="8119" Score="0" Text="&amp; as you have noticed &amp; mentioned elsewhere there is already an &quot;off the shelf&quot; Iphone/ android [dreamify](http://dreamify.io/) image filter" CreationDate="2015-09-17T22:19:32.827" UserId="292" />
  <row Id="7679" PostId="8119" Score="0" Text="another application: generating simulated/ virtual environments for games or movies. similar to [procedural generation](https://en.wikipedia.org/wiki/Procedural_generation)" CreationDate="2015-09-17T22:52:13.787" UserId="292" />
  <row Id="7680" PostId="8120" Score="0" Text="Are you using xgboost bound to Python? The command-line version definitely does what you want: `model_in [default=NULL]&#xA;path to input model, needed for test, eval, dump, if it is specified in training, xgboost will continue training from the input model` - but I don't know whether and how the Python bindings support this option." CreationDate="2015-09-18T12:25:10.447" UserId="836" />
  <row Id="7681" PostId="8124" Score="0" Text="What does the user's class represent?  It's possible that the classes simply aren't easily separable within the part-of-speech tag feature space.  While there are clustering techniques easily capable of handling a problem of this size, clustering seems like an unnecessary processing step if your end goal is binary 0/1 classification.  Also consider that frequency data often requires scaling or normalization before many supervised methods will be helpful.  Did you try that?  At the simplest representation, you might try tf-idf vectors." CreationDate="2015-09-18T15:15:33.427" UserId="12306" />
  <row Id="7683" PostId="8128" Score="0" Text="Please add some additional information and formatting so that your question is more understandable.  If you are having a difficult time describing your question, you could post your data within the question or post a portion of it." CreationDate="2015-09-18T20:24:59.067" UserId="9420" />
  <row Id="7685" PostId="8129" Score="2" Text="This might help: http://www.cs.toronto.edu/~tijmen/tijmen_thesis.pdf . . . although I don't know enough to turn that into a full answer, perhaps someone else will." CreationDate="2015-09-18T21:25:31.197" UserId="836" />
  <row Id="7686" PostId="8120" Score="0" Text="yes I am using the python version, I will check if something like this is supported." CreationDate="2015-09-18T23:30:23.217" UserId="12858" />
  <row Id="7687" PostId="8108" Score="0" Text="@Bey Nope. I need the quality,  support, and infrastructure of a commercial solution. Mathematica is it." CreationDate="2015-09-19T00:11:01.227" UserId="10814" />
  <row Id="7689" PostId="8131" Score="2" Text="I don't think you've said what you are trying to do. Predict the dependent variable? what regression have you tried?" CreationDate="2015-09-19T07:05:51.103" UserId="21" />
  <row Id="7690" PostId="6532" Score="0" Text="@Anony-Mousse I understand your objection. My only point was: **start** learning using 1) simple data sets where you 2) know the answers." CreationDate="2015-09-19T08:10:40.287" UserId="10620" />
  <row Id="7692" PostId="8018" Score="0" Text="@RobertSmith The issue is that without looking at the plots I can not identify which frame is the start and which one is the end of lane change. The lane change 'starts' when the vehicle starts to change the direction to enter a new lane and 'ends' when after entering the new lane the variation in lateral position is quite small. This could not be inferred directly from data." CreationDate="2015-09-19T14:10:32.343" UserId="4933" />
  <row Id="7693" PostId="8120" Score="0" Text="It's not supported in the python module, but there is a fork that shows the simple change needed to enable this.&#xA;&#xA;https://github.com/Far0n/xgboost/commit/c768a042cba117547a10aa7682f7e6fd60cec6ea" CreationDate="2015-09-19T14:36:20.663" UserId="10816" />
  <row Id="7694" PostId="8131" Score="0" Text="I explained that I want to predict the dependent variable and study the coefficients also. I mentioned what analysis I have done so far. Thank you." CreationDate="2015-09-19T21:23:07.717" UserId="12867" />
  <row Id="7695" PostId="8018" Score="1" Text="Yes, I understand that you want to identify the end of the change lane maneuver, but if you already have the lane of the vehicle at each time, then it is not hard to detect those changes. I would start by defining when we should consider that the vehicle is not changing lanes anymore (e.g. after a given number of seconds driving on the same lane). You could use a window to detect segments in which the vehicle keeps the same lane and the points at the start and end of such segments describe your &quot;start of lane change&quot; and &quot;end of lane change&quot;, respectively." CreationDate="2015-09-19T21:35:10.970" UserId="4621" />
  <row Id="7697" PostId="8134" Score="0" Text="Thank you for your detailed information. But I have two questions: 1- How did you get the difference between rows 1 and 2 as 3 for category C, (columns 11, 15, and 16 are all off by one). Don't you consider column 16 which is -1, therefore, it must be 2 instead of 3, am I right?  2- What is the problem for the first approach (which was simpler) rather than being cautious about choosing the seeds? Thank you again." CreationDate="2015-09-20T02:25:27.827" UserId="12867" />
  <row Id="7700" PostId="8131" Score="1" Text=". . . unless you are asking about analysis *for* data collected as part of your PhD (which you are studying now)? In which case I think it may be time to talk things through with your advisor." CreationDate="2015-09-20T08:20:18.210" UserId="836" />
  <row Id="7703" PostId="8131" Score="0" Text="Thank you Neil. I'm doing the analysis for data collected as part of my PhD. I had some graduate statistical analysis courses but they were not as helpful and structured as I expected. On the other hand, my adviser is not well-experienced in this field. Hence, I want to get the experienced advice to build a clear plan for my analysis which must be heavy enough for PhD also. Here are the different steps in my mind so far: &#xA;1-	Plotting the scatter plots of different independent variables vs dependent variable to define outliers and check if the model is linear also with respect to coefficients" CreationDate="2015-09-20T12:39:21.260" UserId="12867" />
  <row Id="7704" PostId="8131" Score="0" Text="2-	Removing the potential outliers &#xA;3-	Splitting the data into two data sets to build the model and validate it after that &#xA;If the model is linear then &#xA;4-	Performing the multiple linear regression &#xA;5-	Performing the multiple linear regression including different transformations to enhance the model &#xA;6-	Validating the model &#xA;7-	Doing the quantile regression &#xA;8-	Doing supervised learning machine etc.&#xA;If the model is not linear use the nonlinear statistical techniques. &#xA;Thank you again." CreationDate="2015-09-20T12:39:49.630" UserId="12867" />
  <row Id="7705" PostId="8017" Score="0" Text="I have failed in tracking Howard Dresner's works, but apparently I am not alone..." CreationDate="2015-09-20T15:41:59.730" UserId="12527" />
  <row Id="7706" PostId="8121" Score="0" Text="Thanks. Can i ask can you offer any guidance as a good rule if thumb for the number of hidden neurons and learning rate as a first approximation?  Given a set number of features? Eg something like sqrt number features for neurons?" CreationDate="2015-09-20T16:11:58.357" UserId="12294" />
  <row Id="7707" PostId="8018" Score="0" Text="@RobertSmith Thanks. I have now used the definition of start and end of lane change as when vehicle leaves the center line of the origin lane and when it first touches the center line of the target lane respectively. I looked at the differences in the `xcoord`inate (y axis on the plot) of center line of the lane and that of the vehicle and set start as the last point where this difference was minimum in the origin lane and end as the first point where the difference between target lane' center line and vehicle's `xcoord` was minimum." CreationDate="2015-09-20T16:14:43.923" UserId="4933" />
  <row Id="7708" PostId="8059" Score="0" Text="I got a chance to dig a bit deeper into vw and it seems that it indeed does support online LDA based on Variable Bayes (VB) which is exactly what I was looking for. Thanks for that! http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf Now I'll have to somehow make this applicable for production use:)" CreationDate="2015-09-20T16:52:52.833" UserId="12663" />
  <row Id="7709" PostId="8018" Score="1" Text="Great. I thought you didn't have origin and target lane but If you always have them, your solution should work and additionally uses the data you already have to construct a definition of lane change." CreationDate="2015-09-20T17:07:08.543" UserId="4621" />
  <row Id="7710" PostId="8136" Score="0" Text="OK, it sounds like both types of learning methods can be for prediction, since both &quot;aim to learn a function ... that predicts ....&quot; I suppose then that it is more accurate for me to say that some unsupervised methods cab be used for description, such as simple clustering or principle component analysis. These don't predict, they just describe the structure of a distribution." CreationDate="2015-09-20T18:55:01.827" UserId="12881" />
  <row Id="7712" PostId="8131" Score="0" Text="@ Neil: Thank you Neil. I know it's too broad but I think different people may have various approaches towards this problem that will eventually help me to build my road map more clear and robust." CreationDate="2015-09-20T21:41:22.293" UserId="12867" />
  <row Id="7714" PostId="8141" Score="1" Text="Just noticed this is self-answered. Could you explain why this is a good answer to your problem (link-only answers are discouraged in Stack Exchange)?" CreationDate="2015-09-21T06:48:05.107" UserId="836" />
  <row Id="7715" PostId="8138" Score="1" Text="I am not an NLP expert by why wouldn't you consider doing an LSA on the whole corpus and then use the corresponding scores in a clustering algorithm? That would easily take care of the fact that different templates have different numerical parameters. Finding the number of clusters $k$ would then follow a standard methodological procedure (eg. $k$-means/AIC)." CreationDate="2015-09-21T07:27:24.557" UserId="12905" />
  <row Id="7716" PostId="8134" Score="0" Text="@Amir, I missed that column 14 was also different, so the total distance should be 4. When calculating distance, we need to use absolute value so that distances are non-negative and symmetric. (If we want '0 distance' to mean that two rows are the same, we can't let +1 in one column cancel out a -1 in another column. If we want to use one number for distance, we need 2 to be 2 away from 4 and 4 to be 2 away from 2, instead of -2 away from 2.)&#xA;&#xA;The problem with the first approach is a centroid with all NA values will be as close as possible to all rows--which makes us unable to cluster!" CreationDate="2015-09-21T14:25:17.987" UserId="12876" />
  <row Id="7717" PostId="8120" Score="0" Text="Thanks @inversion, I will test it out." CreationDate="2015-09-21T17:21:08.947" UserId="12858" />
  <row Id="7718" PostId="8141" Score="0" Text="I will elaborate" CreationDate="2015-09-21T18:16:59.863" UserId="12898" />
  <row Id="7719" PostId="8141" Score="0" Text="Thank you. So your problem is that you have known values (ground truth) for correct ranking of *some* of the options based on their features, but not all of them? And/or correct ranking of the top options, not just selecting the best one, is an important outcome for your predictive model? I'm asking because neither of those things are clearly stated in the original question (so I was going to suggest perhaps reinforcement learning based on answers to my comment, but clearly now that would be completely wrong!)" CreationDate="2015-09-21T18:25:06.877" UserId="836" />
  <row Id="7720" PostId="8131" Score="0" Text="@ Neil: Thank you very much for your help and support." CreationDate="2015-09-21T19:43:02.410" UserId="12867" />
  <row Id="7721" PostId="8143" Score="0" Text="thanks for the suggestion. This will work fine in case that the fix part (template) is leading, followed by the parameters. I will recognize &quot;Temperature today is&quot; as a phrase, but I will probably have problem to distinct if *20* and *centigrade* is parameter or part of the template in my example one." CreationDate="2015-09-21T19:49:28.147" UserId="10620" />
  <row Id="7722" PostId="8138" Score="0" Text="@usεr11852 I appreciative you proposal, but interestingly I get better result using simple distance on TermDocumentMatrix (tm) then with LSA. I suppose this is due to the fixed structure of my templates, e.g. the order of the terms is  relevant for the distance. I intuitively guess there must be a simple elementary solution for my problem, probably based on the distance matrix, but feel free to formulate your proposal as answer, so I can honor it." CreationDate="2015-09-21T20:06:25.347" UserId="10620" />
  <row Id="7723" PostId="8143" Score="0" Text="True - but if you then list out the words most similar to &quot;Temperature today is,&quot;  you'd expect that &quot;centrigrade&quot; is essentially the exact same (and hence part of the template), whereas &quot;20&quot; (or other temps) will be fairly similar, but much less so." CreationDate="2015-09-21T20:33:53.027" UserId="12306" />
  <row Id="7724" PostId="8131" Score="1" Text="I would completely avoid removing &quot;potential outliers&quot;.  You should certainly identify values that seem a bit odd and if there is some clear reason for removing or changing the value (such that from field notes you see that there is a transcription error), only then remove them.  But otherwise, that's data which should be kept.  Everything else might be just noise." CreationDate="2015-09-21T21:24:12.680" UserId="12729" />
  <row Id="7725" PostId="8101" Score="0" Text="@AlbertoD Could you please provide any example of how you used the cp package in your case?" CreationDate="2015-09-22T00:55:58.567" UserId="4933" />
  <row Id="7726" PostId="8149" Score="0" Text="Thanks for answering. I will try to gain more insight into this" CreationDate="2015-09-22T03:20:09.740" UserId="12911" />
  <row Id="7727" PostId="8150" Score="2" Text="I don't think clustering is the best approach.  Instead I suggest treating it as a time series.  Prior to lane change, the x position will probably fit a stationary distribution.  Lane changes could be detected by tests for non-stationarity.  An even simpler approach would be trend change detection via moving averages or similar, as is done in technical stock market analysis." CreationDate="2015-09-22T06:10:45.917" UserId="609" />
  <row Id="7728" PostId="8144" Score="0" Text="Clos - Thank you very much for the answer. I have built a tdm for the corpus and applied K-NN on the dataset. Can i apply cosine similarity distance on the KNN. How can this be done? Can you give a sample R or Python code or reference for the same?" CreationDate="2015-09-22T06:18:08.450" UserId="9793" />
  <row Id="7729" PostId="8144" Score="0" Text="Clos - I also calculated tf-idf and upon this i applied KNN algortihm. But the results were poor. any idea on how it is different from calculating cosine similarity distance and then applying KNN?" CreationDate="2015-09-22T06:39:12.583" UserId="9793" />
  <row Id="7730" PostId="8150" Score="1" Text="Adding to AnonyMousse's answer and MrMeritology's comment, which I agree with, you can consider the following approaches: _trend analysis_ (time series-based) and _change point analysis_ (entropy-based or other). See my relevant answers and links within: [on trend analysis](http://stats.stackexchange.com/a/146261/31372) and [on change point analysis](http://stats.stackexchange.com/a/145472/31372)." CreationDate="2015-09-22T07:16:29.657" UserId="2452" />
  <row Id="7731" PostId="8105" Score="0" Text="IMHO this question (as formulated) will have a hard time being answered properly on any site. This is because choice of the tools for data science (or any other subject domain) heavily depends on existing and future _problems (tasks)_ as well as existing _skills_ and tolerance for learning new ones (i.e., available time)." CreationDate="2015-09-22T07:53:44.643" UserId="2452" />
  <row Id="7732" PostId="8131" Score="0" Text="As the great R A Fisher said &quot;To call in the statistician after the experiment is done may be no more than asking him to perform a postmortem examination: he may be able to say what the experiment died of. &quot;" CreationDate="2015-09-22T08:21:48.027" UserId="471" />
  <row Id="7733" PostId="8144" Score="0" Text="K-NN is made of two things: a representation scheme (how you model your documents in your system, for example unigrams with tf-idf weighting) and a similarity metric between 2 documents which is used to retrieve the k nearest neighbours.&#xA;&#xA;I gave tf-idf and cosine similarity as examples, but you should use whatever you were using. You just need to store the neighbours somewhere and analyze which unigrams put them into the retrieved neighbours." CreationDate="2015-09-22T08:22:08.783" UserId="12909" />
  <row Id="7734" PostId="8144" Score="0" Text="Clos - Thank you again. I really dont have idea on how to store the neighbours. I searched for it and still continuing to do. If you can refer me to this technique it will be really helpful. sorry to trouble you again." CreationDate="2015-09-22T10:13:19.650" UserId="9793" />
  <row Id="7735" PostId="8144" Score="0" Text="That would depend on whatever language and package/library you are using." CreationDate="2015-09-22T10:35:01.127" UserId="12909" />
  <row Id="7737" PostId="8151" Score="0" Text="Are there any tutorials for segmentation in R with similar kind of analysis? My search results in journal articles." CreationDate="2015-09-22T12:07:47.300" UserId="4933" />
  <row Id="7738" PostId="8150" Score="0" Text="Could you please recommend resources for R to implement these techniques? I have never used these before." CreationDate="2015-09-22T12:09:57.677" UserId="4933" />
  <row Id="7740" PostId="8145" Score="0" Text="Excellent -- that is just what I was looking for. And thanks for the reference too." CreationDate="2015-09-22T15:35:50.160" UserId="12881" />
  <row Id="7741" PostId="8151" Score="0" Text="I don't use R much (too slow), so you will have to use Google to find that." CreationDate="2015-09-22T15:48:32.470" UserId="924" />
  <row Id="7742" PostId="8151" Score="0" Text="If you know about any Python resources please share those. Maybe I am not using right keywords in Google because all I get are journal articles pdfs." CreationDate="2015-09-22T16:35:24.240" UserId="4933" />
  <row Id="7743" PostId="8131" Score="0" Text="@ Spacedman, I hear your point but I'm working on a very large and diverse data which were collected by a third party. Now, I'm developing the meaningful dependent and independent variables and extracting the data that can be used for my study. Hence, there was not any design of experiment prior to my data collection; however, I'm assuming the confusion came up from my first sentence ;)" CreationDate="2015-09-22T16:41:22.170" UserId="12867" />
  <row Id="7746" PostId="8152" Score="0" Text="@ Aleksander, I went through all your reference posts which were really helpful but here are some more questions:&#xA;1.	Imagine my research goal is to define the significant variables on the dependents variables, what kind of analysis do you advice rather than regression?&#xA;2.	Do you aware of any regression methods that are more advanced and general (I mean no need all the independent variables follow normal distribution)&#xA;3.	I’m assuming I have some latent variables? How do you approach that?&#xA;4.	Would mind telling me what the difference are between machine learning and regression methods? Thanks." CreationDate="2015-09-22T17:06:22.790" UserId="12867" />
  <row Id="7747" PostId="8152" Score="1" Text="@Amir: I'm glad that my info helped. In regard to your question, unfortunately, it is impossible for me to answer your questions in detail, as each question is quite broad. However, the following are my brief answers. 1. That cannot be your research goal, unless your topic is statistical analysis itself - IMHO, you should formulate research goals in terms of your problem's subject domain. Regardless, your sentence is not clear to me. 2. AFAIK, some multivariate methods do not have data normality assumption (i.e., PLS-SEM). (to be continued)" CreationDate="2015-09-22T17:25:59.237" UserId="2452" />
  <row Id="7748" PostId="8152" Score="0" Text="@Amir: (cont'd) Speaking about regression per se, read about [robust regression](https://en.wikipedia.org/wiki/Robust_regression) methods. Also see [this discussion](http://stats.stackexchange.com/q/12262/31372), which reminds that normality assumption applies to residuals, not data itself. 3. Start with [Wikipedia](https://en.wikipedia.org/wiki/Structural_equation_modeling). Then try [this tutorial](http://ikpp.si/att/26/Ullman%20%20Structural%20equation%20modeling.pdf) and/or [these Mplus tutorials](http://www.ats.ucla.edu/stat/seminars/muthen_08). (to be continued)" CreationDate="2015-09-22T17:48:40.407" UserId="2452" />
  <row Id="7749" PostId="8152" Score="0" Text="@Amir: (cont'd) 4. I would say that the main difference between ML and regression is as follows. Regression is a relatively low-level set of methods to establish relatively simple statistical relationships, whereas ML is a much more sophisticated set of learning from data methods, some of which use regression under the hood. For more reading, see [this](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3345521), [this](https://www.quora.com/When-do-you-use-machine-learning-vs-statistical-regression) as well as [this](http://stats.stackexchange.com/q/6/31372) and links within. Hope that was helpful." CreationDate="2015-09-22T17:57:21.283" UserId="2452" />
  <row Id="7750" PostId="8144" Score="0" Text="Clos - I could able to get the K nearest neighbour in R. Thanks for the help" CreationDate="2015-09-23T08:53:58.740" UserId="9793" />
  <row Id="7753" PostId="8161" Score="0" Text="What do you think the 'model' should predict?&#xA;Getting some certain score by student in future?" CreationDate="2015-09-23T09:13:25.837" UserId="97" />
  <row Id="7754" PostId="8161" Score="0" Text="a score in range of 1-100, as a measure of student ranking" CreationDate="2015-09-23T09:38:28.623" UserId="5091" />
  <row Id="7755" PostId="8165" Score="0" Text="But the issue is there is no validation/test data to compare or measure error. Is there a way to build validation data ?" CreationDate="2015-09-23T09:41:02.073" UserId="5091" />
  <row Id="7756" PostId="8161" Score="0" Text="You want to _predict_ score change in time?" CreationDate="2015-09-23T09:58:52.930" UserId="97" />
  <row Id="7757" PostId="8165" Score="0" Text="You can't build this data. Collect it. And then devide into training and validation data sets." CreationDate="2015-09-23T10:10:09.120" UserId="97" />
  <row Id="7758" PostId="8165" Score="0" Text="I meant: take your existing training set, split it, keep one part as actual training and the other as validation. If you do cross-validation, you are sure to use all the data in either function at some point." CreationDate="2015-09-23T10:49:29.500" UserId="10936" />
  <row Id="7759" PostId="8165" Score="0" Text="score is not availabe to calculate accuracy.I need to predict 'score'." CreationDate="2015-09-23T11:07:45.197" UserId="5091" />
  <row Id="7760" PostId="8165" Score="0" Text="@ IharS yes, that is my question. How to make this validation/testing data ?" CreationDate="2015-09-23T11:08:58.953" UserId="5091" />
  <row Id="7761" PostId="8161" Score="0" Text="no, just give a score based student data. so i have the above mentioned attributes and based on that how can I build a model to predict student score ? also how to validate the score predicted by that model ?" CreationDate="2015-09-23T11:11:30.333" UserId="5091" />
  <row Id="7763" PostId="8152" Score="1" Text="@ Aleksander, thank you for your very helpful response. In fact, I'm thinking to have the following research goals: 1-Finding the relation and significance level of each independent variables with respect to dependent variable. 2– Obtaining the best possible model that could be able to estimate my independent variable 3- Studying each independent variables over the time and use some forecasting methods to predict them over the time horizon. Please let me know if you have any advice." CreationDate="2015-09-23T13:17:24.473" UserId="12867" />
  <row Id="7765" PostId="8165" Score="0" Text="look at my answer below" CreationDate="2015-09-23T14:19:38.090" UserId="97" />
  <row Id="7767" PostId="8121" Score="0" Text="@Chris  - sorry for the delay. There isn't some idealized rule for this, though some general guidelines exist. [This](http://stats.stackexchange.com/a/1097/30369) is the best example i've found for succinct rules to apply when building NNs and [this paper by geoffry hinton](https://www.cs.toronto.edu/~hinton/absps/guideTR.pdf) gives a much more complete view on the matter." CreationDate="2015-09-23T14:31:09.840" UserId="11098" />
  <row Id="7768" PostId="8156" Score="0" Text="Hi Sean, I'm learning Spark and working on different methodologies. SVM is one of them. But when I was looking for some reference about it, I could not find anything relevant. So I put the question here." CreationDate="2015-09-23T15:14:36.550" UserId="10810" />
  <row Id="7769" PostId="8160" Score="0" Text="Thanks Dawny! This link even though very brief, have been helpful." CreationDate="2015-09-23T15:15:13.317" UserId="10810" />
  <row Id="7770" PostId="8160" Score="0" Text="@Beta Glad it helped!" CreationDate="2015-09-23T15:17:31.377" UserId="11097" />
  <row Id="7771" PostId="6209" Score="0" Text="Is what programming language you want are trying to perform your combine transformation?" CreationDate="2015-09-23T16:00:27.553" UserId="5177" />
  <row Id="7772" PostId="8156" Score="0" Text="Have you looks for the Spark official documentation? It contains concrete example about how to use SVM. You might also want to look at the pyspark api doc!" CreationDate="2015-09-23T16:01:57.900" UserId="5177" />
  <row Id="7773" PostId="8173" Score="0" Text="How would the csv file look like?  A small example would be nice" CreationDate="2015-09-23T16:13:03.903" UserId="11097" />
  <row Id="7774" PostId="8173" Score="0" Text="You'll need to write a script to do that! What have you tried so far beside doing in manually?" CreationDate="2015-09-23T16:14:13.407" UserId="5177" />
  <row Id="7775" PostId="8172" Score="0" Text="well have you tried considering the age ranges upon those clothes, items?" CreationDate="2015-09-23T16:15:32.447" UserId="5177" />
  <row Id="7776" PostId="8173" Score="0" Text="name, company, country&#xA;john smith, abc, usa" CreationDate="2015-09-23T16:28:23.387" UserId="12949" />
  <row Id="7777" PostId="8172" Score="0" Text="Good point. But i only have the age ranges ( about 30%) of the people that by the clothes - so most likely the parents and not the age ranges of the kind that are where them." CreationDate="2015-09-23T16:32:32.830" UserId="12947" />
  <row Id="7778" PostId="8169" Score="0" Text="statistical significance does not imply causation either... but it doesn't appear that the user wants to make claims about the relationships rather predict. Tobias's answer hits the nail on the head." CreationDate="2015-09-23T16:46:23.480" UserId="9698" />
  <row Id="7779" PostId="8152" Score="0" Text="@Amir: You're very welcome. Sounds like a good plan. Good luck!" CreationDate="2015-09-23T16:52:53.650" UserId="2452" />
  <row Id="7780" PostId="8172" Score="0" Text="You don't have info about the product? Like product data descriptions?" CreationDate="2015-09-23T17:11:34.470" UserId="5177" />
  <row Id="7781" PostId="8173" Score="0" Text="What's that? If you want to add information to your question please edit it! Your question is not salvageable with the information given." CreationDate="2015-09-23T17:21:01.087" UserId="5177" />
  <row Id="7782" PostId="8172" Score="0" Text="Decision trees model, maybe?" CreationDate="2015-09-23T17:24:56.987" UserId="11097" />
  <row Id="7783" PostId="8015" Score="0" Text="Your question is not clear! What do you mean by &quot;if it is not multinomial, what changes are needed to make it so?&quot;" CreationDate="2015-09-23T17:33:15.290" UserId="5177" />
  <row Id="7784" PostId="6690" Score="0" Text="The only issue with recommenderlab is not memory hungry!" CreationDate="2015-09-23T17:34:38.403" UserId="5177" />
  <row Id="7785" PostId="8173" Score="1" Text="@eliasah I guess the OP wants to build a crawler for scraping the data into a csv.  The question needs some re-wording to be done!" CreationDate="2015-09-23T17:51:20.967" UserId="11097" />
  <row Id="7786" PostId="8156" Score="0" Text="Thanks Eliasah! Dawny actually suggested me the same." CreationDate="2015-09-23T18:02:41.637" UserId="10810" />
  <row Id="7787" PostId="8172" Score="0" Text="Ok, Decision tree. Would you find filling me in why that might be more appropriate is this situation" CreationDate="2015-09-23T18:07:11.693" UserId="12947" />
  <row Id="7788" PostId="8175" Score="0" Text="thanks i'm working on it now with your example scrapper" CreationDate="2015-09-23T18:38:03.593" UserId="12949" />
  <row Id="7789" PostId="8175" Score="1" Text="It's a working code (used it for a research project). But, I'd appreciate if you learn the logic from it and code the crawler on your own. It would be a wonderful learning experience." CreationDate="2015-09-23T18:44:09.677" UserId="11097" />
  <row Id="7791" PostId="5394" Score="1" Text="@ice_lin Hard to tell whether we need this part. If we just use `global_average` or `hot items` to recommend, we save a lot computation resource. But I still want to say, in industry, even a small changes in prediction accuracy would cause a lot!" CreationDate="2015-09-24T04:25:41.777" UserId="1048" />
  <row Id="7793" PostId="8170" Score="0" Text="Thanks for the answer, So in this case need to validate the weigtages used for each attributes in my scoring model in order to prove how effective this ranking model is. correct ? for that I don't have any valid dataset, all weightages are assigned based on intuition." CreationDate="2015-09-24T06:59:33.690" UserId="5091" />
  <row Id="7794" PostId="5996" Score="0" Text="I think you need to expand on what &quot;maximise performance in joins&quot; means to you. Joining what?" CreationDate="2015-09-24T07:23:14.827" UserId="471" />
  <row Id="7795" PostId="8170" Score="0" Text="First of all - define what is _effective_. Once again: You _have_ data set. A set of students and their data. You can devide it into _training_ and _validation_ sets. But which model are you going to _train_? What do you want to _predict_ ?" CreationDate="2015-09-24T08:23:54.377" UserId="97" />
  <row Id="7796" PostId="8170" Score="0" Text="You can _tune_ your weights, model it. But target variable should be defined: for best weights selection the model must know which parameter it optimises." CreationDate="2015-09-24T08:30:25.103" UserId="97" />
  <row Id="7798" PostId="8174" Score="0" Text="Thanks for the reply. Is there anyway to know what their actual value of observed distance and expected distances are? I am using KNN algorithm for a classification problem in  text mining project and i want to know find the keywords influencing  the classification. It wil be helpful if you can let me know how to retrive the absolute values of these distances. Thank you." CreationDate="2015-09-24T08:53:07.473" UserId="9793" />
  <row Id="7799" PostId="8174" Score="0" Text="For that, you use `nn.dist` instead of `nn.index `.  However, the documentation of the `fnn` package would help you understand better  (https://cran.r-project.org/web/packages/FNN/FNN.pdf)" CreationDate="2015-09-24T08:56:11.313" UserId="11097" />
  <row Id="7800" PostId="8173" Score="0" Text="@SeanOwen, I have made an edit, rewording the question to make it clear. Please review and accept." CreationDate="2015-09-24T11:41:16.967" UserId="11097" />
  <row Id="7801" PostId="8015" Score="0" Text="What differences are there, if any, in the above formulas between multinomial and multivariate Naive Bayes?" CreationDate="2015-09-24T13:11:07.930" UserId="8650" />
  <row Id="7802" PostId="8015" Score="1" Text="Multinomial is used for discrete variables and the multivariate normal is used for continuous variable!" CreationDate="2015-09-24T13:16:42.027" UserId="5177" />
  <row Id="7803" PostId="8178" Score="0" Text="This is insightful and I will try if I can use this for what I intend to do. However, I was looking at a recursive and nested function. The above data was used as an example. My actual formula is a little more complicated and `N` is dynamically generated and recursively applied.   I am trying to `bootstrap (finance)` a zero coupon yield curve from fed data." CreationDate="2015-09-24T14:49:51.590" UserId="10345" />
  <row Id="7804" PostId="8188" Score="1" Text="There are a lot of nuances in choosing an aggregation method, such as whether (corpus) normalization has been performed and how.  So much so, that it will be difficult for people to answer this without a specific reference to where you have seen summing employed.  Could you please add a reference to where you are seeing this so we can see the entire featurization, normalization, and aggregation process?  Thanks!" CreationDate="2015-09-24T16:20:43.277" UserId="9420" />
  <row Id="7805" PostId="8015" Score="0" Text="So given the formulas, which are they?" CreationDate="2015-09-24T17:22:09.447" UserId="8650" />
  <row Id="7806" PostId="8190" Score="0" Text="You should not cross-post. As you have already asked in CV, either you can delete it there and let it remain here, else delete this." CreationDate="2015-09-24T17:33:22.063" UserId="11097" />
  <row Id="7807" PostId="8190" Score="0" Text="@Dawny33 got it. deleted the CV question." CreationDate="2015-09-24T17:48:31.193" UserId="3592" />
  <row Id="7808" PostId="8190" Score="0" Text="If no one jumps on with a 3rd order or Nth order implementation, I would suggest that you just feed the 2nd order factors and 1st order back into Vowpal Wabbit's --lrq which will create all of the 3rd order pairings.  You can repeat this ad infinitum to get an Nth order FM." CreationDate="2015-09-24T18:43:57.163" UserId="9420" />
  <row Id="7809" PostId="8190" Score="0" Text="@AN6U5 I'm not sure I am following. Are you proposing to do this in several learning steps? How else would you obtain these &quot;2nd order&quot; factors? the second order factors are the result of the learning algorithm.. can you give an example? Suppose you have three namespaces A,B,C, each with 1000 different feature values. 3-way FM would learn a length k vector for each of the features (3*1000*k parameters). How would you get this otherwise?" CreationDate="2015-09-24T18:48:29.743" UserId="3592" />
  <row Id="7810" PostId="8187" Score="0" Text="- Thanks. I was able to successfully create the object as an ASCII file. However, filtering the records, always lead to the below error  `comparison (1) is possible only for atomic and list types`.I have tried both index positions and `subset()` without much ado. I also tried `input &lt;- subset(source_input, source_input$Record.ID ==&quot;Cast&quot;)`" CreationDate="2015-09-24T20:00:13.317" UserId="10345" />
  <row Id="7811" PostId="8181" Score="0" Text="I can see how repeating the sentence might do that, but my question is specifically about repeating one of the words in the sentence." CreationDate="2015-09-24T20:02:54.110" UserId="12306" />
  <row Id="7812" PostId="695" Score="0" Text="Not sure if it's the same as Marks comment about nolearn, but https://github.com/aigamedev/scikit-neuralnetwork is also some form of a wrapper for a bunch of this stuff." CreationDate="2015-09-24T20:07:52.253" UserId="12980" />
  <row Id="7813" PostId="8195" Score="1" Text="Please don't cross post (http://stats.stackexchange.com/questions/173968/difference-between-training-and-test-data-distribution)" CreationDate="2015-09-25T03:16:47.350" UserId="11097" />
  <row Id="7814" PostId="8195" Score="0" Text="@Dawny33: It seems this question is more relevant to this site than cross validated. That's why I posted here." CreationDate="2015-09-25T03:52:54.527" UserId="12983" />
  <row Id="7815" PostId="8200" Score="0" Text="Thanks. A link to related question here : http://stackoverflow.com/questions/18050891/bin-mallet-train-topics-getting-different-results-at-every-instance?rq=1" CreationDate="2015-09-25T10:05:11.087" UserId="9296" />
  <row Id="7816" PostId="8198" Score="0" Text="So, just that I understand well : 1) It's random because of Gibbs sampling. 2) By fixing the seed I can reproduce my results (yes purpose is scientific) 3) While I can reproduce my results the scientific issue is that there were no particular reason to favor this seed over another and thus for my results to be reliable they should not show too much variability across various seeds." CreationDate="2015-09-25T10:08:11.700" UserId="9296" />
  <row Id="7818" PostId="6078" Score="0" Text="Topic modeling might be a way to learn a representation of your text before applying supervised learning.  However you might be able to use something as simple as tf-idf vectors" CreationDate="2015-09-25T14:23:50.210" UserId="12306" />
  <row Id="7819" PostId="8197" Score="0" Text="Thanks a lot, your answer is comprehensive. As long as I want to take industrial position after PhD, can you please specify where can I find a research group or something?" CreationDate="2015-09-25T15:30:27.103" UserId="10694" />
  <row Id="7820" PostId="8197" Score="0" Text="Hmm can't you research that yourself... how about finding at machine learning conferences and looking up speakers doing things that interest you. Even taking an industrial position being embedded in that academic environment provides great training." CreationDate="2015-09-25T15:34:41.533" UserId="12987" />
  <row Id="7821" PostId="8203" Score="1" Text="I have voted to close the question as opinion-based (because what is good for one person is not good for another, it depends what you already know and how you learn), but you could try Coursera course: https://www.coursera.org/learn/machine-learning - there is a new group session starting in 2 weeks. I have done that course and enjoyed it." CreationDate="2015-09-25T15:45:45.893" UserId="836" />
  <row Id="7822" PostId="8203" Score="0" Text="Thank you man! ok, I understand.!" CreationDate="2015-09-25T17:22:07.547" UserId="12996" />
  <row Id="7823" PostId="8205" Score="1" Text="If I understand correctly, you should look up &quot;(hierarchical) topic modeling&quot; and &quot;ontology/taxonomy induction&quot;; see for example [Topic Models for Taxonomies](http://dirichlet.net/pdf/bakalov12topic.pdf)." CreationDate="2015-09-25T17:58:56.093" UserId="381" />
  <row Id="7824" PostId="8197" Score="0" Text="The reason is I am unlikely to have enough strength for carrying out research alone. I need some initial point and possibly some help through scientific work. Is it a good idea to ask authors by emails for help and coauthoring?" CreationDate="2015-09-25T18:13:22.823" UserId="10694" />
  <row Id="7825" PostId="8197" Score="0" Text="To be honest that is unlikely to work." CreationDate="2015-09-25T18:16:23.193" UserId="12987" />
  <row Id="7826" PostId="6598" Score="1" Text="[Supervised Topic Models](http://papers.nips.cc/paper/3328-supervised-topic-models)" CreationDate="2015-09-25T19:43:14.563" UserId="381" />
  <row Id="7827" PostId="8177" Score="0" Text="Hi Wilson, thank you very much for your answer! Could you tell me how to sample with replacement? I apologize for not stating this clearly in the original thread." CreationDate="2015-09-25T22:46:00.137" UserId="12931" />
  <row Id="7828" PostId="916" Score="0" Text="I'm not sure I agree with this: when designing an algorithm for a statistical problem, a *lot* of concern goes into the complexity of each iterative step (and is usually documented in a manuscript). But as you point out, often it's not that easy to summarize, as two algorithms with the same complexity per iteration may perform very differently due to necessary iterations. That being said, it's very rare that the number of iterations required grows faster than `O(log(n) )`." CreationDate="2015-09-26T02:12:27.850" UserId="13005" />
  <row Id="7829" PostId="8177" Score="1" Text="To sample with replacement you call `sample` with the argument `replace=TRUE`." CreationDate="2015-09-26T08:29:04.873" UserId="12960" />
  <row Id="7830" PostId="8212" Score="0" Text="Thanks @IharS , but actually I'm rating riders valuing their whole career, so there's no current level or future level for this kind of riders. What I want to do is the following:&#xA;1. Train the known riders' data, and predict less known riders' level taking into account their results.&#xA;2. Predict young riders' future level (in this case I should take into account the results the known riders achieved when they were young, train this data, and predict the level of currently young riders).&#xA;On the other hand, I'm looking for the level in each skill, not an overall level." CreationDate="2015-09-26T11:10:09.933" UserId="13000" />
  <row Id="7831" PostId="8212" Score="0" Text="So still you are talking about Rider's class change in time." CreationDate="2015-09-26T11:16:40.717" UserId="97" />
  <row Id="7833" PostId="8171" Score="0" Text="by the way, how many data should one take which be averaged?" CreationDate="2015-09-26T13:53:00.583" UserId="8013" />
  <row Id="7834" PostId="8212" Score="0" Text="Not exactly, I think. I mean, what I want to predict is the level some riders will have, let's say, as a result of their whole career. So I don't want to infer their level at their very early years as a professional, then their improvement, and so on. I would like to take their results as young riders, and deduce what level they could reach, but not a changing level. And there's also the other part, where I have a rider in his maturity, with his results in different races, but without being classified yet." CreationDate="2015-09-26T15:55:57.910" UserId="13000" />
  <row Id="7835" PostId="8204" Score="0" Text="can you provide me the Edx course link? you duplicated the coursera link.thanks" CreationDate="2015-09-27T00:42:13.510" UserId="12996" />
  <row Id="7836" PostId="8204" Score="0" Text="https://www.edx.org/course/data-science-machine-learning-essentials-microsoft-dat203x" CreationDate="2015-09-27T00:43:57.413" UserId="12952" />
  <row Id="7837" PostId="8138" Score="0" Text="@usεr11852 The difficulties with LSA were casued by: 1) problems with ignoring short terms - the parameter wordLengths 2) non-transformed IDF application. I added an answer approaching both problems." CreationDate="2015-09-27T09:48:31.660" UserId="10620" />
  <row Id="7838" PostId="8148" Score="0" Text="I added an answer based on transformed IDF to distinct between the parameters and template terms. As this is my first usage of transformed IDF, I'd appreciate commenting of the approach." CreationDate="2015-09-27T09:55:21.533" UserId="10620" />
  <row Id="7840" PostId="8214" Score="0" Text="Thank you. I added Computational Biology and Quantitative Genetics to my list. Your post also reminded me of computational social science." CreationDate="2015-09-27T14:27:33.957" UserId="3151" />
  <row Id="7841" PostId="5826" Score="0" Text="One possible scenario for you is to work on big data ethics, contracts and legal issues. This way you can learn about data science and use your law skills. This expertise is very rare and you will get a competitive advantage in the market." CreationDate="2015-09-27T15:20:19.327" UserId="3151" />
  <row Id="7844" PostId="8171" Score="0" Text="This is just personal experience (= no science behind it), but I'd say 20-30  (if it's not too time-consuming to run) should cut it." CreationDate="2015-09-28T11:56:21.553" UserId="10936" />
  <row Id="7845" PostId="8148" Score="1" Text="I am glad i could help." CreationDate="2015-09-28T17:03:12.127" UserId="12905" />
  <row Id="7847" PostId="8207" Score="0" Text="The only method I know of that handles 'many-to-one' classification (e.g. each observation has multiple rows of data) is recurrent neural networks and they are sadly pretty difficult to use (http://karpathy.github.io/2015/05/21/rnn-effectiveness/). Likely some kind of feature engineering is needed to create a single row of data for each rider." CreationDate="2015-09-28T19:12:16.657" UserId="3044" />
  <row Id="7851" PostId="8224" Score="0" Text="i am limited to 100 samples, cannot generate anymore...." CreationDate="2015-09-29T03:49:59.177" UserId="8013" />
  <row Id="7852" PostId="8227" Score="0" Text="not as a query in a database. Using text-mining tools might be an option. these are optimized for these kind of tasks." CreationDate="2015-09-29T06:45:43.660" UserId="10517" />
  <row Id="7857" PostId="8230" Score="0" Text="&quot;How to implement PageRank&quot; is pretty broad, since you could want help at many levels. Narrow it down?" CreationDate="2015-09-29T10:17:41.237" UserId="21" />
  <row Id="7858" PostId="8233" Score="0" Text="can you give a link to a python library for the same" CreationDate="2015-09-29T13:03:30.947" UserId="13011" />
  <row Id="7859" PostId="8233" Score="0" Text="Scikit-learn has [a few NN-based algorithms](http://scikit-learn.org/stable/modules/neighbors.html) but if you want something working with your big amount of data you might have to build your own solution from scratch. Thankfully kNN classification is very easy to understand. [This presentation](http://www.math.le.ac.uk/people/ag153/homepage/KNN/OliverKNN_Presentation.pdf) outlines the great lines of K-NN and also of C-NN, condensed nearest neighbours, which might help with your dataset size issue." CreationDate="2015-09-29T13:17:11.053" UserId="12909" />
  <row Id="7860" PostId="6771" Score="0" Text="Well, I'm using ``find . -type f -print0 | xargs -0 md5sum &gt; checksums.md5`` to calculate the checksums and ``md5sum -c checksums.md5`` to checksums, and version control the checksums. That helps to check the data at different locations/on different machines. Seems to be the best we can do at the moment," CreationDate="2015-09-29T21:30:20.153" UserId="8320" />
  <row Id="7861" PostId="8237" Score="0" Text="Thank you IharS for your quick reply. Can you please describe me the whole process from the begining to the end.  Thank you" CreationDate="2015-09-29T21:54:55.077" UserId="8088" />
  <row Id="7862" PostId="8237" Score="0" Text="How to create a data base? Any relational will be OK. Create table? Write to database? Man... that's a basic stuff... Google MySql FAQ" CreationDate="2015-09-29T22:02:33.103" UserId="97" />
  <row Id="7863" PostId="8237" Score="0" Text="No I mean why you think that aggregation is the solution." CreationDate="2015-09-29T22:09:11.993" UserId="8088" />
  <row Id="7866" PostId="8228" Score="0" Text="Really awesome answer, I didnt expect it, I thought my udacity course on ML would be sufficient to solve this problem ! Do you know any good video/blog that gives a good introduction to survival model ? Sorry I'm noob to stats and ML." CreationDate="2015-09-30T07:31:09.047" UserId="13056" />
  <row Id="7867" PostId="8232" Score="0" Text="i will average the class probability. That seems the best idea" CreationDate="2015-09-30T07:35:02.100" UserId="8013" />
  <row Id="7868" PostId="8240" Score="0" Text="Machine learning on algorithm-generated data sounds wrong. For basic functionality I would pursue the regular expressions approach instead." CreationDate="2015-09-30T08:05:09.623" UserId="6550" />
  <row Id="7869" PostId="8240" Score="0" Text="I have difficulties in getting your question, can you be more explicit?" CreationDate="2015-09-30T08:05:30.920" UserId="10169" />
  <row Id="7870" PostId="8237" Score="0" Text="@IharS I don't think a RDBMS is the best solution. It's a tracking profile so data will only ever be added, never changed or deleted. It might be one of the rare use cases where I would advocate for some trendy noSQL database." CreationDate="2015-09-30T08:34:57.950" UserId="12909" />
  <row Id="7871" PostId="8236" Score="0" Text="You need to give more information that this if you want useful feedback. Used by the production site to do what? The use case of that data will guide how you design and implement your user profile." CreationDate="2015-09-30T08:36:11.140" UserId="12909" />
  <row Id="7872" PostId="8237" Score="0" Text="@JérémieClos thank you for your reply, so do yu think that I use hbase and Pig to do this?" CreationDate="2015-09-30T09:32:31.543" UserId="8088" />
  <row Id="7873" PostId="8239" Score="0" Text="yes I had tried to eliminate 80% of the data which had the same category. Still huge size , does it even save the vectorizers for future transforms?" CreationDate="2015-09-30T09:36:42.343" UserId="13011" />
  <row Id="7874" PostId="8237" Score="0" Text="@user17241 I don't have much experience with those two tools but from what I am reading they should do the job." CreationDate="2015-09-30T10:20:55.647" UserId="12909" />
  <row Id="7875" PostId="8240" Score="0" Text="@Valentas I'm under the impression that happens all the time with vision related machine learning. Slight alterations are made to the training data to inflate their numbers." CreationDate="2015-09-30T11:03:57.353" UserId="12321" />
  <row Id="7876" PostId="8241" Score="0" Text="I'm sorry, I tried to be too generic it seems. But you understood me nonetheless mostly right. I'll look into this part-of-speech." CreationDate="2015-09-30T11:06:36.953" UserId="12321" />
  <row Id="7877" PostId="8243" Score="0" Text="Thank you Jeremy" CreationDate="2015-09-30T11:35:52.177" UserId="8088" />
  <row Id="7878" PostId="8228" Score="0" Text="You could take a look at https://cran.r-project.org/web/views/Survival.html" CreationDate="2015-09-30T12:51:37.637" UserId="12384" />
  <row Id="7879" PostId="8246" Score="1" Text="[This paper](http://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf), while long, builds up the CGD from simple linear algebra." CreationDate="2015-09-30T13:11:23.730" UserId="12909" />
  <row Id="7880" PostId="8227" Score="0" Text="Use Named Entity Resolution method." CreationDate="2015-09-30T16:05:40.570" UserId="7848" />
  <row Id="7881" PostId="8246" Score="0" Text="The [wikipedia article](https://en.wikipedia.org/wiki/Conjugate_gradient_method) actually does a pretty good job of illustrating the difference between `conjugate gradient method` and the [`gradient descent`](https://en.wikipedia.org/wiki/Gradient_descent) method and even approaches conjugate gradient from the perspective of both a `direct solve` and an `iterative solve`.  If you don't like math, then Andrew Ng does a pretty good job of describing both with essentially no math in his [Coursera Machine Learning Course](https://www.coursera.org/learn/machine-learning)" CreationDate="2015-09-30T16:36:13.840" UserId="9420" />
  <row Id="7882" PostId="8227" Score="0" Text="Thanks for the recommendation @xeon. I found http://opennlp.apache.org/ that can perform such NER methods." CreationDate="2015-09-30T18:42:19.737" UserId="13057" />
  <row Id="7883" PostId="8234" Score="0" Text="I'll give this a try. Thanks!" CreationDate="2015-09-30T18:42:51.823" UserId="13057" />
  <row Id="7884" PostId="8227" Score="0" Text="@Collarbone I am building lots of custom NER models myself. I can detail you every step such that you can learn as well :)" CreationDate="2015-09-30T23:19:14.200" UserId="7848" />
  <row Id="7885" PostId="8246" Score="0" Text="If I have a curve, like a bell say, then how will one apply conjugate gradient? I went through the wiki, did not understand. I wanted a pictorial description" CreationDate="2015-10-01T03:37:49.903" UserId="8013" />
  <row Id="7886" PostId="8251" Score="0" Text="Well , this is  a good  point, I will take  a look , tnks!" CreationDate="2015-10-01T06:07:56.023" UserId="10094" />
  <row Id="7887" PostId="8252" Score="1" Text="That is an awesome answer.  Beautifully explained.  Thank you." CreationDate="2015-10-01T06:55:21.370" UserId="11097" />
  <row Id="7888" PostId="6771" Score="0" Text="If by modifying your data, you always change its file name, then it might be good solution. Otherwise, I would highly recommend to check on the data itself, for example with `rsync` on (a copy of) the original data. One other possibility which is common in neuroscience (although I do not like it so much because sometimes it is not as well documented as it should be), is to use the nipype python package, which can be seen as a (sort of) workflow manager and it manages the cache of binary data of the intermediate steps of the analysis automatically." CreationDate="2015-10-01T09:11:22.243" UserId="12215" />
  <row Id="7889" PostId="6959" Score="0" Text="Your questions are legitimate but you need to put more effort in formulating them. What are you looking for, an expert user of the rpart library or an explanation on how to explain classifications in decision trees? The more effort you put in your question, the better the answers will be, and the more value it will create for the website." CreationDate="2015-10-01T10:57:18.880" UserId="12909" />
  <row Id="7890" PostId="8254" Score="0" Text="Thanks, I'll check it out. Actually, the 0, 1, and 2 are the index. Also, do you have any idea how sparseness can be handled efficiently here as there are lots of zeroes?" CreationDate="2015-10-01T10:58:35.563" UserId="13027" />
  <row Id="7891" PostId="8254" Score="0" Text="Both pandas and scipy have sparse data structures ([pandas sparse](http://pandas.pydata.org/pandas-docs/stable/sparse.html), [scipy sparse](http://docs.scipy.org/doc/scipy/reference/sparse.html)) for saving memory, but they might not be supported by the machine learning library you use. If the dimensionality of your problem (number of columns) is so large that sparse representation is necessary, you may want to consider also using [dimensionality reduction techniques](http://scikit-learn.org/stable/modules/unsupervised_reduction.html)." CreationDate="2015-10-01T11:41:32.237" UserId="13103" />
  <row Id="7892" PostId="8259" Score="0" Text="Hi @AN6U5 thanks for the answer. &quot;With some time history you can go back in time and definitively label the current customers as churn=&quot;No&quot;.&quot;&#xA;I have time history of each record - date they signed up and then date they cancelled, if they cancelled. So what are you suggesting there? Should I test and train on subscriptions from a certain time frame? Our database goes back several years. But that would not be ideal since the variable &quot;months subscription&quot; would be much higher for these older accounts" CreationDate="2015-10-01T14:32:21.837" UserId="13106" />
  <row Id="7893" PostId="8259" Score="0" Text="Having done some more research I'm going to delete and re-post this question. Thanks for your answer nonetheless" CreationDate="2015-10-01T15:12:52.307" UserId="13106" />
  <row Id="7895" PostId="8266" Score="0" Text="Is there anything specific that you need? For known break points, this can just be modeled by an interaction with a indicator function (0 before, 1 after the break) or a linear spline. The first approach has a jump, the second approach results in a connected piecewise regression line." CreationDate="2015-10-02T04:28:36.700" UserId="13131" />
  <row Id="7896" PostId="8202" Score="0" Text="Have you tried to using the Z scale instead?" CreationDate="2015-10-02T05:58:40.900" UserId="5177" />
  <row Id="7897" PostId="8240" Score="0" Text="Could you provide a representative sample of the data with all of the known series types you currently have , then perhaps people could help you define a detection method or pattern matching algorithm ?" CreationDate="2015-10-02T05:59:46.640" UserId="7980" />
  <row Id="7898" PostId="132" Score="1" Text="@ragingSloth, I think the first one is definitely feature selection - and not feature engineering. While image and text processing examples indeed seem to be feature engineering" CreationDate="2015-10-02T07:26:54.503" UserId="816" />
  <row Id="7899" PostId="8268" Score="0" Text="Do you have any ideas to start the discussion?" CreationDate="2015-10-02T15:03:57.770" UserId="471" />
  <row Id="7900" PostId="8252" Score="1" Text="This is a very well written response, and the diagrams are very helpful. The references are also appreciated." CreationDate="2015-10-03T04:40:40.680" UserId="10663" />
  <row Id="7901" PostId="8280" Score="0" Text="This has given me a starting point, thanks for the link  I am looking into it. As for the four features - source, destination, volume and frequency of transactions, do they relate to LOP or they are other attributes I would consider for signalling a fraudulent transaction? Please, may you explain on the additional features." CreationDate="2015-10-03T06:42:38.273" UserId="13132" />
  <row Id="7902" PostId="8283" Score="0" Text="Well, I am going for the suggested option. Two things, you mentioned about finding similar users - it is sounding like I must have my users into distinct clusters. Do I clusters them based on the range of money that they withdraw - like `$0 - $200, $200 -$300, etc`? And, data aggregation, may you elaborate a little here, I am lost." CreationDate="2015-10-03T07:25:06.720" UserId="13132" />
  <row Id="7903" PostId="8289" Score="0" Text="Well, that is a good explanation indeed. I had missed two attributes, time between two withdrawals and seconds spent on the ATM for each withdrawal. As for the first feature, I am still finding some way of grouping my ATMs - Say in a town `A` I have ATMs `A1, A2, A3,..,An`. Now suppose a user regularly withdraws his money from ATM `A1` - &quot;that is his behavior&quot;, then if that user were to withdraw money from ATM `A7`, I would allocate a high score of suspicion to that transaction. The challenge is on how to group the ATMs or is the approach  wise?" CreationDate="2015-10-03T10:38:07.707" UserId="13132" />
  <row Id="7904" PostId="8289" Score="0" Text="The time between two withdrawals: Let's say that I made one on Thursday and one on Saturday. I had about 48 hours between the two of them. The time spend on the ATM. The seconds that I needed from the time I inserted my card to the time I took it back. For example, I needed 35 seconds to fill all the details on the ATM. As for grouping, it depends of what details you have. If you have lat-long, city, unique code etc." CreationDate="2015-10-03T10:46:31.163" UserId="201" />
  <row Id="7905" PostId="8289" Score="0" Text="Now I am following, and thanks for quick response. As for grouping , I was thinking to consider city and a unique code (each ATM has a unique code that helps to identify it). I was thinking to &quot;hard limit&quot; the ATMs, say for a city I could have a map showing all the ATMs, then I divide the map say in chunks of some area. Then, these chunks become my classes of ATMs, but its not sounding quite intelligent! How would I go about it?" CreationDate="2015-10-03T11:07:55.940" UserId="13132" />
  <row Id="7906" PostId="8289" Score="0" Text="@Giovanrich If you could find the lat and long of ATMs, I would probably made classes from a radius of 50km or 100km close to those coordinates." CreationDate="2015-10-03T11:41:44.693" UserId="201" />
  <row Id="7907" PostId="2269" Score="0" Text="Here you have good options for R Programming: http://www.tutorialspoint.com/execute_r_online.php" CreationDate="2015-09-25T10:33:23.480" UserDisplayName="user12990" />
  <row Id="7908" PostId="8289" Score="0" Text="Let me work on that first and see if I can get hold of those coordinates, I will get back to you thanks." CreationDate="2015-10-03T12:04:18.037" UserId="13132" />
  <row Id="7909" PostId="8289" Score="0" Text="@Giovanrich Keep in your mind this answer about finding ATM's coordinates http://opendata.stackexchange.com/q/5640/505" CreationDate="2015-10-03T12:16:31.890" UserId="201" />
  <row Id="7910" PostId="8268" Score="0" Text="I edited my question. Take a look." CreationDate="2015-10-03T15:48:24.337" UserId="3151" />
  <row Id="7911" PostId="8295" Score="0" Text="Is there any way to find out what features should be clubbed together, because in my case all the features are more or less equally important.. ?" CreationDate="2015-10-03T19:34:09.227" UserId="13155" />
  <row Id="7912" PostId="8280" Score="0" Text="source, destination and frequency of transactions would be separate features. Imagine if a transaction went to a destination account that it has never gone to before, that indicate a fraudulent transaction" CreationDate="2015-10-03T22:37:32.237" UserId="426" />
  <row Id="7913" PostId="8295" Score="0" Text="Try the ones with  small frequency!" CreationDate="2015-10-04T05:46:39.520" UserId="10094" />
  <row Id="7914" PostId="6693" Score="0" Text="Can you be more explicit about what kind of language and framework you are using to get the memory issue?" CreationDate="2015-10-04T09:56:09.183" UserId="5177" />
  <row Id="7915" PostId="8290" Score="0" Text="Disagree. Statistics as an independent discipline is a boring, dull, theoretical enterprise (say my students). An applied &quot;Statistics for Environmental Science&quot; degree is much more popular (say my application counts). Similarly for DS. I posit that &quot;DS for Domain Y&quot; courses would be more popular than &quot;DS&quot; courses. Skills transfers are still possible, as with stats, but most students have an idea of the field they want to work in at their age." CreationDate="2015-10-04T12:29:35.067" UserId="471" />
  <row Id="7916" PostId="8289" Score="0" Text="Forget about &quot;Seconds spent on ATM&quot;: Keep in mind that inside ATM banks usually have plain old PC connected to network (VPN). Now, one can imagine all sorts of possible hiccups, delays etc. Also, keep in mind that ATM has to connect to processing center and check available balance. Where is that center located, and how busy is in that moment can significantly affect response time. And finally, not all ATM have the same interface: for some models it takes 7 'clicks' to get money, while some models requires 10 'clicks'." CreationDate="2015-10-04T12:58:32.703" UserId="13173" />
  <row Id="7917" PostId="8303" Score="1" Text="It's usually a vector model associated to a label." CreationDate="2015-10-04T16:51:02.523" UserId="5177" />
  <row Id="7918" PostId="8301" Score="2" Text="You have to identify good features yourself. We don't have your data." CreationDate="2015-10-04T20:53:34.923" UserId="924" />
  <row Id="7919" PostId="8309" Score="0" Text="You could try _Microsoft Excel_ (if you have it installed on your computer) or similar open source software, such as _OpenOffice/LibreOffice Calc_ or _Gnumeric_: https://en.wikipedia.org/wiki/Comparison_of_spreadsheet_software." CreationDate="2015-10-05T02:47:09.887" UserId="2452" />
  <row Id="7920" PostId="6681" Score="0" Text="Can you be more specific on what you mean by churn? In financial services, churning means a broker making many transactions  in a short period of time for the purpose of collecting additional fees from a client. However, you seem to be talking about something all together different, like a user getting cold feet and making less (not more) transactions." CreationDate="2015-10-05T03:12:28.480" UserId="13184" />
  <row Id="7922" PostId="8296" Score="0" Text="Thanks for the response and well explain answer. In this case I would have to develop a data base that resemble one a bank would use. Also, the scope of my system considers a credit card being used on ATM only, not in a merchant shop or web but on an ATM." CreationDate="2015-10-05T10:41:35.530" UserId="13132" />
  <row Id="7923" PostId="8309" Score="2" Text="*sigh* what is &quot;very large&quot; (bytes, fields per line, lines, columns)? This &quot;multiple lines&quot; thing, are you sure its not just that the data **really is** on multiple lines in the CSV?" CreationDate="2015-10-05T14:20:35.860" UserId="471" />
  <row Id="7924" PostId="8312" Score="0" Text="An awk script to sample 1 in N of the lines either at random or in order is pretty trivial." CreationDate="2015-10-05T14:24:07.110" UserId="471" />
  <row Id="7925" PostId="8227" Score="0" Text="@xeon That'd be great. Feel free to reach out to me via email. My email is on my website http://collarbone.com. :)" CreationDate="2015-10-05T16:40:53.620" UserId="13057" />
  <row Id="7926" PostId="8227" Score="0" Text="@Collarbone I just sent you an email." CreationDate="2015-10-05T17:16:56.530" UserId="7848" />
  <row Id="7927" PostId="8314" Score="0" Text="This is exactly what I was looking for. Thank you." CreationDate="2015-10-05T17:17:12.090" UserId="13168" />
  <row Id="7928" PostId="8318" Score="3" Text="What is your question? If it is &quot;How to apply Kohenen Self Organizing Maps  to do fraud detection?&quot;, then that does seem very broad, and could take a very long time to write an answer (i.e. probably no-one will bother). Could you try to narrow this down by explaining more about what you know already, and focus on just your first problem on understanding what to do next?" CreationDate="2015-10-05T18:52:54.700" UserId="836" />
  <row Id="7929" PostId="8312" Score="0" Text="I'd rather &quot;less -S filename.csv&quot;, as suggested here http://superuser.com/questions/272818/how-to-turn-off-word-wrap-in-less." CreationDate="2015-10-05T19:17:08.027" UserId="6550" />
  <row Id="7930" PostId="8292" Score="0" Text="Well, this is the approach I been looking for; are there alternatives to this method?" CreationDate="2015-10-06T07:20:16.363" UserId="13132" />
  <row Id="7931" PostId="8292" Score="0" Text="There seem to exist quite some **fraud detection** methods which use Neural Nets, Logistic Regression, etc.  I have included the links in the other answer (http://datascience.stackexchange.com/questions/8099/classifying-transactions-as-malicious/8100#8100)  You might want to have a look at it too." CreationDate="2015-10-06T07:23:30.027" UserId="11097" />
  <row Id="7932" PostId="8292" Score="0" Text="Yes, as for fraud detection methods I am following the link. I was asking alternatives approaches to Mahalanobis Distance?" CreationDate="2015-10-06T07:27:59.557" UserId="13132" />
  <row Id="7933" PostId="8292" Score="0" Text="There are quite a lot of methods, which can't fit/explained in a comment.  But, it definitely qualifies as a seperate question." CreationDate="2015-10-06T07:29:46.273" UserId="11097" />
  <row Id="7934" PostId="8312" Score="0" Text="@Spacedman not everybody is familiar with awk though. You should probably add it as an answer so that this question covers a wider array of possible ways of exploring data." CreationDate="2015-10-06T10:34:13.763" UserId="12909" />
  <row Id="7935" PostId="8312" Score="0" Text="I'll wait until the OP has engaged with us - given the current four answers have such wide breadth and the lack of OP comments I do not see the point of further activity at this time." CreationDate="2015-10-06T12:12:03.643" UserId="471" />
  <row Id="7936" PostId="6598" Score="0" Text="@Emre: Interesting paper. Unlike unsupervised LDA, supervised LDA hasn't become popular, and even without trying I have some ideas why this is so." CreationDate="2015-10-06T12:49:40.180" UserId="10169" />
  <row Id="7937" PostId="8293" Score="0" Text="Could you describe a bit more on the type of model you are trying to fit? What kind of dummies?" CreationDate="2015-10-06T13:18:46.770" UserId="12384" />
  <row Id="7938" PostId="8326" Score="0" Text="Thanks... you said that,  first,  pairwise correlation between target variable and a feature is examined... but what if target variable is categorical ( classification into one of the categories) ... in that case how to use correlation score" CreationDate="2015-10-06T13:43:46.720" UserId="13155" />
  <row Id="7940" PostId="8324" Score="0" Text="Do you know any good resource on this topic ? Also how do we know that we have to use mean or median ?" CreationDate="2015-10-06T13:57:34.547" UserId="13155" />
  <row Id="7941" PostId="8324" Score="0" Text="Yould read &quot;Missing Values Analysis and Data Imputation&quot; by David Garson for a basic introduction. Beyond the basics, everything gets very model-specific. Maybe you could add some details about what you want to do with the data. Also don't get too caught up with mean vs. median -- these are just two very simple examples of a broad range of possibilities, including those that use data from other features as well, not just the one with the missing data. Also the decision of mean vs. median will depend very much on your specific problem." CreationDate="2015-10-06T14:18:10.777" UserId="13213" />
  <row Id="7943" PostId="8322" Score="0" Text="In addition to the answers below, the correct way of imputation depends on how you intend to use it.  Is it a target variable, an explanatory variable, or a classification variable?" CreationDate="2015-10-06T16:57:19.757" UserId="8005" />
  <row Id="7947" PostId="8340" Score="2" Text="github &amp; linkedin" CreationDate="2015-10-07T04:21:30.027" UserId="947" />
  <row Id="7948" PostId="8326" Score="0" Text="If either of your feature or target is a categorical variable (also called nominal variable), then we don't find 'Correlation' between them, since correlation implies when one variable increases, an increase/decrease in the other is observed. Categorical variable doesn't possess numerical values, thus increase or decrease doesn't makes sense. We find _association_ when either of your variable is categorical." CreationDate="2015-10-07T06:21:17.293" UserId="10904" />
  <row Id="7949" PostId="8342" Score="1" Text="I think number 3 is really good, because it let's you investigate anything you'd like.  It also forces you to work on storytelling &amp; design, aspects of good data science that are often overlooked." CreationDate="2015-10-07T06:22:47.873" UserId="8041" />
  <row Id="7950" PostId="8322" Score="0" Text="This is a statistics question - you'll get better response on the statistics stackexchange site: http://stats.stackexchange.com/search?q=missing+values" CreationDate="2015-10-07T06:55:37.770" UserId="471" />
  <row Id="7959" PostId="8339" Score="0" Text="Not knowing your data, it is difficult to answer. But **Overfitting** is a strong candidate for this kind of effect." CreationDate="2015-10-07T08:03:50.063" UserId="10169" />
  <row Id="7960" PostId="8339" Score="1" Text="This is not an answer, but rather a comment. And one more thing it's not overfitting." CreationDate="2015-10-07T10:10:42.990" UserId="5177" />
  <row Id="7961" PostId="8339" Score="0" Text="@eliasah: How do you know that it is not overfitting (of course by Random Forest, not by the other three)?" CreationDate="2015-10-07T12:06:10.000" UserId="10169" />
  <row Id="7962" PostId="8339" Score="0" Text="the Random Forest algorithm (statistically) performs better than the other three. But the issue here is that he's considering a strong heuristic &quot;voting&quot; from different classifiers." CreationDate="2015-10-07T12:23:48.290" UserId="5177" />
  <row Id="7963" PostId="8283" Score="0" Text="@Giovanrich, You can cluster them based on any similarity measure. The Mahalanobis distance, mentioned elsewhere, is the ideal measure for normal data (as it transforms out the mean and the scale but not other features of the distribution). Another thing you could do is, for every withdrawal $x_i$ from user $X$, find the closest withdrawal $y(x_i)$ from user $Y$, and the total 'distance' between them is $\sum (x_i-y(x_i))^2$ (or perhaps you'd rather take the absolute value). (Note that this distance isn't symmetric.) A discretization approach could work but I suspect it won't help too much." CreationDate="2015-10-07T13:30:19.260" UserId="12876" />
  <row Id="7964" PostId="8283" Score="0" Text="For aggregation, instead of comparing a new withdrawal from user $X$ to the distribution of user $X$'s past withdrawals, you use user $X$'s past withdrawals to decide they're part of cluster $A$, and you compare the new withdrawal to the distribution of cluster $A$'s past withdrawals. There might be 10 datapoints associated with user $X$, but might be a thousand datapoints associated with cluster $A$, and so we'll be able to be much more statistically confident in our comparison (which we paid for by not being as confident that cluster $A$ is relevant)." CreationDate="2015-10-07T13:33:10.857" UserId="12876" />
  <row Id="7965" PostId="8283" Score="0" Text="Well I got what you are saying and it is making sense; but this information, where does it fall under - i mean which topics should i look into to get my feet wet?" CreationDate="2015-10-07T13:56:22.877" UserId="13132" />
  <row Id="7967" PostId="8347" Score="0" Text="Have you tried loading a *large data set* into that (not that the OP has said what &quot;large&quot; means yet)? Go on, feed it a million rows and see what happens..." CreationDate="2015-10-07T17:29:43.490" UserId="471" />
  <row Id="7968" PostId="8336" Score="2" Text="I suspect you have not looked at the implementation of `lme4`... If you did you would notice that it is implemented in C++ and uses extensively the C++ library [Eigen](http://eigen.tuxfamily.org/index.php?title=Main_Page). Unless you have experience in using BLAS and in particular [CHOLMOD](http://faculty.cse.tamu.edu/davis/suitesparse.html) I would not attempt to port `lme4` in C any time soon..." CreationDate="2015-10-08T02:05:23.910" UserId="12905" />
  <row Id="7969" PostId="8334" Score="0" Text="Have a look at [this](http://stats.stackexchange.com/questions/174668/relation-between-decision-tree-depth-and-number-of-attributes/174901#174901) _very_ similar question." CreationDate="2015-10-08T04:45:29.647" UserId="9085" />
  <row Id="7971" PostId="8351" Score="0" Text="Good that you found an answer, but can you elaborate?" CreationDate="2015-10-08T09:57:47.983" UserId="21" />
  <row Id="7972" PostId="8332" Score="0" Text="Size of the convolutions will have a large impact. Typically for MNIST-solving CNNs, each convolution patch is 5x5 or 7x7, and is convolved over  28x28 space (e.g. around 400 times per patch, if done naively). What are your figures for your 1d convolutions? In 1D, you could easily have a patch 1x100 (larger than MNIST) and running it fully over 18000 length is going to be required almost 18000 times - I can easily see a factor of 1000 creeping in there. You might want to look at FFT style convolutions, not sure if Lasagne supports them (for small-to-mid sized patches, they are not efficient)" CreationDate="2015-10-08T10:36:01.060" UserId="836" />
  <row Id="7974" PostId="8353" Score="2" Text="It's unclear what you mean -- you can pick as many folds as there are subsets of the input. Pick from among them randomly. Is there more to it?" CreationDate="2015-10-08T14:26:14.883" UserId="21" />
  <row Id="7975" PostId="8351" Score="0" Text="Indeed, @SeanOwen, you're right. I've just added the details. All my apologies." CreationDate="2015-10-08T16:36:19.273" UserId="3024" />
  <row Id="7977" PostId="8359" Score="0" Text="Could you clarify? Do you cluster chr1 and chr2 independently (then it could be ignored)? Do you want your clusters to be lists of positions, each within distance 30 from its neighbour, and each position of a different type?" CreationDate="2015-10-09T05:02:57.813" UserId="6550" />
  <row Id="7978" PostId="8362" Score="0" Text="Thank you. Can you give some examples where both are used for different purposes" CreationDate="2015-10-09T07:00:25.540" UserId="13236" />
  <row Id="7979" PostId="6186" Score="0" Text="To add to the confusion with yet another phrase scikit-learn uses &quot;partial fit&quot;." CreationDate="2015-10-09T13:59:23.183" UserId="8111" />
  <row Id="7981" PostId="8322" Score="0" Text="Along with many other answers on the topic on _Cross Validated_, you might find [my related answer](http://stats.stackexchange.com/a/145237/31372) and links within useful." CreationDate="2015-10-10T09:43:43.673" UserId="2452" />
  <row Id="7982" PostId="6691" Score="0" Text="Out of curiosity, have you tried translating to English then using English sentiment analysis?  For some applications it would be enough." CreationDate="2015-10-10T10:35:47.210" UserId="12363" />
  <row Id="7984" PostId="8376" Score="0" Text="If you know there's a more reasonable relationship than linear--maybe you expect a factor to have a multiplicative effect instead of an additive effect--then start off by transforming that factor. But if you're uncertain, I'd start with linear and work up to transformations if the multivariate fit isn't capturing the underlying structure." CreationDate="2015-10-10T14:50:35.043" UserId="12876" />
  <row Id="7985" PostId="8376" Score="0" Text="1. There isn't a statistically significant *linear* relationship, but there very well could be signfiicant nonlinear relationships. I'd check the residuals anyway." CreationDate="2015-10-10T14:54:04.083" UserId="12876" />
  <row Id="7986" PostId="8376" Score="0" Text="2. The residual plot of X7 (I assume that's what you meant by time) looks fine to me, *but* a scatter plot is not the most informative plot when you have non-continuous values like that. Can you easily generate a prob plot of resi, grouped by X7 value?" CreationDate="2015-10-10T14:54:23.460" UserId="12876" />
  <row Id="7987" PostId="5688" Score="0" Text="In certain contexts it is done though.  For example see this [WSJ article on a bot that writes more Wikipedia articles than any human](http://www.wsj.com/articles/for-this-author-10-000-wikipedia-articles-is-a-good-days-work-1405305001)." CreationDate="2015-10-10T18:18:02.277" UserId="12363" />
  <row Id="7989" PostId="310" Score="0" Text="The PU learning is standard two-class classification problem with one caveat - you optimize the area under the curve, not classification accuracy. You can use Sofia ML software package to accomplish exactly this (no programming required). On practical side, you annotate your positive examples with +1 and everything else as -1 (yes, all other unlabeled data that may contain positives)." CreationDate="2015-10-10T23:15:02.403" UserId="7848" />
  <row Id="7990" PostId="8381" Score="0" Text="Check out http://stats.stackexchange.com/tour as that is the SE site for ML." CreationDate="2015-10-11T04:00:32.833" UserId="10319" />
  <row Id="7991" PostId="8381" Score="1" Text="@WBT: _Cross Validated_, the SE site you've referred the OP to, is a site mostly focused on _statistics_. While some people post ML-related questions there and it's a good idea to check it out, referring to CV as &quot;the SE site for ML&quot; is simply wrong. Please spend some time to browse the _Data Science_ SE site to learn that it contains a wealth of ML-focused information." CreationDate="2015-10-11T05:09:50.113" UserId="2452" />
  <row Id="7992" PostId="8381" Score="0" Text="It's called Cross Validated (an ML reference) and says on the tour page, &quot;Cross Validated is a question and answer site for people interested in... machine learning.&quot;  It's also a fully graduated (i.e. no longer in beta) site." CreationDate="2015-10-11T13:38:49.773" UserId="10319" />
  <row Id="7994" PostId="8388" Score="0" Text="These results sound typical for me, with k-means on text. Text is not what k-means was designed for, and cosine is okay for text search, but it doesn't *quantiy* similarity in a reliable way." CreationDate="2015-10-11T20:24:22.173" UserId="924" />
  <row Id="7996" PostId="8381" Score="0" Text="Also, my point wasn't that ML questions wouldn't fit here, but that Cross Validated might be a better place to learn about ML as it has almost an order of magnitude more content on the subject." CreationDate="2015-10-11T23:35:46.493" UserId="10319" />
  <row Id="7997" PostId="8388" Score="0" Text="@Anony-Mousse So then Is there another reliable way to build my cluster? Or How do I estimate my true K?" CreationDate="2015-10-12T02:11:07.933" UserId="13323" />
  <row Id="7998" PostId="8387" Score="0" Text="Can you please elaborate.  Link-only answers are generally discouraged here." CreationDate="2015-10-12T04:38:33.763" UserId="11097" />
  <row Id="7999" PostId="8388" Score="0" Text="There probably *is* no true k. And don't expect clustering to be *reliable*: it is an *explorative* approach, not an automatic." CreationDate="2015-10-12T05:43:43.170" UserId="924" />
  <row Id="8000" PostId="8394" Score="0" Text="Yeah, the tutorials by StramHacker are the best out there for NLP with Python." CreationDate="2015-10-12T05:52:21.243" UserId="11097" />
  <row Id="8001" PostId="8394" Score="0" Text="The top result on Google for [sentiment analysis with python] links to it, but somehow it itself is not up there.  In any case I think adding [with python] drastically shifts the Google results towards practical coding tutorials, my guess is that that exact query formulation simply did not occur to the OP." CreationDate="2015-10-12T05:56:57.547" UserId="12363" />
  <row Id="8003" PostId="8396" Score="0" Text="Sounds reasonable since I have the same data for both cases. However I am not sure how it will work for users that haven't seen an offer before. Neither accept or reject one. I was hoping to do it through similar users with about the same features. Is it possible with decision trees?" CreationDate="2015-10-12T10:20:25.883" UserId="201" />
  <row Id="8004" PostId="8396" Score="0" Text="Yes, it is possible.  Please look up the _read up_ hyperlink in the answer." CreationDate="2015-10-12T10:24:37.523" UserId="11097" />
  <row Id="8007" PostId="8396" Score="0" Text="Sorry, I was on my phone and didn't see that there was a link on your answer. I will check it and if it actually makes sense, I will choose it as the best answer :)...Thank you" CreationDate="2015-10-12T13:15:33.347" UserId="201" />
  <row Id="8008" PostId="8401" Score="1" Text="In a sense you want to *update* the network, ie *online learning*." CreationDate="2015-10-12T16:57:03.547" UserId="12363" />
  <row Id="8009" PostId="8370" Score="0" Text="I'm not sure, whether migrating this question to Cross Validated (CV), as requested by the OP, will be productive. Rationale: 1) it fits much better on this site; 2) it will likely be closed on CV as off-topic. @SeanOwen, what is your opinion?" CreationDate="2015-10-12T18:18:54.483" UserId="2452" />
  <row Id="8010" PostId="8370" Score="0" Text="@AleksandrBlekh It does not seem to be getting much traction here. If it might get closed on CV then this is a better fit.   I am OK with it staying here.  I will probably just put a bounty on it to get more attention." CreationDate="2015-10-12T18:32:52.570" UserId="13285" />
  <row Id="8011" PostId="8370" Score="0" Text="I'm afraid I don't understand your intent. If the question will be migrated to CV, it will disappear from this site (AFAIK). It is not a good idea to produce duplicate questions. Let's wait a bit to see what Sean thinks, OK?" CreationDate="2015-10-12T18:41:24.510" UserId="2452" />
  <row Id="8012" PostId="8370" Score="0" Text="@AleksandrBlekh I am good with it staying here. You are moderator here and I see you are also active on CV.  I trust your opinion." CreationDate="2015-10-12T18:43:35.293" UserId="13285" />
  <row Id="8013" PostId="8370" Score="0" Text="Thank you. We can always migrate the question to CV, if needed. Consider posting a bounty to increase chances of getting (good) answers. Good luck!" CreationDate="2015-10-12T18:49:29.030" UserId="2452" />
  <row Id="8014" PostId="8370" Score="0" Text="@AleksandrBlekh It is not letting me post a bounty yet.  I think it needs more time.  I hope I can use points I have from other site." CreationDate="2015-10-12T18:51:37.527" UserId="13285" />
  <row Id="8016" PostId="8370" Score="0" Text="No problem. Take your time." CreationDate="2015-10-12T18:57:14.500" UserId="2452" />
  <row Id="8018" PostId="8403" Score="1" Text="Right on the spot :) &#xA;Just 2 hours back my colleague also recommended inverse transform sampling to me. I guess these links are enough as an initial set to study. Thank you!" CreationDate="2015-10-12T19:56:06.170" UserId="8338" />
  <row Id="8019" PostId="8403" Score="0" Text="@MangatRaiModi: You are welcome! Don't forget about upvoting and/or accepting answers that you find helpful and/or extremely helpful :-)." CreationDate="2015-10-12T20:02:01.270" UserId="2452" />
  <row Id="8020" PostId="8407" Score="0" Text="I am trying to learn a classifier, where I need sampling for two reasons:- resource limitations and huge selection bias. So I decided to take uniform distribution across each dimension rather than the actual distribution of my data to remove the bias. Thanks for the links, they look interesting." CreationDate="2015-10-12T20:04:35.213" UserId="8338" />
  <row Id="8021" PostId="8379" Score="0" Text="@ Matthew. Please let me know your feedback. Thanks." CreationDate="2015-10-12T20:08:47.587" UserId="12867" />
  <row Id="8023" PostId="8412" Score="0" Text="Not really. Especially if you have many vectors without important information.. If you go that route, then definitely use LSTM :)" CreationDate="2015-10-12T22:01:11.787" UserId="3044" />
  <row Id="8024" PostId="8397" Score="0" Text="I read something about clustering by using latent semantic indexing, but I don't really get it! Do you know some resources to theoretical o practical? Sorry I'm new in DM and TM I'm still learning." CreationDate="2015-10-12T22:25:46.497" UserId="13323" />
  <row Id="8025" PostId="8388" Score="0" Text="Yes your right! But I'm using a bag of words, so at least I could cluster similar words based in the concurrency of different papers. For example:&#xA;`Domain Ontology	GIS	Integrated Geo Systems	Linked Data	Open GIS	Semantic Annotation	Semantic Web	Use Case	Cluster&#xA;1	0	0	1	0	0	1	0	cluster1&#xA;1	0	0	1	0	0	0	1	cluster1&#xA;1	0	0	1	0	1	0	0	cluster1&#xA;0	1	1	0	1	0	0	0	cluster0&#xA;0	1	0	0	1	0	0	0	cluster0`&#xA;&#xA;Am I wrong?" CreationDate="2015-10-12T22:29:04.533" UserId="13323" />
  <row Id="8026" PostId="8388" Score="0" Text="Good luck. Language when you only see bits and bytes is far from easy. I doubt 10% of your cluster assignments will be useable." CreationDate="2015-10-12T22:47:48.610" UserId="924" />
  <row Id="8027" PostId="8414" Score="0" Text="I think this isn't making use of the fact that vector lengths come mostly from the shared idf weights, because of the tf normalization scheme he's using. If a document has a very low norm, that implies that it does not contain rare words (or contains them at a very low fractional frequency), which means that it can be ruled out as similar to a document that only contains rare words.&#xA;&#xA;But how tight this constraint is in general seems unclear to me. It's probably the case that theoretical bounds are very wide compared to observed empirical bounds." CreationDate="2015-10-13T00:06:36.457" UserId="12876" />
  <row Id="8029" PostId="8414" Score="0" Text="@Matthew Graves, All I'm saying is that cosine similarity is agnostic to vector length.  He is asking how differences in vector length can affect the resulting cosine similarity and the answer is: they can't." CreationDate="2015-10-13T00:42:21.417" UserId="9420" />
  <row Id="8031" PostId="8414" Score="1" Text="The empirical correlation cannot be ignored.  There has be a way to correlate the randomness of the corpus to abound if only statistical.  I don't have enough rep on this site for an up vote to register." CreationDate="2015-10-13T01:01:57.410" UserId="13285" />
  <row Id="8034" PostId="8415" Score="1" Text="Nice edit to the question.  Thank you." CreationDate="2015-10-13T02:07:11.243" UserId="13285" />
  <row Id="8035" PostId="8414" Score="0" Text="Here is where I don't agree.  It does not normalize based on length.  It normalizes on the single most common term.    A longer document can only dilute.  I am willing to adjust how normalization is performed to get a bound I can support." CreationDate="2015-10-13T08:58:01.887" UserId="13285" />
  <row Id="8036" PostId="8415" Score="0" Text="I don't agree with ||d|| with seems to serve as a rarity measure.  It is normalized.    &quot;Mary had a little lamb&quot; will have a smaller || than &quot;Marry had a white little lamb&quot;.   And &quot;oddxxA oddxxB oddxxC&quot; will have a smaller || than &quot;oddxxA oddxxB oddxxC  oddxxD&quot; in roughly the same same ratio.  And those two comparisons will have similar cos." CreationDate="2015-10-13T09:18:18.533" UserId="13285" />
  <row Id="8037" PostId="8415" Score="0" Text="@Frisbee, are you sure about that comparison? Supposing the idfs are 0 for 'a', 0.5 for 'had' and 'Mary', 1 for 'little' and 'white', and 2 for 'lamb', I calculate 2.4 for &quot;Mary had a little lamb&quot; and 2.55 for &quot;Mary had a white little lamb&quot;, but 1.83 for &quot;A Mary had a little lamb&quot;. That is, the only way to decrease the norm is by increasing the frequency of the most frequent term, not by adding new words. Or are we not using the same formula?" CreationDate="2015-10-13T14:10:58.593" UserId="12876" />
  <row Id="8038" PostId="8388" Score="0" Text="Ok. Thank you so much. @Anony-Mousse Then I won't waste time trying to do by this way. I'll start searching about LSI." CreationDate="2015-10-13T14:48:26.810" UserId="13323" />
  <row Id="8039" PostId="8397" Score="0" Text="No need to apologise, the best part of data analytics is that it's a continual learning process. The following link should provide a good intro &gt;&gt; [http://fastml.com/dimensionality-reduction-for-sparse-binary-data/]. Also try typing it in on Youtube to see what kind of video resources are available." CreationDate="2015-10-13T14:51:44.190" UserId="13176" />
  <row Id="8040" PostId="8388" Score="0" Text="What you need is a *background knowledge database*. I don't think LSI etc. will be helpful here either. Because they cannot make the association &quot;GIS, probably something with Geo&quot; that a human would make just from the five examples you gave above. None of the standard algorithms can do that." CreationDate="2015-10-13T15:12:33.230" UserId="924" />
  <row Id="8042" PostId="8415" Score="0" Text="I was thinking you normalized the document on the weighted (with IDF) not the raw frequency. That would change things.  It makes more sense to me to normalize on weighted.   Significantly changing a document || by making 'a' the most common term messes with stuff." CreationDate="2015-10-13T15:35:21.940" UserId="13285" />
  <row Id="8043" PostId="8381" Score="0" Text="What happened to my comments? Why did they get deleted? I would love to hear why this happened to improve in the future. Thanks." CreationDate="2015-10-13T15:49:01.547" UserId="7848" />
  <row Id="8044" PostId="8415" Score="0" Text="Sure; I just want to be sure we're working with the same formula. I think I agree with you that one will get superior results by normalizing based on raw tf * idf, that is:&#xA;&#xA;$d_t=w_t(0.5+0.5\frac{w_tf(t,d)}{\mathrm{max}\{w_tf(t,d): t\in d\}})$&#xA;where $w_t=\mathrm{log}\frac{N}{|\{d\in D: t\in d\}|}$. &#xA;(With a somewhat unclear but hopefully acceptable collision between $d$, the vector of weights $d_i$, and $d$, the document associated with that vector.) I'll have to think more tonight about whether or not this would improve the bound (but that probably involves a lot of algebra)." CreationDate="2015-10-13T16:09:31.190" UserId="12876" />
  <row Id="8045" PostId="8414" Score="0" Text="Thanks for modifying your question. It better clarifies what you are trying to accomplish.  Note that your modified normalization makes this actually **not** a cosine similarity, since this is strictly defined.  I would suggest a few additional edits to spell this out.  Take care and good luck." CreationDate="2015-10-13T16:40:26.720" UserId="9420" />
  <row Id="8048" PostId="8425" Score="0" Text="That means that all Transformations in Spark are scan-based operations. Can you give some examples of Spark operations that aren't scan-based?" CreationDate="2015-10-14T06:08:40.177" UserId="339" />
  <row Id="8049" PostId="8425" Score="0" Text="I didn't say that. What I said it that each time there is a predicate to calculate, the operation is scan based." CreationDate="2015-10-14T06:32:24.410" UserId="5177" />
  <row Id="8050" PostId="8425" Score="1" Text="Ah, sorry, my bad. Can you kindly give an example of a transformation with predicate evaluation and one without it? I am (mostly) unfamiliar with Spark API atm and an example would help :) Thanks" CreationDate="2015-10-14T08:57:52.763" UserId="339" />
  <row Id="8051" PostId="893" Score="0" Text="i have to face same problem in my research. but i couldn't find the correct method to solve this issue. so if you can please be kind enough to give me the references you have found." CreationDate="2015-10-14T08:04:25.360" UserId="13336" />
  <row Id="8053" PostId="8414" Score="0" Text="Excellent point, thank you.  Since this is not my expertise I am going to stick with standard tf normalization until I have a solid reason not to." CreationDate="2015-10-14T10:36:02.717" UserId="13285" />
  <row Id="8054" PostId="8370" Score="0" Text="Your argument that concludes with a bound determined by  $\mathrm{cos}=\frac{||d_1||}{||d_2||}$ doesn't work because &quot;The very best d1 dot d2 that can be achieved is d1 dot d1&quot; is incorrect. While $\frac{d_1\cdot d_2}{||d_1||\ ||d_2||}\le\frac{d_1\cdot d_1}{||d_1||\ ||d_1||}$, it is not the case that $d_1\cdot d_2\le d_1\cdot d_1$. For this particular class of vectors, it may work in enough cases that it's a decent approximation, but it'd be considerably harder to establish that it's always the case." CreationDate="2015-10-14T13:45:09.837" UserId="12876" />
  <row Id="8055" PostId="8370" Score="0" Text="@MatthewGraves I think I agree with you.  Not my expertise but I am still hacking away at it." CreationDate="2015-10-14T13:52:08.150" UserId="13285" />
  <row Id="8056" PostId="8290" Score="0" Text="I agree with you @Spacedman. The lack of context really makes the curriculum meaningless. Do you have any formal reference so that I can mention that in my book? For example, a paper or website saying that Statistics as an independent discipline is boring, dull and theoretical? and context and domain knowledge should be added to these programs?" CreationDate="2015-10-14T15:42:37.080" UserId="3151" />
  <row Id="8057" PostId="8429" Score="0" Text="Ah, perfect. Thank you!" CreationDate="2015-10-14T17:25:48.443" UserId="13385" />
  <row Id="8063" PostId="2594" Score="0" Text="Link to http://ai.arizona.edu/intranet/papers/comparative.ieeeis.pdf is broken :-(" CreationDate="2015-10-15T11:36:48.087" UserId="13428" />
  <row Id="8068" PostId="8425" Score="0" Text="@MB_CE I have edited the answer. Finding a non based transformation (not actions) example is actually quite tricky and difficult." CreationDate="2015-10-15T15:39:07.090" UserId="5177" />
  <row Id="8070" PostId="2594" Score="0" Text="I will update thanks" CreationDate="2015-10-15T15:51:24.143" UserId="3466" />
  <row Id="8071" PostId="8425" Score="0" Text="@eliasah what about methods like cache and unpersist? I don't think they count as transformations or actions, but they seem like an example of *something* that's not scan-based." CreationDate="2015-10-15T15:53:09.037" UserId="12876" />
  <row Id="8072" PostId="8425" Score="0" Text="Cache and unpersist aren't transformations nor actions but persistence methods. The `cache` method is a shorthand for using the default storage level, which is `StorageLevel.MEMORY_ONLY`" CreationDate="2015-10-15T15:55:15.063" UserId="5177" />
  <row Id="8074" PostId="8425" Score="0" Text="@eliasah How is the reduce transformation on rdd.map not scan-based? Thanks for the reference, I will take a look :)" CreationDate="2015-10-15T17:04:52.870" UserId="339" />
  <row Id="8075" PostId="8440" Score="0" Text="Not an answer, but I'm sure you've seen: http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html&#xA;But I don't know how to turn the visualizer into a data frame for predictions." CreationDate="2015-10-15T17:07:08.683" UserId="375" />
  <row Id="8076" PostId="8425" Score="0" Text="It just apply a monad operation on the mapped RDD from the step before." CreationDate="2015-10-15T17:08:22.413" UserId="5177" />
  <row Id="8078" PostId="8440" Score="0" Text="@nfmcclure Thanks! Yes, I saw that. I actually thought about writing python code for reverse engineering the paths based on that output, but it's about 250 pages, if you save it to a .txt file. I figure it would be too buggy an approach." CreationDate="2015-10-15T17:15:32.097" UserId="13413" />
  <row Id="8080" PostId="8444" Score="0" Text="how do you define interesting?  If it is activity then you might look for a membrane of maximum entropy or such." CreationDate="2015-10-16T03:40:21.667" UserId="8552" />
  <row Id="8081" PostId="8439" Score="0" Text="Have you heard about &quot;giraffe&quot;?  http://techxplore.com/news/2015-09-giraffe-machine-taught-chess-higher.html http://arxiv.org/abs/1509.01549  What about emrald (a great teacher)?  http://chess.emrald.net/  They could likely be combined in a useful way." CreationDate="2015-10-16T03:41:52.133" UserId="8552" />
  <row Id="8086" PostId="8464" Score="1" Text="You can use Theano backend to implement your own neural net. It will take advantage of multiple CPUs or GPUs" CreationDate="2015-10-16T17:33:01.360" UserId="8040" />
  <row Id="8088" PostId="8203" Score="0" Text="What if the OP added the reference request!" CreationDate="2015-10-16T18:30:54.707" UserId="5055" />
  <row Id="8091" PostId="8460" Score="0" Text="There's an alternative implementation of sklearn HMM that appears to have active contributions that can be found here: https://github.com/hmmlearn/hmmlearn I haven't used it before, so I can't speak to how good it is, but, looking at the examples, it appears to be fairly straightforward." CreationDate="2015-10-16T18:46:10.860" UserId="13413" />
  <row Id="8092" PostId="8459" Score="0" Text="Thanks, and about for EM? Also what would be a good justification for that (The K-Means part)?" CreationDate="2015-10-16T21:14:14.193" UserId="13447" />
  <row Id="8093" PostId="8459" Score="0" Text="Gaussian mixture modeling works for Gaussian distributions. This is a continuous distribution." CreationDate="2015-10-16T21:32:54.030" UserId="924" />
  <row Id="8095" PostId="8464" Score="0" Text="@Betrand: I agree, but in theano can I run learning in batches and directly feed compressed representation ?" CreationDate="2015-10-17T01:58:11.723" UserId="13457" />
  <row Id="8098" PostId="8467" Score="0" Text="I thought that if I can go down to `d` features while retaining most of the variance (&gt; 99.99% in my case), it means that the data roughly lies on a hyper-plane of `d` dimensions. And whatever non-linear aspects there are, they are kept in the `d` features. No? If yes, why would the neural net need to reconstruct the non linear aspects as they are still present?" CreationDate="2015-10-17T09:25:20.303" UserId="12776" />
  <row Id="8099" PostId="6611" Score="0" Text="Have you considered dimension reduction ? PCA is a good candidate for this kind of problem, specially involving the TFIDF vector model. Once you perform the dimension reduction, you can then apply a simple KMeans to cluster your items." CreationDate="2015-10-17T09:42:10.513" UserId="5177" />
  <row Id="8100" PostId="8464" Score="2" Text="Since you have to implement your own network then you can do whatever you want about batches as long as the learning process isn't harmed. About compressed representation I don't think it can be handled, but if I were you, my main concern would be the dimension of the input space which seems to be way too huge to learn anything meaningful." CreationDate="2015-10-17T11:11:12.840" UserId="8040" />
  <row Id="8101" PostId="8452" Score="0" Text="Accepting this answer for now, but if someone comes up with a python solution, I'll likely accept that instead." CreationDate="2015-10-17T14:03:50.737" UserId="13413" />
  <row Id="8103" PostId="8463" Score="0" Text="Thank you for taking the time to go through my code in so much detail! I eventually worked out a solution involving apply, but it wasn't as elegant as yours. My key question is whether apply is faster than an equivalent for loop - i.e. is it a question of optimisation or just of elegance?" CreationDate="2015-10-17T15:07:35.740" UserId="13416" />
  <row Id="8105" PostId="8467" Score="0" Text="No, you are probably throwing away nonlinear contributions." CreationDate="2015-10-17T19:42:55.490" UserId="9420" />
  <row Id="8107" PostId="8483" Score="0" Text="Thanks @amustafa. Now that i think about it, yes I should have been able to figure it out. I think I was caught of guard by the `=` sign in the string." CreationDate="2015-10-17T22:26:55.480" UserId="10345" />
  <row Id="8110" PostId="362" Score="0" Text="Even with false matches this will reduce the number of rows to be compared." CreationDate="2015-10-18T08:50:12.617" UserId="13498" />
  <row Id="8112" PostId="8476" Score="0" Text="Thank you for such a warm greeting! I'm planning to continue studying Data Science - it is such a fascinating subject and with all the sensors and Internet of Things the demand for DS specialists will only increase :) I'm pretty good in math so I will definitely read The Elements of Statistical Learning. As for the application aspect of it - I'm know Python already(I know there are many good statistical and machine learning libraries for it) and heard R is relatively easy to pick up. Which one would you recommend?" CreationDate="2015-10-18T09:26:03.877" UserId="13480" />
  <row Id="8113" PostId="8477" Score="0" Text="Thank you for an excellent suggestion about randomly splitting training data. That's what I'm doing now. Vowpal Wabbit seems to be really popular and is often used during competitions on Kaggle. Will give it a try, thanks!" CreationDate="2015-10-18T09:36:15.863" UserId="13480" />
  <row Id="8114" PostId="8289" Score="0" Text="@Tasos I have read on how to find the lat and long of ATMs. I could implement this in my project but because of time left I have decided to classify my ATMs according to provinces in the country. This is not **not** a good idea but since its final year project and there is little time left." CreationDate="2015-10-18T11:30:05.963" UserId="13132" />
  <row Id="8115" PostId="8289" Score="0" Text="@Tasos One other thing, where can I hook up a sample database of a credit card accounts - one that looks similar to that of a real bank's?" CreationDate="2015-10-18T11:34:25.420" UserId="13132" />
  <row Id="8116" PostId="8477" Score="0" Text="I searched over online docs for VW and couldn't find a good example for decision trees and/or random forests. Could you give an example?" CreationDate="2015-10-18T12:33:12.497" UserId="13480" />
  <row Id="8118" PostId="8487" Score="0" Text="I was under the impression that at every iteration the algorithm would choose the best weak classifier it has available. I guess I've only learned the simple adaboost algorithm and not the stochastic version? Is that where I'm missing knowledge?" CreationDate="2015-10-18T14:49:08.773" UserId="13485" />
  <row Id="8119" PostId="8487" Score="0" Text="Ada implements stochastic boosting. Even if you aren't using it, consider this possibility: you have a huge number of columns of random data. At some point in the boosting process, one of those columns is, by chance, a perfect fit for the margins. But of course, it's useless once you're validating the classifier on other data. This sort of behavior is also possible even if you are using a deterministic algorithm." CreationDate="2015-10-18T14:54:35.663" UserId="9597" />
  <row Id="8120" PostId="8289" Score="0" Text="@Giovanrich For this, you should ask on the Open Data StackExchange site http://opendata.stackexchange.com/" CreationDate="2015-10-18T15:29:07.763" UserId="201" />
  <row Id="8121" PostId="8289" Score="0" Text="@Tasos Thanks, let me see to it" CreationDate="2015-10-18T16:08:11.370" UserId="13132" />
  <row Id="8122" PostId="5381" Score="0" Text="Are you affiliated with this site BTW?" CreationDate="2015-10-18T16:34:32.057" UserId="21" />
  <row Id="8123" PostId="6861" Score="0" Text="This might not be what the OP is looking for, but +1 for laziness!" CreationDate="2015-10-18T16:41:14.353" UserId="5177" />
  <row Id="8126" PostId="6853" Score="0" Text="Based on the examples it seems like it should be front-weighted in some way, as names that should be considered equivalent differ near the end but never im the beginning." CreationDate="2015-10-18T19:13:32.533" UserId="12363" />
  <row Id="8127" PostId="6853" Score="0" Text="For example I would not remove a number from the beginning.  Or at least look to see all the data with numbers in the beginning." CreationDate="2015-10-18T19:18:06.297" UserId="12363" />
  <row Id="8128" PostId="8477" Score="0" Text="VW does not implement decision trees or random forests. You can start with the simple logistic regression. If you notice that your Logistic Regression overfits on your data, then you can use ensembles or random forests to counter this effect. Always start with simple things." CreationDate="2015-10-18T19:20:26.773" UserId="7848" />
  <row Id="8130" PostId="8487" Score="0" Text="I see, I looked up stochastic methods. Makes a lot more sense now. The deterministic piece also makes sense. Thank you!" CreationDate="2015-10-19T04:13:02.377" UserId="13485" />
  <row Id="8131" PostId="8482" Score="0" Text="My samples are fixed. However, what you're saying still makes sense in a fixed data set. I think it has to do with the fact that the ada package in R uses stochastic boosting." CreationDate="2015-10-19T04:16:52.343" UserId="13485" />
  <row Id="8133" PostId="8497" Score="2" Text="Compare performance with different window sizes using cross validation." CreationDate="2015-10-19T06:25:55.037" UserId="6550" />
  <row Id="8134" PostId="8498" Score="0" Text="All the NN libraries will do this if you choose RMSE as the objective (and many will do that automatically for linear output neurons). No effort is required other than setting parameters for the network, and you have already said you want just one output neuron. So just create a NN with one layer, with one linear neuron in it, and train it on the data. If this is something that you don't understand or know how to do, then could you please explain a bit more about where or how you are stuck? Otherwise it is not clear how to answer your question" CreationDate="2015-10-19T06:50:26.160" UserId="836" />
  <row Id="8135" PostId="8498" Score="0" Text="Okay, I'm coming, I've a $200 \times 4$ inputs, and an output vector of $200 \times 1$, I want to **find the weights, $ w_1, w_2, ...$** I'm using Matlab m file for that with for loop and stuff, **also, not sure how to get the proper learning ratem $ \gamma $** , any suggestions ?" CreationDate="2015-10-19T07:26:40.900" UserId="13516" />
  <row Id="8136" PostId="8491" Score="0" Text="In a question like this, you should *always* show us your struggles so far. Also, show us a sample data frame, preferably a tiny one that illustrates the data. Show us how to create that data frame *with some cut and paste code* (use `dput(df)` to get this). Then tell us precisely what you want as output - is it a vector of the name of the last section that the user has completed? A list? Another column added to the data frame? Or a count of how many sections? Show the desired output." CreationDate="2015-10-19T07:30:34.767" UserId="471" />
  <row Id="8138" PostId="8498" Score="0" Text="Learning rate you just need to guess (this is an annoying problem with many ML algorithms). Other than that, this seems like homework or coursework from a basic ML class. If you post **where you are stuck exactly**, explain what *your* problem with understanding is, then maybe the site here can help. Otherwise, you will get much better help from your class - just explain there what it is that you do not understand. Or just watch your lectures again, because it will be explained very well there" CreationDate="2015-10-19T07:49:49.700" UserId="836" />
  <row Id="8139" PostId="8500" Score="0" Text="What do you mean by &quot;make sense of&quot;? What are you using the distribution for?" CreationDate="2015-10-19T12:20:57.967" UserId="7738" />
  <row Id="8140" PostId="8500" Score="0" Text="This distribution will be useful for describing the degree distribution (sorry for repeating myself) of a regular, 2D lattice." CreationDate="2015-10-19T12:22:45.930" UserId="13481" />
  <row Id="8141" PostId="8506" Score="0" Text="This will only rescale (normalize) my sample histogram. My question was more like &quot;what distribution could fit this very dataset, based on the histogram shape?&quot;. Based on the shape of this histogram I am not able to recall a distribution with which to fit the data and try to give them an analytical interpretation." CreationDate="2015-10-19T12:25:29.810" UserId="13481" />
  <row Id="8142" PostId="8506" Score="0" Text="Got it. Would you mind elaborating on your desired analytical properties? It's not clear to me what you are trying to do with the distribution." CreationDate="2015-10-19T12:33:11.557" UserId="7738" />
  <row Id="8143" PostId="8506" Score="0" Text="This is part of a task concerning the analysis of some specific kinds of networks (regular, random, scale-free, etc.). For the random and scale-free cases we have some pretty analytical models that describe their distribution (the degree distribution is a Poisson distribution for a random network, while the scale-free network follows a power law distribution). For the regular network I failed to find something, so I decided to work this out myself by building one and plotting its histogram. But now I am stuck at that precise question I re-formulated before..." CreationDate="2015-10-19T12:38:53.813" UserId="13481" />
  <row Id="8144" PostId="8506" Score="0" Text="So, the properties I am after are simply those of an analytical curve, provided it exists, that fits these data." CreationDate="2015-10-19T12:40:53.000" UserId="13481" />
  <row Id="8146" PostId="8506" Score="0" Text="Hi @Francesco, I have updated my answer. Hope that helps." CreationDate="2015-10-19T13:03:47.057" UserId="7738" />
  <row Id="8147" PostId="8506" Score="0" Text="So, to come to a conclusion, would you agree with the following? There is no single distribution which can fit these data, as a stepwise one, made of three different distributions, is needed. And if you agree, what would these three distributions be called?" CreationDate="2015-10-19T13:06:33.610" UserId="13481" />
  <row Id="8148" PostId="8506" Score="0" Text="I would say that the distribution is a piecewise defined function of N. There is nothing wrong with piecewise distributions. See for example the [Bernoulli distribution](https://en.m.wikipedia.org/wiki/Bernoulli_distribution)." CreationDate="2015-10-19T13:15:29.903" UserId="7738" />
  <row Id="8149" PostId="8506" Score="0" Text="Got it. It really sounds nice. Thanks. I often times know the tools but fail to give them a proper name. I should really look for a thorough statistics MOOC. Anything to suggest?" CreationDate="2015-10-19T13:23:40.987" UserId="13481" />
  <row Id="8150" PostId="8477" Score="0" Text="Thanks. Logistic Regression requires labels {-1,1} but my data has 7 classes represented with integers from 0 to 6. How should I deal with that?" CreationDate="2015-10-19T13:23:45.220" UserId="13480" />
  <row Id="8151" PostId="8506" Score="1" Text="I learned stats in a traditional academic environment, so I don't know much about moocs. I have heard good things about [Think Stats](http://www.greenteapress.com/thinkstats/) if you are already familiar with coding. Good luck! :)" CreationDate="2015-10-19T13:31:12.207" UserId="7738" />
  <row Id="8153" PostId="8477" Score="0" Text="@intellion Logistic Regression is not limited to binary classification. If you are still using VW, take a look at --oaa option. As an argument you specify the number of classes. You can also have string labels for your classes but then you must specify to VW the exhaustive list of labels." CreationDate="2015-10-19T16:20:32.277" UserId="7848" />
  <row Id="8154" PostId="8477" Score="0" Text="That's what I'm trying to do (use the -oaa option) but getting &quot;NAN prediction in example 144219, forcing 0.000000&quot; for almost all rows in the training data. I have asked this question on SO. http://stackoverflow.com/questions/33218138/getting-lots-of-nan-predictions-using-vowpal-wabbit-for-classification" CreationDate="2015-10-19T16:26:45.270" UserId="13480" />
  <row Id="8155" PostId="6853" Score="0" Text="@Adam M. B., Levenshtein is a &quot;base algorithm&quot; and there are many derivations of it that have been tailored for specific advantages and cases.  I haven't found prioritizing beginnings of words over their ends to be effective because my data often contains human error and typos can occur anywhere, but some datasets may benefit.  There are a number of modifications to Levenshtein in the book that I referenced.  I personally use an algo that only allows for an edit distance error of 1 or less.  A key advantage being that it requires a high level of match and scales like $n$ rather than $n^2$." CreationDate="2015-10-19T16:46:20.547" UserId="9420" />
  <row Id="8156" PostId="8502" Score="0" Text="What data are you starting with? If you've got sheet music, this is going to be a lot easier than starting from audio, etc" CreationDate="2015-10-19T16:53:47.687" UserId="13296" />
  <row Id="8157" PostId="8502" Score="0" Text="Hi @PhilipKendall , the data I expect to be the input is an music file (a .wav for example)." CreationDate="2015-10-19T16:55:32.903" UserId="13521" />
  <row Id="8158" PostId="8477" Score="0" Text="@intellion There is a problem with one of your training examples. See my comment to that SO question." CreationDate="2015-10-19T17:02:53.257" UserId="7848" />
  <row Id="8159" PostId="6853" Score="0" Text="All good points.  It's just a suggestion but without seeing the data it is hard to draw specific conclusions.  The thing about frontweighting Levenshtein (as opposed to other optimisations) is that it adds some of the benefit of the other answer (alphasort), which I think has merit." CreationDate="2015-10-19T17:09:03.227" UserId="12363" />
  <row Id="8160" PostId="8502" Score="0" Text="Looks like an interesting area to research on.. All the best.. have a look into the algorithm behind SoundCloud and other popular music classification apps that sorts the music based on their genre / tone etc.." CreationDate="2015-10-19T12:22:24.957" UserId="12952" />
  <row Id="8161" PostId="8510" Score="0" Text="Thank you very much sir! I'm taking a look in Fast Fourier Transform right now." CreationDate="2015-10-19T19:07:38.920" UserId="13521" />
  <row Id="8164" PostId="8502" Score="0" Text="Look harder. e.g. libXtract. People have tried that, but it is kot half as easy as you think it is... it's not really that well defined, either, unless you have the original score (then it may be clear what the author meant, or not). And of course, some change tonality, too." CreationDate="2015-10-19T22:06:06.520" UserId="924" />
  <row Id="8165" PostId="8476" Score="0" Text="rpart is chewing my data for almost 50 hours now, on a quad-core i7 :) Are there ways to speed it up?" CreationDate="2015-10-20T09:44:58.133" UserId="13480" />
  <row Id="8169" PostId="8520" Score="0" Text="Thank you @ov32m1nd . Essentia is exactly what I was searching for. Seems like a great place to start. I'm taking a read on its paper right now (ps: sadly I don't have sufficient reputation to positive your answer :/ )." CreationDate="2015-10-20T15:17:52.543" UserId="13521" />
  <row Id="8170" PostId="8525" Score="0" Text="I think they are looking to solve the more general problem of location identification without having to specify a list of locations themself." CreationDate="2015-10-20T15:39:13.707" UserId="13413" />
  <row Id="8171" PostId="8521" Score="0" Text="Did you try linear models for classification? Try online version of Logistic Regression implemented in Vowpal Wabbit. Go to non-linear when you really need to (VW also supports SVM)." CreationDate="2015-10-20T16:15:11.903" UserId="7848" />
  <row Id="8172" PostId="8476" Score="0" Text="@intellion: That's great! I think Python and R are both great for data science. I tend to use Python on text-mining types of problems, and use R for more traditional regression and statistics, but that's just my personal preference." CreationDate="2015-10-20T16:28:33.720" UserId="13413" />
  <row Id="8173" PostId="8476" Score="0" Text="@intellion: Yes, I've encountered that as well. I've found that to be fairly normal, but I've gotten some performance boost from using rpart.control calls. E.g., rpart(Y ~ X, control=rpart.control(maxcompete=1, maxsurrogate=0). Make sure and check the documentation so you don't use a set of parameters that will break your particular application of rpart!" CreationDate="2015-10-20T16:30:34.893" UserId="13413" />
  <row Id="8174" PostId="8519" Score="0" Text="To clarify, you wish to match the elements of `c` that are locations with an external source of locations (not defined by you in your code), correct?" CreationDate="2015-10-20T16:57:25.633" UserId="10663" />
  <row Id="8175" PostId="8518" Score="0" Text="Best would be to estimate the delay, but you could use Kalman filtering or other multivariate autoregressive models." CreationDate="2015-10-20T16:58:01.057" UserId="154" />
  <row Id="8176" PostId="8527" Score="0" Text="before switching to scikit-learn I manually did the calculation of tf-idf in golang from scratch, it was super painful, no thanks lol (regarding re-implementing)" CreationDate="2015-10-20T17:08:19.440" UserId="13562" />
  <row Id="8177" PostId="8521" Score="0" Text="so far only linear svm, multinomial naive bayes, and logistic regression, none deliver accurate result so far" CreationDate="2015-10-20T17:11:05.687" UserId="13562" />
  <row Id="8178" PostId="8527" Score="0" Text="Oh, &quot;fun&quot;! :) I thought writing a tfidf modeler in Python was pretty educational, actually. I wrote a general text feature modeler class that tfidf, binary, etc. inherent functions from, so I can reuse it easily." CreationDate="2015-10-20T17:14:47.520" UserId="13413" />
  <row Id="8179" PostId="8519" Score="0" Text="Yes, exactly. @ShawnMehan" CreationDate="2015-10-20T17:15:01.243" UserId="13508" />
  <row Id="8180" PostId="8519" Score="0" Text="I am trying to work with OpenNLP's location-model, @ShawnMehan" CreationDate="2015-10-20T17:21:56.673" UserId="13508" />
  <row Id="8181" PostId="8530" Score="0" Text="I'll look it up." CreationDate="2015-10-20T18:04:02.160" UserId="13508" />
  <row Id="8182" PostId="8477" Score="0" Text="@intellion It looks like the phraug version and csv2vw version included with VW are having troubles with variables that contain digits and numbers. Sometimes they get interpreted as categorical values (what we expect) and sometimes as feature value (this is where problem is). I couldn't find yet where the problem in the script is (I don't know Perl)." CreationDate="2015-10-20T18:16:56.330" UserId="7848" />
  <row Id="8183" PostId="8477" Score="0" Text="Actually it is Python and to make csv2vw treat all values as categorical there is -c switch. It gets rid of the NAN problem" CreationDate="2015-10-20T18:28:57.387" UserId="13480" />
  <row Id="8184" PostId="8477" Score="0" Text="You don't want to treat the real valued features as categorical :)" CreationDate="2015-10-20T18:30:37.560" UserId="7848" />
  <row Id="8185" PostId="8528" Score="0" Text="Welcome to the site, @Andy H.! Can you provide a little more information on your question and the format of your data here? I'm not sure your question is yet understandable to someone who hasn't already been thinking about it :)" CreationDate="2015-10-20T20:58:28.210" UserId="13413" />
  <row Id="8186" PostId="8528" Score="0" Text="Thanks, @Andy H.! That's much clearer. I don't think I have the expertise to answer this, but this should help you get the help you need!" CreationDate="2015-10-20T21:59:31.927" UserId="13413" />
  <row Id="8187" PostId="8528" Score="0" Text="Thanks @Kyle. Appreciate the constructive feedback" CreationDate="2015-10-20T22:11:52.827" UserId="13574" />
  <row Id="8188" PostId="8501" Score="0" Text="Do you know they &quot;quality&quot; of each annotator? You can weight their decisions. It might be good idea to simply merge the labels but that can add too much of noise. Another way would be to use the labels as features for your final classifier; the classifier could deal with noise in features, not in the labels." CreationDate="2015-10-20T23:28:18.020" UserId="7848" />
  <row Id="8190" PostId="749" Score="0" Text="Latent variables allow to render the models more powerful in terms what can be modeled. It's up to data and algorithm to define their value. In other words, latent variables are like &quot;step&quot; that bridges the gap between your observed variables and the desired prediction. The wider this &quot;gap&quot; is, the more useful the latent variables are." CreationDate="2015-10-21T00:28:00.517" UserId="7848" />
  <row Id="8191" PostId="8527" Score="0" Text="yea, it was fun, but nowhere as efficient as scikit-learn though (: I would do it again if I am not doing it for my job tho lol" CreationDate="2015-10-21T01:54:49.783" UserId="13562" />
  <row Id="8192" PostId="8477" Score="0" Text="Thanks! I'm getting about 25% accuracy with a model predicting using categorical values. I will see if it increases when I treat them right :)" CreationDate="2015-10-21T05:14:48.150" UserId="13480" />
  <row Id="8194" PostId="8514" Score="0" Text="Can you show us your desired output (the actual values)? Because I fail to both understand you description or your code. Maybe this `apply(df, 1, function(x) c(x[1], x[is.na(x)]))` or maybe `apply(df, 1, function(x) c(x[1], which(is.na(x))))`? Btw, the proper way to convert blanks to `NA` values is just `is.na(df) &lt;- df == &quot;&quot;`" CreationDate="2015-10-21T11:32:32.227" UserId="8479" />
  <row Id="8195" PostId="8542" Score="0" Text="If you are using SVM, then you may be able to use a kernel designed for working with text features. Could you clarify which kernel(s) you have tried with?" CreationDate="2015-10-21T12:55:40.237" UserId="836" />
  <row Id="8196" PostId="6578" Score="0" Text="&quot;most documents look almost the same&quot; - do you mean regardless of organisation, they look the same? Or each organisation's documents look the same and different to other organisations? Is there any trick like looking for the org's logo you could use? Do you know the possible set of org names? Could you just look for that in the text? Do you have a training data set classified with correct orgs?" CreationDate="2015-10-21T13:12:49.527" UserId="471" />
  <row Id="8197" PostId="8542" Score="0" Text="I'm using libsvm, with RBF kernels (because that's what the grid search tool that comes with libsvm uses)." CreationDate="2015-10-21T13:58:26.977" UserId="13596" />
  <row Id="8198" PostId="8544" Score="0" Text="Yep, currently doing just that for string length and a few other things. It helps a lot. &#xA;&#xA;(The most useful feature during training was the class of the previous string (strings occur together in &quot;sentences&quot;). Unfortunately, not very useful for prediction, since a wrong prediction for one  string leads to cascading prediction failure for the next strings)." CreationDate="2015-10-21T14:27:16.907" UserId="13596" />
  <row Id="8200" PostId="8477" Score="0" Text="@intellion I managed to write few regexes to leave categorical values as-is (no feature id) and separated from real values. The performance was not brilliant, ~40% average precision and ~40% average recall. I need to look closer at the numbers though." CreationDate="2015-10-21T15:45:36.690" UserId="7848" />
  <row Id="8202" PostId="8508" Score="0" Text="Hi, thanks for giving a detailed answer. Your first approach (creating new target variable to show which model is the closest to the original label) sounds really interesting. How did you come up with this idea? Did you read about it somewhere? If yes, please provide me with links to the sources, that would be really be helpful.&#xA;&#xA;Also, what does AFAR stands for?" CreationDate="2015-10-22T06:33:44.173" UserId="12377" />
  <row Id="8203" PostId="8508" Score="1" Text="@user2409011 AFAR means 'as far as I recall' :) That approach is called &quot;stacking&quot; and is a typo of an ensemble learning. I suggest a book: http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf" CreationDate="2015-10-22T06:36:10.373" UserId="7969" />
  <row Id="8204" PostId="8508" Score="0" Text=":D I thought AFAR stands for some technique. Anyways, ensemble learning normally means combining models not picking 1 out of a number of models. For ensemble generally people do logistic regression. So that is why asked if you read somewhere about your approach (i.e. picking 1 out of a number of models by creating a new target variable). Does the stanford source specifically deals with your approach?" CreationDate="2015-10-22T06:41:33.583" UserId="12377" />
  <row Id="8205" PostId="8508" Score="0" Text="@user2409011 yes, there is a whole chapter on that just search by keyword 'stacking'. Also keep in mind that ensemble doesn't always mean 'combinig model predictions', there are lots of versions of ensemble learning - bagging, boosting, stacking, blending, bayesian averaging etc. So it is a whole family of different methods, which combine models in SOME way." CreationDate="2015-10-22T06:49:26.923" UserId="7969" />
  <row Id="8206" PostId="8508" Score="1" Text="@user2409011 almost forgot - here is also a good article which has less math - more code and is more accurate in terminology that I was: http://mlwave.com/kaggle-ensembling-guide/" CreationDate="2015-10-22T06:52:48.157" UserId="7969" />
  <row Id="8207" PostId="8548" Score="1" Text="I'm voting to close this question as off-topic because this question belongs on SO" CreationDate="2015-10-22T07:53:10.257" UserId="11097" />
  <row Id="8208" PostId="8508" Score="0" Text="In this second article, which method are you referring to 'voting ensemble' or 'feature weighted linear stacking' or 'Online Stacking' or 'Model Selection'?" CreationDate="2015-10-22T08:05:36.443" UserId="12377" />
  <row Id="8210" PostId="8508" Score="0" Text="It should be some type of 'Stacked Generalization &amp; Blending'." CreationDate="2015-10-22T09:03:40.967" UserId="7969" />
  <row Id="8211" PostId="8554" Score="0" Text="For high-level objects like pianos how deep a network and how many days of training will be needed? Maybe quicker to use the top layer of already trained network such as GoogleNet?" CreationDate="2015-10-22T12:29:00.013" UserId="6550" />
  <row Id="8212" PostId="8543" Score="0" Text="Good suggestion. Using 3- and 4-grams got the accuracy up to ~0.90." CreationDate="2015-10-22T12:59:08.223" UserId="13596" />
  <row Id="8213" PostId="8554" Score="0" Text="Do you have a link to an example / code for reducing dimensionality of images?" CreationDate="2015-10-22T13:01:37.553" UserId="13529" />
  <row Id="8214" PostId="8554" Score="0" Text="It depends on your hardware, of course, Valentas. And sure, there's no harm in starting with a pretrained network. @user2726995: http://deeplearning.net/tutorial/dA.html" CreationDate="2015-10-22T15:03:35.140" UserId="381" />
  <row Id="8216" PostId="8556" Score="0" Text="What did you mean by saying &quot;Because complete linkage is in O(n^3), this approach will not scale to longer videos or higher frame rates.&quot; Does that mean I should not use complete linkage in long videos just for &quot;performance&quot; issues ?" CreationDate="2015-10-22T20:31:04.133" UserId="13606" />
  <row Id="8217" PostId="8450" Score="0" Text="I got tumbleweed badge on on this one." CreationDate="2015-10-22T20:32:39.760" UserId="13285" />
  <row Id="8218" PostId="8556" Score="1" Text="Yes, this approach will not scale to large data because it has cubic complexity." CreationDate="2015-10-22T21:00:12.280" UserId="924" />
  <row Id="8219" PostId="8556" Score="0" Text="If you look at their paper, they decreased the framerate, and used only 680 frames on average (assuming they only used the annotated frames for clustering). At 24 fps, this would be less than 30 seconds of video. O(n^3) means that 10x as many frames will take 1000x as long." CreationDate="2015-10-22T21:30:12.140" UserId="924" />
  <row Id="8220" PostId="8556" Score="0" Text="I think they took every 15th frame.I also did same thing and took every 15th frame this decreased the run time of my code." CreationDate="2015-10-22T21:35:30.233" UserId="13606" />
  <row Id="8221" PostId="8566" Score="0" Text="Excellent answer! Very well-stated." CreationDate="2015-10-22T22:06:04.363" UserId="13413" />
  <row Id="8222" PostId="6567" Score="0" Text="-1: Answers should aim to help the asker, not be a platform for comparing ideas about what should or shouldn't be too difficult! Try to rephrase in a more constructive way!" CreationDate="2015-10-22T22:15:26.790" UserId="13413" />
  <row Id="8223" PostId="8561" Score="2" Text="Two guesses:&#xA;(1) are you shuffling your dataset? Or, are you sampling the data at random for each batch?&#xA;(2) the error starts to raise suspiciously close to epoch 40. Do you change your learning rate or other parameters during training? It might be that there is no significant improvement over say 30 epochs and the logic decides to double/half the learning rate," CreationDate="2015-10-23T00:41:30.470" UserId="7848" />
  <row Id="8224" PostId="8566" Score="0" Text="yea, i was trying to do hierarchical classifying first, but the problem is that if the initial classifier mis-classified a document (roughly 6% chance), then the following result wouldn't make sense anymore lol. I guess I will stick with separating the classifiers, as the more different type of classes are added to the huge, monolithic classifier, parsing the result will be a huge problem..." CreationDate="2015-10-23T02:38:12.777" UserId="13562" />
  <row Id="8225" PostId="8561" Score="0" Text="@xeon thanks, those are really good suggestions! For 2) I'll dig into the source code to see if there is a change that happens at specific intervals. For 1), I don't think the data is shuffled each batch -- should it be?" CreationDate="2015-10-23T03:04:23.963" UserId="12963" />
  <row Id="8226" PostId="8499" Score="1" Text="Quick adds: &#xA;In general&#xA;&#xA;SVM: Less prone to over fitting because of the margin, fast predictions, slower training for large datasets.&#xA;ANN: More prone to over fitting and we need to try out methods as you have mentioned, slower predictions, faster training." CreationDate="2015-10-23T05:06:16.170" UserId="13622" />
  <row Id="8227" PostId="8549" Score="0" Text="when are you having the error? trying to do what?" CreationDate="2015-10-23T06:30:20.420" UserId="5177" />
  <row Id="8228" PostId="6644" Score="0" Text="@NeilSlater - will your example weights work for the outputs that are not 3? I don't see that they will. Please elaborate. Thanks." CreationDate="2015-10-23T06:59:18.913" UserId="13625" />
  <row Id="8229" PostId="8569" Score="0" Text="I'm using a validation set for comparing the different parameter combinations so I'm not cheating as I'm not looking at the test set :) But interesting thought on comparing feature distributions and taking a new sample! How would you do that in practice? What test would you use to decide whether a new sample should be used instead?" CreationDate="2015-10-23T07:41:55.193" UserId="3044" />
  <row Id="8230" PostId="6644" Score="0" Text="@FullStack: Yes it will work, because if $A_3^{old}$ is not active (activation 0), then none of the weights in the example have any impact. You have to construct similar maps for connections from each output neuron in the old layer - each one is associated very simply with its binary representation in the new layer, and they are all independent." CreationDate="2015-10-23T08:31:29.337" UserId="836" />
  <row Id="8233" PostId="8563" Score="1" Text="Hello, first of all, thank you very much for your answer. I see you two major problems: a) if the user is saying yes or not (boolean values) and the duration is a number. How do you compute the correlation between a vector of boolean values and a vector of numbers? b) Training another model excluding the variable has a lot of sense, but we do not know the attribute, and even we do not know if there are two or three non-interesting attributes at the same time, so we should repeat this process many times including all possible combinations." CreationDate="2015-10-23T11:00:22.333" UserId="13618" />
  <row Id="8234" PostId="8569" Score="0" Text="@felbo: If you have continuous-value features you could run KS tests, but I imagine looking at summary statistics (mean, max, etc.) and )comparing them visually using something like `plot(density(dataset1$feature1))`, `points(density(dataset2$feature2))` should be sufficient!`" CreationDate="2015-10-23T11:01:58.903" UserId="13413" />
  <row Id="8235" PostId="8571" Score="0" Text="Note that this implements a set of weights for a linear output layer. In contrast, my answer assumes sigmoid activation in the output layer. Otherwise the two answers are equivalent." CreationDate="2015-10-23T11:21:57.820" UserId="836" />
  <row Id="8236" PostId="1126" Score="0" Text="Averaging is often not the best approach. I would recommend reading http://mlwave.com/kaggle-ensembling-guide/" CreationDate="2015-10-23T12:20:59.137" UserId="3044" />
  <row Id="8237" PostId="1126" Score="0" Text="It might be interesting to extract features from both modalities and apply a single SVM so it would be able to find interactions between text and image features." CreationDate="2015-10-23T12:22:15.093" UserId="3044" />
  <row Id="8238" PostId="8569" Score="0" Text="Wouldn't it be in very rare cases that e.g. the mean/std will be different I have a sample of 5000?" CreationDate="2015-10-23T12:23:21.213" UserId="3044" />
  <row Id="8239" PostId="8569" Score="0" Text="Generally yes, which is why it's important to use a sufficiently large sample size. In practice, it depends on the variance in your features." CreationDate="2015-10-23T13:09:38.940" UserId="13413" />
  <row Id="8240" PostId="8563" Score="1" Text="a) If you believe the dependent variable is normally distributed, a t-test would be appropriate.  I'm also editing my post to rephrase the suggested measure of correlation to the point biserial correlation, which is a special form of the Pearson Correlation." CreationDate="2015-10-23T14:43:04.307" UserId="12306" />
  <row Id="8241" PostId="8563" Score="1" Text="b) you're right that this is a computationally expensive method.  Fortunately if your model is robust, if you remove one variable and it has no measurable effect on performance, you probably don't need to try it in future combinations.  For example if you remove duration and get no change, you can conclude that either it contributed no signal, or that signal is entirely captured by the remaining variables." CreationDate="2015-10-23T14:53:08.000" UserId="12306" />
  <row Id="8242" PostId="8568" Score="0" Text="In general it depends on where you are on the learning curve; if your model has converged for a sample the size of your sample then you should be fine." CreationDate="2015-10-23T15:46:37.973" UserId="381" />
  <row Id="8243" PostId="8568" Score="0" Text="It hasn't converged. The best performing model on 5000 has acc of 0.74, whereas it has 0,76 on the full data." CreationDate="2015-10-23T16:40:30.307" UserId="3044" />
  <row Id="8244" PostId="8549" Score="0" Text="Create an RDD of LabeledPoint. It is not particularly huge, 100K observations x2K feature vector." CreationDate="2015-10-23T17:06:33.593" UserId="13348" />
  <row Id="8245" PostId="8576" Score="0" Text="14g is not a lot??? It's not big data but it is actually a lot!" CreationDate="2015-10-23T18:26:36.640" UserId="5177" />
  <row Id="8246" PostId="8577" Score="1" Text="Depending on what your data looks like, you might be able to simply use the regular expression r'^\d+\s+?\(\S+\.\)$' (see http://rubular.com/r/ZS7vZaR3XD, for an interactive example). Can you post some example rows from your data?" CreationDate="2015-10-23T19:25:57.940" UserId="13413" />
  <row Id="8247" PostId="8577" Score="0" Text="it really is just a line of text with terms separated by a &quot;&lt;&quot; symbol. Each term is a String and corresponds to a feature. The problem is that the number of terms per observation (line of text) is about 2K terms (Strings). Some of those are numeric features, e.g. &quot;width (in.)&quot;, &quot;Height (in.)&quot;, &quot;Capacity&quot; etc, while other terms (Strings) correspond to categorical feature, e.g. &quot;connection type&quot;..etc. If the number of terms per observation is small/manageable, I would just do this manually, but with 2K I think I should use some kind of NER for recognizing numeric features like measurements." CreationDate="2015-10-23T19:44:09.330" UserId="13348" />
  <row Id="8248" PostId="8577" Score="0" Text="I should add that this is needed to avoid numeric features being treated as categorical by something like DictVectorizer() from sickit-learn which would make the data extremely wide and is basically a wrong way of encoding features." CreationDate="2015-10-23T19:48:02.680" UserId="13348" />
  <row Id="8250" PostId="8577" Score="0" Text="Agreed. I suspect NER is overkill for the problem you're working on. You'll likely still have to manually annotate some amount of data to get a good approach working, and there some degree of error you'll have to buy into. From what you've described, it seems like regular expressions is the best approach here, but debugging those is easiest with some sample data." CreationDate="2015-10-23T20:07:46.127" UserId="13413" />
  <row Id="8251" PostId="8577" Score="0" Text="I would do that with regex. Could you post some sample data? You can find numeric and non numeric patterns and look ahead and look behind with regex. For the second time please post some data. OK so then you are already able to extract the terms in the &lt; &gt; ?" CreationDate="2015-10-23T19:11:38.833" UserId="13285" />
  <row Id="8252" PostId="8577" Score="0" Text="I appreciate your help and caring enough to ask again. I can't post any data. However, if you read my comment above to Kyle, you'll know exactly what I'm asking. Or simply answer this: given a line of text consisting of terms separated by a known delimiter ('&lt;'), you can get the individual terms with Split('&lt;'). Then the question is HOW do you find out which of these text terms refer to a numeric feature, e.g. &quot;width&quot; and which refer to a categorical feature, e.g. &quot;connection type&quot;. That's it." CreationDate="2015-10-23T20:02:12.233" UserId="13348" />
  <row Id="8253" PostId="8568" Score="0" Text="I meant to say &quot;size of your _subsample_&quot;. There isn't a big difference between 0.74 and 0.76." CreationDate="2015-10-23T21:15:03.890" UserId="381" />
  <row Id="8255" PostId="8577" Score="0" Text="i thought about that, but... this is NOT an unusual situation, quite the contrary. Many, if not most, machine learning problems have mixed type features, numerical and categorical. So my thinking is that sickit-learn, Spark's MLLib or similar ML libraries MUST have functionality to do this.. I seriously doubt people do this manually." CreationDate="2015-10-23T21:50:30.513" UserId="13348" />
  <row Id="8256" PostId="8577" Score="0" Text="@Kai: I'm not sure what python/sklearn libraries there are for NER, but I suspect using Stanford NER is a good place to start: http://textminingonline.com/how-to-use-stanford-named-entity-recognizer-ner-in-python-nltk-and-other-programming-languages" CreationDate="2015-10-23T22:35:21.673" UserId="13413" />
  <row Id="8257" PostId="8582" Score="0" Text="Can you provide some references for your &quot;general rule for choosing a classifier&quot; statement? I agree that parsimony should be a guiding principle in choosing an ML approach, but I disagree that that means starting with nearest neighbors. For example, linear SVM has a single parameter to optimize ($c$), whereas knn has two (choice of $k$ and the distance metric to use)." CreationDate="2015-10-24T13:07:08.897" UserId="13413" />
  <row Id="8258" PostId="8583" Score="1" Text="You'll need to perform dimension reduction, otherwise you'll not be able to visualize the R^n vector space." CreationDate="2015-10-24T14:29:12.133" UserId="5177" />
  <row Id="8259" PostId="8584" Score="0" Text="I don't think there will be much salvageable information left after projecting from $R^{39}$ down to $R^{3}$ using PCA.  This just isn't realistic nor will it provide any insight.  Further, the vast majority of the variance will thrown away and you will have mashed together 39 dimensions into 3 without any insight into which tiny piece comes from which original dimension.  I would be interested to see an example of successfully deriving actionable visual insight from $PCA(R^{39})==&gt;R^{3}$ if you have one." CreationDate="2015-10-24T18:55:54.933" UserId="9420" />
  <row Id="8260" PostId="8584" Score="2" Text="PCA will allow you compute the principal components of your vector model, so the information are not lost but &quot;synthesized&quot;. Unfortunately there is no other imaginable way to display 39 dimensions on a 2/3 dimension screen. If you wish to analyze correlations between your 39 features, maybe you should consider another visualization technique. I would recommend a scatter plot matrix in this case." CreationDate="2015-10-24T19:08:17.817" UserId="5177" />
  <row Id="8261" PostId="8583" Score="1" Text="For visual analysis, I would focus on the primitive variables and not worry about the derivatives for the time being since you can infer slopes and inflection points so easily visually.  Totally separate from the visualization question: Are the derivatives helping the ANN at all?  It should actually have the ability to learn the derivatives on its own since nth order derivatives are just linear adds and subtracts if the abscissa are uniformly spaced." CreationDate="2015-10-24T19:08:50.157" UserId="9420" />
  <row Id="8263" PostId="8587" Score="0" Text="I'm not quite sure I understand what do you mean by &quot;precise references&quot;. Would you care to explain that please! Your question raises my interests." CreationDate="2015-10-24T21:41:01.643" UserId="5177" />
  <row Id="8264" PostId="6583" Score="0" Text="Not particularly useful because no specific insight was provided" CreationDate="2015-10-25T07:39:10.830" UserId="9123" />
  <row Id="8265" PostId="8588" Score="1" Text="I appreciate your global-to-local answer with keywords, and especially the reference to the [NIST/SEMATECH e-Handbook of Statistical Methods](http://www.itl.nist.gov/div898/handbook/) which was in my reference base; but forgotten" CreationDate="2015-10-25T08:32:22.987" UserId="12527" />
  <row Id="8267" PostId="8589" Score="0" Text="If you think of the HDP, how would *you* imagine this?" CreationDate="2015-10-25T12:19:55.847" UserId="924" />
  <row Id="8269" PostId="8588" Score="0" Text="@LaurentDuval: You are very welcome." CreationDate="2015-10-25T13:16:11.210" UserId="2452" />
  <row Id="8271" PostId="8583" Score="0" Text="&quot;Are the derivatives helping the ANN at all?&quot; - Good question! ... i thought that it could be an extra unuseful information (i mean the derivatives), but in more than 1 scientific paper i've noticed that these are used. Furthermore very often the energy is used, which is nothing more than a squared sum of the values. So even energy should be automatically inferred by ANN, but is used anyway." CreationDate="2015-10-25T17:19:39.337" UserId="13648" />
  <row Id="8276" PostId="8580" Score="0" Text="Ok, thanks for the details. Let me ask the question differently; machine learning is being applied to wide datasets (# of features is large, thousands or more). There MUST be standard tools, libraries etc that people use in order to distinguish between numeric and categorical features .. I doubt people are going through the data manually or even with regular expressions designating the type of each feature... this is an important step before applying your classifier, there's no escaping it, hence I think there must be existing tools to do this. IF not, what do Machine Learning Gurus do?" CreationDate="2015-10-25T23:50:07.630" UserId="13348" />
  <row Id="8277" PostId="8598" Score="1" Text="I think that the answer wants to say: take the pruning depth as another parameter to tweak, i.e. take a tree algorithm with a fixed set of parameters (i.e. depth is fixed) and then let it run over different data sets and compute some function (usually the mean) of the performances over all the data sets and assign this as the total performance of this parameter setting." CreationDate="2015-10-26T09:06:17.797" UserId="9856" />
  <row Id="8278" PostId="8580" Score="0" Text="Yes, there are good standard tools for it, but they are based on input of feature values (many per field), not the arbitrary feature name.  If you do indeed have a list of such values (eg &quot;4.2&quot;...), then it's easy.  If the values are mixed (eg &quot;4.2 in&quot;) then it's also solvable.  But in your examples you give strings that do not contain numbers but rather look like column names.  In same cases these could be written as &quot;length_inches&quot; or &quot;leninches&quot; or even &quot;field 32&quot; - much less solvable." CreationDate="2015-10-26T11:41:23.403" UserId="12363" />
  <row Id="8280" PostId="8580" Score="0" Text="OK, so it appears that this step of recognizing numeric vs categorical is highly application dependent; if the problem space guarantees meaningful col names (&quot;width&quot;) then we can use some NER API, otherwise you have to create some kind of algorithm that looks at the feature **values** to recognize feature type... there is no universal solution. Can you point me t these &quot;standard tools&quot; for recognizing feature type from feature values? as in the example you give where the feature value is a String: &quot;4.2 in.&quot; ? I am inclined to write this myself now, but I still like to see existing solutions." CreationDate="2015-10-26T14:05:55.347" UserId="13348" />
  <row Id="8281" PostId="8610" Score="0" Text="First, why does your co-occurrence matrix have values other than 1 or 0? The resulting matrix should be binary.  Next, the result of Jaccard should be a statistic rather than a matrix e.g. keep reading the wikipedia article that you referenced and it details how to sum the matrix correctly to recover a scalar value.  Could you maybe edit the question and include the two source matrices then the co-occurrence matrix and finally the Jaccard index and the community can try to assess where things stand?  Thanks!" CreationDate="2015-10-26T15:50:48.443" UserId="9420" />
  <row Id="8283" PostId="8592" Score="0" Text="Also, the column layout, presence of delimiter images, font, inter-character/line spacing, text-density. Pre-processing the documents with a document-image understanding framework can provide the basis for extracting these layout features. `Cermine` is an open-source framework, which among other things, extracts meta-data from PDF's." CreationDate="2015-10-26T18:15:40.863" UserId="8630" />
  <row Id="8284" PostId="8580" Score="0" Text="Pandas will do it.  I would preproc out the &quot; in.&quot; though." CreationDate="2015-10-26T19:03:13.030" UserId="12363" />
  <row Id="8285" PostId="5860" Score="1" Text="I'm voting to close this question as off-topic because it is too broad.  Just asking for references in AI is okay. But, asking for which field to start, is completely opinion based and would have millions of answers for that." CreationDate="2015-10-27T04:23:04.793" UserId="11097" />
  <row Id="8286" PostId="8616" Score="0" Text="What is the x axis on your first graph? Position of word in sentence?" CreationDate="2015-10-27T13:49:30.910" UserId="836" />
  <row Id="8287" PostId="8616" Score="0" Text="Yes it is ! Same as the second graph" CreationDate="2015-10-27T14:19:50.060" UserId="10232" />
  <row Id="8288" PostId="2438" Score="0" Text="This pakage is on CRAN now https://cran.rstudio.com/web/packages/analogsea/" CreationDate="2015-10-28T00:28:31.460" UserId="13719" />
  <row Id="8289" PostId="8603" Score="0" Text="Thanks for pointing to mixture models - I thing this could be relevant. Clustering on other site, seems not helpful for my problem, especially K-Means, as the **K** (the number of the main categories) is the sought solution." CreationDate="2015-10-28T08:27:12.700" UserId="10620" />
  <row Id="8292" PostId="8633" Score="0" Text="The only way I currently see is using bigger bloom filter with more acceptable error rate like 1 per million and store the whole filter on ssd storage(it can be easily over 1TB nowdays for affordable price while having great random read/write speed). This at least looks like an implementable approach. Any better ideas?" CreationDate="2015-10-28T14:36:28.590" UserId="13735" />
  <row Id="8293" PostId="8634" Score="0" Text="I am using .NET C# and I found an implementation but I am not following how to use it.  I will study up." CreationDate="2015-10-28T14:41:14.880" UserId="13285" />
  <row Id="8294" PostId="8636" Score="0" Text="Sounds like a good deal, currently reviewing what it offers and it seems to be a perfect fit for this particular task. Is there any idea how much disk space would it take to store 1 billion of 20 bytes hashes inside a set? My dumb estimate would be ~20gb, am I close enough?" CreationDate="2015-10-28T15:38:02.553" UserId="13735" />
  <row Id="8295" PostId="8636" Score="1" Text="Disk space is cheap. Not too sure why this would be a concern. See here for estimating sizes. http://docs.datastax.com/en/cassandra/1.2/cassandra/architecture/architecturePlanningUserData_t.html" CreationDate="2015-10-28T15:53:19.463" UserId="13684" />
  <row Id="8296" PostId="8636" Score="0" Text="Thank you very much @init-random for pointing me to cassandra - a _perfect_ fit" CreationDate="2015-10-28T16:20:39.753" UserId="13735" />
  <row Id="8297" PostId="8604" Score="0" Text="I think it depends on the data. If Y is fairly consistent every day (something like memory usage on a server), then a random sample could be ok. If Y were something like sales, then weekends and holidays will probably be disproportionate to normal weekdays, so a truly random sample would not be valid." CreationDate="2015-10-28T16:51:45.423" UserId="13633" />
  <row Id="8298" PostId="8482" Score="0" Text="Boosting samples bootstrap subsets of your data. So if it samples, if You don't fix set.seed(123) or any other number You will receive different results of sampling which leads to different model results" CreationDate="2015-10-28T18:52:56.070" UserId="5224" />
  <row Id="8299" PostId="8638" Score="1" Text="Lots of details and context is missing. It is unclear what those sequences are and what does it mean to transform.&#xA;A wild wild guess: you have a set of sequences with corresponding output sequences. RNN can learn such transoframation (similar to machine translation, e.g. from english to french)." CreationDate="2015-10-28T20:47:02.327" UserId="7848" />
  <row Id="8300" PostId="8638" Score="0" Text="Thanks for the feedback, add it more info." CreationDate="2015-10-28T20:49:35.917" UserId="12384" />
  <row Id="8301" PostId="8631" Score="0" Text="&quot;I am able to connect to the server and import few rows in Dataframe&quot; - show us your Python code that does that. This is really just a database connection question - you just need to save the table to disk in chunks rather than reading the whole thing into a Python structure, yes?" CreationDate="2015-10-29T00:12:23.100" UserId="471" />
  <row Id="8303" PostId="8631" Score="0" Text="@Spacedman : Yes.&#xA;&#xA;      import odbc&#xA;      import pandas as pd&#xA;      import numpy as np&#xA;&#xA;&#xA;      db = odbc.odbc('Netezza/username/password')&#xA;&#xA;      cursor = db.cursor()&#xA;      script = &quot;&quot;&quot;&#xA;      SELECT * FROM table_server limit 1000000&#xA;      &quot;&quot;&quot;&#xA;      cursor.execute(script)&#xA;&#xA;      list1 = cursor.description&#xA;&#xA;      columns = [desc[0] for desc in cursor.description]&#xA;      data = cursor.fetchall()&#xA;&#xA;      df = pd.DataFrame(list(data), columns=columns)" CreationDate="2015-10-29T04:17:56.343" UserId="13734" />
  <row Id="8304" PostId="8631" Score="0" Text="@Spacedman : Yes.&#xA;&#xA;`      import odbc&#xA;&#xA;      import pandas as pd&#xA;&#xA;      import numpy as np&#xA;&#xA;&#xA;      db = odbc.odbc('Netezza/username/password')&#xA;&#xA;      cursor = db.cursor()&#xA;&#xA;      script = &quot;&quot;&quot;&#xA;      SELECT * FROM table_server limit 1000000&#xA;      &quot;&quot;&quot;&#xA;&#xA;      cursor.execute(script)&#xA;&#xA;      list1 = cursor.description&#xA;&#xA;      columns = [desc[0] for desc in cursor.description]&#xA;      data = cursor.fetchall()&#xA;&#xA;      df = pd.DataFrame(list(data), columns=columns)&#xA;`" CreationDate="2015-10-29T04:24:47.113" UserId="13734" />
  <row Id="8305" PostId="6899" Score="0" Text="Maybe this paper on a theoretically optimal way of combining classifiers for ROC (or papers that cite it) can help you to understand the state of art: M. Barreno, A. Cardenas, J.D. Tygar, Optimal ROC Curve for a Combination of Classifiers, Advances in Neural Information Processing Systems, 2008." CreationDate="2015-10-29T09:23:40.787" UserId="6550" />
  <row Id="8306" PostId="8638" Score="0" Text="Are you wanting to aggregate events by some common features of their &quot;type&quot; (independent of where they appear in the sequence) e.g. you consider `a'` in your example to cover `a` and `b` because they are similar in some way? Or are you wanting to cluster events by when they occur in the sequence e.g. you are combining `a` and `b` *because* they occur together?" CreationDate="2015-10-29T10:09:22.253" UserId="836" />
  <row Id="8307" PostId="8638" Score="0" Text="They occur in sequence. Not because they have any kind of resemblance." CreationDate="2015-10-29T12:58:01.120" UserId="12384" />
  <row Id="8309" PostId="8649" Score="0" Text="@Kyle Thank you" CreationDate="2015-10-29T16:32:11.607" UserId="13767" />
  <row Id="8310" PostId="8603" Score="0" Text="Note that 1) the normal data itself may have more than one distribution 2) the outlier &quot;low volume&quot; class may occur very infrequently compared to the normal data ... as is usually the case in outlier analysis. &#xA;&#xA;These would be important considerations in the suggested approach." CreationDate="2015-10-29T16:37:14.507" UserId="619" />
  <row Id="8311" PostId="8634" Score="0" Text="I have spent some time with this and I just don't get how logistic regression on tf-idf vectors will handicap.   The regression is not aware of cosine similarity.  Can you provide more instructions?" CreationDate="2015-10-29T17:43:38.483" UserId="13285" />
  <row Id="8312" PostId="8634" Score="0" Text="Is there an inherent dependence on cosine similarity in your problem?  If you are simply trying to separate responsive documents from the others, you are attempting a classification problem.  One class is &quot;responsive&quot; and the other is &quot;irrelevant.&quot;  If you label the responsive documents as responsive == 1 and others as not == 0, then your classifier will learn to distinguish between responsive and not based on tf-idf as an input." CreationDate="2015-10-29T18:01:03.403" UserId="12306" />
  <row Id="8314" PostId="8634" Score="0" Text="Yes there is an inherent dependence on cosine similarity.  I am not aware how to make the logistic regression aware of that dependence.  How does my classifier learn?" CreationDate="2015-10-29T18:22:02.090" UserId="13285" />
  <row Id="8315" PostId="8632" Score="1" Text="Are you asking specifically about multivariate logistic regression?  As in you want to perform many classifications at once?  Multivariate _linear_ regression is certainly implemented.  Logistic regression would have to be framed differently to use the sklearn library." CreationDate="2015-10-29T18:34:37.420" UserId="12306" />
  <row Id="8316" PostId="8634" Score="0" Text="I'm really not sure where the requirement to use cosine similarity comes in.  A logistic regression would simply separate out the two classes (which I believed to be your objective)." CreationDate="2015-10-29T18:37:01.817" UserId="12306" />
  <row Id="8317" PostId="8634" Score="0" Text="Why use tf-idf to characterize a document if you are not going to use cosine similarity?" CreationDate="2015-10-29T18:41:14.150" UserId="13285" />
  <row Id="8318" PostId="8634" Score="0" Text="tf-idf vectors are just a way of representing your document for any classification task.  they turn a document into a vector, and vectors form the inputs to many basic classifiers/regressions etc.  Using a tf-idf vector as the input to a logistic regression is a good place to start when trying to classify documents." CreationDate="2015-10-29T18:43:18.563" UserId="12306" />
  <row Id="8319" PostId="8634" Score="0" Text="Thanks for your help but I am not following.  A tf-idf are not just A way of representing.  Tf-idf are a specifically designed for cosine similarity.   In that link the example is hours studied and pass/fail.  I don't see any facility to pass a tf-idf vector and a pass / fail to a logistic regression. Again thanks for trying to help." CreationDate="2015-10-29T19:15:01.753" UserId="13285" />
  <row Id="8320" PostId="8651" Score="0" Text="This fits better into the scope of Manhattan Distance (https://en.wikipedia.org/wiki/Taxicab_geometry), in my opinion." CreationDate="2015-10-29T19:15:45.633" UserId="13413" />
  <row Id="8321" PostId="8634" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/30884/discussion-between-jamesmf-and-frisbee)." CreationDate="2015-10-29T19:16:21.583" UserId="12306" />
  <row Id="8322" PostId="8651" Score="1" Text="The issue with Manhattan Distance, is that while that is a very logical distance to apply to this problem, the question specifically says &quot;As the crow flies.&quot;  https://en.wikipedia.org/wiki/As_the_crow_flies  In this two dimensional representation, this straight line would be measured by Euclidean distance (i.e. a crow can fly over buildings)" CreationDate="2015-10-29T19:22:21.893" UserId="12306" />
  <row Id="8323" PostId="8651" Score="1" Text="Hmm...good point! It's weird to measure crow flights in blocks!" CreationDate="2015-10-29T19:23:59.033" UserId="13413" />
  <row Id="8324" PostId="8646" Score="1" Text="Have you thought about regression instead of classification?  Remember that your classes are ordered and therefore correlated, so you may be losing information representing 2 failures as [0,0,1,0,0,0,0,0].  That example did have one failure as well as 2 failures." CreationDate="2015-10-29T19:48:20.183" UserId="12306" />
  <row Id="8325" PostId="8651" Score="0" Text="@jamesmf. Thanks for your answer. I am wondering how probability is calculated in random walk simulation.  I mean how random walk simulation will be able to answer the 8 questions?" CreationDate="2015-10-29T19:56:23.000" UserId="13767" />
  <row Id="8326" PostId="8339" Score="0" Text="Imagine the case where RF guesses 85/100 correctly and SVM guesses 78/100 correctly.  if the SVM predicts (78+15 =) 93 items the same as RF, but gets the other 7 wrong, it is easy to see how voting would be worse than RF alone (SVM made all the same mistakes and then some).  So it's certainly possible, particularly if one classifier regularized more strongly for example." CreationDate="2015-10-29T20:03:08.150" UserId="12306" />
  <row Id="8327" PostId="8651" Score="0" Text="Run the simulation N times (N is a large number).  Then calculate the proportion of times that each thing occurred (or the average timestep at which each criterion was met)." CreationDate="2015-10-29T20:08:21.480" UserId="12306" />
  <row Id="8328" PostId="8649" Score="0" Text="Are you supposed to answer this mathematically and exactly? Because you've asked in the wrong place. A computer simulation can only give approximate answers." CreationDate="2015-10-29T20:27:05.120" UserId="471" />
  <row Id="8329" PostId="8652" Score="1" Text="Thanks a lot, Matthew" CreationDate="2015-10-29T21:40:17.190" UserId="3024" />
  <row Id="8330" PostId="8651" Score="0" Text="@jamesmf. Thanks again. I will run it and post the code and result here so you can comment on it." CreationDate="2015-10-29T21:52:20.913" UserId="13767" />
  <row Id="8334" PostId="8632" Score="0" Text="Whoops, sorry I misread, I was reading the sklearn.linear_model.LogisticRegression documentation thinking about linear regression. I'll remove my comment to avoid confusing future readers. Thanks!" CreationDate="2015-10-30T03:30:09.200" UserId="843" />
  <row Id="8335" PostId="8631" Score="0" Text="If you are looking for a way to dump data from Oracle into a CSV file on the disk, then Oracle provided tools such as export may be a better option" CreationDate="2015-10-30T06:01:49.820" UserId="13145" />
  <row Id="8336" PostId="8657" Score="0" Text="Good for you for trying to write your own implementation! Try increasing your step size (eta) and checking your error converges. Adaptively reduce the step size if necessary." CreationDate="2015-10-30T07:43:54.243" UserId="381" />
  <row Id="8337" PostId="8651" Score="0" Text="@jamesmf. The post is updated." CreationDate="2015-10-30T13:58:04.977" UserId="13767" />
  <row Id="8338" PostId="8651" Score="0" Text="The code you're running is a random walk, and it's doing a lot of what you want.  You just need variables for each of the questions.  For the first question, you'd need to do a check on each of the first 10 steps seeing if the distance is greater than 3.  then you store that value (0 if they didn't, 1 if they did).  And you can average that over every iteration and that will get you a probability." CreationDate="2015-10-30T14:06:42.537" UserId="12306" />
  <row Id="8339" PostId="8651" Score="0" Text="@jamesmf. I have added the plot" CreationDate="2015-10-30T14:14:04.063" UserId="13767" />
  <row Id="8340" PostId="8651" Score="0" Text="@jamesmf, What does &quot;ever&quot; signify in questions 3 to 6? What is the different between &quot;ever&quot; greater than or equal to 5 blocks and greater than or equal to 5 blocks? In other words, what is the difference between Q2 and Q4" CreationDate="2015-10-30T15:02:17.750" UserId="13767" />
  <row Id="8341" PostId="8644" Score="0" Text="Thank you for the answer. Do you have any general course or book about this precise methodology?" CreationDate="2015-10-30T15:49:39.697" UserId="13754" />
  <row Id="8343" PostId="8651" Score="0" Text="to answer Q2 you need only know where the person is at time t=60.  to answer Q4, you need to keep track of whether or not the person ever got more than 10 blocks away at any t&lt;60" CreationDate="2015-10-30T17:12:43.340" UserId="12306" />
  <row Id="8344" PostId="8657" Score="0" Text="Thanks @Emre for the suggestion.  I tried adaptively updating the step size, but I still don't see the loss decreasing after successive iterations.  Any more thoughts?" CreationDate="2015-10-30T17:30:29.510" UserId="13775" />
  <row Id="8345" PostId="6899" Score="0" Text="@Valentas : Thank you.  This looks very interesting.  I wonder if the fact that they are working with false positive rate rather than PPV makes a difference.  I also wonder if the special structure of my problem (ie no two classifiers say yes to the same piece of data) makes a difference." CreationDate="2015-10-30T19:46:12.807" UserId="12438" />
  <row Id="8346" PostId="8663" Score="0" Text="Thank you for the feedback.  I originally tried to compute the logistic function directly, but I had some trouble with the exponential leading to overflow errors.  I tried to implement Fabian Pedregosa's code (see code and link) considering that he clearly thought a lot about solving that particular problem.&#xA;&#xA;All that said, this has helped me tremendously." CreationDate="2015-10-30T21:14:41.917" UserId="13775" />
  <row Id="8347" PostId="8663" Score="0" Text="Glad it helped. &#xA;&#xA;Yeah, overflow might be a problem. I haven't considered it my self - that's why your code looked so weird to me. But scaling all your features before training might be helpful (subtracting means and potentially dividing by the standard deviation) to get all the features within a tighter range of values centered around zero. By doing this, you may not need a more complicated implementation..." CreationDate="2015-10-30T21:25:53.937" UserId="12580" />
  <row Id="8348" PostId="8666" Score="0" Text="Welcome to the site @ChrisC  :)" CreationDate="2015-10-31T07:16:39.217" UserId="11097" />
  <row Id="8350" PostId="8663" Score="0" Text="The reason to implement phi &quot;in such a complicated way&quot; is to avoid overflows. For example, your implementation will overflow for values of t below - 100. State of the art implementations like the ones found on liblinear use this trick.And the multiplication by y can be done only if the two classes are - 1 and 1, in which it defaults to the same loss you wrote." CreationDate="2015-10-31T07:50:40.827" UserId="13786" />
  <row Id="8351" PostId="8657" Score="0" Text="It would be better if you can be more precise in your questions. What do you mean that the loss doesn't change much? What is the expected outcome and what are you seeing?  It isn't expected to move much once it's in a vicinity of the solution..." CreationDate="2015-10-31T07:53:12.153" UserId="13786" />
  <row Id="8352" PostId="8666" Score="0" Text="Thanks @Dawny33, I see you mention it a lot on CV and followed you here ;)" CreationDate="2015-10-31T13:17:20.470" UserId="9131" />
  <row Id="8353" PostId="8671" Score="0" Text="@Kyle: Thanks for the edit!" CreationDate="2015-10-31T14:02:02.013" UserId="13518" />
  <row Id="8354" PostId="8665" Score="0" Text="Even is you chose statistics you can select classes to prepare for a data science career." CreationDate="2015-10-31T16:17:38.227" UserId="13285" />
  <row Id="8355" PostId="8662" Score="0" Text="Yes, indeed, but my question is about which statistical test is used. Thanks" CreationDate="2015-10-31T17:14:58.437" UserId="3024" />
  <row Id="8363" PostId="8631" Score="0" Text="@Ramana : thanks! but I specifically want to achieve it using python." CreationDate="2015-11-01T08:46:25.847" UserId="13734" />
  <row Id="8366" PostId="8682" Score="0" Text="Which algorithms would be used for concept learning?" CreationDate="2015-11-02T05:54:22.447" UserId="8820" />
  <row Id="8367" PostId="8682" Score="0" Text="This slide: http://ml.informatik.uni-freiburg.de/_media/documents/teaching/ss09/ml/version.pdf outlined a few algorithms for concept learning" CreationDate="2015-11-02T06:04:15.457" UserId="13818" />
  <row Id="8368" PostId="8681" Score="0" Text="I don't think any clustering will return meaningful results on such data. Make sure to *validate* your findings. Also consider implementing an algorithm yourself, and *contributing* it to sklearn. But you can try to use e.g. DBSCAN with dice coefficient or another **distance function for binary/categorial data**." CreationDate="2015-11-02T07:08:31.713" UserId="924" />
  <row Id="8371" PostId="8692" Score="0" Text="Could you clarify your problem, because it is not clear to me exactly what the issue is. I think what is relevant is how you are defining &quot;best&quot; for a company and what data you do have about whether company A is &quot;better&quot; than company &quot;B&quot; - e.g. do you have any ranking data?" CreationDate="2015-11-02T14:00:46.043" UserId="836" />
  <row Id="8372" PostId="8681" Score="1" Text="It is common to convert categorical to numeric in these cases. See here http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html. Doing this you will now only have binary values in your data, so will not have scaling issues with clustering. You can now try a simple k-means." CreationDate="2015-11-02T14:16:41.833" UserId="13684" />
  <row Id="8373" PostId="8639" Score="0" Text="If it is highly skewed data, like finding credit fraud, then yes, generally algorithms have trouble deciphering the signal from the noise. A common technique is to sub-sample the non-spam. So, for example if the spam/non-spam ratio is 10000 to 1 then randomly select a subet of non-spam for training so that you might have a more even 60/40 split." CreationDate="2015-11-02T14:25:31.153" UserId="13684" />
  <row Id="8374" PostId="8650" Score="0" Text="That works great! Thanks." CreationDate="2015-11-02T16:06:11.563" UserId="12384" />
  <row Id="8375" PostId="8692" Score="0" Text="Hi Neil,&#xA;&#xA;Thanks for the quick response&#xA;&#xA;I have list of companies A, B, C, D...&#xA;&#xA;Each company has following variables: social media, website traffic, employee count, revenue of the company etc...., All variables have numeric data.&#xA;&#xA;To Do:&#xA;1. Need to rank the company based on the above variables&#xA;2. Need to find which variables are mostly influencing the company" CreationDate="2015-11-02T16:08:20.310" UserId="13834" />
  <row Id="8376" PostId="8692" Score="0" Text="Rank the companies for what purpose? Is there an order to this rank, what is it supposed to mean? If there is a rank #1 company, does that have a use or purpose? Is there a &quot;better&quot; rank for a company? Or are you  looking to find &quot;similar&quot; companies in the data?" CreationDate="2015-11-02T16:17:42.600" UserId="836" />
  <row Id="8377" PostId="8692" Score="0" Text="When you say find which variables are most influencing the company, influencing in terms of what? If you mean influencing there ranking, that will entirely depend on how you choose to rank them? Or am I missing something." CreationDate="2015-11-02T16:25:47.127" UserId="13630" />
  <row Id="8378" PostId="8140" Score="0" Text="Looks like contextual bandit problem to me. You are trying to come up with the best decision to make at each time moment given only some previous and current observations. You don't have access to some loss function which is required in classic supervised approaches for classification." CreationDate="2015-11-02T16:38:06.017" UserId="7848" />
  <row Id="8379" PostId="8692" Score="0" Text="@Neil, we have thousands of companies list. So we will have to rank them and show in the website with rank to each website. So each website will have unique rank." CreationDate="2015-11-02T16:49:42.577" UserId="13834" />
  <row Id="8380" PostId="8692" Score="0" Text="What do you need to be different about your rank than e.g. sorting by name? Are users searching for companies - if so why not use search relevance? Does &quot;best&quot; in your case mean &quot;most likely to be of interest to current user&quot; or &quot;user will click on the link to the website&quot;?" CreationDate="2015-11-02T16:54:05.453" UserId="836" />
  <row Id="8381" PostId="8692" Score="0" Text="@Daniel, Influencing the rank of the website based on the variables. We dont have any dependent variable to apply a multi regression analysis. DO you suggest any algorithm" CreationDate="2015-11-02T16:56:02.617" UserId="13834" />
  <row Id="8382" PostId="8693" Score="0" Text="@init-ramdom thanks for the ideas. I need to test them carefully and get back to you. The only thing I don't see is how to incorporate the size but anyways mosaic seems to be a good idea" CreationDate="2015-11-02T17:28:21.467" UserId="13778" />
  <row Id="8383" PostId="8692" Score="0" Text="@Neil, We are ranking these companies to find the top performing ones  based on variables. These ranking helps people to look for jobs, information, investments etc.," CreationDate="2015-11-02T17:33:42.980" UserId="13834" />
  <row Id="8384" PostId="8692" Score="0" Text="So do you have data regarding the performance of (at least some of) the companies? That would seem to be critical. You cannot make such data up from other variables with no clue about what &quot;top performing&quot; is, or how to value or rank it. How would you even know when your ML got it right or wrong?" CreationDate="2015-11-02T17:36:53.377" UserId="836" />
  <row Id="8385" PostId="8692" Score="0" Text="We have checked some data performance based on few basic calculations like &quot;scale()&quot; functions to the variables. But That is not a grate performance indicator. What do you suggest as the best approach, then? &#xA;&#xA;Can you please share your email? I can get in touch with you directly." CreationDate="2015-11-02T17:47:29.087" UserId="13834" />
  <row Id="8386" PostId="8692" Score="0" Text="You are thinking in circles. You will not get anywhere trying to generate a numeric index or rank for &quot;performance&quot; until you can define it well enough to measure it and test it, for at least some companies. E.g. if someone on your project has some feel for what the exact ranking should be for 100 of the companies, then you could get them to create that, and use that data to help rank the others by similarity. I do not want to give personal email support, sorry." CreationDate="2015-11-02T17:52:18.060" UserId="836" />
  <row Id="8387" PostId="8665" Score="0" Text="You can consider domain-specific data science programs as well to learn data science + domain knowledge. Take a look at this question:http://datascience.stackexchange.com/questions/8199/domain-specific-data-science-programs" CreationDate="2015-11-02T17:57:47.827" UserId="3151" />
  <row Id="8388" PostId="8692" Score="0" Text="We have data in TB. As you said, we can go with the test(20/80) set. We can say the ranking either as growth of the company in the last duration selected. &#xA;&#xA;In brief, If you select any date range, it should show the list of companies which have more growth in variables(social media, website traffic, employee count, revenue of the company etc)." CreationDate="2015-11-02T18:05:15.613" UserId="13834" />
  <row Id="8389" PostId="8692" Score="0" Text="The algorithm should analyze the variables data and show the companies in order." CreationDate="2015-11-02T18:09:08.590" UserId="13834" />
  <row Id="8390" PostId="8662" Score="0" Text="Thanks @Shawn. Very helpful" CreationDate="2015-11-02T19:22:53.350" UserId="3024" />
  <row Id="8391" PostId="8140" Score="0" Text="The problem seems to be somewhat similar one I have now. But this us little tricky I have an issue in analyzing some data. I have to predict the best company and rank it based on some variables we considered. The issue here is we dont have any dependent variable to do a regression. All variable are independent. So I have done Z-score to all the variable and generated a score for each company. Now i'm considering the score as dependent variable and applied multiple regression analysis between the dependent variable(score generated using z-score) and other independent variables. But the generate" CreationDate="2015-11-02T13:38:24.937" UserId="13834" />
  <row Id="8392" PostId="8295" Score="0" Text="...or you could make the values more granular. e.g. Northeast, mid-atlantic, etc. instead of the 50 states. Is there any sort of domain knowledge you could use to merge certain factors together?" CreationDate="2015-11-02T21:10:16.920" UserId="13684" />
  <row Id="8393" PostId="8694" Score="0" Text="Mahout has a map reduce implementation. https://mahout.apache.org/users/classification/partial-implementation.html. I don't know what implementation you are using, but in theory each tree can be built independently so the full data set should never need to be in memory. you could also try sub-sampling to remove some of the easily classifiable cases to even out your distribution." CreationDate="2015-11-02T21:25:55.780" UserId="13684" />
  <row Id="8394" PostId="8294" Score="1" Text="Look into [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing)" CreationDate="2015-11-03T00:38:05.853" UserId="381" />
  <row Id="8395" PostId="8694" Score="0" Text="@init-random That is good to know and I will have to check that out! Still, I'm looking for more of a theoretical / statistical answer on this one, so that I'll know what's the best approach in general (or the tradeoffs of either choice) when HPC has been exhausted or when you may not have time to go that route." CreationDate="2015-11-03T01:12:30.983" UserId="2723" />
  <row Id="8396" PostId="8694" Score="0" Text="I can't say for certain, but I would be surprised to find the comparison you are looking for in the literature. You are basically comparing different data sets between (1) and (2) _and_ different models. It would be best keep one stationary and then compare the differences in the moving part. I would think (2) would be your best alternative. See here http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Fourth%20Printing.pdf p. 320. You could think of (2) as a test/train split for cross validation and choose the model which generalizes best. I would certainly test with OOB error or cross validation." CreationDate="2015-11-03T02:02:04.947" UserId="13684" />
  <row Id="8397" PostId="2591" Score="0" Text="Using a tree is certainly a way to discretize, i.e. finding break points. Quantiles is another alternative. Not sure why you need to discretize, although it is a valid thing to do, but check your accuracy. You could create the model on this transformed data set. Also not sure what you mean by prior in this context. It might be better to subset on the class which is easy to predict as opposed to &quot;over-sample&quot; on the low volume class, as not to over-fit the model." CreationDate="2015-11-03T02:38:50.653" UserId="13684" />
  <row Id="8398" PostId="1028" Score="0" Text="@AndyBlankertz ISTR = http://www.internetslang.com/ISTR-meaning-definition.asp  ?" CreationDate="2015-11-03T03:15:01.010" UserId="2723" />
  <row Id="8399" PostId="8661" Score="0" Text="Thanks a lot for your sharing. I've gone through these two pieces and like the first one very much. However, it aims at a different problem as what I'm expecting. I might not make that point clearly in my questions. I'm actually working on time series data with fixed time gap for each action. Therefore the variable that consists the key part of the model is invariant in my case. The second piece seems relevant to mine. It mentioned the Cantelli’s inequality that serves as burst detection theorem." CreationDate="2015-11-03T08:29:12.670" UserId="13709" />
  <row Id="8400" PostId="8654" Score="0" Text="The way you explain chaining doesn't seem quite right. Are you appending 6 indicator columns in _feature space_ to each of the 7 datasets and updating their values from the previous model in the chain? Prediction is done in a similar way. This method fails if classes are strongly correlated. This is for multi-label learning, but see here http://www.cs.kent.ac.uk/people/staff/aaf/pub_papers.dir/ICTAI-2013-Goncalves.pdf" CreationDate="2015-11-03T13:12:48.970" UserId="13684" />
  <row Id="8401" PostId="8684" Score="0" Text="Thank you for the input. Yes, I lack knowledge in statistics, and I already started studying it. I'll check the Udacity course." CreationDate="2015-11-03T15:58:41.690" UserId="13754" />
  <row Id="8402" PostId="8700" Score="0" Text="Thanks for the feedback, this is basically the line of thoughts I had. In practice the number of added examples is very small compared to the complete dataset size. I would need to add an AWFUL amount of such small fixes to bias it, I believe. My question to you is how the model's short comings can be fixed then? I cannot just throw some random data (cannot afford to label large number of arbitrary examples) and hope it works for that one fix. Not speaking about other small issues." CreationDate="2015-11-03T16:33:27.183" UserId="7848" />
  <row Id="8403" PostId="8700" Score="0" Text="That is part of the point, it might not get fixed, which should be an expectation. If the misclassification rate is within your pre-defined threshold, then it is an expected error. If the rate is above the threshold you may need to retrain. My advice is expect errors, set a threshold, retrain. It is sometimes possible to automate labeling. I don't know what models you are using, but from the sounds of it a well defined rules engine like Drools sounds like a viable alternative. Your models could help drive the rules." CreationDate="2015-11-03T17:28:14.147" UserId="13684" />
  <row Id="8404" PostId="8295" Score="1" Text="How do you know that all of the categorical values are equally important?What was your methodology... did you do Pearson correlation with the target, lasso regression, decision tree, ...?  How are you assessing feature importance?" CreationDate="2015-11-03T17:45:44.950" UserId="9420" />
  <row Id="8405" PostId="8694" Score="0" Text="in the [vignette of randomunitforest](https://cran.r-project.org/web/packages/randomUniformForest/vignettes/randomUniformForestsOverview.pdf) chapter 5.3 is a piece about incremental learning. It is a useful reading, but only the start. There is little to be found at the moment." CreationDate="2015-11-03T20:26:13.000" UserId="10517" />
  <row Id="8406" PostId="8582" Score="0" Text="I was thinking along the lines of simplicity of the method. Everyone understands NN and is able to implement it if necessary. Linear SVM requires more thought. Additionally NN without the k and by just using euclidean distance has no parameters at all." CreationDate="2015-11-03T23:20:53.730" UserId="12798" />
  <row Id="8408" PostId="8705" Score="1" Text="Interesting!  How would you model the problem statement into a graph?" CreationDate="2015-11-04T05:27:06.423" UserId="11097" />
  <row Id="8410" PostId="8705" Score="0" Text="as a simple example say the nodes and relations are like,                                                   &#xA;&#xA;user_name_1 -&gt; BOUGHT -&gt; product_name_1, &#xA;user_name_1 -&gt; WORK_AT -&gt; company_name_1, &#xA;&#xA;user_name_2 -&gt; BOUGHT -&gt; product_name_1, &#xA;user_name_2 -&gt; WORK_AT -&gt; company_name_1, &#xA;&#xA;user_name_3 -&gt; works_at -&gt; company_name_1, &#xA; &#xA;then recommend &#xA;&#xA;product_name_1 to user_name_3, this can be done using graphs as well as ML right ?" CreationDate="2015-11-04T07:02:32.407" UserId="5091" />
  <row Id="8411" PostId="8705" Score="0" Text="An interesting idea.  But, don't you think it'd be computationally intensive.  The graph can also be modelled as a neural network too.  But still, my intuition on the first glance would be it's computationally intensive(compared to the traditional sparse matrix computations).  But, I might be wrong too." CreationDate="2015-11-04T08:01:12.003" UserId="11097" />
  <row Id="8412" PostId="8705" Score="0" Text="yes that what I need to know. what all are the advantages and disadvantages ?" CreationDate="2015-11-04T08:06:47.573" UserId="5091" />
  <row Id="8413" PostId="8704" Score="0" Text="This explanation may help  https://en.wikipedia.org/wiki/Vector_space_model" CreationDate="2015-11-04T10:28:00.040" UserId="13285" />
  <row Id="8414" PostId="8705" Score="0" Text="I agree with @Dawny33 that using a graph model is computationally intensive. But you ought using or graph algorithms or ML algorithms." CreationDate="2015-11-04T13:39:51.620" UserId="5177" />
  <row Id="8415" PostId="8679" Score="0" Text="Well, thanks, a nice explanation indeed. Can I apply Mahalanobis distance technique in this case  or it is works for multivariate data?" CreationDate="2015-11-04T14:19:51.073" UserId="13132" />
  <row Id="8416" PostId="8680" Score="0" Text="Well, thanks for the response. I am certain that the numbers are not normally distributed, does your method still apply in that context?" CreationDate="2015-11-04T14:32:08.380" UserId="13132" />
  <row Id="8417" PostId="8679" Score="0" Text="You can.  But, it's over-qualified for univariate data.  Just a Boxplot analysis with some set threshold value should do the job for this data." CreationDate="2015-11-04T14:39:07.153" UserId="11097" />
  <row Id="8418" PostId="8719" Score="0" Text="That's a very clever solution jamesmf :) thanks" CreationDate="2015-11-04T15:11:17.470" UserId="13891" />
  <row Id="8419" PostId="8715" Score="0" Text="Is there an inherent need to use Naive Bayes?  the LDA feature space is well-suited to many supervised classifiers" CreationDate="2015-11-04T15:33:19.203" UserId="12306" />
  <row Id="8420" PostId="8679" Score="0" Text="Well, thanks, i got it." CreationDate="2015-11-04T15:50:35.187" UserId="13132" />
  <row Id="8421" PostId="8715" Score="0" Text="@jamesmf No need, but I just assumed it should be a simple starting point to compare classifiers. Ideally, I'd like to use probabilistic models too. I want to compare it with other classifiers like SVM which I don't believe I need to assume the LDA distribution but for NB I am still not sure how to define it." CreationDate="2015-11-04T15:56:50.633" UserId="13890" />
  <row Id="8422" PostId="8720" Score="0" Text="Thanks for your suggestion. However, as I'm learning about NMF, that's the algorithm I have to implement." CreationDate="2015-11-04T17:08:22.800" UserId="13884" />
  <row Id="8423" PostId="8707" Score="0" Text="Can you clarify what kind of categories? Will it need to be able to handle new categories and/or unseen words? The requirements regarding infrequent terms and unseen categories will help the design of the system." CreationDate="2015-11-04T19:27:19.183" UserId="11136" />
  <row Id="8424" PostId="8680" Score="0" Text="Yes, there is always a underlying model that implies predictions about the data.  If we get a new data point, we can make a guess about what that value is.  If we have no other information about the model (it's just a collection of numbers), then the best guess is that it will look similar to the numbers we already have.  If you do end up with a specific model for the data, it's important to incorporate that into your prediction.  Using mean doesn't imply we're assuming normal distribution, but other models could use something other than the mean." CreationDate="2015-11-04T19:52:52.710" UserId="12626" />
  <row Id="8425" PostId="8722" Score="1" Text="This is not a good approach and [here](http://datascience.stackexchange.com/a/8346/5177) is there reason why." CreationDate="2015-11-04T22:05:45.807" UserId="5177" />
  <row Id="8426" PostId="8722" Score="0" Text="Thanks Elisha, even though I don't agree with some points in the link you provided. I would appreciate if you can elaborate more your answer and provide a good approach. Thanks" CreationDate="2015-11-04T22:32:32.600" UserId="13891" />
  <row Id="8427" PostId="8718" Score="0" Text="thank you. yes I am aware of hybrid approaches, but would like to know what all are the key aspects to consider before choosing graph based or ML approach. Yes complexity/dimensional-space reduction is a issue." CreationDate="2015-11-05T03:51:00.777" UserId="5091" />
  <row Id="8428" PostId="8722" Score="0" Text="I agree that that explanation is not entirely relevant (particularly if you scale your input for the whole dataset at once and use appropriate sampling/weighting schemes). But it might still be hard to train that many classifiers. You might consider a multiple regression with your output as 0s and 1s for each label. You can then apply a threshold to your prediction to get a binary vector of labels. This also has limitations, but you get to use all your data instead of having to oversample for each individual classifier" CreationDate="2015-11-05T04:06:44.183" UserId="12306" />
  <row Id="8429" PostId="8707" Score="0" Text="Thanks @NBartley. Unseen words will also be a high probability. The input paras will be user generated content, hence the possibility of new unseen words will be very high. The categories would be defined, but we will need to expand the category list over time. Thanks" CreationDate="2015-11-05T05:57:36.197" UserId="13881" />
  <row Id="8430" PostId="8721" Score="0" Text="Thanks @rushimg. If the categories are closely related, i.e. the para of text that are used as input have a large amount of common words, which of the two approaches  would be better at understanding the context and differentiating between the two?" CreationDate="2015-11-05T06:02:05.570" UserId="13881" />
  <row Id="8431" PostId="8722" Score="0" Text="thanks @jamesmf, this what I am trying to do now, and actually a binary output is not a most in my case, the order of the predicted categories is what I will be looking at. I am just wondering if applying a random forest is more relevant for that scale of problems?" CreationDate="2015-11-05T08:28:58.180" UserId="13891" />
  <row Id="8432" PostId="8715" Score="0" Text="Paul, your problem is a typical multiclass classification problem, you can easily use spark and Mllib to solve it. Just put the data in the proper format of Mllib: did you have a look at: http://spark.apache.org/docs/latest/mllib-naive-bayes.html" CreationDate="2015-11-05T08:33:32.870" UserId="13891" />
  <row Id="8433" PostId="8716" Score="1" Text="I would first join what you can then incrementally try to join in the rest of the data. If you are concerned about mis-typed IDs you could try a string distance https://cran.r-project.org/web/packages/stringdist/stringdist.pdf. This could be problematic is the IDs are incremental instead of a hash type ID. Are there timestamps? If so, you could help narrow the possible join selections. In general, however, your problem is just hard work. Be very precise about your data sets so you can easily retrace your steps and explain what you did." CreationDate="2015-11-05T13:15:23.017" UserId="13684" />
  <row Id="8434" PostId="8716" Score="0" Text="Thanks for the suggestions! Hard work is where I've been focusing so far &quot;Incrementally&quot; going data frame by data frame, reducing &quot;variation&quot; as I go." CreationDate="2015-11-05T14:28:44.160" UserId="2781" />
  <row Id="8435" PostId="8722" Score="1" Text="If it's just an order, then regression is even more applicable.  If you wanted to train 300 models,  you could use random forest.  If you wanted one 300 dimensional output, you might try a regression (or a neural net can do this).  The applicability of a random forest (and decision trees in general) probably depends on the nature of your feature space." CreationDate="2015-11-05T16:16:44.423" UserId="12306" />
  <row Id="8436" PostId="8729" Score="0" Text="Could you expand on what you want to use this feature to do?" CreationDate="2015-11-05T16:18:01.853" UserId="12306" />
  <row Id="8437" PostId="6342" Score="0" Text="If you are coming from matlab, numpy will feel familiar.  Most python data science packages are built off of it.  Pandas can be nice for reading in data (in memory), and scikit-learn has an implementation of many ML algorithms and models." CreationDate="2015-11-05T16:28:34.263" UserId="12306" />
  <row Id="8438" PostId="8721" Score="0" Text="I would use the Doc2Vec model due to the fact that it removes the bag-of-words assumption of the max-ent model. If tf-idf is used as features in the max-ent model then this would also reduce the impact of common words. I think trying out both methods and tweaking them would be the best course of action." CreationDate="2015-11-05T19:58:53.830" UserId="13896" />
  <row Id="8439" PostId="8727" Score="0" Text="What would be the output?" CreationDate="2015-11-05T20:06:22.630" UserId="924" />
  <row Id="8440" PostId="8017" Score="0" Text="A few days later, not much more... @Seyed Mohammad still waiting for updates?" CreationDate="2015-11-05T22:51:52.717" UserId="12527" />
  <row Id="8441" PostId="8667" Score="0" Text="How do you recognize an outlier on this small set? How would you do &quot;by hand&quot; on slightly bigger data?" CreationDate="2015-11-05T23:02:42.463" UserId="12527" />
  <row Id="8442" PostId="8727" Score="0" Text="The output is the cluster information. For example, suppose that there are 2 clusters 1 and 2. Then MOA can output `(1,15,25,1)` after inputting first instance (point) `(1,15,25)`. Note that the cluster that a point belongs to might change when the point is updated. For example, after inputting updated point `(1,15,26)` MOA can return `(1,15,26,2)`" CreationDate="2015-11-06T07:22:05.103" UserId="9310" />
  <row Id="8443" PostId="8727" Score="0" Text="For k-means that may be okay (you would still want to know *something* on the cluster), but what if you have less trivial cluster models?" CreationDate="2015-11-06T07:25:30.290" UserId="924" />
  <row Id="8444" PostId="424" Score="0" Text="If you want to find their explanation, I believe there was a paper on the underlying method called Google Sets." CreationDate="2015-11-06T16:39:09.177" UserId="12306" />
  <row Id="8445" PostId="8741" Score="0" Text="and pay 6 figures for if you want to deploy it in an enterprise" CreationDate="2015-11-06T18:29:38.363" UserId="710" />
  <row Id="8446" PostId="8744" Score="0" Text="What about a test document that has no similar training points?  Clearly you need a representative training set." CreationDate="2015-11-06T20:51:50.800" UserId="13285" />
  <row Id="8447" PostId="8745" Score="3" Text="Isn't this just like binary classification? 0 for first item equal or greater than the second item, otherwise 1? Why can't you do a binary (or k=3) classification?" CreationDate="2015-11-07T02:51:33.987" UserId="9123" />
  <row Id="8448" PostId="8744" Score="0" Text="I don't quite follow the question. Naive Bayes is simply a concept of breaking the conditional probability due to independence. It's distribution independent. You can do it with our MLE if you wish." CreationDate="2015-11-07T02:54:50.950" UserId="9123" />
  <row Id="8449" PostId="8745" Score="0" Text="I'm not saying you can't, but implicitly you're measuring how one piece of data fits in with all the other data the algorithm has seen. Perhaps you only care about the difference between two pieces of data. I guess you could derive some features using proportions and differences. But already that's different from simply doing binary classification. And what if I know that in a given pair exactly one of them is 1 and the other is 0? Perhaps the way I'm paring the data matters because the two pieces of data aren't independently drawn." CreationDate="2015-11-07T02:57:50.187" UserId="9768" />
  <row Id="8450" PostId="8614" Score="0" Text="Thank you so much, that are good resources, they helped a lot." CreationDate="2015-11-07T11:20:08.003" UserId="9771" />
  <row Id="8453" PostId="8745" Score="0" Text="What would you do with this direct comparison?  What would greater than mean?" CreationDate="2015-11-07T16:44:24.623" UserId="13285" />
  <row Id="8454" PostId="8747" Score="0" Text="thank you for the answer. It gives a nice overview of the state of the organization at specific decades. What I would like to see that the GANTT like diagrams do not provide, is to also group/cluster the individuals according to the background areas they have in their CV. I will try to explain it better in my question" CreationDate="2015-11-07T17:43:36.633" UserId="13778" />
  <row Id="8455" PostId="8741" Score="0" Text="@IharS. Thank you for the answer. It gives a very good overview of the development over time. Ideally, I would also like to have the individuals grouped according to similarity of their backgrounds" CreationDate="2015-11-07T17:45:41.417" UserId="13778" />
  <row Id="8456" PostId="8747" Score="0" Text="Instead of having a single y-axis of persons you could give each decade its own persons axis or drop person axis.  You can sort the records in each decade by the skills as well get some clustering. One way this can be done in Mathematica is using `Manipulate` so you get interactive clustering." CreationDate="2015-11-07T18:15:30.743" UserId="10814" />
  <row Id="8457" PostId="8741" Score="0" Text="@hrbrmstr the same you can get in any BI tool. I've just shown the concept" CreationDate="2015-11-07T18:24:28.873" UserId="97" />
  <row Id="8458" PostId="8017" Score="0" Text="Actually I forgot about it! ... Since it seems that no body else has anything to add, so I'm marking this as the answer." CreationDate="2015-11-07T19:15:53.750" UserId="10181" />
  <row Id="8460" PostId="8017" Score="0" Text="I'll update if I found some more. Best" CreationDate="2015-11-07T20:43:20.093" UserId="12527" />
  <row Id="8461" PostId="8744" Score="0" Text="Yes it's distribution independent. But we may need to assume a distribution to perform MLE. The distribution can be any arbitrary distribution, though." CreationDate="2015-11-07T22:24:55.757" UserId="13943" />
  <row Id="8462" PostId="8747" Score="0" Text="@YannisP. I've added an interactive example that clusters by decade by background. It actually may come in handy for me as I am about to look at something similar soon at the office." CreationDate="2015-11-08T13:12:53.763" UserId="10814" />
  <row Id="8464" PostId="8747" Score="0" Text="I 'll see if I can get my hands into a copy of Mathematica and try it, cause I have never used it" CreationDate="2015-11-08T15:04:50.093" UserId="13778" />
  <row Id="8465" PostId="8743" Score="0" Text="argh, i meant character n-gram, for instance &quot;I like dosg&quot; becomes `[&quot; I &quot;, &quot; li&quot;, &quot;lik&quot;, &quot;ike&quot;, &quot;ke &quot;, &quot; do&quot;, &quot;dos&quot;, &quot;osg&quot;, &quot;sg &quot;]`" CreationDate="2015-11-09T01:49:26.723" UserId="13562" />
  <row Id="8466" PostId="8743" Score="0" Text="That makes sense. That is a good feature space, though the best solution might be to use a default dictionary and map unknown (i.e. misspelled) terms to the nearest (edit distance) valid term. Of course that isn't always perfect." CreationDate="2015-11-09T04:48:44.860" UserId="12306" />
  <row Id="8469" PostId="8743" Score="0" Text="ooooo, yea, I am trying this character 3-gram with my prototype, the result looks interesting, but it is a lot slower than using unigram word model (:" CreationDate="2015-11-09T05:28:50.267" UserId="13562" />
  <row Id="8471" PostId="8763" Score="0" Text="Exact so do you have any idea ? Maybe one of the answers above ?" CreationDate="2015-11-09T12:22:35.927" UserId="13915" />
  <row Id="8472" PostId="8766" Score="0" Text="Can you explain a bit more?" CreationDate="2015-11-09T12:23:08.813" UserId="13915" />
  <row Id="8474" PostId="8763" Score="0" Text="What is a *good* metric depends on the problem you're trying to solve. You could for instance use one of the geoIP services to get a geolocation for every IP address - but we can't tell you whether that's good for you because we don't know what your problem is." CreationDate="2015-11-09T12:36:52.777" UserId="13296" />
  <row Id="8475" PostId="8762" Score="1" Text="To you what would make two IP addresses similar?" CreationDate="2015-11-09T13:04:49.203" UserId="13285" />
  <row Id="8476" PostId="8766" Score="0" Text="@marcL , I was on my phone.  Check the update." CreationDate="2015-11-09T13:11:01.313" UserId="10814" />
  <row Id="8477" PostId="8743" Score="0" Text="That would be the advantage of defining a dictionary first - you could use a unigram model with the preprocessing step of evaluating which tokens are a small distance away from an existing term. Most NLP packages include a dictionary." CreationDate="2015-11-09T13:13:52.350" UserId="12306" />
  <row Id="8478" PostId="8766" Score="1" Text="So you are assuming that Sim(&quot;192.168.1.1&quot;, &quot;192.168.1.2&quot;) == Sim(&quot;191.168.1.1&quot;, &quot;192.168.1.1&quot;)?" CreationDate="2015-11-09T13:53:01.597" UserId="12909" />
  <row Id="8479" PostId="8766" Score="0" Text="Correct. They are equally similar to one another with this measure. But I see the issue here with the hierarchical nature of the IP addresses.  Let me think about that." CreationDate="2015-11-09T14:01:18.397" UserId="10814" />
  <row Id="8480" PostId="8766" Score="0" Text="@JérémieClos So I'm thinking that each element of the vector could be non-linearly scaled before computing the norm of the distance vector.  This would give more weight to elements higher in the hierarchy." CreationDate="2015-11-09T14:08:16.943" UserId="10814" />
  <row Id="8482" PostId="8766" Score="0" Text="That's what I was thinking too, but I didn't really take the time to formulate it well so I went with a simpler solution. Decomposing the similarity into a logarithmically weighted sum of pointwise similarities might do the job better (where each component is divided by log(n) where n is the position of the component).&#xA;&#xA;Then sim(aaa.bbb.ccc.ddd, eee.fff.ggg.hhh) = sim(aaa,eee)/$log_2$(2)  + sim(bbb,fff)/$log_2$(3) + sim(ccc,ggg)/$log_2$(4) + sim(ddd,hhh)/$log_2$(5)" CreationDate="2015-11-09T15:21:20.860" UserId="12909" />
  <row Id="8483" PostId="8766" Score="0" Text="My bad, I didn't see that you updated your answer with something similar." CreationDate="2015-11-09T15:34:32.847" UserId="12909" />
  <row Id="8484" PostId="8767" Score="0" Text="Very nice. Thanks for critique. I obviously need to brush up on my Linear Algebra. ;-)" CreationDate="2015-11-09T21:12:48.953" UserId="10814" />
  <row Id="8485" PostId="8767" Score="0" Text="As I mentioned, eventually one could introduce a more complex &quot;IP space-time&quot; by considering also non-vanishing off-diagonal elements in $g$. This I guess would depend on the problem he wants to solve, i.e., on the data. However, my first guess is that it easily is overkill. At least one should first try with a diagonal form as in the example above. In such case, there wouldn't be any need to dust-off your LA books, ;-) just following the example, which is, btw, like a problem of vectors in 2 dimensions." CreationDate="2015-11-09T21:24:54.820" UserId="12603" />
  <row Id="8486" PostId="8762" Score="0" Text="You could use a GeoIP lookup and literally compare the distance on the earth between them...." CreationDate="2015-11-09T21:25:54.063" UserId="471" />
  <row Id="8487" PostId="8762" Score="0" Text="@Spacedman That only works for WWW public IP.  OP has not even provided that information." CreationDate="2015-11-09T21:43:12.100" UserId="13285" />
  <row Id="8488" PostId="8762" Score="0" Text="Yes, I was hoping some clarification would be forthcoming, instead of everyone piling in with &quot;answers&quot;... @marcL?" CreationDate="2015-11-09T21:53:55.437" UserId="471" />
  <row Id="8489" PostId="5967" Score="0" Text="The `-p` flag means that you don't have to create the parent directory first." CreationDate="2015-11-09T22:26:47.487" UserId="12626" />
  <row Id="8490" PostId="8764" Score="0" Text="So, if both addresses are the same you code assigns a distance of 1, not zero. Is that a typo or it does this 1 have a special meaning? (You could make it return a zero at that point so as to avoid a NAN at the return at the bottom.)" CreationDate="2015-11-09T22:48:45.580" UserId="12603" />
  <row Id="8491" PostId="8767" Score="0" Text="It's *not* a fourudimensional space. It's a 32 dimensional space. The dot-representation is just easier to remember." CreationDate="2015-11-09T23:36:09.960" UserId="924" />
  <row Id="8492" PostId="8769" Score="0" Text="Thanks! I'm still not 100% convinced: &quot;Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy.&quot; Now, of course &quot;combinations of the variables&quot; means new features, basically learned. I agree that this is a discussion about terminology, but I'm trying to learn what's what." CreationDate="2015-11-09T23:41:45.100" UserId="14009" />
  <row Id="8493" PostId="8764" Score="0" Text="An IP is *one* 32-bit number." CreationDate="2015-11-09T23:50:10.277" UserId="924" />
  <row Id="8494" PostId="8757" Score="1" Text="Does &quot;my dissimilarity measure does not exists sometimes&quot; mean that you have missing data ro the dissimilarity is zero?" CreationDate="2015-11-09T23:53:31.783" UserId="8878" />
  <row Id="8495" PostId="8769" Score="0" Text="Feature extraction is more applying some pre-defined rule/function on data and getting those features. Feature learning is more task-oriented, e.g. you learn features to recognize digits as your ultimate goal. Both are similar in a sense that they are just a mean to get a learning algorithm clues to do its predictions." CreationDate="2015-11-10T00:27:46.553" UserId="7848" />
  <row Id="8496" PostId="8767" Score="0" Text="@Anony-Mousse You are right, there is this other way of looking at it, namely bit-wise. What I propose is independent of how one describes the &quot;IP-space, i.e., whether 4 or 32-dimensional. That said, without other info on the OP's problem, it's seems reasonable to distinguish the usual 4 numbers of an IP and say that 191.0.0.0 and 192.0.0.0 differ more than 0.0.0.0 and 0.0.0.1." CreationDate="2015-11-10T00:33:06.663" UserId="12603" />
  <row Id="8497" PostId="8771" Score="0" Text="I hadn't seen your answer before I replied to your comment on mine. I understand now what you meant. Simple di/similarity measures on the IP's won't help one to distinguish any sort of geographical location. Maybe not even any other kind of useful measure of proximity -organizations have IP ranges assigned at ad-hoc, such that make a distance function superfluous/meaningless." CreationDate="2015-11-10T00:41:17.817" UserId="12603" />
  <row Id="8498" PostId="8764" Score="0" Text="@MASL This is a similarity metric, I should have precised it. 1 means complete similarity here." CreationDate="2015-11-10T01:16:05.667" UserId="12909" />
  <row Id="8499" PostId="8764" Score="0" Text="Also I am probably abusing the word &quot;metric&quot; here,  forgive me for that." CreationDate="2015-11-10T01:22:33.773" UserId="12909" />
  <row Id="8500" PostId="8773" Score="2" Text="Aargh!  You beat me to it  :)&#xA;&#xA;Indeed, TensorFlow is super good. Has a better compile time than the big guns out there like Theano, Torch, etc" CreationDate="2015-11-10T06:31:27.433" UserId="11097" />
  <row Id="8501" PostId="8772" Score="0" Text="What does &quot;every combination of numbers from left to right&quot; even mean? Why isn't `[3,5]` in your first list? Its one of the combinations of those numbers and it goes from left to right. Sure, it skips 4, but maybe you have another meaning of &quot;left to right&quot;. If whatever it is you are doing (partitioning?) is always going to result in a triangle of numbers like in your two examples, then the number of elements in the triangle will be, given N elements in the list, `(N-1)*N/2`. You don't even have to generate the partitions to count them. If your list is 1000 numbers long, you get 1000*999/2" CreationDate="2015-11-10T08:10:16.233" UserId="471" />
  <row Id="8502" PostId="8680" Score="0" Text="Hello. I usually don't find that's a good idea to use average for non normal distributed data. In fact, for example, many non parametric statistical tests are using median instead of mean. But that's only an opinion..." CreationDate="2015-11-10T08:11:12.900" UserId="3024" />
  <row Id="8503" PostId="8757" Score="0" Text="The dissimilarity is a function between two points that is sometimes not defined. There is no reasonable value to give." CreationDate="2015-11-10T12:48:05.143" UserId="13978" />
  <row Id="8504" PostId="8762" Score="0" Text="How they are similar in terms of network not based on earth distance. Sorry for the delay" CreationDate="2015-11-10T14:05:47.140" UserId="13915" />
  <row Id="8510" PostId="1177" Score="0" Text="Do you want to combine your features (vector dimensions) with nonlinear weights (like a squaring to give greater weight to larger values)? Or do you just want a linear combo? When you say &quot;values 10..22&quot;, are 10 and 22 'values' of features or indexes to elements of your vector? Linear combinations are typically computed with just a vector (inner) product of your weights with the vectors your want to score. It turns out this product (a scalar) is proportional to the angle between your weight vector and each feature vector, so it's easy to explain/justify your weight choices." CreationDate="2015-11-10T18:53:11.543" UserId="14031" />
  <row Id="8511" PostId="1180" Score="0" Text="It's trivial to convert a similarity to a distance. Just invert (`d=1/s`) or normalize (`0&lt;s&lt;1`) and subtract from 1 (`d=1-s`). That's why &quot;distance&quot; and &quot;similarity&quot; are used interchangably in books like PCI, because they are interchangeable ;)" CreationDate="2015-11-10T18:59:12.390" UserId="14031" />
  <row Id="8512" PostId="8762" Score="0" Text="What to you makes an IP similar in terms of a network?  Is this a public (WWW) network?" CreationDate="2015-11-10T19:27:23.860" UserId="13285" />
  <row Id="8513" PostId="8787" Score="0" Text="Excellent. Thank you! :)" CreationDate="2015-11-10T20:05:12.933" UserId="14029" />
  <row Id="8514" PostId="8680" Score="0" Text="Fair enough, but I think it's safe to say if you believe your data is distributed symmetrically, mean is a pretty good idea, otherwise median is a better measure of a 'typical' value." CreationDate="2015-11-10T22:06:07.707" UserId="12626" />
  <row Id="8515" PostId="8791" Score="0" Text="Where did `[.95, .45, .6]` come from? The way I defined the problem, the result is already normalized. Your solution works for the example I gave - thanks for that - but it does not generalize to when there are more than 2 minerals (I revised my question to make this more clear)" CreationDate="2015-11-10T23:32:56.060" UserId="10683" />
  <row Id="8516" PostId="8790" Score="0" Text="This seems reasonable, I will give this a try. However my feeling is that it may find solutions that are impossible, such as a negative amount of a mineral, and also that I may get better performance from a model that is built to constrain the sum to 1 because the search space will be smaller." CreationDate="2015-11-10T23:41:31.447" UserId="10683" />
  <row Id="8517" PostId="8770" Score="1" Text="Looks interesting. I only glanced at the link, but the types of models mentioned would be highly useful. Is it a regular C library you can use in a program though? I'll have to investigate further." CreationDate="2015-11-11T00:06:22.510" UserId="2723" />
  <row Id="8518" PostId="6799" Score="0" Text="Awesome! Thank you. I only wish they were for plain C too, but still helpful." CreationDate="2015-11-11T00:06:44.367" UserId="2723" />
  <row Id="8519" PostId="5361" Score="0" Text="I know what you mean about the overwhelming number of tools! I tell my intern not to be distracted and to focus on just 1 or 2 things, but it's hard to take my own advice." CreationDate="2015-11-11T00:09:10.247" UserId="2723" />
  <row Id="8520" PostId="6800" Score="0" Text="Another highly useful answer. Do you know if we are allowed to freely reproduce that image (in case i want to put it in a presentation or blog, etc)? Also, when it says things like &quot;&gt; 20k samples&quot; I wonder if it really means &quot;samples&quot; or &quot;observations in your sample&quot;?" CreationDate="2015-11-11T00:11:08.853" UserId="2723" />
  <row Id="8521" PostId="5997" Score="0" Text="Very good links, specially so the first one which looks more detailed and thorough. When searching for year (a string) and returning the record id (an int), potgresql is about 4x faster, but when returning author, the order of magnitude is the same. MongoDB is only about 20% slower when returning author. Is there a fundamental difference between returning a int and returning a string that could explain this? That is, if recid were a string, would the advantage of postgresql vanish and both be around the same as in the case of author?" CreationDate="2015-11-11T00:40:09.017" UserId="12603" />
  <row Id="8522" PostId="8743" Score="0" Text="hmm, i don't see one for scikit-learn unfortunately" CreationDate="2015-11-11T01:46:11.693" UserId="13562" />
  <row Id="8523" PostId="8789" Score="0" Text="https://en.m.wikipedia.org/wiki/Non-negative_least_squares I haven't thought too hard about it but this seems more intuitive to me than lasso." CreationDate="2015-11-11T03:03:25.283" UserId="12306" />
  <row Id="8524" PostId="8743" Score="0" Text="I use PyEnchant for an english dictionary" CreationDate="2015-11-11T03:03:58.263" UserId="12306" />
  <row Id="8525" PostId="8743" Score="0" Text="THANKS! checking it to see if it is useful (: (unfortunately my work is not for english data lol)" CreationDate="2015-11-11T03:12:34.987" UserId="13562" />
  <row Id="8526" PostId="8743" Score="0" Text="Should also have other languages, http://pythonhosted.org/pyenchant/tutorial.html" CreationDate="2015-11-11T04:04:31.837" UserId="12306" />
  <row Id="8527" PostId="8786" Score="0" Text="Can you comment the link under my comment here?  I shall edit and add it to your answer" CreationDate="2015-11-11T04:42:16.597" UserId="11097" />
  <row Id="8528" PostId="8743" Score="0" Text="oh, found this, should be good as my document is multi-lingual :S http://pythonhosted.org/pyenchant/tutorial.html#personal-word-lists" CreationDate="2015-11-11T07:39:24.773" UserId="13562" />
  <row Id="8529" PostId="8795" Score="0" Text="Thanks for the thorough explanation!" CreationDate="2015-11-11T07:51:19.737" UserId="13625" />
  <row Id="8530" PostId="8762" Score="0" Text="You can't tell the network similarity without the netmask value.... Again, people are piling in with answers when the question is unclear." CreationDate="2015-11-11T09:57:16.387" UserId="471" />
  <row Id="8531" PostId="8791" Score="0" Text="This is the result of `0.50m1+1.50m2`. And yes, I know it can't be what you are looking for; I post it only to *sharp* your question. Good luck!" CreationDate="2015-11-11T10:44:53.887" UserId="10620" />
  <row Id="8532" PostId="6768" Score="0" Text="Very possible that the EEG data isn't separable, but have you inspected the training v test sets to ensure they have aren't biased (e.g. one has only positive examples or is normalized differently)?" CreationDate="2015-11-11T13:05:03.950" UserId="12306" />
  <row Id="8534" PostId="8799" Score="0" Text="I don't have an answer myself but [this question](http://stats.stackexchange.com/questions/13237/som-clustering-for-nominal-circular-variables) seems to be related." CreationDate="2015-11-11T14:36:40.790" UserId="12909" />
  <row Id="8535" PostId="8799" Score="0" Text="Can you pass a function or method for calculating the difference?  Min of (larger - smaller), (smaller + 24 - larger)" CreationDate="2015-11-11T14:44:36.440" UserId="13285" />
  <row Id="8536" PostId="8786" Score="0" Text="Now that my answer was upvoted, I now have the required rep and was able to add the links. Thanks though!" CreationDate="2015-11-11T15:04:59.773" UserId="14030" />
  <row Id="8537" PostId="8799" Score="0" Text="I'm using R's kmeans, I don't see such an option. What languages/packages would allow me to do that?" CreationDate="2015-11-11T15:42:14.743" UserId="14044" />
  <row Id="8538" PostId="8799" Score="0" Text="The solution's in Jeremie's link. Thank you!" CreationDate="2015-11-11T15:55:15.343" UserId="14044" />
  <row Id="8539" PostId="8804" Score="0" Text="[Here](https://github.com/bread-tan/canopyClusteringPython)'s a Hadoop-based python implementation (not tested)." CreationDate="2015-11-11T18:47:26.717" UserId="381" />
  <row Id="8540" PostId="8807" Score="0" Text="Ok. And what if my `adjusted R squared` was `0.9` with a statistical significance of, for instance, `0.09`? Such an high `R` means that the two datasets are very well described by the current model but that... there is strong evidence in favour of `R=0`? Isn't this a contradiction? I am perhaps doing wrong in seeing the `p-value` as a confirmation parameter..." CreationDate="2015-11-11T20:50:02.147" UserId="13481" />
  <row Id="8541" PostId="8793" Score="0" Text="Thanks for that. I've actually alraedy come across that quote. That's what I meant by &quot;arcane&quot;. This is an assertion. I'm trying to understand what's the differences between the too approaches and when they might be expressed." CreationDate="2015-11-11T20:54:09.333" UserId="7999" />
  <row Id="8543" PostId="4888" Score="2" Text="A beautiful explanation this is!" CreationDate="2015-11-12T05:26:19.133" UserId="11097" />
  <row Id="8545" PostId="8812" Score="0" Text="Thanks. How is this loss function different from binary:logistic for binary classification?" CreationDate="2015-11-12T07:30:49.610" UserId="1151" />
  <row Id="8546" PostId="8812" Score="1" Text="It's just a generalization of logistic function for multi-class case, should be no significant difference." CreationDate="2015-11-12T07:39:18.090" UserId="14059" />
  <row Id="8547" PostId="8032" Score="0" Text="Doesn't it output probabilities by default with the settings you used? I mean: have you examined pred_s and you are certain those are not probabilities?" CreationDate="2015-09-08T11:58:01.303" UserId="10936" />
  <row Id="8548" PostId="8032" Score="0" Text="No its having negative values. Probability should vary between 0 to 1." CreationDate="2015-09-08T12:05:08.043" UserId="1151" />
  <row Id="8549" PostId="8808" Score="0" Text="Thanks, very thorough and competent. I will clarify one aspect, though: I used `adjusted R squared` as my sample size is limited. In these videos (https://www.youtube.com/watch?v=VEQPX6d-EQw) it is being said that it is meaningful to use the adjusted version as long as the sample size does not reach 100." CreationDate="2015-11-12T08:38:36.260" UserId="13481" />
  <row Id="8550" PostId="8804" Score="0" Text="You should email the very nice scikit-learn mailing list." CreationDate="2015-11-12T11:02:40.157" UserId="910" />
  <row Id="8555" PostId="8816" Score="1" Text="Can the `Box-Cox Transformation` be categorized as an `exponential transformation`?" CreationDate="2015-11-12T14:47:18.610" UserId="13481" />
  <row Id="8556" PostId="8816" Score="1" Text="@FrancescoCastellani Yes, Box-Cox is also an option.  I have edited my answer to include how it fits in, along with a quick implementation in R.  (Sorry, I'm not experienced at SPSS)" CreationDate="2015-11-12T15:07:32.357" UserId="11097" />
  <row Id="8557" PostId="8811" Score="1" Text="Minor suggestion to check the F1 score also, since your binary classes might not be balanced." CreationDate="2015-11-12T15:14:38.510" UserId="9420" />
  <row Id="8559" PostId="8811" Score="1" Text="yes the binary classes are heavily skewed, so using undersampling / oversampling to mitigate it" CreationDate="2015-11-12T15:33:35.657" UserId="14057" />
  <row Id="8560" PostId="8817" Score="0" Text="I've added some information on the variable names I'm using to my question." CreationDate="2015-11-12T16:09:39.750" UserId="14053" />
  <row Id="8561" PostId="8817" Score="0" Text="OK, I think your $\delta_i$ etc are my $\frac{\partial J}{\partial z_i}$ - i.e. the gradients pre-activation-function. Whilst my answer assumes they are the matching $\frac{\partial J}{\partial a_j}$ . . . I will check and correct later. The main thrust of the answer is the same." CreationDate="2015-11-12T16:20:26.597" UserId="836" />
  <row Id="8562" PostId="8808" Score="0" Text="Good luck. I don't think that it's best to consider the size of your sample, but rather against the number of other parameters in the model. Anyway, [here is](http://stats.stackexchange.com/questions/34312/whether-to-use-r-square-or-adjusted-r-square-with-a-small-sample-size-that-may-r) a relevant question and some answers that might provoke some insights." CreationDate="2015-11-12T16:39:58.450" UserId="10663" />
  <row Id="8563" PostId="8811" Score="1" Text="But using accuracy to score highly skewed classes is incorrect.  Consider a rare cancer that occurs in 1/10,000 people.  I could design a test that always returns &quot;No Cancer&quot; and it would be 99.99 % accurate.  Read about `precision` and `recall` and how `F1-score` is the harmonic mean of those two metrics." CreationDate="2015-11-12T17:10:32.730" UserId="9420" />
  <row Id="8566" PostId="8817" Score="0" Text="To be more precise, my $\delta_i$ are your $-\frac{\delta J}{\delta z_i}$." CreationDate="2015-11-12T17:27:42.353" UserId="14053" />
  <row Id="8567" PostId="8811" Score="0" Text="sorry for not giving all results. yes precision, recall and f-score is quite low for minority, i.e crime. its 0.81,0.62 and 0.70 respectively" CreationDate="2015-11-12T18:07:52.533" UserId="14057" />
  <row Id="8569" PostId="8817" Score="0" Text="@mcb: OK, I think I am done updating the answer to better match your terms." CreationDate="2015-11-12T19:42:58.993" UserId="836" />
  <row Id="8570" PostId="8792" Score="0" Text="In terms of optimizers that are different between the two, see the logistic regression section of this page. https://en.m.wikipedia.org/wiki/Cross_entropy" CreationDate="2015-11-12T22:42:30.360" UserId="12306" />
  <row Id="8571" PostId="8828" Score="0" Text="Do not worry. For sure, the school designed an engaging way for students to learn about data. They will not learn heavy material. But to learn how to understand data and how to question based on data." CreationDate="2015-11-13T09:19:13.730" UserId="3151" />
  <row Id="8575" PostId="2392" Score="0" Text="It's not hard to imagine that 16G of data would be too much for 16G of memory. Do you read the data straight into numpy arrays? If not, you're doubling the memory footprint. Can you use sparse matrices?" CreationDate="2015-11-13T13:09:09.123" UserId="12306" />
  <row Id="8576" PostId="8837" Score="0" Text="Are the section headings tags in the xml, or are you trying to derive them via natural language processing methods?" CreationDate="2015-11-13T19:39:51.447" UserId="13413" />
  <row Id="8577" PostId="8837" Score="0" Text="There will inevitably be some kind of tags associated with the headings, but its not standard between documents. Here's one example (which happens to be html) https://www.sec.gov/Archives/edgar/data/320340/000143774915020747/ins20150930_10q.htm" CreationDate="2015-11-13T19:54:22.857" UserId="6403" />
  <row Id="8578" PostId="8837" Score="0" Text="NLP is not a tool to extract headings.   Extract the text and use tags as features.  And html is not the same as xml - it will be different tools for extraction." CreationDate="2015-11-13T20:25:09.587" UserId="13285" />
  <row Id="8579" PostId="8839" Score="0" Text="Thanks. Why we need the % sign in front of sysfunc but not in front of trim?" CreationDate="2015-11-14T04:00:08.183" UserId="10522" />
  <row Id="8580" PostId="8842" Score="0" Text="Thank you for the answer. So in this case how does one go about improving the recall?" CreationDate="2015-11-14T06:31:00.620" UserId="11110" />
  <row Id="8581" PostId="8839" Score="0" Text="The argument to sysfunc is the precise code that sysfunc should execute as though it were not in a macro - so it interprets trim correctly" CreationDate="2015-11-14T15:12:30.987" UserId="12306" />
  <row Id="8582" PostId="8842" Score="0" Text="It is unlikely that there is a straightforward answer in this case, unless you know something is wrong with your prediction (you aren't normalizing the same way at test time, say). You might want to try visualizing your data to convince yourself your features are meaningful. If you find that your features don't correlate well with the output and that simple solutions don't work, your feature space is likely bit informative enough for prediction." CreationDate="2015-11-14T15:19:22.820" UserId="12306" />
  <row Id="8583" PostId="5459" Score="0" Text="You can use :var1=1 or var1=2" CreationDate="2015-11-14T16:08:28.143" UserId="10522" />
  <row Id="8584" PostId="8842" Score="0" Text="Thank you for the reply..I will try this." CreationDate="2015-11-14T18:28:19.430" UserId="11110" />
  <row Id="8585" PostId="8847" Score="0" Text="You mentioned advisor, so I'd assume this is part of a Graduate School assignment? Do you have access to any commercial software, or are you expected to do this with only Python and open-source packages? What are you learning about in class at the moment and what is the name of the class? Also, is there a performance requirement in terms of time it should take to give an answer?" CreationDate="2015-11-15T03:22:34.307" UserId="9318" />
  <row Id="8587" PostId="8847" Score="0" Text="I am expected to only use Python and open source packages. Writing my own source code is discouraged, even.&#xA;This is a master's level course.&#xA;The class is an introductory Data Science course. The last thing we covered is feature selection, though almost all of the discussion is about text data.&#xA;There are no performance requirements outside of an accuracy ~70%" CreationDate="2015-11-15T18:35:09.760" UserId="14112" />
  <row Id="8588" PostId="8849" Score="1" Text="Thank you so much. I posted about this on a few places and yours was by far the most informative answer. I realized that I was misunderstanding how feature extraction of images works conceptually." CreationDate="2015-11-15T19:38:09.187" UserId="14112" />
  <row Id="8589" PostId="8849" Score="0" Text="Glad that my answer helped you :)" CreationDate="2015-11-16T00:53:01.777" UserId="11097" />
  <row Id="8596" PostId="2344" Score="0" Text="Sorry, what you are doing here `df1 = df[&quot;foo&quot;].map(lambda x:x[&quot;id&quot;])`? And also could you post few lines of sample data?" CreationDate="2015-11-16T07:43:28.127" UserId="14109" />
  <row Id="8598" PostId="8457" Score="1" Text="See: [How to apply piecewise linear fit in Python?](http://stackoverflow.com/questions/29382903/how-to-apply-piecewise-linear-fit-in-python)" CreationDate="2015-11-16T08:42:37.827" UserId="13123" />
  <row Id="8599" PostId="8855" Score="2" Text="The fundamental issue with this is that changing the pool_size affects the dimensionality of the output from that layer.  Therefore weights learned above that layer would have to be either re-initialized to be of the same dimension, or up/downsampled to match its size." CreationDate="2015-11-16T14:14:43.100" UserId="12306" />
  <row Id="8604" PostId="8607" Score="0" Text="Thank you for the answer. Yeah, it is one of the methods. And I have found Bandit algorithms perform really nice (better than Logistic and A/B).  Please have a look at my answer below :)" CreationDate="2015-11-16T18:01:28.540" UserId="11097" />
  <row Id="8605" PostId="8861" Score="0" Text="No metrics include words?   https://en.wikipedia.org/wiki/Cosine_similarity has a similarity that can be used to rank.  I don't follow the question." CreationDate="2015-11-16T18:04:01.183" UserId="13285" />
  <row Id="8606" PostId="8861" Score="0" Text="Good question!  Though cosine is for generating the features for ranking algorithms.  Not for the evaluation of performance." CreationDate="2015-11-16T19:24:27.590" UserId="1097" />
  <row Id="8607" PostId="8861" Score="0" Text="I don't follow but not important.  I can tell you that I use tf-idf cosine similarity to rank search results." CreationDate="2015-11-16T19:53:01.240" UserId="13285" />
  <row Id="8608" PostId="8862" Score="0" Text="When you say you have a dataset, do you mean you only have those descriptors of the dataset (the ones you mentioned)?" CreationDate="2015-11-16T22:40:31.330" UserId="12306" />
  <row Id="8609" PostId="8859" Score="0" Text="eh, semi-supervised learning, let me do some reading on that" CreationDate="2015-11-17T02:32:03.860" UserId="13562" />
  <row Id="8611" PostId="8866" Score="0" Text="+1 for a nice explanation.  Do you know of any links where a Python implementation is done?  Any blog or maybe Github?" CreationDate="2015-11-17T05:16:44.593" UserId="11097" />
  <row Id="8612" PostId="8867" Score="0" Text="No I performed PCA on the whole set of images, and then in the inputs replaced each image with its PCA transformation. If I do the replacement just in the inputs, I can still learn, but if I also replace the targets, it stops working. Anyway, thanks for the suggestions, I'll try them. I'd still like to understand why my approach is wrong (if it is) though..." CreationDate="2015-11-17T05:34:34.117" UserId="12776" />
  <row Id="8613" PostId="8869" Score="0" Text="How are you going to find the most ngrams if you don't calculate the ngrams?   Compare one by one?  That is pretty simple single query." CreationDate="2015-11-17T05:49:02.387" UserId="13285" />
  <row Id="8614" PostId="8862" Score="0" Text="Yes, I mean exactly that." CreationDate="2015-11-17T07:47:09.970" UserId="13680" />
  <row Id="8616" PostId="8867" Score="0" Text="Okay I'm updating my answer now that I know what you've done" CreationDate="2015-11-17T13:07:42.280" UserId="12306" />
  <row Id="8617" PostId="8867" Score="0" Text="Have you tried performing PCA on the target images separately?" CreationDate="2015-11-17T13:32:42.380" UserId="12306" />
  <row Id="8618" PostId="8846" Score="0" Text="Are you asking of a programmatic implementation of Fuzzy Logic?" CreationDate="2015-11-17T14:46:50.173" UserId="11097" />
  <row Id="8619" PostId="8866" Score="0" Text="There's a decent one here: http://deeplearning.net/tutorial/lenet.html , though I prefer the style of this one:  http://neuralnetworksanddeeplearning.com/chap6.html .&#xA;  For implementation and great demos, I like keras (fairly easy to pip install) http://keras.io/" CreationDate="2015-11-17T14:48:44.563" UserId="12306" />
  <row Id="8620" PostId="8869" Score="0" Text="The ngrams for the documents are all calculated as well as the query. The issue is that now I want to see which of the document in the database is most similar to the query. Or rather, get the top 10 documents most similar to the query, based on their ngrams." CreationDate="2015-11-17T17:04:49.543" UserId="14154" />
  <row Id="8621" PostId="8869" Score="0" Text="The stated question is the ngram have been extracted.  Now are you saying they are calculated.  Query document to me is document.  The answer is only as good as the question.  What database?  The syntax varies." CreationDate="2015-11-17T17:10:43.203" UserId="13285" />
  <row Id="8622" PostId="8869" Score="0" Text="I meant extracted. Either way, the ngrams are obtained from each document so I am just working with the ngrams itself. You have a database of ngrams that represent each document in the database. You have a query document where you want to find lets say the top 10 document that appears most similar to your query. By database, lets just say that there is a huge list of the ngram model that represents the document" CreationDate="2015-11-17T17:15:53.260" UserId="14154" />
  <row Id="8623" PostId="8869" Score="0" Text="And you have an answer from me.  A database with not stated table design does not really narrow it down." CreationDate="2015-11-17T17:18:06.163" UserId="13285" />
  <row Id="8624" PostId="6227" Score="0" Text="You should post this on DBA or StackExchange." CreationDate="2015-11-17T17:36:47.510" UserId="13285" />
  <row Id="8625" PostId="726" Score="0" Text="Cross-posted on [Mathematics](http://math.stackexchange.com/questions/865206/trying-to-understand-the-math-in-a-neuroscience-article-by-karl-friston),&#xA;&#xA;[Cross Validated](http://stats.stackexchange.com/q/107653/32036),&#xA;&#xA;[Cognitive Sciences](http://cogsci.stackexchange.com/questions/7805/trying-to-understand-equations-in-karl-friston-article)" CreationDate="2015-11-17T19:01:42.853" UserId="14171" />
  <row Id="8627" PostId="8873" Score="1" Text="I think you answered your own question. You should be recording a concept of an impression, or show. If you showed a product and their was no click, then this is what you are looking for." CreationDate="2015-11-17T21:41:08.857" UserId="13684" />
  <row Id="8628" PostId="8867" Score="0" Text="Are your outputs also contained in input examples?  It looks like they roughly are in example data you posted.  If that is the case, my answer isn't relevant." CreationDate="2015-11-17T23:21:13.360" UserId="12306" />
  <row Id="8630" PostId="8846" Score="0" Text="@ Dawny 33. I didn't exactly get that what you meant by &quot;programmatic implementation of Fuzzy Logic&quot;. I'm using triangular membership function in Matlab for both input/output. However, I was thinking if there is any statistical or mathematical analysis to verify and validate the fuzzy rules rather than opinions' experts only." CreationDate="2015-11-18T01:51:35.887" UserId="12867" />
  <row Id="8631" PostId="8846" Score="0" Text="Ohh okay.  I get it now.  I think it would be better if you added the reference-request tag." CreationDate="2015-11-18T01:58:26.280" UserId="11097" />
  <row Id="8632" PostId="8868" Score="0" Text="TIL about kaggle, looks interesting (: I am still trying to process the source data which is a collection of unstructured text (e.g. address) with components often wrongly parsed (may be due to wrongly entered postcode in source, typing error, crappy mock data, or wrong value like city in postcode column)." CreationDate="2015-11-18T02:34:14.783" UserId="13562" />
  <row Id="8637" PostId="8828" Score="0" Text="Yes, @HamidehIraj is correct.  There are very basic concepts in Data Science which are suited for middle school.  And I believe the curriculum would be designed around those.  So, you don't need to worry about it.  He'll definitely enjoy it." CreationDate="2015-11-18T07:20:59.877" UserId="11097" />
  <row Id="8638" PostId="8867" Score="0" Text="Yes, they are, that's what I meant in the sentence just below the example image (I know it's hard to understand what I am trying :))" CreationDate="2015-11-18T08:47:10.187" UserId="12776" />
  <row Id="8639" PostId="8877" Score="0" Text="thanks, but this is not what I'm asking. I'm interesting in knowing `P(M &amp; A | L)` and I know that `M` and `A` are dependent (I also know P(M &amp; A)), but I don't know if `L` depends on `M &amp; A`, so they only thing I could do was to assume `P(M &amp; A | L) = P(M &amp; A) P(L)`. So now I need to understand how bad this assumption is." CreationDate="2015-11-18T09:02:42.210" UserId="13680" />
  <row Id="8640" PostId="8883" Score="0" Text="You know you can ask this on the Kaggle forum, and it is *already answered*:  https://www.kaggle.com/c/rossmann-store-sales/forums/t/17137/how-to-handle-missing-customers-field-in-test-dataset/97276 and https://www.kaggle.com/c/rossmann-store-sales/forums/t/16730/two-questions-customers-and-scoring/95138" CreationDate="2015-11-18T09:22:35.420" UserId="836" />
  <row Id="8641" PostId="8883" Score="0" Text="Oooo, pardon my kaggle noobiness. First time kaggle without anyone holding my hands =)" CreationDate="2015-11-18T09:28:52.237" UserId="122" />
  <row Id="8642" PostId="8883" Score="1" Text="No problem. The `Customers` data is very specific to the competition. If you are not sure how to deal with missing values in general for ML (such as empty `Promo` values), it might be worth changing this question to be about that issue only. There are already some answers about that on this site, e.g. http://datascience.stackexchange.com/questions/8322/filling-missing-data-with-other-than-mean-values" CreationDate="2015-11-18T09:35:42.667" UserId="836" />
  <row Id="8643" PostId="8873" Score="0" Text="But actually this is not available in recorded data. Thats what I mentioned. Data only contain what user-product pair has click label. What was shown and what was clicked is not recorded." CreationDate="2015-11-18T10:00:51.437" UserId="14167" />
  <row Id="8644" PostId="8873" Score="0" Text="Also, even if let say - user1 was shown prod1, prod2,prod3 (and he clicked prod1) - Then user1 with prod2 and prod3 will have no-click label. But what about rest of products (prod4,prod5,...). Just because they were not shown, user had no opportunity to decide click/no-click. This does not say whether user would be interested in not-shown products - so labeling all other combination as no-click may not be true in real. This is what I also explained in question." CreationDate="2015-11-18T10:03:28.260" UserId="14167" />
  <row Id="8647" PostId="8892" Score="0" Text="Can you please elaborate a bit more on your answer" CreationDate="2015-11-18T13:54:58.807" UserId="11097" />
  <row Id="8648" PostId="8892" Score="0" Text="edited. Check it out. Hope its useful." CreationDate="2015-11-18T15:00:41.613" UserId="14167" />
  <row Id="8649" PostId="8846" Score="0" Text="Thank you Dawny." CreationDate="2015-11-18T15:11:44.540" UserId="12867" />
  <row Id="8650" PostId="8851" Score="0" Text="Awesome, thanks for the response. I edited my question to add sample data" CreationDate="2015-11-19T03:04:55.327" UserId="13917" />
  <row Id="8651" PostId="8835" Score="0" Text="Thanks for the response, edited my question to include data" CreationDate="2015-11-19T03:05:28.393" UserId="13917" />
  <row Id="8654" PostId="8891" Score="1" Text="this isn't true.  Your just not engineering your features carefully enough.  The method I describe below works perfectly fine.  the centroid locations lie along arcs of the unit circle.  The mean of 23 and 0 in my Sin/Cos space is just the 23:30 location.  When I get a minute I'll show you in an example." CreationDate="2015-11-19T04:42:44.547" UserId="9420" />
  <row Id="8655" PostId="8898" Score="2" Text="+1  Wonderfully written!" CreationDate="2015-11-19T05:55:24.723" UserId="11097" />
  <row Id="8656" PostId="8891" Score="0" Text="I'm not trying to say it cannot be done with good feature transformations. What you are suggesting is a transformation into a space where squared Euclidean and the mean can be used. My answer is tailored to the suggestions of using &quot;distances&quot; like (x-y) mod 24." CreationDate="2015-11-19T07:27:17.330" UserId="924" />
  <row Id="8658" PostId="8891" Score="1" Text="But the question was &quot;How can I map the data so that the boundary will be cyclic?&quot;.  Your answer said that this is &quot;abusing k-means&quot;.  You should just delete the answer as it misleads all but the cognoscenti." CreationDate="2015-11-19T08:05:14.110" UserId="9420" />
  <row Id="8663" PostId="6070" Score="0" Text="Here's an idea way-out-of-left field - a time-series of pdfs can be thought of as a solution to a Fokker-Planck-type PDE (yes/no/maybe?). Would it be feasible to fit such a PDE to your samples and then cluster the PDE's coefficients?" CreationDate="2015-11-19T08:24:35.733" UserId="14189" />
  <row Id="8666" PostId="8909" Score="0" Text="https://github.com/dmlc/MXNet.jl" CreationDate="2015-11-19T09:40:42.067" UserId="9634" />
  <row Id="8667" PostId="8897" Score="0" Text="Thanks a lot @AN605. That's so nice of you ! &#xA;I have a few quesitons :&#xA;For the 4)&#xA;&#xA;- When you say &quot;to train on A' and test on B'&quot;, do you mean validate ? &#xA;- &quot;generate learning curves for C&quot; &amp; &quot;F1(C) score is under/similar to F1(B)&quot;. I though that, for the learning curve, we had to plot the error metric for the training set (A or A' here)  and the error metric for the validating set (B or B') only. Aren't you &quot;validate&quot; on C here ?" CreationDate="2015-11-19T09:49:26.670" UserId="14200" />
  <row Id="8668" PostId="8897" Score="0" Text="About using the &quot;class weights&quot;, correct me if I'm wrong (I just had a quick look about it) but, this trick involves to &quot;modify&quot; the cost function by adding a coefficient/weight &quot;k&quot; in front of the unbalanced class, right ? :￼￼ &#xA;Cost( h(x) , y ) = -y * k * log( h(x) ) - (1-y) * log( (h(x) )&#xA;Like that, the algorithm should considers a misclassification of the positive class as more important.&#xA;But the thing is that I &quot;have to&quot; use Apache Spark &amp; MLlib to build my all model. And I'm not sure that I can modify easily my cost function with spark.&#xA;&#xA;Anyway thanks for your time !" CreationDate="2015-11-19T09:49:34.923" UserId="14200" />
  <row Id="8669" PostId="8896" Score="0" Text="Hi @jamesmf, thanks for that cool answer.&#xA;For the F1Score, the problem I had is that I may want to focus more on eliminate the False Positive more than the False Negative. Would that be right to add different &quot;weight&quot; for FP and FN in the computing of precision and recall ?" CreationDate="2015-11-19T09:57:57.610" UserId="14200" />
  <row Id="8670" PostId="8911" Score="0" Text="That sounds right :) Thanks for the answer man." CreationDate="2015-11-19T09:58:31.990" UserId="14200" />
  <row Id="8673" PostId="8909" Score="0" Text="@itdxer Thank you for the link. Can you put that as an answer by elaborating about it?" CreationDate="2015-11-19T10:17:56.380" UserId="11097" />
  <row Id="8690" PostId="8919" Score="0" Text="Increasing the depth of the tree would lead to more overfitting, and thereby a more non-linear structure." CreationDate="2015-11-19T11:49:11.030" UserId="11097" />
  <row Id="8691" PostId="8922" Score="0" Text="Is it after a specific character, or after a specific index?" CreationDate="2015-11-19T13:39:31.183" UserId="11097" />
  <row Id="8692" PostId="8896" Score="0" Text="That makes sense to me. Also your description of class weighting is correct, and I don't see it implemented in MLib, but it might be worth a feature request" CreationDate="2015-11-19T14:11:06.173" UserId="12306" />
  <row Id="8695" PostId="8896" Score="0" Text="Okay thanks james !  &#xA;&#xA;I'm currently trying to do it by myself by extending the LogisticGradient class and overwritting the compute method. I will let you know if this give me good results.&#xA;&#xA;Have a good day." CreationDate="2015-11-19T15:30:20.833" UserId="14200" />
  <row Id="8696" PostId="8924" Score="0" Text="Thank you. and What if I want to do it for the last © in the text. Consider this: c(&quot; © aaa © bbb&quot;) --&gt; c( &quot;© aaa&quot;)" CreationDate="2015-11-19T15:36:55.533" UserId="3151" />
  <row Id="8697" PostId="8922" Score="0" Text="After a specific character: ©" CreationDate="2015-11-19T16:24:20.907" UserId="3151" />
  <row Id="8698" PostId="8924" Score="0" Text="@HamidehIraj  You can make use of [regexes](http://stackoverflow.com/questions/7449564/regex-return-all-before-the-second-occurrence) for executing that." CreationDate="2015-11-19T16:26:29.630" UserId="11097" />
  <row Id="8699" PostId="8922" Score="0" Text="Then, seems like the existing answer solved your question  :)" CreationDate="2015-11-19T16:26:56.510" UserId="11097" />
  <row Id="8700" PostId="8924" Score="1" Text="You are welcome. Once you get use to regex you'll see that it is as easy to remove from the last @ char. I've edited my answer to include this case as well." CreationDate="2015-11-19T17:27:22.567" UserId="12603" />
  <row Id="8701" PostId="8813" Score="1" Text="Did any of the existing answers helped you solve the question?  If yes, then please mark it as accepted  :)" CreationDate="2015-11-19T17:45:29.793" UserId="11097" />
  <row Id="8702" PostId="8813" Score="0" Text="You say &quot;two different datasets&quot; but then plot them as if they are one dataset with two features.  Is that what you really want?  Also, you say &quot;want to find the correlation&quot;, but then never check the linear correlation, do you want to check the linear correlation before moving to nonlinear?  Though it might not &quot;look&quot; like a line is a good fit, there are plenty of cases where points get stacked on top of one another so the distro is linear and its only the outliers that are nonlinear. Lots of posters seem to be jumping to nonlinear without fully vetting linear.  I advise being disciplined." CreationDate="2015-11-19T20:39:33.353" UserId="9420" />
  <row Id="8703" PostId="8427" Score="0" Text="Word2vec has an interesting option that identifies phrases for you, you could try that." CreationDate="2015-11-19T22:30:06.267" UserId="12306" />
  <row Id="8704" PostId="8485" Score="0" Text="+1 Thanks for the link. I have been using adjacency list but did not know all the technical names of the others." CreationDate="2015-11-20T00:41:45.193" UserId="10814" />
  <row Id="8705" PostId="8926" Score="2" Text="To test the impact you could do cross validation on a test set. Even then you would only be able to test the accuracy of one class. But, yes, I would think this is an issue. Generally, SVMs are binary classifiers and your non-L will be very noisy. Have you considered a nearest neighbour or clustering to find those similar to the L records?" CreationDate="2015-11-20T02:09:09.403" UserId="13684" />
  <row Id="8706" PostId="8813" Score="0" Text="The dots you see have their x values coming from one dataset and the y values coming from another. How would you check if there is a regression line that could be fit, if you don't plot them this way?" CreationDate="2015-11-20T08:56:04.910" UserId="13481" />
  <row Id="8707" PostId="8813" Score="0" Text="As for the linear correlation: I failed to imagine how a linear regression could even fit this scatterplot. Look at its shape: it suggests some exponential curve fits them best. This is also confirmed by the two answers that I got. You are right, there could be a partial linear regression which may serve the cause, but look at those that would become outliers: aren't they too many to &quot;get rid&quot; of them?" CreationDate="2015-11-20T08:58:02.487" UserId="13481" />
  <row Id="8708" PostId="8926" Score="0" Text="I had not considered clustering, no. It seems my problem is more similar to a semi-supervised learning problem than straight binary classification. Thanks for the idea." CreationDate="2015-11-20T09:51:50.980" UserId="13596" />
  <row Id="8709" PostId="5404" Score="1" Text="R will be integrated with SQL Server 2016. Now you have the best of both!" CreationDate="2015-11-20T11:35:38.657" UserId="13498" />
  <row Id="8711" PostId="8892" Score="0" Text="I really didn't apply this till now.  (Would do it though).  As I have self-answered, using the Bandit algorithms proved very useful." CreationDate="2015-11-20T12:30:01.680" UserId="11097" />
  <row Id="8712" PostId="8928" Score="0" Text="Thanks for quick response. I missed a point here when you said I can think of ATMs as changing states. How is that possible? I do not intend to combine all the transaction in one model instead I want to analyze each transaction on its own. Like before it is authorized, it is passed to the detection system for analyses," CreationDate="2015-11-20T13:01:03.230" UserId="13132" />
  <row Id="8713" PostId="8941" Score="0" Text="Good question - but in its current formulation it isn't conducive to Stack Exchange format as no single answer can be accepted. You might want to consider removing the single conference per answer." CreationDate="2015-11-20T13:33:18.643" UserId="12306" />
  <row Id="8714" PostId="8941" Score="0" Text="@jamesmf Thank you for the suggestion. Edit done!" CreationDate="2015-11-20T14:01:18.867" UserId="11097" />
  <row Id="8715" PostId="8359" Score="0" Text="Is the goal to create as many clusters of (1,2,3) as possible, leaving some unassigned?  Or to create as many clusters of at least size 2 as possible?  If there isn't a concrete metric you'd like to optimize, you will have trouble defining the problem as a clustering one." CreationDate="2015-11-20T14:52:32.487" UserId="12306" />
  <row Id="8716" PostId="8395" Score="0" Text="Did any of the answer below help you?" CreationDate="2015-11-20T16:52:40.320" UserId="11097" />
  <row Id="8717" PostId="8928" Score="0" Text="If thinking of ATMs as changing states doesn't seem to make sense to you, then maybe HMM is not the right model. Your case does sounds like a fairly straightforward binary classification problem (given a location, time, and amount, flag as either suspicious or not). Is that true?" CreationDate="2015-11-20T20:14:28.673" UserId="14218" />
  <row Id="8718" PostId="8948" Score="0" Text="Makes sense... but what about how to choose what method to feature scale and can each feature be manipulated differently than any other feature?" CreationDate="2015-11-20T21:58:44.623" UserId="13578" />
  <row Id="8719" PostId="5404" Score="0" Text="Yeah, I saw that, but you still have to upgrade or purchase 2016. Not easy to do that at the drop of a hat." CreationDate="2015-11-20T22:27:00.120" UserId="8842" />
  <row Id="8720" PostId="8948" Score="0" Text="That entirely depends on the data." CreationDate="2015-11-20T22:29:22.807" UserId="14177" />
  <row Id="8721" PostId="8930" Score="0" Text="Ah, yes. I think this is probably the right way of thinking about it, a partial ordering. It looks like a fun little rabbit hole to go down. Thanks." CreationDate="2015-11-21T00:56:11.017" UserId="9768" />
  <row Id="8722" PostId="8965" Score="0" Text="thanks!&#xA;is there any way to read the 1st link's material without enrolling and waiting for it to be posted?" CreationDate="2015-11-21T09:58:47.667" UserId="14240" />
  <row Id="8723" PostId="8965" Score="0" Text="Yes, you can watch the videos from the archive [here](https://class.coursera.org/ml-005/lecture/preview)" CreationDate="2015-11-21T10:28:12.390" UserId="11097" />
  <row Id="8724" PostId="8928" Score="0" Text="Yes, that is how I want it - I want to flag an ongoing transaction as suspicious or not. I am checking with SVM" CreationDate="2015-11-22T05:57:44.187" UserId="13132" />
  <row Id="8725" PostId="8980" Score="0" Text="Thank you. Can you also include the link to the videos in your answer, please?" CreationDate="2015-11-22T17:55:55.733" UserId="11097" />
  <row Id="8726" PostId="8979" Score="0" Text="Thanks for the quick reply.  Can you be more specific on the required numeric data type?  Can it take vectors?  (Or to what must one convert labels?)  Does it do any type or range inference?" CreationDate="2015-11-22T19:54:05.017" UserId="12363" />
  <row Id="8727" PostId="8979" Score="1" Text="@AdamM.B. Nice doubt.  XGBoost takes inputs in the format of DMatrices.  So, you need to convert your data into that form.  [Link](https://github.com/dmlc/xgboost/blob/master/doc/input_format.md)" CreationDate="2015-11-23T01:36:14.720" UserId="11097" />
  <row Id="8728" PostId="8943" Score="0" Text="It is unclear to me exactly what you are asking. You fit a linear model but your description of the time series sounds quite non-linear. What is your error metric to determine best fit? You say the time series is noisy, but the example looks rather stable. Are there patterns to the discontinuities? Have you looked at ARIMA models?" CreationDate="2015-11-23T01:56:55.187" UserId="13684" />
  <row Id="8730" PostId="8982" Score="0" Text="Are you using this for playing around with the dataset (for learning process) or for solving a specific problem?" CreationDate="2015-11-23T05:51:43.917" UserId="11097" />
  <row Id="8731" PostId="8982" Score="0" Text="For both. I am trying to implement the skills I got on the courses and also I want to do some kind of project that I can offer to real estate magazine and use it in my CV. I'm new in data science and I need as more done work as I can." CreationDate="2015-11-23T05:56:52.733" UserId="14267" />
  <row Id="8732" PostId="8982" Score="0" Text="Okay.  And welcome to the site! :)" CreationDate="2015-11-23T05:58:52.847" UserId="11097" />
  <row Id="8733" PostId="8982" Score="0" Text="Thanks ))))))))" CreationDate="2015-11-23T06:01:32.947" UserId="14267" />
  <row Id="8734" PostId="8947" Score="0" Text="Thanks for that.  I have seen that the presentations are [downoadable](https://www.wolfram.com/events/technology-conference/2015/presentations/).  Any idea where the talk videos are?" CreationDate="2015-11-23T06:53:22.750" UserId="11097" />
  <row Id="8735" PostId="8898" Score="0" Text="Hands down the best answer I got to any of my questions." CreationDate="2015-11-23T09:08:51.640" UserId="14044" />
  <row Id="8736" PostId="6577" Score="0" Text="Did any of the existing answers solved your problem?" CreationDate="2015-11-23T09:59:06.943" UserId="11097" />
  <row Id="8737" PostId="8982" Score="3" Text="I suggest looking at recent previous real estate publications to see what the industry is talking about. Then base your analysis on answering a few questions on those concepts." CreationDate="2015-11-23T10:50:56.337" UserId="10814" />
  <row Id="8738" PostId="8982" Score="0" Text="hmmm... good idea)))) I haven't thought about this way. I will try to find such kind of statistics ans graphics. Thanks!" CreationDate="2015-11-23T10:57:00.610" UserId="14267" />
  <row Id="8739" PostId="8677" Score="0" Text="(+1) There's no _one algo to rule them all_ model." CreationDate="2015-11-23T12:35:22.167" UserId="11097" />
  <row Id="8741" PostId="6580" Score="0" Text="I'm sorry Duncan, I thought I tagged it already.  Thanks much. This did work for me." CreationDate="2015-11-23T15:22:01.147" UserId="10881" />
  <row Id="8742" PostId="8993" Score="1" Text="You can definitely do sales prediction and analytics.  But, what do you really mean by: _select a bunch of optimized data from a larger data set_?" CreationDate="2015-11-23T16:51:20.697" UserId="11097" />
  <row Id="8743" PostId="8993" Score="0" Text="I have some products, each product have their own selling information, like # of units sold before, review, ratings, etc, want to select a subset from the data to recommend to the retailers, so to increase the # of units being sold for the retailers. I want the subset to be like &quot;optimized&quot; subset, so that it can increase the retailers' # of units sold as much as possible." CreationDate="2015-11-23T16:57:42.867" UserId="4619" />
  <row Id="8744" PostId="8993" Score="0" Text="Unless you have a compelling reason why, generally you do not want to subset data. Rather, you will train on all data. I think the optimization you are referring to is the ability to provide an optimal recommendation. For this you could look into collaborative filtering techniques." CreationDate="2015-11-23T19:24:24.110" UserId="13684" />
  <row Id="8746" PostId="8993" Score="0" Text="Thank you so much. For the collaborative filtering, there is a project about it: http://blog.yhathq.com/posts/recommender-system-in-r.html. I took a look, but when you do it, you need a reference data point for referring. Here we do not have a reference data point. So I am wondering how. Thank you." CreationDate="2015-11-23T21:17:52.367" UserId="4619" />
  <row Id="8747" PostId="6580" Score="0" Text="No problem! I'm glad that it works." CreationDate="2015-11-23T21:25:33.647" UserId="10884" />
  <row Id="8748" PostId="8993" Score="0" Text="Nice blog btw... By reference data point do you mean that you do not have a similarity measure for users in your system? You will create one, e.g. in your case it might be finding the Jaccard similarity of users based on their purchases. See http://datascience.stackexchange.com/questions/8873/user-product-positive-click-data-available-how-to-generate-negative-no-click/8894#8894 for links on collaborative filtering and creation of a utility matrix." CreationDate="2015-11-23T21:31:55.493" UserId="13684" />
  <row Id="8749" PostId="8947" Score="1" Text="Those are just a fraction of the presentations.  Many more were conducted.  There are tons of videos on the Wolfram Language and the Mathematica platform at http://www.wolfram.com/broadcast/" CreationDate="2015-11-23T22:13:37.010" UserId="10814" />
  <row Id="8750" PostId="8993" Score="0" Text="reference data point is that something like an input, a target  data point in the system. like in the beer system, we try to recommend beer to the user, then, we need to get some beer information about the user, the metrics of the beer the user like, then based on the metric, calculate and recommend beer to the user, here, we do not have a certain user, or the products' information they like. we just machine learning from a bigger data set, and the data set has the selling data." CreationDate="2015-11-23T23:00:59.490" UserId="4619" />
  <row Id="8752" PostId="695" Score="3" Text="FYI: [Pylearn2 has no more developer.](https://github.com/lisa-lab/pylearn2/pull/1565)" CreationDate="2015-11-24T00:59:21.673" UserId="843" />
  <row Id="8754" PostId="8984" Score="1" Text="Just went to MLConf last week. I was quite disappointed in that each speaker was only given 20 min. each to speak. So, the talks were very high-level and not deep technical talks. Quite honestly a lot of local meetups are quite good and they tend to have an interesting variety of topics." CreationDate="2015-11-24T05:09:53.927" UserId="13684" />
  <row Id="8755" PostId="8984" Score="0" Text="@init-random That should be frustrating as an attendee.  I only watched Xavier(from Quora)'s talk." CreationDate="2015-11-24T05:40:18.223" UserId="11097" />
  <row Id="8758" PostId="8980" Score="0" Text="They have a YouTube playlist of them here: https://www.youtube.com/playlist?list=PLB2SCq-tZtVl2qWH4CjizIb-IrkT_RA1f&#xA;&#xA;I don't think all of the videos are there, but most are, including some of the workshops." CreationDate="2015-11-24T07:06:06.943" UserId="14258" />
  <row Id="8759" PostId="8980" Score="0" Text="Thank You. I have edited the answer for you with the links." CreationDate="2015-11-24T07:08:18.240" UserId="11097" />
  <row Id="8761" PostId="9004" Score="0" Text="(+1) Thanks!  That's a nice one.  Heard a couple from that podcast.  I would vouch for their quality." CreationDate="2015-11-24T12:32:50.947" UserId="11097" />
  <row Id="8764" PostId="9002" Score="0" Text="You could aggregate the training set by user over the training period. Then normalize your predictor variables, e.g. clicks/session as opposed to raw number of clicks. You could try this and just using your current data, which looks like it might be aggregated by session. Test both models with cross validation and confirm which aggregation is better." CreationDate="2015-11-24T13:36:03.577" UserId="13684" />
  <row Id="8765" PostId="9002" Score="0" Text="If the dataset reflects the true distribution of logins, wouldn't you be more concerned with balancing between positive/negative examples rather than individual users?" CreationDate="2015-11-24T13:52:30.067" UserId="12306" />
  <row Id="8768" PostId="9002" Score="0" Text="@jamesmf , Well the Login to monetization ratio is already at around 70% so the number of positives are already pretty high enough I think. Its not really like a case of say something like a credit card fraud with positives in the range of 2%-3% only" CreationDate="2015-11-24T16:26:30.407" UserId="14294" />
  <row Id="8769" PostId="9002" Score="0" Text="@init-random Yes, the current data is aggregated on a daily basis (Whether the user monetized on the same day that he logged in). The predictor variables that are strong learners (based on the success some of the other similar models) are derived out of the user activity of the previous 7 days. Since the day of the week of the visit is also a very important predictor, I am out of ideas on how to aggregate this by user. Following your cue, I'll try to make 7 models, one for each weekday. Then this could be aggregated." CreationDate="2015-11-24T16:39:01.817" UserId="14294" />
  <row Id="8770" PostId="9002" Score="0" Text="@Vaibhav Srivastava Balancing your data doesn't just mean 'having enough positives.'  But my question is why are you aggregating?  If your goal is to predict a the probability of monetizing per login, you should train on per-login information.  You can easily include historical data (7 day activity, for example) for each login.  But if you aggregate during training, you lose the granularity of per-login." CreationDate="2015-11-24T16:54:21.377" UserId="12306" />
  <row Id="8772" PostId="9008" Score="0" Text="https://en.wikipedia.org/wiki/Longitudinal_study" CreationDate="2015-11-24T21:06:44.577" UserId="381" />
  <row Id="8773" PostId="8539" Score="0" Text="Are you saying that at all times `ti` you have a set of features `xi`, and then if you perform the specific measurements, you'll have `xi` plus some new features, `x2i`?  If the dimensionality changes if you perform these measurements, you'll need to formulate the problem carefully." CreationDate="2015-11-24T21:37:20.390" UserId="12306" />
  <row Id="8774" PostId="9002" Score="0" Text="@jamesmf Okay I got you now. Also, if a users multiple logins are very similar, maybe the decision trees would automatically take care care of that. Anyway, I'll make a few models and check which works best." CreationDate="2015-11-25T04:01:13.153" UserId="14294" />
  <row Id="8775" PostId="9013" Score="0" Text="Sounds like you're right but I opened it with SublimeText and it didn't have those characters, so I re-saved it without the `^M` and I still have the same problem in R." CreationDate="2015-11-25T04:52:35.273" UserId="14327" />
  <row Id="8777" PostId="9015" Score="0" Text="Glad you realized your mistake &#xA;&#xA;Can you please mark your answer as accepted, to avoid any more answers :)" CreationDate="2015-11-25T05:00:16.313" UserId="11097" />
  <row Id="8778" PostId="9015" Score="0" Text="@dawny33 will do in 2 days, that's when it says I'll be allowed to accept my answer." CreationDate="2015-11-25T06:09:54.710" UserId="14327" />
  <row Id="8781" PostId="9016" Score="0" Text="This is an IT question, not a data science question so is off-topic here." CreationDate="2015-11-25T09:21:51.387" UserId="13296" />
  <row Id="8782" PostId="6111" Score="0" Text="In SAS proc dtree is for this exact problem." CreationDate="2015-09-26T02:24:29.280" UserId="8005" />
  <row Id="8783" PostId="6111" Score="0" Text="It looks like you want to do a market basket analysis. But maybe I understand your question incorrectly. If you are interested in market basket analysis try [arules package](http://lyle.smu.edu/IDA/arules/)." CreationDate="2015-08-27T01:35:50.793" UserId="12384" />
  <row Id="8785" PostId="9012" Score="0" Text="Please refer to this answer to a problem similar to yours. http://stackoverflow.com/questions/5843495/what-does-m-character-mean-in-vim" CreationDate="2015-11-25T03:02:30.073" UserId="14328" />
  <row Id="8786" PostId="9012" Score="0" Text="So I'm not good with VIM and did use the solution on that page, but I opened the file in SublimeText and there it didn't have the `^M` so I saved it like that.  I'm still having the same problem when I try to run this code in R." CreationDate="2015-11-25T04:53:25.990" UserId="14327" />
  <row Id="8787" PostId="9012" Score="0" Text="Link only answers are discouraged here." CreationDate="2015-11-25T12:53:42.473" UserId="21" />
  <row Id="8788" PostId="9018" Score="0" Text="Is your worry that the categories are too correlated? If a human couldn't manually separate the classes, you likely won't get far with most NLP models" CreationDate="2015-11-25T14:17:05.923" UserId="12306" />
  <row Id="8789" PostId="9023" Score="3" Text="Almost any algorithm will do the trick. On the other hand it is worth spending some thought on what features of the texts you want to use as a base for the clustering. This depends very much on your research question." CreationDate="2015-11-25T15:43:01.057" UserId="10169" />
  <row Id="8790" PostId="9028" Score="0" Text="@knappen Yes, I do believe that this is viable approach. However I'm guessing that for example in PAC (Probably Approximately Correct) context when one learns &quot;something&quot;, then knowledge of the underlying distribution of this &quot;something&quot; can be very useful and this distribution can be derived from unlabeled data. So, I'm guessing there must be a different and perhaps more precise way of using unlabeled data than clustering. For example, clustering by itself is oftentimes problematic." CreationDate="2015-11-25T15:47:28.627" UserId="14009" />
  <row Id="8791" PostId="9021" Score="0" Text="What platform are you using to fit your model?" CreationDate="2015-11-25T17:46:27.740" UserId="13413" />
  <row Id="8792" PostId="9030" Score="0" Text="What's K9? You linked to k-means." CreationDate="2015-11-25T18:06:17.230" UserId="381" />
  <row Id="8793" PostId="9030" Score="0" Text="@Emre Thanks, fixed" CreationDate="2015-11-25T18:15:12.707" UserId="13285" />
  <row Id="8794" PostId="8982" Score="1" Text="Please share your data set. Upload it to somewhere public like Github or Google Docs." CreationDate="2015-11-25T18:15:53.040" UserId="471" />
  <row Id="8795" PostId="9010" Score="0" Text="Thank you so much for your reply James! That really helps." CreationDate="2015-11-25T19:05:23.960" UserId="14293" />
  <row Id="8796" PostId="9026" Score="0" Text="k-means is not the knn algorithm (and not a good choice)" CreationDate="2015-11-25T23:12:49.583" UserId="924" />
  <row Id="8797" PostId="9026" Score="0" Text="@Anony-Mousse Thank you for spotting the spelling error.  Rectified.  As the OP asked about _any_ algorithm, I included k-means which I have used once, and is a relevant answer." CreationDate="2015-11-26T02:22:13.117" UserId="11097" />
  <row Id="8798" PostId="9029" Score="0" Text="The link on paragraph vectors doesn't work." CreationDate="2015-11-26T02:53:17.323" UserId="9123" />
  <row Id="8799" PostId="9029" Score="0" Text="Should be fixed, thanks for pointing that out." CreationDate="2015-11-26T03:04:38.650" UserId="12306" />
  <row Id="8801" PostId="9021" Score="0" Text="I am using SPSS." CreationDate="2015-11-26T05:04:55.167" UserId="9052" />
  <row Id="8802" PostId="9039" Score="0" Text="(+1) Nice explanation with the example!" CreationDate="2015-11-26T06:33:09.753" UserId="11097" />
  <row Id="8804" PostId="9034" Score="0" Text="Thanks. What does make_scorer() do?" CreationDate="2015-11-26T09:51:28.767" UserId="1151" />
  <row Id="8805" PostId="9060" Score="1" Text="Is this not the right place for me to ask this question? Why is the question downvoted?" CreationDate="2015-11-25T14:05:51.753" UserId="14332" />
  <row Id="8806" PostId="9060" Score="2" Text="This is the right place, provided you ask correctly. See [ask]. Perhaps if you told us what features you liked/disliked about those you googled? What operating system? Free, or do you have a budget? The more (specific) information you can give us, the more that we can help you. I reversed that downvote, sicne you are new, but please help us to help you in future &amp; give more detail in this question if you can (did you ask your professor?)" CreationDate="2015-11-25T14:22:32.113" UserDisplayName="Mawg" />
  <row Id="8807" PostId="9060" Score="1" Text="Ok. I will surely update my question with all those details. Thanks for the feedback." CreationDate="2015-11-25T14:25:11.907" UserId="14332" />
  <row Id="8808" PostId="9061" Score="0" Text="Thanks for your answer. I will check about Syncfusion today." CreationDate="2015-11-25T14:04:33.360" UserId="14332" />
  <row Id="8809" PostId="9062" Score="0" Text="Thanks for your answer. i will discuss your points with my team. :)" CreationDate="2015-11-26T10:48:07.043" UserId="14332" />
  <row Id="8810" PostId="9060" Score="1" Text="Welcome to the site!  Good luck with the project  :)" CreationDate="2015-11-26T10:49:38.803" UserId="11097" />
  <row Id="8811" PostId="9060" Score="0" Text="Thank you.... :)" CreationDate="2015-11-26T10:50:14.087" UserId="14332" />
  <row Id="8813" PostId="9041" Score="0" Text="Ok , I have researched more about clustering evaluation  by counting.&#xA;The evaluation depends on 4 pairs of items&#xA;&#xA;SD = Same Cluster , Different Category&#xA;DS = Different Cluster , Same Category &#xA;DD = Different both cluster and category&#xA;SS = Same cluster and category &#xA;&#xA;and with rand index the evaluation will be SS+DD  / SS + SD + DS + DD &#xA;&#xA;I understand the concept , but I still wonder how to count the pairs.&#xA;&#xA;For example I have data 1 2 3 4 5 6 7 8 9&#xA;&#xA;There are 3 categories &#xA;(123) (456) (789)&#xA;&#xA;and 2 clusters&#xA;&#xA;(1256) (34789)&#xA;&#xA;How to count SD,DS,SS and DD ?" CreationDate="2015-11-26T11:31:30.393" UserId="14357" />
  <row Id="8814" PostId="9041" Score="0" Text="And another question is if I need F1 result , how to calculate it  (how to calculate , precision and recall )?" CreationDate="2015-11-26T11:34:00.710" UserId="14357" />
  <row Id="8815" PostId="9036" Score="1" Text="For those of us without powerpoint, or not willing to download and process a large file to get a little more context, could you paraphrase the description in the slide (I am assuming it is not much text) - specifically what context and rationale is given. The fact that you are calling this shuffling implies only certain type of algorithm (e.g. ranking). Context is important: It might be quite sensible approach in a recommendation engine for instance." CreationDate="2015-11-26T11:37:15.043" UserId="836" />
  <row Id="8817" PostId="9036" Score="0" Text="@NeilSlater Thanks. I forgot to add the context." CreationDate="2015-11-26T11:41:11.160" UserId="9123" />
  <row Id="8818" PostId="9036" Score="0" Text="I don't know a good theory-driven answer, but the assertion may in part be due to measuring real-world performance of the engine (e.g. customer purchases or clicks based on recommendations). In other words, the shuffling has some useful effect of making up for errors in the model, even as simple as showing the user/customer a wider range of things to click on over time." CreationDate="2015-11-26T11:45:32.860" UserId="836" />
  <row Id="8819" PostId="9036" Score="0" Text="@NeilSlater You might want to add it as an answer!" CreationDate="2015-11-26T11:46:25.727" UserId="9123" />
  <row Id="8820" PostId="9036" Score="0" Text="It's just a guess, I have only ever written one recommendation engine in Andrew Ng's course, so I'd prefer to leave an actual answer to someone with more experience." CreationDate="2015-11-26T11:47:16.673" UserId="836" />
  <row Id="8822" PostId="9034" Score="0" Text="`make_scorer()` converts metrics into callables that can be used for model evaluation. The `scoring` argument expects a function `scorer(estimator, X, y)`. So it will fail, if you try to pass `scoring=cohen_kappa_score` directly, since the signature is different, `cohen_kappa_score(y1, y2, labels=None)`. The `make_scorer` also accepts additional arguments, like `labels` from `cohen_kappa_score`." CreationDate="2015-11-26T11:58:40.537" UserId="14320" />
  <row Id="8823" PostId="9018" Score="0" Text="Categories are correlated but separable. Problem is that the content contains many terms that are overlapping as the objects have the same functions, because they are under the same taxonomy. For example &quot;Teacher&quot; and &quot;Lecturer&quot; could be under the same taxonomy &quot;Staff&quot; and have the same functionalities." CreationDate="2015-11-26T12:42:07.780" UserId="14334" />
  <row Id="8824" PostId="9063" Score="0" Text="Define accuracy?" CreationDate="2015-11-26T14:01:16.503" UserId="13285" />
  <row Id="8825" PostId="9041" Score="0" Text="@user3801162 &quot;How to count SD,DS,SS and DD ?&quot; By considering each pair and checking to what of these indexes it contributes to. Take 1,2. This adds one to SS; 1,3 adds one to DS, etc." CreationDate="2015-11-26T14:21:59.883" UserId="12603" />
  <row Id="8826" PostId="9037" Score="0" Text="Are your *classes* mutually disjoint? Do they also *cover* the data set, i.e., each item of it belongs to one and only one class? I'm trying to make sure I understand what you are talking about.." CreationDate="2015-11-26T14:26:22.567" UserId="12603" />
  <row Id="8827" PostId="9063" Score="0" Text="Number of correctly parsed companies / Total predicted companies by the parser" CreationDate="2015-11-26T14:55:47.947" UserId="14368" />
  <row Id="8828" PostId="9064" Score="0" Text="The HTML is obtained from Apache Tika. And it varies from resume to resume. There is no fixed structure of HTML." CreationDate="2015-11-26T14:57:12.573" UserId="14368" />
  <row Id="8829" PostId="9063" Score="0" Text="That is precision." CreationDate="2015-11-26T15:02:01.380" UserId="13285" />
  <row Id="8830" PostId="9063" Score="0" Text="Thanks for correcting. I will edit the details." CreationDate="2015-11-26T15:04:30.380" UserId="14368" />
  <row Id="8831" PostId="9018" Score="0" Text="The fact that the terms collide fairly common. You should try a basic tf-idf approach with logistic regression as a first pass to see if you really have an issue" CreationDate="2015-11-26T15:45:19.477" UserId="12306" />
  <row Id="8832" PostId="9041" Score="0" Text="If a cluster is (123) then you get pairs (12), (13), (23)." CreationDate="2015-11-26T15:54:46.417" UserId="924" />
  <row Id="8833" PostId="694" Score="3" Text="I'm voting to close this question as off-topic because is become a poster example of why recommendations and &quot;best&quot; questions don't work in the format. The accepted answer is factually inaccurate after 12 months (PyLearn2 has in that time gone from &quot;active development&quot; to &quot;accepting patches&quot;)" CreationDate="2015-11-26T17:33:07.673" UserId="836" />
  <row Id="8834" PostId="6176" Score="0" Text="this is a hard problem - have you benchmarked against another model?  say a simple logistic regression?  It's possible that in the format your current data is in, you can't do much better.  The results where you test on data the model has already seen aren't useful - you're likely overfitting." CreationDate="2015-11-27T04:59:42.627" UserId="12306" />
  <row Id="8835" PostId="9071" Score="0" Text="the above post is mine (something goofy is happening and after I registered it is not showing as mine). So, if my features are say like persons geographic details,age,occupation etc and my target prediction value need to be say persons monthly spending then how would I make those non numeric features to be encoded so as to make the prediction  for numeric value?" CreationDate="2015-11-27T05:47:42.450" UserId="14384" />
  <row Id="8836" PostId="9070" Score="0" Text="So, if my features are say like persons geographic details,age,occupation etc and my target prediction value need to be say persons monthly spending then how would I make those non numeric features to be encoded so as to make the prediction for numeric value?" CreationDate="2015-11-27T05:48:56.127" UserId="14384" />
  <row Id="8837" PostId="9071" Score="0" Text="@JasonDonnald I edited my answer to include a link discussing methods for vectorizing categorical data.  Things like age are already numeric." CreationDate="2015-11-27T05:55:48.867" UserId="12306" />
  <row Id="8838" PostId="9070" Score="0" Text="Yes, encode them and use them in the algorithm.  A  [helpful link](http://appliedpredictivemodeling.com/blog/2013/10/23/the-basics-of-encoding-categorical-data-for-predictive-models) which might help you." CreationDate="2015-11-27T05:57:48.583" UserId="11097" />
  <row Id="8839" PostId="9065" Score="0" Text="Thanks. But I think Stanford NER uses CRF and that is particularly useful when you want to predict Named Entities using context of surrounding words. In a resume that is not the case." CreationDate="2015-11-27T07:24:52.290" UserId="14368" />
  <row Id="8842" PostId="9065" Score="0" Text="Infact how useful will be NER when the text is not completely unstructured. Resumes have some structural information associated with them." CreationDate="2015-11-27T08:27:51.820" UserId="14368" />
  <row Id="8843" PostId="9075" Score="0" Text="Thanks!  Nice find." CreationDate="2015-11-27T09:12:43.283" UserId="11097" />
  <row Id="8844" PostId="9016" Score="0" Text="you can create a cron job that runs your script and automatically sends an email to the users" CreationDate="2015-11-27T10:10:40.640" UserId="6550" />
  <row Id="8845" PostId="9077" Score="0" Text="Thank you. How can I do the replacement on the same dataset?" CreationDate="2015-11-27T10:33:49.603" UserId="3151" />
  <row Id="8846" PostId="9077" Score="0" Text="You can simply replace `newdata` with `test_stopword`" CreationDate="2015-11-27T10:37:45.120" UserId="11097" />
  <row Id="8847" PostId="9077" Score="0" Text="It doesn't work. it creates a new column and puts the old column in row.names." CreationDate="2015-11-27T10:57:26.187" UserId="3151" />
  <row Id="8848" PostId="9077" Score="0" Text="@HamidehIraj Oh!  Okay. You want to replace the existing one with the new ones, right? It's just a small tweak in that function :)   Anyway, I'll edit the answer in a while. I'm travelling!" CreationDate="2015-11-27T10:59:19.163" UserId="11097" />
  <row Id="8849" PostId="9065" Score="0" Text="Resumes aren't written in complete sentences (most of the time). So standard NLP methods aren't a good match." CreationDate="2015-11-27T12:42:53.193" UserId="14368" />
  <row Id="8850" PostId="9065" Score="0" Text="That's an interesting point - though if you have meaningful structure, why do you need this extraction? I'll think about the no-full-sentences hurdle" CreationDate="2015-11-27T13:27:01.477" UserId="12306" />
  <row Id="8851" PostId="9065" Score="0" Text="The problem is the structure varies a lot. Infact I should say that resumes are poorly structured. We have tried to extract HTML using Apache Tika and bold, italic, tables, bullets of text are intact but visual formatting gets mangled. &#xA;&#xA;By structure I mean to say that you can identify visually similar information from a resume. And guess that one part of text is infact a company based on visually similar structure." CreationDate="2015-11-27T13:36:03.647" UserId="14368" />
  <row Id="8853" PostId="9077" Score="0" Text="In addition, I have test_stopword$texts not texts vector, so I am looking for something that works on the dataframe." CreationDate="2015-11-27T15:14:56.867" UserId="3151" />
  <row Id="8854" PostId="9065" Score="0" Text="A visual approach using OCR is interesting. You could also probably simply hit an entity resolution api or Freebase for each word that is capitalized and see if any institutions/companies come back as the first result" CreationDate="2015-11-27T18:18:40.590" UserId="12306" />
  <row Id="8855" PostId="5357" Score="0" Text="I appreciate wanting to have a wide variety of skills, however, if your company allows for teams of a variety of backgrounds, you're real best bet is to bring in a rock solid software engineer.  C/C++ can be extremely efficient, however, getting the efficiency out of these languages takes a lot of experience.  IMO, you're better off developing your ability as a scientist, which is a scarce skill set, and benefit from a diverse team." CreationDate="2015-11-27T21:31:51.253" UserId="8041" />
  <row Id="8856" PostId="9068" Score="0" Text="I think word2Vec is really the most accurate answer to the question.  There is a really nice Kaggle tutorial on how to train &amp; use one:&#xA;https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors" CreationDate="2015-11-27T21:50:55.333" UserId="8041" />
  <row Id="8857" PostId="9088" Score="0" Text="Yes it uses lineage for fault tolerance and lazily evaluates. So this saving causes faster execution?" CreationDate="2015-11-27T21:51:11.850" UserId="14391" />
  <row Id="8858" PostId="9006" Score="1" Text="What's the Point is great." CreationDate="2015-11-27T21:52:17.110" UserId="8041" />
  <row Id="8859" PostId="9088" Score="0" Text="It can; learning the right places to cache results is a bit of an art when it comes to spark programing.  I've been able to speed up programs by a good amount of time by caching in strategic points in time." CreationDate="2015-11-27T21:56:30.577" UserId="8041" />
  <row Id="8860" PostId="9088" Score="0" Text="So just to sum up, the 10x speed that Spark achieves on disk compared to Map reduce is due to lazy evaluation?" CreationDate="2015-11-27T22:07:49.423" UserId="14391" />
  <row Id="8861" PostId="9088" Score="1" Text="No, it's just a part of it.  The majority of the speed up is the in memory performance aspect you noted earlier.  Caching is stored in memory as well, as opposed to map-reduce which actually writes to disk between stages (and is the biggest bottleneck)." CreationDate="2015-11-27T23:52:20.233" UserId="8041" />
  <row Id="8862" PostId="9089" Score="0" Text="Is it a compulsion that you need a similar visualization?  Cause, there are better ones which can cater to your problem statement" CreationDate="2015-11-28T01:47:31.833" UserId="11097" />
  <row Id="8863" PostId="5357" Score="0" Text="@j.a.garner I hear you and my main company does have scaled agile scrum teams. However (i) I do science with C/C++ and I think better so than I could with just scripting languages (ii) I'm also a 1 man entrepreneur/consultant on the side and (iii) I don't work at a software/tech company so I'm about as high tech and programming knowledgeable as we get." CreationDate="2015-11-28T01:48:06.693" UserId="2723" />
  <row Id="8864" PostId="9089" Score="0" Text="Doesn't have to be similar, just has to be a network visualisation of sorts." CreationDate="2015-11-28T02:05:33.897" UserId="14397" />
  <row Id="8865" PostId="9068" Score="0" Text="Thanks, I forgot the word2vec link, I'll add one (in addition to the one you list here!)" CreationDate="2015-11-28T04:54:15.837" UserId="12306" />
  <row Id="8866" PostId="9085" Score="0" Text="You could give later votes (good or bad) a lower weight.  But then someone that gets friend to vote early will get a higher weight." CreationDate="2015-11-28T14:12:19.713" UserId="13285" />
  <row Id="8867" PostId="9092" Score="0" Text="So the DAG is the contributing factor then? Just like the way Tez speeds up Hive" CreationDate="2015-11-28T16:02:17.127" UserId="14391" />
  <row Id="8868" PostId="8621" Score="0" Text="Why can't you mix all the data sets and select validation set properly, so that is contains required amount of _significant_ attribute?&#xA;Is your data time-dependant? Please add the dataset into the question body. This is a cross-validation problem. Please check out relative threads: http://datascience.stackexchange.com/questions/tagged/cross-validation" CreationDate="2015-10-28T07:46:04.480" UserId="97" />
  <row Id="8869" PostId="9085" Score="0" Text="There's a good analogue to this in time series analysis- smoothing is accomplished via a moving window.  Windows were once step functions (just like your &quot;window&quot;), but perform much better when they are gaussians.  You should turn your window into a gaussian and weight the votes by the value of the gaussian.  I'll try to turn this into an answer, but am really busy now.  Check out exponentially weighted moving average (EWMA) but don't move it i.e. EWA.  Its nice since everything becomes smooth and you get away from the quantum jumps from contributing scores to non-contributing scores." CreationDate="2015-11-28T18:24:42.237" UserId="9420" />
  <row Id="8870" PostId="9111" Score="0" Text="Your argument that it would sound awkward is strange. Of course, I would rather write &quot;Semantic Classification of 3D Entities Based on Spatial Feature Descriptors&quot; instead of &quot;Semantic Classification: Classification of 3D Entities Based on Spatial Feature Descriptors&quot;. Writing a paper is usually a lot of work. I'm certain that people spend quite a bit of work in the title. But I guess I should simply try to get an author of one of those papers to answer my question." CreationDate="2015-11-28T21:06:13.603" UserId="8820" />
  <row Id="8871" PostId="9086" Score="0" Text="If you did have predictive capability to believe a certain user had a high likelihood of an exception what would you do? The goal is optimize the application. Are you trying to refine which bugs engineers should spend their time on instead of just fixing known bugs in the application? Feature engineering may be very important to this task. Also, you may want to consider logistic regression which will produce a 0..1 value which may be interpreted as a probability." CreationDate="2015-11-28T21:31:16.143" UserId="13684" />
  <row Id="8872" PostId="9111" Score="0" Text="My point was that it's generally regarded as awkward to repeat the same word multiple times in a title/sentence - hence the use of the word &quot;labeling&quot; instead of repeating &quot;classification&quot;. Really a minor point, I guess. Perhaps the authors used &quot;Labeling&quot; as a search-engine trip to show up for queries on that topic, despite the fact that classification is the same thing. &#xA;&#xA;I personally like the title they decided to go with the best :)&#xA;&#xA;Also the proof is in the pudding, I'm assuming you've read the papers and noticed that what they are doing is classification at its core." CreationDate="2015-11-28T23:11:18.050" UserId="12575" />
  <row Id="8873" PostId="9074" Score="0" Text="See also: https://www.researchgate.net/post/Is_there_a_difference_between_classification_and_labeling#view=565a459664e9b26c378b4567" CreationDate="2015-11-29T07:58:59.993" UserId="8820" />
  <row Id="8874" PostId="8813" Score="0" Text="I have tried to export the graph values, with limited success. Could you please share the data?" CreationDate="2015-11-29T08:32:00.860" UserId="12527" />
  <row Id="8875" PostId="9086" Score="2" Text="I think you are overshooting in the objectives. Don't treat ML as a black box to do magic. You have to ask the right questions (and have adequate data for that) to get any result." CreationDate="2015-11-29T11:10:09.987" UserId="924" />
  <row Id="8876" PostId="9085" Score="2" Text="What if there is a reason for that? Say in Google Play Store, a new version of an app is uploaded. And oops, it is broken. Of course a lot of people will give a 1 star rating then, without being not reasonable. I would look for other indicators of manipulation." CreationDate="2015-11-29T11:13:35.337" UserId="924" />
  <row Id="8877" PostId="6660" Score="0" Text="I have the similar problem, in my dataset, V1 is my targeted variable, it has 8 classes, V2 to V5 are all binary variables. When I do KNN, I need to use Jaccard to measure my &quot;distance&quot;, but there r no KNN package in R using Jaccard. Could you please tell me how to solve this problem? Thank you very much" CreationDate="2015-11-29T02:49:18.693" UserId="14409" />
  <row Id="8878" PostId="9036" Score="0" Text="I would imagine that dithering acts as a cross between simulated annealing and stochastic resonance.  In simulated annealing the noise is added to make sure the global optimal is reached, not just a local one.  In stochastic resonance the noise couples to a weak nonlinear phenomena - it is how the orbit of Jupiter can influence climate on earth." CreationDate="2015-11-29T13:52:07.847" UserId="8552" />
  <row Id="8879" PostId="9038" Score="0" Text="It is not just about the &quot;pretty picture&quot; but the purpose of visualizing high dimensional data is similar for visualizing regular 2/3 dimensional data. e.g correlation, boundaries and outliers." CreationDate="2015-11-29T16:53:14.813" UserId="5177" />
  <row Id="8880" PostId="8846" Score="0" Text="Dawny, how can I add the reference-request tag?" CreationDate="2015-11-29T18:15:07.177" UserId="12867" />
  <row Id="8881" PostId="9118" Score="0" Text="Thanks for the recommendation, glad to know the technical terms of what I am after. &#xA;&#xA;The issue with a trimmed mean is that outfits who never appear on the first page almost get no 1-votes, whereas those on the front page get 10% or more. If I trim the top and bottom 2% the problem is still there for front-pagers and it hurts second-pagers greatly.&#xA;&#xA;To prevent manipulation I do delete votes that seem far from the mean when grouped by IP and geographic region. However this is a clean up task I run every few months, what is in question here is how to handle it on the fly" CreationDate="2015-11-29T18:44:18.890" UserId="14392" />
  <row Id="8882" PostId="9117" Score="0" Text="So, performance on the same holdout (test) set was worse training on the full set vs. a subset or was the test set different? And the distribution of +/- records were the same? If the majority records are easily classifiable you can try sub-sampling this class and retaining all minority records to even out the distribution a bit." CreationDate="2015-11-29T19:51:51.723" UserId="13684" />
  <row Id="8883" PostId="9118" Score="2" Text="If you do this on-the-fly as you discussed above, you become much slower. What if there is a reason many people start to give low marks (e.g. because it has become known the product has a problem?)" CreationDate="2015-11-29T19:52:24.560" UserId="924" />
  <row Id="8884" PostId="9038" Score="0" Text="@eliasah: I understand that. But the space that you project your data to isn't the original space anymore, which can distort some of the shapes in the high dimensions. Say you have a blob in 4 dimensions. As soon as you project it down to 2D or 3D, your structure is already destroyed." CreationDate="2015-11-30T00:02:19.160" UserId="8774" />
  <row Id="8885" PostId="8846" Score="0" Text="It is already added :)" CreationDate="2015-11-30T01:30:26.327" UserId="11097" />
  <row Id="8888" PostId="9120" Score="0" Text="Having a mapping from the projected space to the original space makes sense. However, are there any other use cases?" CreationDate="2015-11-30T02:18:07.950" UserId="8774" />
  <row Id="8890" PostId="2587" Score="0" Text="Just a small confirmation.  n-fold and k-fold CV's are both same, right?" CreationDate="2015-11-30T05:55:08.847" UserId="11097" />
  <row Id="8891" PostId="9120" Score="0" Text="I also looked at chapter 4 of &quot;Illuminating the Path, The Research and Development Agenda for Visual Analytics&quot;. It mentions nothing about high dimensional visualizations on a visible subspace." CreationDate="2015-11-30T07:29:58.230" UserId="8774" />
  <row Id="8892" PostId="2587" Score="0" Text="yes 'n','k' is variable" CreationDate="2015-11-30T09:21:21.987" UserId="5091" />
  <row Id="8894" PostId="2587" Score="0" Text="Thanks for clarifying!&#xA;&#xA;And, did the answer helped you?  Noticed that it has been unanswered since a long time, so did a quick Google search, and added the most convincing result.  :)" CreationDate="2015-11-30T10:26:31.243" UserId="11097" />
  <row Id="8895" PostId="6116" Score="0" Text="Nice question.  Been there, done(and doing) that :D" CreationDate="2015-11-30T11:12:01.837" UserId="11097" />
  <row Id="8896" PostId="8686" Score="0" Text="There's also the [crab library](http://muricoca.github.io/crab/install.html) however I've never used it so I cannot comment on its performance." CreationDate="2015-11-02T13:22:29.403" UserId="13833" />
  <row Id="8897" PostId="2587" Score="0" Text="Yes, see the following links: http://www.quora.com/In-what-way-can-I-use-the-CrossFoldLearner-class-in-Mahout-v0-8-0-9-to-perform-K-fold-cross-validation https://mahout.apache.org/users/classification/logistic-regression.html" CreationDate="2014-12-04T20:25:54.433" UserId="5280" />
  <row Id="8898" PostId="3719" Score="0" Text="This article shows some details on scalability, special advantage for Titan.&#xA;https://groups.google.com/forum/#!topic/orient-database/CpPh42ukfH4" CreationDate="2015-01-12T10:31:34.573" UserId="7764" />
  <row Id="8899" PostId="9117" Score="1" Text="What metric are you using? The smaller dataset has a chance of containing almost no positive samples. In that case you'd learn to basically always guess the negative class, which will perform very well in terms of accuracy." CreationDate="2015-11-30T13:34:46.827" UserId="12306" />
  <row Id="8900" PostId="9129" Score="0" Text="There's an infinite number of transformations, eventually you can find one that can give you any answer you want. Just ignore the fact you chose a transformation from the infinity of transformations when you write it up with your significant p-value." CreationDate="2015-11-30T13:45:01.350" UserId="471" />
  <row Id="8901" PostId="9129" Score="0" Text="Less sarcastic response: What is the question you are asking of the data? Chasing transformations to get a &quot;good fit&quot; or a large R-squared is not the way we do statistics." CreationDate="2015-11-30T13:45:59.760" UserId="471" />
  <row Id="8902" PostId="9129" Score="0" Text="I didn't mean to force a fit - I just wanted to make sure there is any." CreationDate="2015-11-30T13:54:40.693" UserId="13481" />
  <row Id="8904" PostId="9129" Score="0" Text="You can fit a line to any data. Are you trying to determine trend, predict from regression, cluster, classify, ...?" CreationDate="2015-11-30T14:56:11.573" UserId="13684" />
  <row Id="8905" PostId="9129" Score="0" Text="I wanted to predict from regression." CreationDate="2015-11-30T14:56:37.697" UserId="13481" />
  <row Id="8906" PostId="9129" Score="1" Text="The data certainly appears non-linear. Have you considered regression trees (CART)?" CreationDate="2015-11-30T15:21:30.930" UserId="13684" />
  <row Id="8907" PostId="9129" Score="0" Text="Would this bring me to a point where I come up with a prediction model (equation) or something? I also wonder what the applicability limits of such an approach are." CreationDate="2015-11-30T15:26:06.910" UserId="13481" />
  <row Id="8908" PostId="9129" Score="0" Text="Basically it gives you a conjunction_of_features =&gt; value. The decision boundaries are linear. See here for details. http://www.stat.cmu.edu/~cshalizi/350-2006/lecture-10.pdf" CreationDate="2015-11-30T15:33:47.690" UserId="13684" />
  <row Id="8909" PostId="9136" Score="1" Text="IMHO `data-wrangling` should be a tag here, but I lack the reputation to create it." CreationDate="2015-11-30T17:45:55.777" UserId="12963" />
  <row Id="8910" PostId="9135" Score="1" Text="Thanks for your answer. I've read xgboost docs. But I meant a bit different thing. Not the code of using a framework. But the code of the algorithm of classifier itself." CreationDate="2015-11-30T18:00:25.140" UserId="14434" />
  <row Id="8911" PostId="9135" Score="0" Text="Ohh.  Thanks for letting me know :)   I guess I've misunderstood your question.  And welcome to the site!" CreationDate="2015-12-01T01:45:08.553" UserId="11097" />
  <row Id="8912" PostId="9137" Score="0" Text="@ Michaelg, In fact, the dependent variable (a productivity index) was significantly improved over the study period (10 years). Therefore, the year of the observation was considered as a predictor variable which had a positive impact on the dependent variable. However, the year itself doesn’t have any effect on the dependent variable per se; rather, it is the other factors occurring in the same time period which result in improvements.  Hence, the latest observations are more close to the reality and more important." CreationDate="2015-12-01T04:00:13.653" UserId="12867" />
  <row Id="8913" PostId="9137" Score="0" Text="You meant multiply the dependent variable by the year of observation? Can you provide some references? Thank you." CreationDate="2015-12-01T04:00:32.610" UserId="12867" />
  <row Id="8914" PostId="9137" Score="0" Text="I see. It is more clear now. You want to weight by the date the independent factors used for the regression. Thus using the timestamp integer to transform your variables may improve your model." CreationDate="2015-12-01T04:16:25.110" UserId="12490" />
  <row Id="8915" PostId="9137" Score="0" Text="For a reference, I would recommend to have a look into the weighting average techniques used for recommender systems. I found the recommender system course on [coursera](https://www.coursera.org/learn/recommender-systems/home/welcome) very useful to understand correlation-weighted average." CreationDate="2015-12-01T04:20:48.087" UserId="12490" />
  <row Id="8917" PostId="9140" Score="0" Text="Did you try doing k-means clustering?" CreationDate="2015-12-01T06:39:16.720" UserId="11097" />
  <row Id="8918" PostId="9140" Score="0" Text="@Dawny33 :yes, similar results." CreationDate="2015-12-01T06:53:30.970" UserId="14427" />
  <row Id="8919" PostId="9129" Score="0" Text="Just eyeballing the blue squares, it looks no different to a random scatterplot. Whatever value of X you give me, I'll tell you the Y value is equally likely to be anywhere between 0 and 7000. That data just has *no* predictive power. The purple look the same and the red is impossible to tell because the graphic is not well designed. Show the three sets of data on separate plots because they tell different stories." CreationDate="2015-12-01T08:45:52.387" UserId="471" />
  <row Id="8920" PostId="9143" Score="0" Text="Yes! Scaling solved the problem. I had scaled the variables individually but not with respect to each other -how stupid. Thanks a lot!" CreationDate="2015-12-01T09:43:43.190" UserId="14427" />
  <row Id="8921" PostId="9142" Score="1" Text="Can you please include also a small example of your training, testing, validating data AND the data you try to use for the prediction? Sometimes, a slightly different structure on the datasets could lead to issues on the prediction." CreationDate="2015-12-01T09:49:08.823" UserId="201" />
  <row Id="8922" PostId="9135" Score="0" Text="@shda I have edited the answer with the relevant links. Do let me know if you find it useful :)" CreationDate="2015-12-01T11:53:57.980" UserId="11097" />
  <row Id="8924" PostId="9138" Score="0" Text="which functionality do you want?  In &quot;abcdef&quot; do you want to count &quot;a&quot;, &quot;ab&quot;, &quot;bc&quot;, ..... &quot;abcdef&quot;, or do you want it to only show up as &quot;abcdef&quot;?" CreationDate="2015-12-01T14:15:57.120" UserId="12306" />
  <row Id="8926" PostId="9138" Score="0" Text="As I mentioned above, I only want to extract `abcd` here, and `ef` maybe. since `abcdef` is not a `high frequency` phrase, and `a` or `ab` or `abc` is most likely a sub-string of the high frequency phrase `abcd`." CreationDate="2015-12-01T14:35:12.477" UserId="14445" />
  <row Id="8927" PostId="9138" Score="0" Text="I see - it was the &quot;maybe&quot; piece that was confusing. It sounded like you had predefined substrings you cared about/wanted to ignore, not just based on frequencies" CreationDate="2015-12-01T14:55:34.767" UserId="12306" />
  <row Id="8929" PostId="9149" Score="0" Text="tf-idf is a good idea, but it would require me to know beforehand the size of the boilerplate text. I guess I could keep taking steps up and threshold them," CreationDate="2015-12-01T21:09:35.757" UserId="12963" />
  <row Id="8930" PostId="9149" Score="0" Text="This uses counts, not tf-idf. The idf would diminish a token value if it appeared in many documents. Also if you had a frequent text 'a b c d e' where each letter represents a word and you only decided to have a maximum of 3-word phrases then a_b_c, b_c_d, and c_d_e would also be frequent. So, it may not be necessary to know the maximum length of frequent phrases in advance." CreationDate="2015-12-01T21:22:55.027" UserId="13684" />
  <row Id="8931" PostId="9156" Score="0" Text="It seems appropriate to match Timmy with the closest feature cluster and predict via that until you have enough data to change models." CreationDate="2015-12-01T21:47:09.720" UserId="13578" />
  <row Id="8932" PostId="9109" Score="0" Text="Try using R? Here is an example: http://www.r-bloggers.com/genetic-algorithms-a-simple-r-example/&#xA;I also agree with the answer below. Writing your own code to do a simple GA wouldn't be too difficult." CreationDate="2015-12-01T22:23:51.430" UserId="141" />
  <row Id="8933" PostId="9142" Score="0" Text="@Tasos - The data I used for training, testing, validating and predicting are all the same format, i.e. col 0 is the classification and cols 1-59 are numerical inputs. In the code above, I just used the training data to do the prediction. I edited the question accordingly. Thank you for your help!" CreationDate="2015-12-02T04:06:53.537" UserId="13625" />
  <row Id="8934" PostId="8642" Score="0" Text="Nice question.  Are there any online links, which can help us learn more about the concept of _concept learning_?" CreationDate="2015-12-02T07:11:10.437" UserId="11097" />
  <row Id="8935" PostId="9156" Score="0" Text="Knn approaches seem worth a try, yes." CreationDate="2015-12-02T07:28:48.890" UserId="924" />
  <row Id="8936" PostId="9165" Score="1" Text="Sorry for a noob question :)&#xA;&#xA;What do you mean by *Y dataset is more influenced by the X dataset?*" CreationDate="2015-12-02T08:55:10.647" UserId="11097" />
  <row Id="8937" PostId="9165" Score="0" Text="It is visible that an increase in X produces a decrease in both Y1 and Y2. My question could be interpreted as follows: which Y dataset decreases the most, given the increase in X? Is the slope everything I need?" CreationDate="2015-12-02T08:56:39.793" UserId="13481" />
  <row Id="8938" PostId="9165" Score="1" Text="Thanks for explaining.  I think ANCOVA is the technique you're looking for.  Pl have a look at the answer and let me know if it answered your problem :)" CreationDate="2015-12-02T09:02:33.447" UserId="11097" />
  <row Id="8939" PostId="9135" Score="0" Text="Full xgboost source is rather complicated example) But i'll give it a try" CreationDate="2015-12-02T12:10:35.883" UserId="14434" />
  <row Id="8940" PostId="9167" Score="0" Text="A [similar question](http://stats.stackexchange.com/q/56302/84191)" CreationDate="2015-12-02T14:40:47.857" UserId="11097" />
  <row Id="8943" PostId="9159" Score="0" Text="This is too broad -- start with a description of your data and what your constraints are?" CreationDate="2015-12-02T14:44:50.763" UserId="21" />
  <row Id="8944" PostId="5973" Score="0" Text="Hey If you get your answer please suggest me some steps , I am doing stock prediction with 25 unknown feature(Parameter) , weight and rate for 5 windo (day-2,day-1,intraday,day+1,day+2) ...&#xA;I am doing with python .please suggest me something ." CreationDate="2015-12-01T21:12:58.973" UserId="14471" />
  <row Id="8945" PostId="9172" Score="0" Text="I'm not sure if &quot;shell&quot; is the right word. Probably &quot;spheres&quot; is better? Google finds for both, &quot;concentric spheres&quot; and &quot;concentric shells&quot; images." CreationDate="2015-12-02T16:17:39.303" UserId="8820" />
  <row Id="8946" PostId="9172" Score="1" Text="It sounds like you'd do best explicitly calculating the magnitude of each vector and then clustering based on magnitude explicitly." CreationDate="2015-12-02T16:31:46.493" UserId="12306" />
  <row Id="8947" PostId="9172" Score="0" Text="@jamesmf If I know that my data looks like this, I cluster the integer distance to 0, of course. But that would be &quot;cheating&quot;. You can always apply functions to make your data linearly separable. The question is how complicated the data has to get until the algorithm fails. For single layer neural neurons it was XOR, for example. Of course, you could say that the single layer neuron could classify XOR correct if you give it XOR as input, but that is not the point..." CreationDate="2015-12-02T22:00:17.833" UserId="8820" />
  <row Id="8948" PostId="9176" Score="0" Text="This is not from a textbook. I've asked this question myself." CreationDate="2015-12-02T22:18:22.510" UserId="8820" />
  <row Id="8949" PostId="9176" Score="0" Text="Sorry, I have a really low opinion on clustering chapters of machine learning textbooks... ;-)" CreationDate="2015-12-02T22:36:49.360" UserId="924" />
  <row Id="8950" PostId="9172" Score="1" Text="I understand your point, I simply meant  that choosing an arbitrary distance metric for kmeans was not meaningfully different than explicitly calculating magnitude and using &quot;traditional&quot; distance metrics." CreationDate="2015-12-02T22:46:07.817" UserId="12306" />
  <row Id="8951" PostId="9172" Score="0" Text="@jamesmf Ah, ok, now I get what you mean. Yes, you're right. I didn't think of that before. However, I'm still happy I've asked the question here, because I could learn something by the answer :-)" CreationDate="2015-12-02T22:59:04.707" UserId="8820" />
  <row Id="8952" PostId="9173" Score="0" Text="So in order to check if my RMSE is good or bad, I can compare it with the standard_deviation to get a rough idea? The standard_deviation should be calculated in actual label values? How can I calculate the standard_deviation?" CreationDate="2015-12-02T23:07:27.957" UserId="14384" />
  <row Id="8953" PostId="9172" Score="0" Text="Agreed, Anony-Moose expressed it well." CreationDate="2015-12-02T23:09:18.597" UserId="12306" />
  <row Id="8954" PostId="9173" Score="0" Text="Depends on the package you're using, but I'm certain every toolkit for ML will have an easy stdev function. It's also easy to calculate yourself. Basically the StDev is a rough idea of if you're learning anything at all. If you want a better idea, compare to other models." CreationDate="2015-12-02T23:14:40.790" UserId="12306" />
  <row Id="8955" PostId="9138" Score="0" Text="Most likely? And if there were two abcdef?  Define the rules." CreationDate="2015-12-02T23:44:49.077" UserId="13285" />
  <row Id="8956" PostId="9032" Score="0" Text="@{neil-slater} thanks for the input! What doesn't feel quite right (to me) in these Deep Learning Examples, is that they are more of a recipe: yes, you train an autoencoder to simplify the data and you also use labeled data to train the DLN. It's true that autoencoder trick &quot;helps&quot;, but it doesn't appear to be theoretically necessary: it's just **harder** to train without." CreationDate="2015-12-03T01:17:38.560" UserId="14009" />
  <row Id="8957" PostId="9178" Score="0" Text="Where is your data stored? If it is a SQL database then you should just group by the cluster and concept to get the confusion matrix. Pandas his a similar functionality and you can read quite easily most data sources into it." CreationDate="2015-12-03T07:17:31.280" UserId="13727" />
  <row Id="8958" PostId="6316" Score="0" Text="Did my answer provide help?" CreationDate="2015-12-03T07:55:33.940" UserId="10834" />
  <row Id="8959" PostId="9186" Score="0" Text="You are correct that I am asking for a library. Thanks for your suggestion. Taking a look now..." CreationDate="2015-12-03T09:42:36.177" UserId="13625" />
  <row Id="8961" PostId="9031" Score="1" Text="Thanks a lot, this is what I want." CreationDate="2015-12-03T11:28:09.090" UserId="14339" />
  <row Id="8963" PostId="4904" Score="0" Text="@SarathaPriya  Bag of phrases is typically for text processing (not images)." CreationDate="2015-12-03T23:43:24.700" UserId="14518" />
  <row Id="8969" PostId="6508" Score="1" Text="You are already in the black-hole of &quot;Curse of Dimensionality&quot;. Nothings works before a dimensionality reduction." CreationDate="2015-12-04T15:52:37.413" UserId="8878" />
  <row Id="8970" PostId="9199" Score="0" Text="I already have a set of predictor variable . I dont understand why I would add noise in this scenario ." CreationDate="2015-12-04T17:08:46.820" UserId="14353" />
  <row Id="8972" PostId="9110" Score="0" Text="I want to use java specifically." CreationDate="2015-12-04T18:32:42.080" UserId="14340" />
  <row Id="8973" PostId="9109" Score="0" Text="There is another R package called &quot;GA&quot; which I use from time to time, despite my answer below :)  It has the advantage of [excellent documentation](http://www.jstatsoft.org/article/view/v053i04/v53i04.pdf) with many examples." CreationDate="2015-12-04T19:04:56.353" UserId="1077" />
  <row Id="8976" PostId="9209" Score="1" Text="Clustering (e.g. logistic regression) might provide some interesting results." CreationDate="2015-12-04T22:40:48.753" UserId="13578" />
  <row Id="8979" PostId="44" Score="1" Text="Two new alternatives that are worth mentioning are: SparkR https://databricks.com/blog/2015/06/09/announcing-sparkr-r-on-spark.html and h2o.ai http://h2o.ai/product/ both well suited for big data." CreationDate="2015-12-05T06:03:32.720" UserId="13023" />
  <row Id="8981" PostId="9196" Score="0" Text="Thanks... but i was really hoping for a piece of code or a software bundle... guess I have to do it myself... but isn't it curious that there is no single free application out there to do some simple Image Clustering (even based on some simple features such as color ?!)" CreationDate="2015-12-05T08:55:12.553" UserId="14504" />
  <row Id="8982" PostId="9190" Score="0" Text="Thank you... Your resource seems to be good enough... haven't got the time to implement it yet but it seems at least applicable enough... I wish there was a clean and ready code just for some simple Image Clustering..." CreationDate="2015-12-05T08:56:16.907" UserId="14504" />
  <row Id="8983" PostId="9190" Score="0" Text="@Cypher You can download the code of the article directly from there. There is a link in the first paragraph. However, I wouldn't expect a cleaner or simpler code, since clustering images is not the most trivial task." CreationDate="2015-12-05T09:01:34.217" UserId="201" />
  <row Id="8985" PostId="9196" Score="0" Text="Maybe it is not as simple, or not as useful?" CreationDate="2015-12-05T11:25:05.350" UserId="924" />
  <row Id="8986" PostId="9172" Score="0" Text="See also: [K-means Mahalanobis vs Euclidean distance](http://stats.stackexchange.com/a/37781/25741)" CreationDate="2015-12-05T11:27:16.237" UserId="8820" />
  <row Id="8990" PostId="135" Score="0" Text="Oh my goodness! &quot;As an evidence, in a data mining course in my university, the best final project was written in Python, although the others has access to R's rich data analysis library.&quot; And you want to have readers respect your analysis? wow. Could there be any other factors involved in the best project being a python project other than the language it was written in? really...." CreationDate="2015-12-05T18:35:17.533" UserId="10663" />
  <row Id="8991" PostId="6798" Score="1" Text="The Allen Institute for Artificial Intelligence (AI2) challenge is currently live on Kaggle. https://www.kaggle.com/c/the-allen-ai-science-challenge  I suppose the dataset will remain there after the competition has ended in case anyone needs it." CreationDate="2015-12-05T20:30:56.073" UserId="13023" />
  <row Id="8992" PostId="8707" Score="0" Text="You should check out sense2vec too http://arxiv.org/abs/1511.06388. In a nutshell it's word embeddings combined with Part-Of-Speech tagging. It's reported it made word embeddings more accurate by disambiguating homonyms. It would be interesting to see if it also improves performance in classification tasks." CreationDate="2015-12-05T20:50:33.397" UserId="13023" />
  <row Id="8993" PostId="8943" Score="0" Text="Could you share with us what kind of time series we are talking about? Is it financial, biological? that's because there are certain pre-processing steps specific for different types of problems." CreationDate="2015-12-05T20:58:30.040" UserId="13023" />
  <row Id="8994" PostId="6798" Score="0" Text="@wacax Thanks, do some Kaggle contests sometimes remove the data sets after the end of the contest?" CreationDate="2015-12-05T21:02:13.720" UserId="843" />
  <row Id="8995" PostId="6798" Score="1" Text="Not really, as far as I know. However this particular challenge does have a restriction, that the questions in this contest should not be published or shared and should only be used for your work in the competition. More info here: https://www.kaggle.com/c/the-allen-ai-science-challenge/forums/t/16891/reminder-questions-in-this-contest-should-not-be-published-or-shared" CreationDate="2015-12-05T21:13:29.330" UserId="13023" />
  <row Id="8996" PostId="6798" Score="0" Text="@wacax thanks, what a shame." CreationDate="2015-12-05T21:16:27.140" UserId="843" />
  <row Id="8997" PostId="6798" Score="0" Text="@wacax FYI [Do some Kaggle contest organizers remove the data sets after the end of the contest?](http://opendata.stackexchange.com/q/6596/1652)" CreationDate="2015-12-05T21:28:50.247" UserId="843" />
  <row Id="8998" PostId="9231" Score="0" Text="What are the _other features_, and why do you want to find their values?" CreationDate="2015-12-06T11:07:43.000" UserId="11097" />
  <row Id="8999" PostId="1069" Score="0" Text="&quot;I am able to generate a confusion matrix and compute a kappa.&quot; - what does that mean? What is $\kappa$? (So what exactly do you compute?)" CreationDate="2015-12-06T12:18:57.420" UserId="8820" />
  <row Id="9000" PostId="8943" Score="0" Text="The data represents temperatures taken from meat cooking in the oven." CreationDate="2015-12-06T22:07:59.953" UserId="14231" />
  <row Id="9001" PostId="9233" Score="2" Text="FYI: [Comprehensive list of activation functions in neural networks with pros/cons](http://stats.stackexchange.com/q/115258/12359)" CreationDate="2015-12-07T01:13:53.543" UserId="843" />
  <row Id="9002" PostId="683" Score="1" Text="The post should be added the tags semi-supervised and pu-learning. These tags still do not exist and currently I cannot create them." CreationDate="2015-12-07T07:11:29.437" UserId="13727" />
  <row Id="9003" PostId="9241" Score="1" Text="How do you initialise the weights? Have you tried checking the gradients produced by your backprop algorithm (this involves running the feed-forward network many times, with a small variation in each weight and measuring the difference at the output)?" CreationDate="2015-12-07T08:01:06.497" UserId="836" />
  <row Id="9004" PostId="9241" Score="0" Text="@NeilSlater I initialise the weights as  follows: rnd()×2×e -e [basically picks a small number between -e and e where e is just some near zero value]. I haven't done gradient checking yet, I'm not 100% clear on that process yet. I do plot J(theta) and J(theta)1-J(theta)0 [current J(theta) - last]." CreationDate="2015-12-07T08:11:33.187" UserId="14586" />
  <row Id="9005" PostId="9207" Score="0" Text="Looks like you may want to look at the Fourier transform" CreationDate="2015-12-07T08:57:07.590" UserId="816" />
  <row Id="9006" PostId="683" Score="0" Text="@DanLevin  Yeah, [tag: semi-supervised-learning] makes sense.  Added :)&#xA;&#xA;I'm not sure with the pu-learning part(atleast I'm not aware of it), so someone else can do it!" CreationDate="2015-12-07T10:51:11.097" UserId="11097" />
  <row Id="9007" PostId="683" Score="1" Text="PU-learning is a specific case of semi supervised learning. It is less common (7K results at Google) then semi supervised (298K results at Google) that this question is PU (the labeled dataset is just positives).   While the topic is discussed the academy (e.g., https://www.cs.uic.edu/~liub/NSF/PSC-IIS-0307239.html) it is possible that this question will be alone with this tag for quite a while." CreationDate="2015-12-07T11:19:23.817" UserId="13727" />
  <row Id="9008" PostId="9245" Score="0" Text="You might want to read about ridit https://en.m.wikipedia.org/wiki/Ridit_scoring" CreationDate="2015-12-07T13:45:43.647" UserId="12306" />
  <row Id="9009" PostId="9245" Score="0" Text="That is something I am looking for. Are there any other ideas?" CreationDate="2015-12-07T18:18:28.827" UserId="14596" />
  <row Id="9010" PostId="9226" Score="0" Text="Thank you for a great answer!" CreationDate="2015-12-07T19:21:08.880" UserId="14556" />
  <row Id="9011" PostId="9259" Score="0" Text="Thanks for the input (pun intended) but since both inputs are scores out of 100   both features are of the same scale so feature scaling isn't required (at least that's my understanding?). Nevertheless I did think of this as well and I have tried it with each value scaled to between 0 and 1 (score / 100) but the same problems arose." CreationDate="2015-12-08T08:46:26.143" UserId="14586" />
  <row Id="9012" PostId="5302" Score="0" Text="Might be a better fit on StackOverflow.   This is just data formatting.   What dev tools are comfortable with.  I am surprised Access is crashing at 1.5 million lines." CreationDate="2015-12-08T09:22:14.280" UserId="13285" />
  <row Id="9013" PostId="9262" Score="0" Text="By KL, do you mean Kullback-Leibler divergence?" CreationDate="2015-12-08T10:55:06.823" UserId="11097" />
  <row Id="9014" PostId="9262" Score="0" Text="Yes, exactly that!" CreationDate="2015-12-08T10:57:04.823" UserId="8206" />
  <row Id="9015" PostId="9262" Score="0" Text="By running `sklearn.metrics.mutual_info_score([1.346112,1.337432,1.246655], [1.033836,1.082015,1.117323])`, I get the value `1.0986122886681096`." CreationDate="2015-12-08T11:05:11.860" UserId="11097" />
  <row Id="9016" PostId="9262" Score="0" Text="Sorry, i was using values1 as [1, 1.346112,1.337432,1.246655] and values2 as values2 as [1,1.033836,1.082015,1.117323] and hence the difference value." CreationDate="2015-12-08T11:07:36.000" UserId="8206" />
  <row Id="9017" PostId="9264" Score="1" Text="I tried this too but this was returning negative values which, I think, is not a valid value. A little bit of research then got me to this result http://mathoverflow.net/questions/43849/how-to-ensure-the-non-negativity-of-kullback-leibler-divergence-kld-metric-rela which talks about how the input has to be a probability distribution. Guess that is where I made my mistake." CreationDate="2015-12-08T11:11:10.450" UserId="8206" />
  <row Id="9018" PostId="9264" Score="0" Text="Thanks for the help!" CreationDate="2015-12-08T11:11:24.347" UserId="8206" />
  <row Id="9019" PostId="9264" Score="0" Text="@Nanda  Thanks for the link. Mine returns `0.775279624079` for your inputs and the sklearn metrics return `1.3862943611198906`.  Confused still!   But, seems like including those value checks according to the qn, into the script should do :)" CreationDate="2015-12-08T11:14:38.283" UserId="11097" />
  <row Id="9020" PostId="9264" Score="1" Text="I know what you mean! I have tried 3 different functions to get 3 different values with the only thing common between them being that the result didn't &quot;feel&quot; right. The input values is definitely a logical error so changing my approach altogether!" CreationDate="2015-12-08T11:25:53.210" UserId="8206" />
  <row Id="9021" PostId="9264" Score="0" Text="@Nanda  Ahh, that's clear now :)  Thanks for explaining" CreationDate="2015-12-08T11:27:39.593" UserId="11097" />
  <row Id="9022" PostId="9259" Score="0" Text="Have you looked at the activations of each layer for a few iterations?" CreationDate="2015-12-08T13:06:08.907" UserId="12306" />
  <row Id="9023" PostId="9241" Score="0" Text="FYI: [How to verify that an implementation of a neural network works correctly?](http://stats.stackexchange.com/q/184290/12359)" CreationDate="2015-12-08T14:47:21.070" UserId="843" />
  <row Id="9024" PostId="9267" Score="0" Text="Thank you. The regularisation term keeps the weights small. I understand from your link and reading around that this helps avoid overfitting. How's keeping the weights from growing too much helping avoid overfitting?" CreationDate="2015-12-08T14:57:25.173" UserId="13736" />
  <row Id="9025" PostId="9267" Score="1" Text="@user79303 You might want to have a look at [this](http://stats.stackexchange.com/q/64208/84191)" CreationDate="2015-12-08T15:07:39.383" UserId="11097" />
  <row Id="9028" PostId="9273" Score="0" Text="Thanks. Can you pls explain how svd solves the problem in PCA and how taking sum of logs reduces the problem? Where is this sum of logs used in the naive bayes classifier?" CreationDate="2015-12-09T02:53:17.627" UserId="1151" />
  <row Id="9029" PostId="9273" Score="0" Text="These are more in depth questions, but I can provide some pointers. it &quot;solves&quot; it because you can obtain PCA from SVD. See here for an excellent article: http://arxiv.org/pdf/1404.1100.pdf. SVD is preferred because of the lack of the covariance matrix in its computation.  Sum of logs in naive bayes: http://blog.datumbox.com/machine-learning-tutorial-the-naive-bayes-text-classifier/" CreationDate="2015-12-09T03:37:40.210" UserId="13684" />
  <row Id="9030" PostId="9129" Score="0" Text="If you provide some of the data we can test it out.  At first glance it looks like  the data are in a polar coordinate system.  Plot the data in cartesian coordinates  assuming that x and y are polar coordinates.  If there is, do a regression between the new cartesian variables." CreationDate="2015-12-09T04:51:20.590" UserId="2981" />
  <row Id="9032" PostId="694" Score="0" Text="I can't vouch for it, but [Brainstorm](https://github.com/IDSIA/brainstorm) seems to be the spiritual successor to PyBrain and looks promising." CreationDate="2015-12-08T17:59:13.587" UserId="14617" />
  <row Id="9033" PostId="9276" Score="0" Text="Yes I did exactly that- scaled them and averaged the two. Also been playing with the weights manually. Eg : 0.2*Matrix1+0.8*Matrix2. and have some reasonable output(intuitively/business sense wise reasonable). I was wondering if there was a less...err..intuition driven way and more solid math-y way to do it. Though now I guess not, since there is no 'right' answer. (Too used to have data tagged with dependent variables )" CreationDate="2015-12-09T07:36:29.430" UserId="14427" />
  <row Id="9034" PostId="9279" Score="0" Text="Thank you. Can you please explain a bit more about them?" CreationDate="2015-12-09T10:15:25.823" UserId="11097" />
  <row Id="9035" PostId="9279" Score="1" Text="Added information in my comment. Please note that in my opinion any of these algorithm can't really be explained in one line, you'll have to read the articles." CreationDate="2015-12-09T13:57:46.810" UserId="1093" />
  <row Id="9036" PostId="9279" Score="0" Text="In addition, do you have code implementation of a Bandit algorithm? I would give it try on my dataset." CreationDate="2015-12-09T14:01:50.160" UserId="1093" />
  <row Id="9037" PostId="9282" Score="2" Text="and one more &#xA; - https://www.kaggle.com/c/criteo-display-ad-challenge" CreationDate="2015-12-09T14:30:27.463" UserId="14636" />
  <row Id="9038" PostId="9274" Score="1" Text="doc2vec is substantially different than performing word2vec on a corpus of articles rather than sentences.  doc2vec will learn representations of the articles themselves, rather than just the words." CreationDate="2015-12-09T15:33:03.497" UserId="12306" />
  <row Id="9039" PostId="9280" Score="0" Text="I dont have information of next year's data .No way predict can give any prediction without having a data for the predictor variable .Forecast uses a linear model (ARIMA is  linear) so it gives very crude predictions . However that is going to be my first step towards solving the problem . a time series model for the predictor's then a predictive model for the target" CreationDate="2015-12-09T20:50:55.143" UserId="14353" />
  <row Id="9040" PostId="9198" Score="0" Text="I liked this idea . currently building the model ." CreationDate="2015-12-09T20:51:20.387" UserId="14353" />
  <row Id="9041" PostId="9289" Score="0" Text="As I mentionned, summing temp does not make sense except to calculate an average.  I mean two days at 150 Ranking or two liters of water at don't make 4 at 300 R whereas two days of sales  or two liters of  wine at $150 do make  4  at  $300." CreationDate="2015-12-09T21:26:46.350" UserId="9503" />
  <row Id="9044" PostId="9289" Score="0" Text="Does't may sense to YOU.   Cook in 1 hour at 275 versus 45 minutes at 300.  Heat exposure versus time.  I have BS in chemical engineering and masters in math.  Entropy is the integral heat / temp dt from 0 to t.  That is summing temperature on steroids.   A physical property or measure does not have artificial analytical bounds." CreationDate="2015-12-09T21:45:22.467" UserId="13285" />
  <row Id="9045" PostId="9289" Score="0" Text="Well, cooking a piece of meat at 150 for one hour or 300 for 30 minutes do not get the same results in real life. But buying two pieces at 20  each does make a 40 total.   And no need for the agressive &quot;YOU&quot; or to make an exposé on surface vs volume, chemical composition, protein folding and heat exposure... I understand where you're coming from and thank you for your try at answering my question. &#xA;What make sense in your field does not necessary translate into other pratical domains." CreationDate="2015-12-09T22:19:16.123" UserId="9503" />
  <row Id="9046" PostId="9289" Score="0" Text="Other practical domains?   Work is is the integral f dot ds.  YOU don't need to italic bold.   You have no basis to assume a measure has limited analytics.  One counter example does not dismiss.  Here is social domain.  If I am real loud in class I may get kicked out in one day.   If I am semi loud for three day I may get kicked out.   Acoustics are not physically additive but the social effects may be.  In a ML environment where I am trying to identify relationships I would be stupid exclude summation as a possible correlation." CreationDate="2015-12-09T22:35:38.520" UserId="13285" />
  <row Id="9047" PostId="9292" Score="0" Text="One naive solution is to simply combine the time series data over some fixed window (say the last N months) with the immutable product features and train on that feature set." CreationDate="2015-12-09T23:03:47.077" UserId="12306" />
  <row Id="9048" PostId="9292" Score="0" Text="found this, panel data analysis:http://imai.princeton.edu/teaching/files/panel.pdf" CreationDate="2015-12-09T23:36:58.493" UserId="14642" />
  <row Id="9049" PostId="9278" Score="0" Text="Right on spot. I used the embeddings in a model and, just like you mentioned, there was a big improvement in the model's predictive performance when I used the entire article. So in what case would training sentence by sentence be superior." CreationDate="2015-12-10T00:11:31.923" UserId="13023" />
  <row Id="9050" PostId="9297" Score="0" Text="additional question. i'm crawling data from twitter. as we know, twitter will return data with json format. i do some process to reduce data, like i do when i get status from user that return group of tweet (200 tweet). i just calculate URL number after get data from one user, and make URL number feature. can i still named it as crawling when it return not original data from twitter?" CreationDate="2015-12-10T01:56:20.943" UserId="14647" />
  <row Id="9051" PostId="9285" Score="2" Text="You are thinking of [intensive and extensive properties](https://en.wikipedia.org/wiki/Intensive_and_extensive_properties)." CreationDate="2015-12-10T02:56:09.960" UserId="2981" />
  <row Id="9052" PostId="9297" Score="0" Text="I would call the first step as dummy encoding though. (Even, transformation is also close  :) )" CreationDate="2015-12-10T03:31:14.663" UserId="11097" />
  <row Id="9053" PostId="9280" Score="0" Text="you can pick some reasonable values for the attributes for the future (or assume last years' attributes remain constant) . good luck." CreationDate="2015-12-10T08:18:12.313" UserId="14588" />
  <row Id="9054" PostId="9303" Score="1" Text="I think this is off topic...." CreationDate="2015-12-10T09:17:06.613" UserId="9123" />
  <row Id="9057" PostId="9304" Score="0" Text="+1 for willing to help an off-topic question :)" CreationDate="2015-12-10T10:10:27.603" UserId="11097" />
  <row Id="9060" PostId="9306" Score="0" Text="I found [this](https://www.boozallen.com/media/file/The-Field-Guide-to-Data-Science.pdf) a lot helpful when I was starting to learn about the domain :)" CreationDate="2015-12-10T11:39:19.097" UserId="11097" />
  <row Id="9061" PostId="9235" Score="0" Text="The ambiguity you mention is exactly why I was looking for a different word.  I was trying to describe a setting in which I had all my robotic data (so, offline), but was updating the network with every data point separately (ie. online).&#xA;As a side note, I wouldn't say online training of SOMs is more common---it's just conceptually purer or simpler. In fact, according to &quot;Mr. SOM&quot;, Teuvo Kohonen himself, batch training in SOMs is faster and safer than online learning[1].&#xA;[1] Essentials of the Self-Organizing Map. Neural Networks, Vol. 37 (January 2013), pp. 52-65, by Teuvo K. Kohonen" CreationDate="2015-12-10T12:08:31.770" UserId="8714" />
  <row Id="9064" PostId="9309" Score="0" Text="&quot;you're correct that softmax guarantees a non-zero output&quot; - I know that this is theoretically the case. In reality, can it happen that (due to numeric issues) this becomes 0?" CreationDate="2015-12-10T14:30:54.380" UserId="8820" />
  <row Id="9065" PostId="9309" Score="0" Text="Good question. I assume it's perfectly possible for the exponentiation function to output 0.0 if your input is too small for the precision of your float. However I'd guess most implementations do add the tiny positive term to guarantee non-zero input." CreationDate="2015-12-10T14:50:35.473" UserId="12306" />
  <row Id="9067" PostId="9285" Score="0" Text="@Biswajit Banerjee Could you put that as an answer?  It's very close to what I'm looking for, I think. Enough that it add to the debate.  I'll try to reread the article to understand better. thanks." CreationDate="2015-12-10T15:52:44.243" UserId="9503" />
  <row Id="9069" PostId="9311" Score="0" Text="Note there is some ambiguity in the presentation of the second formula - it could in theory assume just one class and $i$ would then enumerate the examples." CreationDate="2015-12-10T16:24:12.747" UserId="836" />
  <row Id="9072" PostId="9313" Score="0" Text="Thank you! Is a good ideia use a cutoff prob and define a class and then compute accuracy.!" CreationDate="2015-12-10T17:23:32.380" UserId="14644" />
  <row Id="9073" PostId="9302" Score="0" Text="@jamesmf Right. I fixed that. I'm sorry." CreationDate="2015-12-10T17:52:15.920" UserId="8820" />
  <row Id="9074" PostId="9314" Score="1" Text="+1 for the information about Facebook-Insights library, any chance you've got a demo of how that works? I'd love to see code mining the Facebook graph API" CreationDate="2015-12-10T18:09:46.217" UserId="13023" />
  <row Id="9075" PostId="9314" Score="0" Text="@wacax We use it in our Django framework, in my team. But, I don't have any code in public repos, though!   However, the md doc there, is self-explanatory :)" CreationDate="2015-12-10T18:11:54.033" UserId="11097" />
  <row Id="9076" PostId="9280" Score="0" Text="can you quantify &quot;reasonable &quot; ?" CreationDate="2015-12-10T18:29:05.327" UserId="14353" />
  <row Id="9077" PostId="9307" Score="1" Text="It works fine for me! Try running it line by line, that can be a good way to debug code like this!" CreationDate="2015-12-10T18:45:26.250" UserId="13413" />
  <row Id="9079" PostId="9317" Score="0" Text="Suppose `A = c(1,1,2,2,3,3,4,4,5,5,6,6,9,11,15,17,19,19)` and `B = c(1,2,2,2,3,3,3,4,5,6,7,8,8,8)` what will your result look like? One possibility is `C=[1 1 2 2 3 3 4 4 5 5 6 6]`.  Another is `D=[1 2 2 2 3 3 3 4 5 6]`. Do you need `E = [1 1 2 2 2 3 3 4 4 5 5 6 6]`?" CreationDate="2015-12-10T23:29:00.403" UserId="2981" />
  <row Id="9080" PostId="8793" Score="0" Text="@UriMerhav That would have been nice to say so in the question." CreationDate="2015-12-11T02:55:57.327" UserId="843" />
  <row Id="9081" PostId="9241" Score="0" Text="As far as I see, there is no major flaw in your code. However, many little things can go wrong. I suggest you first try to implement the code using the provided skeleton codes, e.g., ex4.m, etc. In particular, using sigmoidGradient.m can help you check if the your gradient calculation is correct." CreationDate="2015-12-11T03:49:03.853" UserId="14463" />
  <row Id="9084" PostId="5836" Score="0" Text="Don't agree too much with this answer. There is only so much time to spend in exploration: the point is to make it count. This answer does not support that CNN would be a *likely* best fit for the OP's scenario." CreationDate="2015-12-11T17:03:53.847" UserId="14518" />
  <row Id="9086" PostId="9340" Score="0" Text="You could sum and then normalize on page count - square root of the sum of the squares - harmonic mean - average - product." CreationDate="2015-12-11T17:43:47.513" UserId="13285" />
  <row Id="9087" PostId="9326" Score="0" Text="Thank you for you answer, in fact it is 2D-array indexing using Numpy, for more details have a look at the link bellow (section: Indexing Multi-dimensional arrays): http://docs.scipy.org/doc/numpy-1.10.1/user/basics.indexing.html" CreationDate="2015-12-11T09:05:17.410" UserId="14674" />
  <row Id="9088" PostId="9343" Score="0" Text="Do you mean for(nm in names){ data &lt;- read.csv(&quot;https://...../api/dataservice?sql = select * from db where name IN nm &quot;) and then append the data together ?" CreationDate="2015-12-11T20:06:35.760" UserId="3314" />
  <row Id="9089" PostId="9343" Score="0" Text="Sorry, I should have added example code! I've updated my answer for you." CreationDate="2015-12-11T20:09:17.733" UserId="13413" />
  <row Id="9090" PostId="9343" Score="0" Text="I see. I will try it out. Thanks @Kyle." CreationDate="2015-12-11T20:11:30.907" UserId="3314" />
  <row Id="9091" PostId="9343" Score="0" Text="I am having difficulty: I need to input the names like this: where IN (\'name1\',\'name2\') .  How do I insert that \ (back slash) in there. I tried paste0, Everytime, i tried to use past0, it comes out as: (\\'name1\\',\\'name2\\') which is giving error. So I do need a single back slash followed by the single quote. Any help as to how to use paste to get me the single backslash ?" CreationDate="2015-12-11T20:57:40.427" UserId="3314" />
  <row Id="9092" PostId="9343" Score="0" Text="I'm not sure I completely follow...does the replacing the print line with the following fix it?   print(paste(&quot;https://...../api/dataservice?sql = select * from db where name IN \'&quot;, n, &quot;\'&quot;, sep=&quot;&quot;)) From that, I get the following example output: &quot;https://...../api/dataservice?sql = select * from db where name IN 'a'&quot;" CreationDate="2015-12-11T22:04:20.177" UserId="13413" />
  <row Id="9093" PostId="9344" Score="0" Text="I believe I addressed this in the comments on your other question (http://datascience.stackexchange.com/questions/9342/how-do-write-the-following-piece-of-code-in-r)! If that adequately fixes your problem, can you close this one? Thanks!" CreationDate="2015-12-11T22:08:30.623" UserId="13413" />
  <row Id="9094" PostId="9343" Score="0" Text="As I understand, in order to use sql command inside read.csv, in the WHERE clause, we need to enter the names as `\' name1\',\'name2\',etc` and not simply as 'name1,name2, etc' .... now when i am trying to use the paste function, it is giving me `\\'name1\\',\\'name2\\',etc`. I couldn't get rid off those extra backslashes. I need single backslash not double." CreationDate="2015-12-11T22:15:10.633" UserId="3314" />
  <row Id="9095" PostId="9344" Score="0" Text="Please see the comments. As I mentioned here, I couldn't get single backslash. paste function is giving me double backslashes like `\\'name1\\', etc`. Inside read.csv, double backslashes produces error. It only take single backslash." CreationDate="2015-12-11T22:17:09.710" UserId="3314" />
  <row Id="9096" PostId="9344" Score="0" Text="I am kind of in a time crunch. That's why I opened another question in the hope of reaching others who may not have seen this. No other reason." CreationDate="2015-12-11T22:18:43.510" UserId="3314" />
  <row Id="9097" PostId="9343" Score="0" Text="Ah! I understand now. I think the updated code will address your issue! I also updated it to hopefully be easier to follow, by assigning the strings to variables before getting to the tricky bit." CreationDate="2015-12-11T22:34:33.953" UserId="13413" />
  <row Id="9098" PostId="9343" Score="0" Text="thanks @Kyle. I will try it out. Thanks." CreationDate="2015-12-11T22:55:29.373" UserId="3314" />
  <row Id="9099" PostId="9346" Score="0" Text="&quot;RTS AI: Problems and Techniques&quot;, http://webdocs.cs.ualberta.ca/~cdavid/pdf/ecgg15_chapter-rts_ai.pdf" CreationDate="2015-12-12T07:20:09.450" UserId="5279" />
  <row Id="9100" PostId="9346" Score="0" Text="Could be useful towards an answer: http://ijcai.org/papers07/Papers/IJCAI07-168.pdf and review of same: http://aigamedev.com/open/review/transfer-learning-rts/" CreationDate="2015-12-12T08:34:07.140" UserId="836" />
  <row Id="9101" PostId="9331" Score="0" Text="can you also help me with how i should go forward with learning and building a hold onto data science?" CreationDate="2015-12-12T11:31:03.557" UserId="14681" />
  <row Id="9102" PostId="9331" Score="0" Text="@SagarGhai I might not be able to, due to time constraints. However, there is plenty of material over Quora and this SE for getting up to speed. Good Luck :)" CreationDate="2015-12-12T15:48:31.263" UserId="11097" />
  <row Id="9103" PostId="9228" Score="1" Text="KNN is NOT unsupervised. Perhaps the answer was thinking about k-means?" CreationDate="2015-12-12T14:31:48.213" UserDisplayName="user14705" />
  <row Id="9106" PostId="9353" Score="0" Text="I edited my question." CreationDate="2015-12-12T17:18:07.327" UserId="3151" />
  <row Id="9107" PostId="9347" Score="0" Text="Does the area you want to create a product in (price recommendation) usually have a highly regular set of input data across all kinds of clients? I would guess not, which would make creating a single one-fits-all product very hard. You could create a product around the technical model, but there would still be a lot of work adapting to specifics of each client. How well it could scale depends critically on how much qualitative difference there will be between clients' data." CreationDate="2015-12-12T22:55:22.783" UserId="836" />
  <row Id="9108" PostId="9347" Score="0" Text="How do you define 'regular' when you say regular set of input data? Also, what did you mean by &quot;technical model&quot;?" CreationDate="2015-12-13T03:02:53.470" UserId="13100" />
  <row Id="9109" PostId="5836" Score="0" Text="@javadba Once again, more voice of reason.  Perhaps my answer should have been in the form of a comment.  My point was to encourage going through the process.  If he's looking for a reason to learn, using one's own data can be motivating." CreationDate="2015-12-13T05:43:04.820" UserId="3457" />
  <row Id="9110" PostId="9340" Score="0" Text="What are you classifying? If you are classifying what the document &quot;looks like&quot; then using an image would make sense.  But if you are classifying what the text actually says, then using an RNN built using the text or a bag of words model may be better suited." CreationDate="2015-12-13T05:45:56.960" UserId="3457" />
  <row Id="9111" PostId="9347" Score="0" Text="Probably I should say &quot;standardised&quot; instead of &quot;regular&quot;. I mean arranged in a very similar way, having an identical ontology, so that the same process can be used to extract features each time. By &quot;technical model&quot;, I mean the code that generates a predictive model from the input - often the easy bit once you have arranged, understood and cleaned the data." CreationDate="2015-12-13T08:37:27.780" UserId="836" />
  <row Id="9113" PostId="9347" Score="0" Text="Thanks for your help on this. So i assume that answer to my original question is option1, which is every time new client on boards, that statistical model will have to be built from scratch so that mathematical equation can be developed and then it can be deployed. although technical model may remain same. If we are not going off topic, i find some companies write this in their site, which is misleading." CreationDate="2015-12-13T10:12:41.313" UserId="13100" />
  <row Id="9114" PostId="9347" Score="0" Text="Actually I do not know the answer. It depends on your answers to my question - for the *price recommendation* problem that you are looking at, how &quot;standardised&quot; can you expect client data to be for your service? If the answer to that is &quot;not at all&quot;, then it is not possible to fully automate it. If the answer is &quot;most clients can and will arrange the data to have very similar input structure and fields&quot;, then you have a good chance." CreationDate="2015-12-13T10:23:59.310" UserId="836" />
  <row Id="9115" PostId="9358" Score="0" Text="Yes, I think I'm a little confused. It's still not clear to me. If I have standard data (no affinity related), I can make it an affinity matrix A by taking the pairwise distance between the data samples. Now if I see A as a graph, I can take the Laplacian and solve for the eigenvectors and get a solution; if I don't see A as a graph, I could simply solve for the matrix eigenvectors (PCA) and get a solution. What's the difference?" CreationDate="2015-12-13T11:44:56.897" UserId="14665" />
  <row Id="9116" PostId="9363" Score="0" Text="Is not the logistic regression like classification of two class (In case of your example) ?" CreationDate="2015-12-13T12:00:50.957" UserId="14716" />
  <row Id="9117" PostId="9362" Score="0" Text="@Sean Owen, In particular, I want to know When should I choose Linear Regression and When to choose Logistic regression?" CreationDate="2015-12-13T12:03:42.427" UserId="14716" />
  <row Id="9118" PostId="9363" Score="0" Text="In this case, Yes.  But, in general the difference is simply that: The dependent variable of linear regression is continuous and that of logistic regression is categorical" CreationDate="2015-12-13T12:38:13.147" UserId="11097" />
  <row Id="9119" PostId="9347" Score="0" Text="Ok, to give context, a client in Europe might have same data structure in their sql database in terms of name, variable type in comparison to sql database of a client who is based in Singapore. However, Singapore data might have different trend, seasonality and pattern in comparison to data in Europe. Is there any way that a single Machine Learning or Artificial Intelligence or Deep Learning or Neural Net algorithm can be built and used for all regions and clients without having to update that model manually for any new client?" CreationDate="2015-12-13T13:09:54.130" UserId="13100" />
  <row Id="9120" PostId="9342" Score="1" Text="Aside: what happens if I call that API with `sql=DROP TABLE students; --`?" CreationDate="2015-12-13T14:32:04.943" UserId="13296" />
  <row Id="9121" PostId="9347" Score="0" Text="That seems more tractable. I don't know the answer, but I suggest you put that last comment of yours into the question, because it may help for someone who knows your problem domain better." CreationDate="2015-12-13T14:42:25.840" UserId="836" />
  <row Id="9122" PostId="9359" Score="1" Text="As said by sb0709, Just keep in mind that there is no magic; you should learn about modeling, programming, math and statistics so computers do not do anything out of nothing for you. However, it is worth giving it a try. Any of the courses you mentioned are good. You can enroll for free, get involved in learning and see how you feel. Good luck" CreationDate="2015-12-13T15:47:32.030" UserId="3151" />
  <row Id="9123" PostId="9358" Score="0" Text="I read your question again. The answer is the properties (e.g. the one I mentioned in my answer) Laplacian matrix provides for better decomposition. However you absolutly can sole eigenfunction for any similarity related matrices and get some results which are different just in details. For instance about the PCA you mentioned: PCA takes the covariance matrix so it captures where the variance is high but in general the concept follows the same direction as the other spectral decomposition techniques. I'll proofread my answer soon as I see some &quot;Saturday Night&quot; sentences ;)" CreationDate="2015-12-13T16:36:30.427" UserId="8878" />
  <row Id="9124" PostId="9359" Score="0" Text="That's a nice advice from @HamidehIraj. In case you want to pursue, you can have a look at [this](http://datascience.stackexchange.com/q/8963/11097). But, the decision is yours to make, and thus the close vote from my side as _opinion based_. Hope you understand :)" CreationDate="2015-12-13T17:11:55.057" UserId="11097" />
  <row Id="9125" PostId="9340" Score="0" Text="I don't have OCR text of the documents. But yes I'm trying to achieve the classification of what the documents contents &quot;actually says&quot;, without the OCR text corpus as input. My thought is the `X` would be the image pixels and the `Y` would be the type of document it is (example: scify, romance novel, etc..)." CreationDate="2015-12-13T18:23:48.807" UserId="14694" />
  <row Id="9126" PostId="9351" Score="0" Text="Uh, value1 - value2?  Maybe normalize so they have the same range or same average?" CreationDate="2015-12-13T18:47:46.057" UserId="13285" />
  <row Id="9127" PostId="9337" Score="0" Text="Thank you for your answer. It struck me too that I could use it as another hyper-parameter. However, this question is to understand the concept of nonnegativity i.e. if I can determine nonnegativity directly from my application input data i.e. implicit Ratings." CreationDate="2015-12-13T23:27:17.037" UserId="14667" />
  <row Id="9128" PostId="9347" Score="0" Text="I suppose it's technically possible, but I doubt that it would give you good/efficient learning. Multinational corporations that invest in this sort of thing, like Walmart, don't rely on automating price determination because there are extensive time lags and human considerations that can't be put into a model. Product recommendation engines work because there is a constant and reliable stream of data of what customers click on to purchase. The same is not true for all the factors that determine a product's price." CreationDate="2015-12-13T23:39:54.433" UserId="6529" />
  <row Id="9131" PostId="9362" Score="0" Text="The difference is obviously the dependent variable (as stated in  the answer)." CreationDate="2015-12-14T01:11:25.637" UserId="9123" />
  <row Id="9132" PostId="9318" Score="0" Text="Hi , thanks for your response. I am actually trying to code the algorithm given on page 4 of this paper: http://www.machinelearning.org/proceedings/icml2004/papers/221.pdf . You are right, i get the error while calling the function." CreationDate="2015-12-14T02:06:34.443" UserId="14427" />
  <row Id="9133" PostId="9368" Score="0" Text="Wow, that's excellently explained.  Maybe, I'll wait for a day before accepting :)" CreationDate="2015-12-14T05:46:45.900" UserId="11097" />
  <row Id="9134" PostId="9228" Score="0" Text="Decision tree learning is also not unsupervised. https://en.wikipedia.org/wiki/Supervised_learning." CreationDate="2015-12-14T08:50:22.613" UserId="6550" />
  <row Id="9135" PostId="9370" Score="1" Text="Are you using the RandomForest model?" CreationDate="2015-12-14T11:18:15.433" UserId="11097" />
  <row Id="9136" PostId="9229" Score="0" Text="Clarification: Clustering, but a single k-sized cluster around a given input vector. It is not necessarily true that all features must be numeric. For example, you could use Jaccard similarity to define a distance where features are nominal." CreationDate="2015-12-14T12:15:36.240" UserId="13684" />
  <row Id="9137" PostId="9351" Score="1" Text="You could normalize with a z-score for each column and then take their average to determine a score for each record." CreationDate="2015-12-14T13:23:31.663" UserId="13684" />
  <row Id="9138" PostId="9369" Score="0" Text="You might look into how well your model can predict words in each document given their context. Hopefully perplexity should be higher on documents unlike those it was trained on. https://en.m.wikipedia.org/wiki/Perplexity" CreationDate="2015-12-14T13:53:28.080" UserId="12306" />
  <row Id="9140" PostId="9375" Score="1" Text="Why not have the inputs be $\mathbb R^{26}$ vectors like $x_1 = (1,4,15, 3, 0, \dots)$? The unmentioned columns are presumably zero?" CreationDate="2015-12-14T17:16:37.170" UserId="381" />
  <row Id="9141" PostId="9369" Score="0" Text="Great idea, thank you. I will research. Looks like I need to roll my own, which might not be too bad? There is a log_perplexity method for LDA, but LSI does not look like there any such method." CreationDate="2015-12-14T17:43:30.350" UserId="13684" />
  <row Id="9142" PostId="9378" Score="0" Text="Cross Posting is not encouraged in SE :)" CreationDate="2015-12-14T17:49:24.567" UserId="11097" />
  <row Id="9144" PostId="9379" Score="0" Text="Ok. Can you point me to a specific publication detailing it? The most i've seen skim over it" CreationDate="2015-12-14T18:14:24.980" UserId="1426" />
  <row Id="9145" PostId="9375" Score="0" Text="I think there needs to be more consistent terminology for this to be clearly articulated. Generally &quot;classes&quot; are categorical outputs for a model. I think hear you are saying there are many features (which for now you have labeled A, B... Z), and you have counts of each feature. If the numbers here are not counts, but rather something categorical as well, then your second approach makes more sense. More information about what each of these features (A-Z) means would help." CreationDate="2015-12-14T18:17:45.490" UserId="12306" />
  <row Id="9146" PostId="9380" Score="2" Text="Link-only answers are discouraged -- links can stop working. Can you inline relevant info?" CreationDate="2015-12-14T18:23:14.067" UserId="21" />
  <row Id="9147" PostId="9375" Score="0" Text="@jamesmf that's correct - these are features (ABC...Z) and numbers are their counts. &#xA;It's a description of a molecules. There are many molecules in one file. Each one has several compounds with their respective count. &#xA;F.e. one of molecule has &quot;C10C1007 3 C10C1012 4 C40C3104 1 Cl10C1004 8&quot; etc. Total number of features is 119. &#xA;The file I have (160 MB) contains overall 7700 unique compounds (features). I've seen 8+ GB file, although I'm not sure if number of compounds (features) there much bigger." CreationDate="2015-12-14T18:53:09.737" UserId="14740" />
  <row Id="9148" PostId="9376" Score="0" Text="Although they're in a way similar, I'd say the first case allows me to use continuous features as the last one treat all of them as categorical. It might have implications on the algo's performance in terms of accuracy or speed, or both." CreationDate="2015-12-14T18:55:35.380" UserId="14740" />
  <row Id="9150" PostId="9229" Score="1" Text="Actually, they're both supervised. Supervised just means that the learner has access to a labeled training set. Unsupervised algorithms do things like clustering, not label prediction." CreationDate="2015-12-15T01:47:44.473" UserId="9483" />
  <row Id="9152" PostId="8681" Score="0" Text="Perhaps this approach would be useful: http://zeszyty-naukowe.wwsi.edu.pl/zeszyty/zeszyt12/Numerical_Coding_of_Nominal_Data.pdf" CreationDate="2015-12-14T19:29:04.897" UserDisplayName="user14742" />
  <row Id="9153" PostId="9389" Score="0" Text="There are gazillion ways to mine this data. Do you have a problem statement in mind?" CreationDate="2015-12-15T03:47:02.490" UserId="11097" />
  <row Id="9154" PostId="9393" Score="0" Text="That's an interesting idea. Thanks for sharing" CreationDate="2015-12-15T07:28:21.207" UserId="14427" />
  <row Id="9155" PostId="9351" Score="0" Text="Is z-score the most commonly used technique for normalising?" CreationDate="2015-12-15T07:56:01.887" UserId="14706" />
  <row Id="9156" PostId="9394" Score="0" Text="The addition of new features in lieu of the single feature will greatly increase the flatness of the original data. Wont this affect the accuracy of the model- as the number of features here will then be disproportionate to the number of data instances. &#xA;&#xA;Which approach would be better in such a scenario, implementing the technique you described in your answer, or by simply performing a 1:N join. if you have had experience with such data, which technique fared better in your case?" CreationDate="2015-12-15T08:52:20.067" UserId="14763" />
  <row Id="9157" PostId="9390" Score="0" Text="It's not clear what you're asking. You already have a mapping from docs to hash buckets. You just want smaller buckets? use more buckets." CreationDate="2015-12-15T09:28:16.640" UserId="21" />
  <row Id="9158" PostId="22" Score="0" Text="Perhaps this approach would be useful: http://zeszyty-naukowe.wwsi.edu.pl/zeszyty/zeszyt12/Numerical_Coding_of_Nominal_Data.pdf" CreationDate="2015-12-14T19:24:05.343" UserDisplayName="user14742" />
  <row Id="9159" PostId="9378" Score="0" Text="I'm well aware of that. I wasn't sure where it fits best." CreationDate="2015-12-15T10:24:21.977" UserId="1426" />
  <row Id="9160" PostId="9378" Score="0" Text="I think it is best suited here!" CreationDate="2015-12-15T10:26:27.060" UserId="11097" />
  <row Id="9162" PostId="9393" Score="0" Text="Note that I assumed that in the cases the similarity measures agree, the result is indeed as you intended. If in not so, do some manual labeling there too." CreationDate="2015-12-15T11:18:18.673" UserId="13727" />
  <row Id="9163" PostId="9398" Score="0" Text="The $\frac{1}{2N}$ factor in the cost function is there to make usage of derivatives simpler..." CreationDate="2015-12-15T13:29:58.843" UserId="3024" />
  <row Id="9169" PostId="9390" Score="0" Text="Apologize about missing details, added more information." CreationDate="2015-12-15T15:49:05.670" UserId="14758" />
  <row Id="9170" PostId="9375" Score="0" Text="So you have molecules, each of which has 'sub-graphs' (features) C10C1007, C10C10112... and the number following that is a count, right?  So if a molecule has 2 sets of C10C1007, I'd see &quot;C10C1007 2&quot; correct?  And there are 119 of these features, with 7700 rows, or molecules?" CreationDate="2015-12-15T15:58:56.270" UserId="12306" />
  <row Id="9171" PostId="9375" Score="0" Text="@jamesmf partially true. I have a file with 37291 molecules. These molecules have features like C10C1007, C10C10112, etc. For each feature there is a corresponding count. The first molecule only has 119 such a features. Overall, for all molecules in the file, there are 7700 unique features." CreationDate="2015-12-15T16:10:45.140" UserId="14740" />
  <row Id="9172" PostId="9375" Score="0" Text="But you don't know what that number means?" CreationDate="2015-12-15T16:11:11.887" UserId="12306" />
  <row Id="9173" PostId="9375" Score="0" Text="Nope, I do not." CreationDate="2015-12-15T16:12:48.380" UserId="14740" />
  <row Id="9174" PostId="9329" Score="0" Text="What data do you have, beyond temperature values? What features are associated with each data point? Time, location?" CreationDate="2015-12-15T18:36:06.310" UserId="12306" />
  <row Id="9175" PostId="9398" Score="0" Text="Neural networks are, for the most part, are trained using gradient descent. Although your problem may not be convex, using gradient descent with a good type of regularization should give you a &quot;good enough&quot; solution. Using global optimizers, such as genetic algorithms, does not guarantee a global optimum will be found, and it will be much more computationally expensive than using gradient descent. Are you strictly against using first order optimization?" CreationDate="2015-12-15T19:42:43.973" UserId="14779" />
  <row Id="9176" PostId="9404" Score="0" Text="Thanks, so if $n$ is the size of the mini-batch, we update the weights after each mini-batch, right: $\frac{\partial L}{ \partial w}$, so if we have 100 mini-batches, we update weights 100 times in 1 epoch. If I understood you correctly, I'm not sure where normalized gradient (i.e. divided by the norm) fits." CreationDate="2015-12-15T21:59:18.120" UserId="1426" />
  <row Id="9177" PostId="9398" Score="0" Text="Thanks Armen. No, I'm not against first order optimizations ... for the good reason that I don't know what first order means ;-)" CreationDate="2015-12-15T22:07:46.310" UserId="3024" />
  <row Id="9178" PostId="9404" Score="0" Text="@Alex Yep exactly. 100 mini-batches means updating the weights 100 times. For the normalizing the gradient I was just explaining why mini-batch is preferred to stochastic gradient descent." CreationDate="2015-12-15T22:16:57.640" UserId="14779" />
  <row Id="9179" PostId="9398" Score="0" Text="By first order optimization, I mean optimizations that take into account the first derivative. Just a fancy way of saying gradient descent :)" CreationDate="2015-12-15T22:59:29.120" UserId="14779" />
  <row Id="9180" PostId="9404" Score="0" Text="Great thanks Armen" CreationDate="2015-12-15T23:11:26.820" UserId="1426" />
  <row Id="9181" PostId="9398" Score="0" Text="Thanks. Good to know. I think I'll use gradient descent." CreationDate="2015-12-16T08:38:28.577" UserId="3024" />
  <row Id="9182" PostId="9287" Score="0" Text="Interesting!  Can you include more details in the answer :)" CreationDate="2015-12-16T14:40:17.773" UserId="11097" />
  <row Id="9183" PostId="9410" Score="0" Text="I tried guessing immediately, i.e. 0 training steps, and I still get the same result, not a standard distribution. Am I missing something on how numpy randomises values?" CreationDate="2015-12-16T15:20:03.903" UserId="13736" />
  <row Id="9184" PostId="9413" Score="0" Text="Do you want your model to be interpretable? Or is that not a strict constraint?" CreationDate="2015-12-16T17:13:18.067" UserId="14779" />
  <row Id="9185" PostId="9413" Score="0" Text="Can you clarify that? The purpose is that it can be used to measure other places." CreationDate="2015-12-16T17:16:45.057" UserId="14800" />
  <row Id="9186" PostId="9413" Score="0" Text="Yes, but do you want to be able to understand why the model is making the decisions that it is making? Essentially do you need a white-box model? Or does it not matter?" CreationDate="2015-12-16T17:19:04.020" UserId="14779" />
  <row Id="9187" PostId="9413" Score="0" Text="It doesn't matter. Black box is alright." CreationDate="2015-12-16T17:20:33.890" UserId="14800" />
  <row Id="9188" PostId="9287" Score="0" Text="I have no more information except the link." CreationDate="2015-12-16T17:35:11.877" UserId="3151" />
  <row Id="9189" PostId="9409" Score="0" Text="Rate 1-5 is neither an up or down vote.  You would penalize a reputation for a 1 (top) rating." CreationDate="2015-12-16T17:37:52.933" UserId="13285" />
  <row Id="9190" PostId="9398" Score="0" Text="Do you need help deriving the gradients?" CreationDate="2015-12-16T17:59:35.923" UserId="14779" />
  <row Id="9191" PostId="9410" Score="0" Text="Hmm interesting - have you plotted the class frequencies?" CreationDate="2015-12-16T18:47:31.507" UserId="12306" />
  <row Id="9192" PostId="9398" Score="0" Text="No thanks. It will be fun to do some derivatives like 15 years ago ... And sympy will be my friend at the end :-)" CreationDate="2015-12-16T20:26:23.593" UserId="3024" />
  <row Id="9193" PostId="9258" Score="0" Text="It looks OK but if you will see carefully then you will find that for value_0, it doesn't have 1 in all rows. Since 0 is present in all rows therefore value_0 should have 1 in all row. Same for value_5856, Value_25081 etc. It seems this logic is picking values from a column and then not going back instead move forward." CreationDate="2015-12-16T20:55:31.297" UserId="14604" />
  <row Id="9194" PostId="9410" Score="0" Text="I found the problem. It was the classical &quot;bug&quot;. See the answer I wrote. Thank you!" CreationDate="2015-12-17T07:08:19.850" UserId="13736" />
  <row Id="9195" PostId="9311" Score="0" Text="I'm sorry, I've asked something different than what I wanted to know. I don't see a problem in $\log(y_i) = 0$, but in $y_i = 0$, because of $\log(y_i)$. Could you please adjust your answer to that?" CreationDate="2015-12-17T08:47:46.050" UserId="8820" />
  <row Id="9196" PostId="9397" Score="0" Text="Probably because it can map different features to the same bucket. But usually it's not a big deal" CreationDate="2015-12-17T09:21:29.210" UserId="816" />
  <row Id="9197" PostId="9329" Score="0" Text="year, month, min temp, max temp.&#xA;these are the only attributes that I have." CreationDate="2015-12-17T09:21:50.440" UserId="14681" />
  <row Id="9198" PostId="6694" Score="0" Text="http://stats.stackexchange.com/questions/130130/textbook-on-reinforcement-learning" CreationDate="2015-12-17T15:44:30.217" UserId="5279" />
  <row Id="9199" PostId="8195" Score="0" Text="This is a tough one for two reasons.  If turning the images into a distribution using a KDE were viable, I'd tell you to apply a [two sample Kolmogorov–Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test).  But, the two dimensional nature of the image will render this difficult.  Also there is a tiling effect in images that won't be recovered well with K-S.  I thus suggest image processing: [Haussdorff distance](http://www.cs.cornell.edu/~dph/papers/HKR-TPAMI-93.pdf).  Also [check out this post](http://stackoverflow.com/questions/6499491/comparing-two-histograms)." CreationDate="2015-12-17T17:54:28.340" UserId="9420" />
  <row Id="9201" PostId="9426" Score="0" Text="I understand what are you trying to say and I kind of agree. But I cannot completely remove all the rows with NAs because each column has different amount of NA values. I suppose I could remove a threshold of 277000 values and substitute the remaining with the mean value of the column." CreationDate="2015-12-17T20:18:42.903" UserId="10894" />
  <row Id="9202" PostId="9419" Score="1" Text="Likely contributing factors are poor performance (Java), platform/vendor attachment (C#), and complexity (C++). Python is easily learned, open source, and has good performance." CreationDate="2015-12-17T21:44:15.353" UserId="13578" />
  <row Id="9205" PostId="9344" Score="0" Text="cat(&quot;\\'&quot;,name,&quot;\\'&quot;) function is giving desired results, but let me try to fit this into your function" CreationDate="2015-12-18T07:11:02.090" UserId="12425" />
  <row Id="9206" PostId="9344" Score="0" Text="try to write it as csv `write.csv(t,&quot;t.csv&quot;)` it gives the desired result means that R internally stores it as expected" CreationDate="2015-12-18T09:22:09.410" UserId="12425" />
  <row Id="9207" PostId="9258" Score="0" Text="Hi Sanoj. It's not really fair to use my solution and vote me down. The least you can do is to update your question with the new progress you made instead of opening a new question. If you want people to help you, you should play nice with them." CreationDate="2015-12-18T13:05:48.520" UserId="12490" />
  <row Id="9208" PostId="9258" Score="0" Text="Hello michaeld: I had no intention to vote you down. I just took off click sign since this solution did not fulfill my needs as asked in question. Initially I thought OK but later when I investigated I found the discrepancies as mentioned in reply above. I was not getting any reply of this therefore I created a new question where I mentioned my original answer and included your reply with correction needed. Sorry I did not mention your name there. I will update that." CreationDate="2015-12-18T17:02:44.210" UserId="14604" />
  <row Id="9209" PostId="9261" Score="0" Text="This doesn't say how you will dynamically get dummy value (25041) and column names (i.e. dx1) both in the for loop. I can get only one at a time." CreationDate="2015-12-18T17:11:35.447" UserId="14604" />
  <row Id="9212" PostId="9346" Score="0" Text="Have you seen https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf ?" CreationDate="2015-12-19T03:05:27.163" UserId="3470" />
  <row Id="9213" PostId="9426" Score="0" Text="Anyhow, I recommend creating a model as a baseline. When you do not have a baseline with measures, it is impossible to say what action (such as deleting) is useful and what action is not." CreationDate="2015-12-19T04:02:28.157" UserId="3151" />
  <row Id="9215" PostId="6154" Score="4" Text="Although the links are good, a brief summary of the model in your own words would have been better." CreationDate="2015-12-19T13:34:15.347" UserId="9123" />
  <row Id="9217" PostId="9407" Score="1" Text="I'm voting to close this question as off-topic because it was a typo the author spotted." CreationDate="2015-12-19T16:45:33.423" UserId="471" />
  <row Id="9219" PostId="9440" Score="0" Text="http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html" CreationDate="2015-12-19T16:53:43.297" UserId="381" />
  <row Id="9221" PostId="9261" Score="0" Text="Take a look now. I added all of the details." CreationDate="2015-12-19T17:52:21.863" UserId="9420" />
  <row Id="9222" PostId="9446" Score="0" Text="Great answer! I would also add to your &quot;bonus&quot; section that if you are going to use an analogue based classification method, your best results will come from a support vector machine.  kNN is more of a historical vestige since SVM's are much more powerful and tunable.  If you really understand how to cross validate, you can tune your SVM very quickly to provide excellent results." CreationDate="2015-12-20T07:34:19.187" UserId="9420" />
  <row Id="9223" PostId="9416" Score="0" Text="Can you add a visualization to this question highlighting the points? This may help others understand your situation. You can use rattle package as well. It contains random forest." CreationDate="2015-12-20T07:45:13.737" UserId="3151" />
  <row Id="9225" PostId="9318" Score="0" Text="@UD1989 I'd suggest to post a dedicated question on this task. Possible on SO with R tag. In my understanding the main challenge is to find implementation that doesn't use for loop but is vectorized; i.e. the function `f` returns a vector of result for all ratings. You should simplify the requirements by avoiding the iteration loop and provide the initial data.frame and the function `F` in some trivial implementation. I guess this could be a very interesting question. I have some solution, but possible far from optimal one, so let me know:)" CreationDate="2015-12-20T09:39:24.467" UserId="10620" />
  <row Id="9229" PostId="9446" Score="0" Text="Cheers AN6U5. Yes, of course, thanks for adding that. You might be interested in this paper which discusses the benefits of kNN in the age of SVMs: http://www.wisdom.weizmann.ac.il/~irani/PAPERS/InDefenceOfNN_CVPR08.pdf" CreationDate="2015-12-21T00:15:09.423" UserId="2515" />
  <row Id="9230" PostId="9407" Score="0" Text="I am fine with closing this question. My mistake, sorry." CreationDate="2015-12-21T07:32:14.263" UserId="13736" />
  <row Id="9231" PostId="9456" Score="0" Text="So the inputs are queries?  Over what are you trying to match them?" CreationDate="2015-12-21T12:14:21.217" UserId="12363" />
  <row Id="9232" PostId="9458" Score="0" Text="Alice, it is perfectly on-topic here; and it's a very nice question.  Welcome to the site :)" CreationDate="2015-12-21T12:30:06.197" UserId="11097" />
  <row Id="9233" PostId="9452" Score="1" Text="Does `myCorpus` have the right names or are they somehow lost in the construction of the term-document matrix? What does `names(myCorpus)` give?" CreationDate="2015-12-21T15:06:50.013" UserId="471" />
  <row Id="9234" PostId="9456" Score="0" Text="@A.M.Bittlingmayer The input is a dataset of very sparse vectors. The dataset consists of examples that can be split into 10-20 groups depending on the annotation (which is available). The task is to transform this dataset into much lower dimensionality while keeping the information." CreationDate="2015-12-21T16:45:51.703" UserId="7848" />
  <row Id="9235" PostId="9261" Score="0" Text="Your solution looks good if I need to create dummy values based in one column only as you have done from &quot;E&quot;. But when I have to create it from multiple columns and those cell values are not unique to a particular column then do I need to loop your code again for all those columns? If that is the case then how repetition of values will be taken care of? Otherwise it will over write the previous dummy column created with the same name. I have added my result in question above to make it clear if there was any confusion. Thanks anyway for you looking into it." CreationDate="2015-12-21T17:55:18.873" UserId="14604" />
  <row Id="9238" PostId="9447" Score="0" Text="Thank you very much - this is very helpful and makes a lot of sense. Are there any other encoding schemes you use for specific/edge cases? Do you ever find that you're in a situation where you'll use different encoding schemes for different features?" CreationDate="2015-12-21T20:36:59.493" UserId="10462" />
  <row Id="9239" PostId="9472" Score="1" Text="Define &quot;by response type&quot;?" CreationDate="2015-12-21T22:46:45.110" UserId="21" />
  <row Id="9240" PostId="9472" Score="0" Text="Just updated the question to reflect what I'm looking for. Let me know if that clarifies things." CreationDate="2015-12-22T00:03:15.130" UserId="14910" />
  <row Id="9241" PostId="8773" Score="0" Text="@Dawny33 What do you mean with &quot;compile time&quot;? Theano, Torch and most of the time also TensorFlow are used in an interpreted fashion." CreationDate="2015-12-22T01:02:15.140" UserId="8820" />
  <row Id="9242" PostId="8773" Score="0" Text="@moose In Theano at least, some code is compiled in C++ or CUDA." CreationDate="2015-12-22T01:03:31.477" UserId="843" />
  <row Id="9243" PostId="8773" Score="0" Text="@FranckDernoncourt Ok, but does that time really matter? When you have the training running for about 20 minutes, isn't only a couple of seconds the time it needs to generate the CUDA code? (How can I measure that?)" CreationDate="2015-12-22T01:05:59.440" UserId="8820" />
  <row Id="9244" PostId="8773" Score="2" Text="@moose In Theano generating the CUDA / C++ code takes around 30 seconds to one minute for a reasonably sized model. It makes debugging quite tedious. To measure CUDA / C++ code generation time, you can time before/after a Theano function is compiled." CreationDate="2015-12-22T01:21:17.487" UserId="843" />
  <row Id="9245" PostId="9446" Score="0" Text="If I was doing kNN regression, would $R^2$ metric be appropriate?" CreationDate="2015-12-22T01:44:17.383" UserId="838" />
  <row Id="9246" PostId="9477" Score="0" Text="Ah I see, so there is no native way of doing this directly in lattice then! Thank you for your answer, works well, much obliged!" CreationDate="2015-12-22T08:56:54.757" UserId="11116" />
  <row Id="9247" PostId="5737" Score="0" Text="why not... sure.. :P" CreationDate="2015-12-22T09:13:21.147" UserId="9035" />
  <row Id="9248" PostId="9481" Score="1" Text="[Helpful Reference](http://datascience.stackexchange.com/q/8606/11097)" CreationDate="2015-12-22T11:10:15.647" UserId="11097" />
  <row Id="9249" PostId="9483" Score="1" Text="Please do not [cross-post](http://stackoverflow.com/q/34414255/4993513)" CreationDate="2015-12-22T12:03:20.667" UserId="11097" />
  <row Id="9251" PostId="9441" Score="0" Text="Simple R programming questions like this are better asked on Stack Overflow. There are more R experts there and its a bit trivial to call this &quot;Data Science&quot;" CreationDate="2015-12-22T15:20:40.337" UserId="471" />
  <row Id="9253" PostId="9446" Score="0" Text="short answer is 'Yes'. I did a bit of reading on nonparametric regression and evaluation, and so far the list of evaluation metrics is MSE, RMSE, MAE, and $R^2$. https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html#regression-model-evaluation  and  https://www.kaggle.com/wiki/Metrics .  Have a look at http://www.stat.cmu.edu/~larry/=sml/nonpar.pdf .  Conclusions: If $R^2$ is stable, i.e. low variance for multiple random sub-samples of data, then I'd say that is good enough.  (full disclosure: at this point I am still not convinced and I'll update this thread when I learn more.)" CreationDate="2015-12-22T16:54:34.980" UserId="2515" />
  <row Id="9254" PostId="9484" Score="0" Text="regarding this line: `double net=0;`   the posted code exhibits several details that. individually,  might not matter, but do indicate problems.  For instance, a literal value being assigned to a `double`.   The literal value is expected to contain a decimal point.  so the line should be: `double net=0.0;`" CreationDate="2015-12-22T17:50:05.043" UserId="14933" />
  <row Id="9255" PostId="9484" Score="0" Text="for readability by us humans, strongly suggest following the axiom: only one statement per line and (at most) one variable declaration per statement." CreationDate="2015-12-22T17:57:11.600" UserId="14933" />
  <row Id="9256" PostId="9484" Score="0" Text="for those of us that are very good with 'C' but do not know the `Neural network with multiple layer: learning function` algorithm, could you post a link to that algorithm." CreationDate="2015-12-22T18:02:20.983" UserId="14933" />
  <row Id="9257" PostId="9420" Score="0" Text="I got a reasonable answer here: http://stats.stackexchange.com/questions/187186/whats-the-relationship-between-an-svm-and-hinge-loss" CreationDate="2015-12-22T22:10:04.687" UserId="1301" />
  <row Id="9258" PostId="9471" Score="0" Text="Do you have stats only for the end of the game, or do you have them on any type of regular interval?  For many games, your lead 'accelerates' as you amass an advantage over your opponents." CreationDate="2015-12-23T00:51:56.243" UserId="8041" />
  <row Id="9259" PostId="6576" Score="0" Text="(1) Andrew's Ng course on Machine Learning;&#xA;(2) Yaser Abu-Mostafa course on Learning from the Data;&#xA;Both are accessible (time is not included) and will get you good level of understanding." CreationDate="2015-12-23T01:01:11.553" UserId="7848" />
  <row Id="9260" PostId="9473" Score="2" Text="I would also add h2o's deeplearning to the list. It's one of the most powerful implementations of NNets accessible from R. And I think it scales up really well." CreationDate="2015-12-23T01:02:43.690" UserId="13023" />
  <row Id="9262" PostId="9482" Score="0" Text="Look up [Spatial Transformer Networks](http://torch.ch/blog/2015/09/07/spatial_transformers.html)." CreationDate="2015-12-23T01:43:26.797" UserId="381" />
  <row Id="9263" PostId="9481" Score="0" Text="thanks mate.. its really helped..!" CreationDate="2015-12-23T05:44:58.653" UserId="9035" />
  <row Id="9264" PostId="9452" Score="0" Text="dimnames(myTdmBase)$Docs returns NULL. myCorpus and my main dataframe have no problem." CreationDate="2015-12-23T07:06:00.033" UserId="3151" />
  <row Id="9265" PostId="9452" Score="0" Text="Well there's a problem between your corpus and your TDM somewhere... Simplify: Do you get the same `character(0)` document names *without* the `control` argument to `TermDocumentMatrix`? If so, edit your Q to show this. Can you then show `str(myCorpus)` or dump the object somewhere we can look at it? What version of `tm` and R are you on? etc. Please edit the question and not reply in comments." CreationDate="2015-12-23T08:02:13.543" UserId="471" />
  <row Id="9266" PostId="9495" Score="1" Text="Neural networks don't extrapolate well, so once outside the target range, it will not work. The NN here never &quot;learns the function&quot;, instead it learns how to best (lowest error) approximate the function in the range given by the training data. The $y=x^2$ function should work though, and I would expect there is a problem with your NN implementation. However, there is too much unfamiliar code here for me to find it." CreationDate="2015-12-23T09:10:41.130" UserId="836" />
  <row Id="9267" PostId="6547" Score="0" Text="h2o library not importing in this module." CreationDate="2015-12-23T06:08:10.810" UserDisplayName="user14945" />
  <row Id="9268" PostId="9495" Score="0" Text="@NeilSlater Do you have a source / argument for your claim that NNs (there are lots of them, not only MLPs!) don't extrapolate well? - I use Google TensorFlow. The model is defined under &quot;Create Model&quot;. I use $x$ as input, a hidden layer with 100 neurons and sigmoid activation, then a hidden layer with another 100 neurons and sigmoid activation, then a linear activation and a single neuron. I use MSE and momentum." CreationDate="2015-12-23T10:04:00.170" UserId="8820" />
  <row Id="9269" PostId="9495" Score="1" Text="It's a well-known property of NNs. In general, they interpolate between training data, they do not extrapolate. You could deliberately fit the form of the function by choosing your architecture and activation functions to match the desired end result, but that defeats the &quot;general function approximation&quot;. Here's a discussion: https://groups.google.com/forum/#!topic/comp.ai.neural-nets/CH4C63CzhVo" CreationDate="2015-12-23T10:23:10.797" UserId="836" />
  <row Id="9272" PostId="9471" Score="0" Text="@j.a.gartner: No, I only have the mentioned data from the end of the games. Its a board game, so it is difficult to get additional data. I have to make due with what I have." CreationDate="2015-12-23T16:38:03.510" UserId="14909" />
  <row Id="9273" PostId="9483" Score="0" Text="@Dawny33 deleted from SO." CreationDate="2015-12-23T18:14:34.830" UserId="14925" />
  <row Id="9274" PostId="9473" Score="0" Text="I primarily use h2o now, built in parallel processing and ram allocation is very nice." CreationDate="2015-12-23T18:45:22.953" UserId="14913" />
  <row Id="9277" PostId="9493" Score="0" Text="Thanks for your answer - its really helpful to see a coded example. How does the magnitude of the weighting function coefficients affect the model? I looked through xgboost docs, but I can't find information about the significance of these numerical values." CreationDate="2015-12-23T19:29:44.163" UserId="14895" />
  <row Id="9278" PostId="6576" Score="0" Text="Check out [my most recent question](http://datascience.stackexchange.com/questions/9435/how-to-self-learn-data-science/9436#9436)" CreationDate="2015-12-23T20:02:30.320" UserId="8960" />
  <row Id="9279" PostId="9493" Score="0" Text="didn't know this trick, nice. there's a little tidbit in the xgboost doc under the function `setinfo()`, though its not very descriptive" CreationDate="2015-12-24T15:39:57.007" UserId="14913" />
  <row Id="9280" PostId="9505" Score="0" Text="If 10 is the min value the response can take on, I would say any prediction below 10 should be set to equal 10.  You can also look into random forest/bagging, or possibly taking the average prediction of many Boosted trees/Neural net models to see if it helps your results a little.  Also, another loss function might be nice (estimate 11 and truth 10 is a 10% error, but only loss 1, where estimate 1,100,000 and truth 1,000,000 is still 10% error, but the loss is 100,000 so it is prioritizing those higher values.  Just something to consider, a lot depends on the context of the variables." CreationDate="2015-12-24T17:44:30.203" UserId="14913" />
  <row Id="9281" PostId="9507" Score="0" Text="Can you give an example of what you mean? I'm confused whether you're asking about a single input variable mapped to multiple different output variables, or a single input variable being mapped to a list of the same variable." CreationDate="2015-12-24T22:21:18.797" UserId="11136" />
  <row Id="9285" PostId="9517" Score="0" Text="Yes, I got this. In fact, haar cascades performs better. But I still wanted to know exactly why hog descriptors fail miserably on my images, is it because the human is not completely included in the picture?" CreationDate="2015-12-25T12:18:27.313" UserId="982" />
  <row Id="9286" PostId="9517" Score="1" Text="The reason is the one I gave you. The default classifier you are using takes images that are nothing like the ones you are using" CreationDate="2015-12-25T16:11:09.077" UserId="14994" />
  <row Id="9287" PostId="511" Score="0" Text="FYI: [K-fold vs Monte Carlo cross-validation (aka. Repeated random sub-sampling)](http://stats.stackexchange.com/q/51416/12359)" CreationDate="2015-12-25T18:00:50.930" UserId="843" />
  <row Id="9288" PostId="9519" Score="0" Text="Can you be more specific?? I didn't understand the question. How can you have a cluster with only 2 points???" CreationDate="2015-12-25T20:06:00.050" UserId="14994" />
  <row Id="9290" PostId="9523" Score="0" Text="If we increase number of points in the cluster are we guaranteed to have unique medoid in non-Euclidean case? I am guessing that some non-Euclidean metrics will have unique medoid and some not. To start, I am not sure if weighted Euclidean will have unique medoid. I read PAM description and it looks like it only takes &quot;euclidean&quot; and &quot;manhattan&quot; metrics. It looks like flexclust works with arbitrary metric, but I have not tried it yet." CreationDate="2015-12-26T14:36:56.267" UserId="14999" />
  <row Id="9291" PostId="9524" Score="0" Text="Thank you. I updated my question. My concern is that center of the cluster is not unique. I will surely expand to more points, it is just simpler to analyze question of uniqueness with two points. I would like to answer question of uniqueness with weighted Euclidean first." CreationDate="2015-12-26T14:54:04.893" UserId="14999" />
  <row Id="9292" PostId="9523" Score="0" Text="Medoids are never unique, e.g. with the discrete metric. The algorithm PAM works with any metric - the implementation you have been looking at may have limitations. The `flexclust` R package is horribly slow (like anything that is pure R)." CreationDate="2015-12-26T15:14:37.433" UserId="924" />
  <row Id="9293" PostId="9523" Score="0" Text="Thank you. Is there way to estimate non-uniqueness? How bad does it get? Is possible that set of medoids (for one cluster) is so big that whole clustering analysis is useless?" CreationDate="2015-12-26T15:22:52.897" UserId="14999" />
  <row Id="9294" PostId="9522" Score="0" Text="I don't understand your question. Could you put an example of what is you input and what is your desired output? Are you trying to do a dimensionality reduction from m to 1??" CreationDate="2015-12-26T17:14:42.410" UserId="14994" />
  <row Id="9295" PostId="9480" Score="0" Text="That's difficult!! Does the files have some associate information, like when they were created, the compiler or something like that?" CreationDate="2015-12-26T17:20:25.587" UserId="14994" />
  <row Id="9296" PostId="9480" Score="0" Text="Yes, the file name has creation date embedded in it and the records in the database can be approximately mapped to the source file using this date." CreationDate="2015-12-26T17:51:32.987" UserId="13145" />
  <row Id="9297" PostId="9528" Score="0" Text="In the math minimize (derivative = 0) the squared probably turns out to be an easier equation" CreationDate="2015-12-26T20:20:35.390" UserId="13285" />
  <row Id="9298" PostId="9530" Score="0" Text="Thanks for the detailed guidelines. I will try these ideas" CreationDate="2015-12-26T21:24:16.617" UserId="13145" />
  <row Id="9299" PostId="9523" Score="1" Text="Does it matter? Never trust a clustering result, but *analyze* it carefully and draw conclusions." CreationDate="2015-12-27T00:33:58.620" UserId="924" />
  <row Id="9300" PostId="9533" Score="0" Text="Just to check if I understood it: Instead of writing $\geq 1$ we could also use any constant $\epsilon$ and write $\geq \epsilon$, where $\epsilon &gt; 0$?" CreationDate="2015-12-27T10:53:40.330" UserId="8820" />
  <row Id="9301" PostId="9533" Score="0" Text="In principle, yes. E.g. in soft-margin SVMs (where you allow for some misclassifications or points within the margin), you use $\geq 1-\xi_i$ so you can be $\xi_i$ from the margin. Of course then you need some penalty term which forces most $\xi_i$ to be zero or at least very low." CreationDate="2015-12-27T11:00:49.497" UserId="14906" />
  <row Id="9302" PostId="9507" Score="2" Text="All the examples that you have shown thus far have a single output that can be deterministically mapped to a variable length list.  Here is the single input/single output: &lt;1,1&gt;,&lt;2,2&gt;,&lt;2,4&gt; and a simple deterministic script can turn this into &lt;1,[1]&gt;,&lt;2,[1,1]&gt;,&lt;2,[1,1,1,1]&gt;.  I suggest you split the problem into the machine learning piece and the deterministic piece." CreationDate="2015-12-27T17:27:14.767" UserId="9420" />
  <row Id="9303" PostId="9536" Score="0" Text="A million inputs is not considered that big anymore, but for argument's sake, you could use an alternative basis such as wavelets, DCT, SVD, NMF, etc. Alternatively you could downsample; does the learning task really require full resolution?" CreationDate="2015-12-27T17:52:07.940" UserId="381" />
  <row Id="9305" PostId="9536" Score="1" Text="@Emre Really? Please give me reference to 3 papers which make use of input vectors of a dimension of at least 5 million to substantiate your claim." CreationDate="2015-12-28T12:02:32.440" UserId="8820" />
  <row Id="9306" PostId="9544" Score="1" Text="[this might help](http://igraph.org/r/doc/aaa-igraph-package.html)" CreationDate="2015-12-28T14:56:04.940" UserId="14913" />
  <row Id="9307" PostId="9522" Score="1" Text="The question will be IMO better answered on SO (with R tag). If I get you correctly you have performed a *one hot encoding*, so the question should be formulated such as **How to perform one hot DEcoding (inverse operation to one hot encoding)**" CreationDate="2015-12-28T18:31:12.017" UserId="10620" />
  <row Id="9308" PostId="9536" Score="0" Text="Pick any paper with categorical variables. NLP is a good place to look. If you want to represent every word in the language as an input, and you use the typical BoW representation, you will need as many elements as there are words in the vocabulary. And that's just for representing one word. What if you wanted several neighboring words for context? With a language of 100k-1M vocabulary, you could easily blow through a million neurons. Look at the input of [word2vec](http://stats.stackexchange.com/questions/177667/input-vector-representation-vs-output-vector-representation-in-word2vec)." CreationDate="2015-12-28T18:56:08.863" UserId="381" />
  <row Id="9311" PostId="9542" Score="0" Text="I still wonder how to implement&#xA;&#xA;I try this in java &#xA;&#xA;	for (int i = 0; i &lt; CENTROIDS_SIZE; i++) {&#xA;&#xA;				for (int j = 0; j &lt; dataList.size(); j++) {&#xA;&#xA;					double accumerateDistance = accumeratedSqrDistanceCal(dataList, centroids);&#xA;&#xA;					double rand = Math.random();&#xA;&#xA;					double distance = minDistanceFromClosetCentroidsCal(dataList.get(j), centroids);&#xA;&#xA;					double distanceSquare = Math.pow(distance, 2);&#xA;&#xA;					double chance = distanceSquare / accumerateDistance;&#xA;&#xA;					if (chance &gt; rand) {&#xA;&#xA;						centroids.add(dataList.get(j));&#xA;						break;&#xA;&#xA;					}&#xA;&#xA;				}&#xA;			}&#xA;&#xA;The selection chance is very low" CreationDate="2015-12-29T01:09:13.840" UserId="14357" />
  <row Id="9312" PostId="9547" Score="0" Text="What do you mean by 'the predictions originated from train'. Can you set an example? I don't really understand your problem" CreationDate="2015-12-29T06:44:32.030" UserId="14994" />
  <row Id="9313" PostId="9542" Score="0" Text="Change your script a little bit, if you add all the probabilities of the datapoints, does it add up to 1? That part looks ok to me.&#xA;&#xA;Two points about your script, you should remove datapoints that have been selected as centroid from your datapoint collection (I think) and the second is that I would change the randomized picking of a centroid to something like this: Make a list of cumulative probabilities, and then check where your rand falls, that way, if they do add up to 1 you will always have selected one." CreationDate="2015-12-29T10:16:20.420" UserId="14904" />
  <row Id="9314" PostId="9550" Score="0" Text="Tnks for this   importand and usefull clarification" CreationDate="2015-12-29T11:05:55.097" UserId="14946" />
  <row Id="9315" PostId="9522" Score="0" Text="@gm1 The other comments are fine. The tone and content of your comments are out of line. Keep it clean and on-topic." CreationDate="2015-12-29T21:28:38.583" UserId="21" />
  <row Id="9316" PostId="9551" Score="0" Text="This isn't feasible if there's no maximum length, correct?" CreationDate="2015-12-29T22:11:38.337" UserId="14989" />
  <row Id="9317" PostId="9551" Score="0" Text="yes, that's a problem if you don't have a limit. Let me edit the answer" CreationDate="2015-12-29T22:14:49.900" UserId="14994" />
  <row Id="9318" PostId="9551" Score="0" Text="That's a reasonable workaround. Any response to the first question? Is there an algorithm that can produce a varying number of outputs?" CreationDate="2015-12-29T22:24:21.387" UserId="14989" />
  <row Id="9319" PostId="9551" Score="0" Text="I'm sorry but it doesn't occur to me. I don't know how a variable output could be managed mathematically. I've always worked with fixed inputs and outputs." CreationDate="2015-12-29T22:27:24.537" UserId="14994" />
  <row Id="9320" PostId="9551" Score="0" Text="No worries. I've never heard of such an algorithm, so I'm not surprised it hasn't been done yet. And as far as I know, most data sets people are interested in are or can be set up with fixed input and output lengths. I'll wait a few days in case someone else knows something we don't, but this is roughly what I expected." CreationDate="2015-12-29T22:33:35.020" UserId="14989" />
  <row Id="9321" PostId="9551" Score="0" Text="I'm interested as well if somebody has an answer to your question" CreationDate="2015-12-29T22:35:43.310" UserId="14994" />
  <row Id="9323" PostId="9507" Score="0" Text="Thanks for the additional information. However, I don't think there is enough information provided to formulate a response beyond a high level heuristic discussion as seen in the answer that is provided. The unlimited bound precludes classification algorithms as discussed below, so this looks more like a problem for a Hidden Markov Model.  But the example you provided still lacks a statistical component that points to solution by a statistical learning method.  Is the 'look-and-say sequence' the real problem or is there a statistically distributed data set that you are really working with?" CreationDate="2015-12-29T23:04:03.417" UserId="9420" />
  <row Id="9324" PostId="6601" Score="0" Text="Since you've had some success with k-nearest neighbor and KNN is the simplest of the analogue based classifiers and is usually significantly outperformed by SVM (i.e. the best analogue based classifier), have you considered just changing the KNN to an SVM?" CreationDate="2015-12-29T23:12:20.463" UserId="9420" />
  <row Id="9325" PostId="6601" Score="0" Text="I can help you if you provide some details? What is your time-series exactly? and you want to classify different time-series or different part of a single time-series?" CreationDate="2015-12-30T11:40:53.680" UserId="8878" />
  <row Id="9326" PostId="9556" Score="2" Text="+1  I think it is a great project for someone to contribute. Thank you for sharing. Atleast, I don't think it is irrelevant :)" CreationDate="2015-12-30T15:29:57.880" UserId="11097" />
  <row Id="9327" PostId="9540" Score="0" Text="Thank you for the advice. I am exploring that scenario." CreationDate="2015-12-30T17:10:47.413" UserId="15032" />
  <row Id="9328" PostId="9507" Score="0" Text="The actual problem I'm interested in is the [Collatz Conjuecture](https://en.wikipedia.org/wiki/Collatz_conjecture). In particular, what insights can I gain from a learning algorithm trying to learn &lt;[Mersenne number](http://mathworld.wolfram.com/MersenneNumber.html), hailstone sequence&gt;." CreationDate="2015-12-30T17:39:15.523" UserId="14989" />
  <row Id="9329" PostId="9507" Score="0" Text="Off-topic because the question is a deterministic pure mathematics problem recast in the guise of machine learning. A statistical sample is not provided, rather, a small set of cases from an infinite mathematical series is referenced. The OP is hoping to recover a nonlinear relationship between the input feature and the list of prime numbers corresponding with the input feature's Mersenne Number. This is not well posed, nor does it fall within the data science topic list." CreationDate="2015-12-30T20:30:26.267" UserId="9420" />
  <row Id="9330" PostId="9554" Score="2" Text="Check out ELKI (on github, Java; see also Wikipedia). I use this a lot because it is the most complete project for clustering and anomaly detection; and usually way faster than R, too." CreationDate="2015-12-30T21:46:05.947" UserId="924" />
  <row Id="9331" PostId="9561" Score="1" Text="This is genius!  Thank you so much.  I am new to data science (as a university student) and I had no idea where to start to learn for myself.  Those links are perfect." CreationDate="2015-12-31T00:25:10.837" UserId="15083" />
  <row Id="9332" PostId="9559" Score="1" Text="statisticians call it &quot;fishing for significance&quot;. Check the XKCD cartoon on jelly beans. And try the statistics Stack Exchange site, http://stats.stackexchange.com/" CreationDate="2015-12-31T10:00:01.143" UserId="471" />
  <row Id="9333" PostId="9564" Score="0" Text="Thank you for your answer. Could you please name this method of image processing? I would like to read about it more detailed." CreationDate="2015-12-31T16:56:28.517" UserId="15024" />
  <row Id="9334" PostId="9564" Score="0" Text="http://cs231n.github.io/convolutional-networks/ is a good source for CNNs." CreationDate="2015-12-31T16:59:38.697" UserId="14047" />
  <row Id="9335" PostId="9553" Score="0" Text="I don't follow.  Clearly if you include words of length one then they will be used by cosine similarity (anything that uses the vector)." CreationDate="2015-12-31T17:02:09.157" UserId="13285" />
  <row Id="9336" PostId="9568" Score="0" Text="I think this would be a better fit on DBA" CreationDate="2015-12-31T19:31:25.287" UserId="13285" />
  <row Id="9337" PostId="9568" Score="1" Text="If the mods want to move it, that's fine with me!" CreationDate="2015-12-31T19:33:47.553" UserId="3466" />
  <row Id="9339" PostId="800" Score="0" Text="Here is a related question regarding NoSQL, JSON, and Drill: http://datascience.stackexchange.com/questions/9568/database-options-for-json-storage-queried-with-apache-drill" CreationDate="2015-12-31T20:54:53.660" UserId="3466" />
  <row Id="9341" PostId="9553" Score="0" Text="what if I feed the corpus to LSI to reduce the dimension count?" CreationDate="2016-01-01T12:21:25.510" UserId="13562" />
  <row Id="9344" PostId="9573" Score="0" Text="Supervised learning by definition requires data with labels, so, it's by definition? not clear what you mean." CreationDate="2016-01-01T21:37:34.357" UserId="21" />
  <row Id="9345" PostId="9579" Score="0" Text="How high of dimensions?  4D or 10,000D?  What robot?  Part 2 kind of comes out of nowhere.  I suggest adding some details and laying out your problem to elicit responses.  Is this a purely theoretical mind experiment or is this something you are actually doing?  I think you will have more chance of a meaningful answer if you ground things a little better." CreationDate="2016-01-01T22:20:11.830" UserId="9420" />
  <row Id="9347" PostId="9579" Score="0" Text="256 maximum so far but not likely to go up.&#xA;No specific robot yet.&#xA;It is something that I am actually doing.&#xA;It is vague because I am trying to figure out what to do.  The front end collecting data from sensors and translating them into higher dim's is defined (so far).  There still needs to be a back end that allows it to do things.  E.g. say I now possess clustered data derived from RGB and depth say in std::list's of list's.  Rather than to run the data for a single defined task e.g. classification,  I want a general purpose programmable environment that allows manipulation of this HD data." CreationDate="2016-01-02T02:09:03.163" UserId="15040" />
  <row Id="9348" PostId="9370" Score="0" Text="Yes! I am using RandomForest. But I would wonder the same thing for other algorithms, like GBRT. Sorry for the late reply, was away for a few weeks. Thanks!" CreationDate="2016-01-02T02:56:41.387" UserId="14729" />
  <row Id="9355" PostId="9557" Score="1" Text="I confirm - it is very easy to commit to sklearn, just open a pull request and that's it." CreationDate="2016-01-02T09:43:18.183" UserId="816" />
  <row Id="9357" PostId="9583" Score="2" Text="Such an answer will always be opinion based, except if you give an objective ranking criterion." CreationDate="2016-01-02T11:16:14.537" UserId="8820" />
  <row Id="9358" PostId="9583" Score="0" Text="@moose Yes but I'm looking for not just an opinion. Maybe some research paper or rank as you suggested. Something like we are good with Face detection (exactly people achieved a 99.6% of performance) like in this [paper](http://arxiv.org/abs/1503.03832). However, the performance of X is Y% which is low, and people is trying hard to push that limit." CreationDate="2016-01-02T11:24:05.290" UserId="14994" />
  <row Id="9362" PostId="9583" Score="1" Text="What you posted is just one of hundreds of papers in machine learning. Most of them have some kind of evaluation where they compare their results with other results. That is just how science works. But is computer vision more important than speech recognition? Is question understanding more important than recognizing emotions? - I don't think there is any meaningful ranking criterion. This means it is not possible to answer your question objectively, but only by opinion. And I can guarantee you that there isn't any trustworthy paper which makes a statement like &quot;CV is more important than ASR)" CreationDate="2016-01-02T11:43:42.420" UserId="8820" />
  <row Id="9363" PostId="9583" Score="0" Text="@moose I don't agree my friend. Can't you say that having a 99.63% of performance in Face Recognitions makes that problem solved??? ImageNet performance a couple of years ago was around 60% now is 96%. You said that there are hundreds of papers in ML answering my question. Do you have a good and recent example?" CreationDate="2016-01-02T12:01:00.507" UserId="14994" />
  <row Id="9364" PostId="9583" Score="0" Text="Take any of http://arxiv.org/list/cs.CV/recent - e.g. http://arxiv.org/abs/1411.4038" CreationDate="2016-01-02T13:12:05.110" UserId="8820" />
  <row Id="9365" PostId="9583" Score="0" Text="&quot;Can't you say that having a 99.63% of performance in Face Recognitions makes that problem solved?&quot; - No. There is still room to 100%. And less memory usage. And less time. Working on less training data. Making the algorithm simpler." CreationDate="2016-01-02T13:14:02.100" UserId="8820" />
  <row Id="9366" PostId="9583" Score="0" Text="ML is the response to the problems that mathematics and later AI had with such *programs*. And yes I agree, this is my opinion and I'll have a hard time to collect some evidence for this conjecture." CreationDate="2016-01-02T17:10:52.060" UserId="10620" />
  <row Id="9367" PostId="9586" Score="1" Text="I agree with you. It's very difficult to learn like a human. It's really curious that the most successful algorithms right now, which are deep learning, are not the same as how the humans learn. There was a very good paper of Rodney Brooks taking about this. I'll try to find it" CreationDate="2016-01-02T17:36:11.407" UserId="14994" />
  <row Id="9368" PostId="9585" Score="1" Text="I'm voting to close this question as off-topic because there's probably a Stack Exchange site packed with neuro-biologists who might actually know something about this. http://biology.stackexchange.com/questions/9656/molecules-and-human-memory might help" CreationDate="2016-01-02T18:13:24.420" UserId="471" />
  <row Id="9370" PostId="9585" Score="0" Text="I don't really think biology is that close to computational neuroscience. Cognitive Science is closer though https://cogsci.stackexchange.com/. They even have a computational-neuroscience tag" CreationDate="2016-01-02T21:43:58.593" UserId="13023" />
  <row Id="9371" PostId="9575" Score="1" Text="False. RUnit has a new maintainer." CreationDate="2016-01-03T00:39:55.607" UserId="515" />
  <row Id="9372" PostId="9588" Score="0" Text="[You can access columns pandas-style using DataFrame notation](https://databricks.com/blog/2015/08/12/from-pandas-to-apache-sparks-dataframe.html)." CreationDate="2016-01-03T04:34:00.263" UserId="381" />
  <row Id="9373" PostId="9575" Score="1" Text="@DirkEddelbuettel, adjusted based on your info. tnx" CreationDate="2016-01-03T08:13:41.363" UserId="10517" />
  <row Id="9374" PostId="9589" Score="0" Text="Take a look at Knuth's humorous paper on the complexity of songs. https://www.cs.utexas.edu/users/arvindn/misc/knuth_song_complexity.pdf &#xA;It won't be very helpful for your project but it is funny ;-)" CreationDate="2016-01-03T10:32:14.553" UserId="13727" />
  <row Id="9375" PostId="9590" Score="1" Text="I'm voting to close this question as off-topic because it belongs on Software Recommendations SE" CreationDate="2016-01-03T13:16:36.810" UserId="11097" />
  <row Id="9376" PostId="9590" Score="0" Text="@Dawny33, I did search through that SE but did not find any tool. Since this seems like a specialized requirement, I was hoping I could get suggestions in this SE." CreationDate="2016-01-03T13:19:41.437" UserId="15140" />
  <row Id="9377" PostId="9590" Score="0" Text="Pattern Recognition is an entire domain in itself. In fact, one of the primary goals of data science and statistical modelling domain is pattern recognition.  So, qns about algorithms and techniques would be more suited here, rather than software suggestions! :)" CreationDate="2016-01-03T13:22:53.537" UserId="11097" />
  <row Id="9378" PostId="9590" Score="0" Text="@Dawny33, thanks for that. As a *complete* beginner, this is good for me to know. Given that this is the kind of information I'm looking for, what resources would you suggest I start with?" CreationDate="2016-01-03T13:26:51.610" UserId="15140" />
  <row Id="9379" PostId="9590" Score="0" Text="You can learn about the software used for pattern recognition [here](http://datascience.stackexchange.com/questions/tagged/software-recommendation) and [these](http://datascience.stackexchange.com/questions/tagged/beginner) posts would help you get a nice headstart!" CreationDate="2016-01-03T13:31:08.973" UserId="11097" />
  <row Id="9384" PostId="987" Score="0" Text="I was doing basic the same experiment last year and encounter the exactly the same problem you have. Can your word2vec vector after l2-normalizing process beat BOW? I haven't done l2-normalizing, but even after testing many post processing method semantic vector is still 2-4 absolute percent behind BOW tf/idf features I wonder is that direction a deadend. My original sought is to combine a densely semantic vector with traditional BOW and see if it can enhance topic classification/modeling performances. BTW: what data set have you been working on, mine is 20newsgroup." CreationDate="2016-01-03T15:42:39.227" UserDisplayName="user15150" />
  <row Id="9385" PostId="9590" Score="0" Text="@Dawny33, thanks!" CreationDate="2016-01-03T21:24:41.680" UserId="15140" />
  <row Id="9386" PostId="9595" Score="0" Text="It's not at all commercial. I'm just trying to learn something I'm interested in. Did you see the link in the question? Seems to suggest it can find &quot;hidden&quot; patterns." CreationDate="2016-01-03T21:28:01.463" UserId="15140" />
  <row Id="9388" PostId="9595" Score="0" Text="You won't find a tool that can understand the business context of a factory, formulate an interesting question based on it, relate the question to the data, answer it and then explain the answer to you. If you find one, you've stumbled on an almost complete Artificial Intelligence. I'd suggest you look up an Operational Research syllabus and get reading :)" CreationDate="2016-01-03T22:44:36.163" UserId="5303" />
  <row Id="9389" PostId="9599" Score="0" Text="thanks @Emre, my confusion is that how do I treat an entire year's worth of data as 1 point? Doesn't each row of data (representing one day) constitute a sample in scikit-learn nomenclature? How do I treat an entire year as one sample rather than 365?" CreationDate="2016-01-04T00:26:06.027" UserId="12985" />
  <row Id="9390" PostId="9599" Score="1" Text="I was not addressing the specifics of sklearn, but since you asked, you want to be using the `sklearn.cross_validation` methods with &quot;Label&quot; in the name, such as [sklearn.cross_validation.LabelKFold](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.LabelKFold.html#sklearn.cross_validation.LabelKFold)." CreationDate="2016-01-04T00:33:38.687" UserId="381" />
  <row Id="9391" PostId="9599" Score="0" Text="thanks @Emre, so the idea is to assign each year a single label, right?" CreationDate="2016-01-04T01:05:26.460" UserId="12985" />
  <row Id="9392" PostId="9599" Score="0" Text="Yes, @user308827." CreationDate="2016-01-04T01:51:12.003" UserId="381" />
  <row Id="9393" PostId="9584" Score="0" Text="The reason I included 1-length tokens, is that I have documents that has very slight difference, e.g. `book 1 of Meow`, `book 2 of Meow`. `book 3 of Meow`" CreationDate="2016-01-04T04:49:19.817" UserId="13562" />
  <row Id="9394" PostId="9599" Score="0" Text="thanks @Emre! ." CreationDate="2016-01-04T05:50:07.340" UserId="12985" />
  <row Id="9395" PostId="9595" Score="0" Text="Of course. I'm not looking for something that can think of and solve business questions - that would be revolutionary! I was hoping for a tool that could look purely from a mathematical/statistical point of view. That tries to identify common patterns. Maybe we're not there yet. Thanks for the suggestion, will look at it." CreationDate="2016-01-04T07:10:15.560" UserId="15140" />
  <row Id="9398" PostId="9604" Score="0" Text="A [helpful reference](http://datascience.stackexchange.com/q/8847/11097)" CreationDate="2016-01-04T11:24:53.793" UserId="11097" />
  <row Id="9399" PostId="9596" Score="0" Text="Out of curiosity and as it is related to the answers which might make sense: What is a &quot;large collection of blog posts&quot; in your case? Several hundreds of posts? Thousands?" CreationDate="2016-01-04T12:12:20.303" UserId="8820" />
  <row Id="9400" PostId="9596" Score="0" Text="About 80,000 posts." CreationDate="2016-01-04T12:49:17.897" UserId="15157" />
  <row Id="9401" PostId="9596" Score="0" Text="Out of curiosity: Are all of the blog posts written in the same language? And how accurate should the geo-location be? Country? Area (e.g. &quot;West Coast United States&quot;)? City?" CreationDate="2016-01-04T12:55:07.017" UserId="14588" />
  <row Id="9402" PostId="9596" Score="0" Text="There a two languages, English and Spanish and place names will be US mostly, if it fails to pick cities outside of the US then that isn't a massive problem. In the answer below it talks about doing a basic search which is great, but I'm interested in playing around with machine learning and I'm looking for an algorithm to learn more about it, especially dealing with false positives such as `John Denver` (bad example maybe) but I don't to mark that post with `Denver, Colorado`." CreationDate="2016-01-04T13:06:37.430" UserId="15157" />
  <row Id="9403" PostId="9606" Score="0" Text="Thank you for your answer, the location solution would work great, but what about (as mentioned above) false positives such as `John Denver` (bad example maybe) but I don't to mark that post with `Denver, Colorado` - I guess looking at using Stanford NLP might be a solution. But out of interest any other ways to go about this will be of interest." CreationDate="2016-01-04T13:08:57.833" UserId="15157" />
  <row Id="9405" PostId="9599" Score="0" Text="thanks again @Emre, please have a look at the follow-up question: http://datascience.stackexchange.com/questions/9612/assigning-values-to-missing-target-vector-values-in-scikit-learn" CreationDate="2016-01-04T15:18:10.057" UserId="12985" />
  <row Id="9406" PostId="987" Score="0" Text="I was working with a dataset for the CIKM 2014 competition. For me, the vector representations were never able to beat BOW with tf-idf weights. My plan was to use them in addition to improve quality. In my experience (for text classification) some form of tf-idf + a linear model with n-grams is an extremely strong approach. I am currently experimenting with convolutional neural networks and even with these (more or less) complex models that approach hard to beat." CreationDate="2016-01-04T16:56:42.383" UserId="2979" />
  <row Id="9408" PostId="9615" Score="1" Text="Discussions elsewhere:&#xA;&#xA;https://www.reddit.com/r/MachineLearning/comments/3xvx6d/which_of_the_nips_2015_papers_are_most/&#xA;&#xA;https://www.quora.com/What-were-the-scientific-highlights-of-NIPS-2015" CreationDate="2016-01-04T18:25:10.717" UserId="12386" />
  <row Id="9409" PostId="9614" Score="0" Text="I have a similar problem and have been looking into probablistic graphs without too much progress. Time between interactions/purchase could also be useful" CreationDate="2016-01-04T18:27:32.617" UserId="14913" />
  <row Id="9410" PostId="9616" Score="2" Text="If you're willing to use R, the maps package should do the trick! https://cran.r-project.org/web/packages/maps/maps.pdf" CreationDate="2016-01-04T18:55:10.053" UserId="13413" />
  <row Id="9411" PostId="9619" Score="1" Text="0. What dataset do you plan to use?" CreationDate="2016-01-04T21:42:16.573" UserId="843" />
  <row Id="9412" PostId="9614" Score="0" Text="Is a purchase a one time event or does a customer potentially make multiple purchases over time?" CreationDate="2016-01-05T00:06:22.260" UserId="8005" />
  <row Id="9413" PostId="9565" Score="0" Text="Could you please explain the [sign(E(G)$weight)/2 + 1.5] part? @jean lescut" CreationDate="2016-01-05T04:28:05.173" UserId="13953" />
  <row Id="9414" PostId="9565" Score="0" Text="Another thing is I want an edge to appear only when the value in the adjacency matrix is beyond a particular threshold. Please Help." CreationDate="2016-01-05T04:43:36.037" UserId="13953" />
  <row Id="9415" PostId="9609" Score="0" Text="In a weighted and directed graph, what is the best measure of Influence? Eigen centrality, Betweeness Centrality or Weighted Avg. Degress, In-Degree" CreationDate="2016-01-05T05:47:34.743" UserId="14204" />
  <row Id="9416" PostId="9619" Score="1" Text="*&quot;learn from the pool of articles/research papers published by the Publisher/Journal till date.&quot;* - if you &quot;learn&quot; from a dataset of papers that have all been accepted (as presumably, all published papers have been) then your machine learning algorithm will &quot;learn&quot; that all papers are accepted, and will always predict 100% probability of acceptance. On the bright side, your algorithm will have 100% accuracy on your training set!" CreationDate="2016-01-05T08:19:55.670" UserId="15189" />
  <row Id="9417" PostId="9609" Score="0" Text="All centrality measures are actually talking about the same concept but with different languages. The best is to take a sample data and see which one works better for your purpose. Please note that 1) Not all of them have a wighted directed version and 2) If your graph is big then the computational complexity matters a lot (e.g. Eigen centrality is not comfortable for gigantic graphs)" CreationDate="2016-01-05T08:20:58.043" UserId="8878" />
  <row Id="9419" PostId="9623" Score="0" Text="Awesome, this helps. Should I convolute with 3D feature maps or should I split the channels into 2D maps and do the usual 2D feature maps?" CreationDate="2016-01-05T10:35:51.250" UserId="13625" />
  <row Id="9420" PostId="9632" Score="2" Text="This problem is solvable analytically using calculus and does not require statistical learning. Assuming you want a numerical solution, its more readily solvable using stochastic gradient descent rather than a genetic algorithm.  Also, notice that you have defined a function that is linear in y and the x term that scales fastest goes like -x^2, so for most parameter regimes, the solution is uninteresting (xmax,ymin).  I suggest spending a little more time finding an example that is more meaningful and deciding between SGD and GA. [Here's a true genetic algorithm example](http://boxcar2d.com/)" CreationDate="2016-01-05T12:50:28.163" UserId="9420" />
  <row Id="9421" PostId="9632" Score="0" Text="Hi , in fact it is  just  an  example. In practice  my function is a combination of 2  nested  functions in which  I  dont even have a hessian." CreationDate="2016-01-05T12:58:05.160" UserId="14946" />
  <row Id="9422" PostId="9623" Score="0" Text="You're splitting into 3 2D maps, usually." CreationDate="2016-01-05T13:18:15.653" UserId="12306" />
  <row Id="9426" PostId="9632" Score="0" Text="But do you see the relation between stochastic gradient descent and genetic algorithms?  And that the example you provided is so simplistic that there is no difference between the two? All I'm getting at is that you need a more complex example to illicit the differences and hence better understand the latter." CreationDate="2016-01-05T16:44:17.693" UserId="9420" />
  <row Id="9427" PostId="9632" Score="0" Text="I was looking  for an example where it is described a trivial example like the one aboce in order to   generalize from it." CreationDate="2016-01-05T16:48:21.593" UserId="14946" />
  <row Id="9429" PostId="9633" Score="0" Text="What do the buy vs non-buy probabilities look like?  Random forests typically doesn't overfit that much, so I would look more into the forest and your data to figure out what is going on.  For your second question, AUC is a solid measure for this, as is measuring the lift in each segmentation group." CreationDate="2016-01-05T17:57:12.220" UserId="14913" />
  <row Id="9430" PostId="9640" Score="0" Text="How is SGD a subset of Genetic Algorithms? SGD isn't population-based, doesn't use any of the genetic operators, and genetic algorithms do not use gradient-based optimization." CreationDate="2016-01-05T18:00:35.970" UserId="12909" />
  <row Id="9431" PostId="9582" Score="0" Text="So is the goal to create a general algorithm that will tell you if a statement is in support/opposition of an arbitrary topic. Or is the the goal to have your algorithm work for a single topic (immigration for example)?" CreationDate="2016-01-05T18:33:29.190" UserId="14779" />
  <row Id="9432" PostId="9637" Score="0" Text="IIRC (from a course), then the fully enumerated state+action/reward map in Q-Learning is for cases when you *don't* have a model. The NN in your case is there to *replace* the very large map with a model that can estimate the reward from features of the state (and hopefully generalise in cases where the game is in a new but similar state). Unfortunately I don't know the full detail (still learning), but suspect the right answer will point you at basic architecture for combining RL with an NN to estimate reward (and therefore it will enact a policy, but not explicitly represent it)" CreationDate="2016-01-05T19:44:38.953" UserId="836" />
  <row Id="9433" PostId="9640" Score="1" Text="Hi ANSU5 , excelent   reference , just wait to  put it in pracitce tommorow" CreationDate="2016-01-05T19:45:15.697" UserId="14946" />
  <row Id="9434" PostId="9640" Score="0" Text="@Jérémie Clos, you are correct. I have edited the answer to reflect this.  I was caught up in the discussion above and was trying to illustrate some similarities in various optimization techniques.  But this likely obfuscated more than in elucidated." CreationDate="2016-01-05T19:50:06.163" UserId="9420" />
  <row Id="9435" PostId="9582" Score="0" Text="Lets go for a single topic. FYI the topic is more specific than simply immigration- more like a bill whose impact is on students in STEM fields with F1 visas in USA." CreationDate="2016-01-05T21:15:25.217" UserId="15128" />
  <row Id="9436" PostId="9637" Score="0" Text="@NeilSlater Yes, that's my goal." CreationDate="2016-01-05T21:40:27.250" UserId="15197" />
  <row Id="9438" PostId="9619" Score="0" Text="Hi, @FranckDernoncourt what I was trying to do was learn from the patterns of published paper and use that in the present paper at hand to do the prediction. I get what you implying, I should have a training datasets consisting of mixture of papers every published and papers rejected so that I can classify the testing datasets to check for accuracy and then predict the current paper at hand. Actually I am not get how to approach this task (weather in form of supervised learning or unsupervised learning)." CreationDate="2016-01-06T05:23:19.920" UserId="15182" />
  <row Id="9439" PostId="9638" Score="0" Text="Thanks, FISM seems the way to go. Since it looks at discrete values, it can't form association rules like &quot;X &lt;= 100&quot; or &quot;Y &gt; 0&quot;. I could discretize each column individually but as far as I can see I would then lose information with respect to the other columns. Any thoughts on that?" CreationDate="2016-01-06T07:26:01.427" UserId="15196" />
  <row Id="9440" PostId="987" Score="0" Text="To Mod: Sorry that I don't have 50 reputation, so I can't write in the comment area. Hi elmille: Yes, that is what I experience in all the test. However, do you find that word vec + BOW help? In my experience, when I concatenate word vec with BOW tf-idf(in my case this vec is actually a overall vector within entire article, its not word-vec but very similar), the performance get even lower. I originally think it should be BOW+vec &gt; BOW &gt; vec. Since they contain mutually assistant information. The actually result is BOW&gt;vec&gt;BOW+vec. Then I do standard scaling and normalization to bow and vec in" CreationDate="2016-01-05T01:52:12.973" UserDisplayName="user15150" />
  <row Id="9443" PostId="6794" Score="0" Text="Why do you ask &quot;What is the standard&quot; and then start your question with the assumption that there's really no &quot;standard&quot;?" CreationDate="2016-01-06T12:17:58.010" UserId="471" />
  <row Id="9445" PostId="9630" Score="0" Text="Used your method @kasra manshaei but got the following error : Error in inherits(v, &quot;igraph.vs&quot;) : &#xA;  (list) object cannot be coerced to type 'double'" CreationDate="2016-01-06T13:29:29.483" UserId="13953" />
  <row Id="9448" PostId="9630" Score="0" Text="Check your R matrix after line 6. Whatever it is should be also after line 7 (double, int, ...). then converting it to this type will solve. Unfortunately I do not have access to R otherwise I would have sent you the code." CreationDate="2016-01-06T14:58:13.217" UserId="8878" />
  <row Id="9451" PostId="9630" Score="0" Text="It's a 4*4 matrix @Kasra Manshaei, where do i find the 6th line :P" CreationDate="2016-01-06T15:05:13.053" UserId="13953" />
  <row Id="9454" PostId="9651" Score="0" Text="It would be really useful if you can print your dataframe and show us what the fields look like. I know this is probably obvious for people working with JSON regularly, but I think this community is more familiar with rectangular formats, so you will get a pythonic solution faster by providing some additional details. Thanks!" CreationDate="2016-01-06T16:05:18.380" UserId="9420" />
  <row Id="9455" PostId="9651" Score="0" Text="@AN6U5 added, thanks!" CreationDate="2016-01-06T16:11:40.317" UserId="982" />
  <row Id="9456" PostId="9652" Score="0" Text="Welcome to the site :)" CreationDate="2016-01-06T16:31:10.713" UserId="11097" />
  <row Id="9457" PostId="9654" Score="0" Text="Thank you for the valuable answer! The data is post-processed to be structured, I will try TinyDB, never heard of it before. I read some people have been using SQLite with mixed opinions about speed. Regarding the plotting, I guess I need to have a custom program that uses IPython since I will have to internally read the data from SQL, run some standard functions that should not be exported at the report to send to my users, right?" CreationDate="2016-01-06T16:57:29.833" UserId="14487" />
  <row Id="9458" PostId="9630" Score="0" Text=":)))) I mean 6th line in the code above dude ... not inside your matrix" CreationDate="2016-01-06T17:59:23.113" UserId="8878" />
  <row Id="9460" PostId="9654" Score="0" Text="Yeah, right!  If you don't want to use Ipython, then you can write a [Luigi](http://luigi.readthedocs.org/en/stable/) script for the workflow, and connect the whole thing to a server for visualization!" CreationDate="2016-01-06T18:04:37.410" UserId="11097" />
  <row Id="9461" PostId="9255" Score="0" Text="Any idea how to solve this? I am still waiting for this to resolve as my data getting bigger and bigger and existing solution takes for ever to generated dummy columns." CreationDate="2016-01-06T21:25:04.430" UserId="14604" />
  <row Id="9462" PostId="9619" Score="0" Text="But ML is better at determining topic.  A journal is going to look for new topics not duplicate.  Acceptance is more about quality of the article and the research.  Not having articles rejected is a key component." CreationDate="2016-01-06T21:36:02.637" UserId="13285" />
  <row Id="9463" PostId="6794" Score="0" Text="*Saying &quot;nearly half of the toys didn't have colors listed&quot; doesn't really add much to the conversation.* It adds an enormous amount to the conversation in my opinion and would be a statement I would **expect** to be made in the document (if not in a note on the figure itself!) if the nulls were dropped out." CreationDate="2016-01-07T00:41:38.060" UserId="905" />
  <row Id="9464" PostId="6513" Score="0" Text="If you want to stop early you should use a [sequential hypothesis test](https://en.wikipedia.org/wiki/Sequential_analysis) like [SPRT](https://en.wikipedia.org/wiki/Sequential_probability_ratio_test)" CreationDate="2016-01-07T01:34:02.660" UserId="381" />
  <row Id="9465" PostId="9038" Score="0" Text="Not if the data lies in a low-dimensional manifold, just as in your illustration. Determining this manifold is the goal of manifold learning." CreationDate="2016-01-07T01:41:47.073" UserId="381" />
  <row Id="9468" PostId="9659" Score="1" Text="But there are problems with these kinds of models. Specially since they work in discrete space, so it does not gives a good similarity measure between documents. The main reason I read was due to the use of discrete entities (bag-of-words model) any small changes in the doc will skew the distribution of doc a lot. While using Neural Embeddings it gives a continuos vector representation of a doc so the similarity measure works well in this space." CreationDate="2016-01-07T05:19:58.517" UserId="13518" />
  <row Id="9469" PostId="9662" Score="0" Text="Welcome to the site :)  What do you mean by `performing ad hoc sql (style) queries` on csv data?   Unless and until your data is in a relational data store, you **can't** do SQL style queries(obviously)" CreationDate="2016-01-07T06:29:09.103" UserId="11097" />
  <row Id="9471" PostId="9654" Score="1" Text="Thank you, I will have a look at both Luigi and TinyDB" CreationDate="2016-01-07T10:03:56.550" UserId="14487" />
  <row Id="9472" PostId="9670" Score="0" Text="Cross posting is generally discouraged in SE. So, please delete one of the two posts.  I'm sure you'd get a nice answer here, and welcome to the site! :)" CreationDate="2016-01-07T10:07:27.197" UserId="11097" />
  <row Id="9473" PostId="9670" Score="0" Text="I'll go take the other one down if it's against the rules, interested to hear your guys take on this! :D" CreationDate="2016-01-07T10:13:55.563" UserId="15233" />
  <row Id="9474" PostId="9672" Score="1" Text="Just wanted to tell you:  &quot;I'm a huge fan of your questions&quot;  :)" CreationDate="2016-01-07T10:28:50.390" UserId="11097" />
  <row Id="9475" PostId="9672" Score="0" Text="@Dawny33 Thank you very much :-)" CreationDate="2016-01-07T10:33:10.180" UserId="8820" />
  <row Id="9477" PostId="8682" Score="0" Text="So with techniques like the ones used in deep dream / the ones presented by Matt Zeiler in [Visualizing and Understanding Deep Neural Networks](https://www.youtube.com/watch?v=ghEmQSxT6tw) one can say that the CNN is learning the concept of &quot;cats&quot; and can of course be used to classify images as &quot;cats&quot;?" CreationDate="2016-01-07T10:53:54.717" UserId="8820" />
  <row Id="9478" PostId="8682" Score="0" Text="It still seems to me that any learning algorithm for classification can be used to generate instances of the class. Hence all classification algorithms would also be concept learning algorithms?" CreationDate="2016-01-07T10:54:45.360" UserId="8820" />
  <row Id="9479" PostId="9671" Score="0" Text="Thank you, I;m reading into training my own right now." CreationDate="2016-01-07T11:33:10.593" UserId="15233" />
  <row Id="9481" PostId="9677" Score="2" Text="I'm sorry I need to vote it close as opinion-based  :(&#xA;&#xA;Can you edit it and be a bit more specific?" CreationDate="2016-01-07T12:14:21.040" UserId="11097" />
  <row Id="9482" PostId="9677" Score="0" Text="@Dawny33 I mean the matters strictly connected to the machine learning - not programming or libraries. I just don't know how to put it into words. :(" CreationDate="2016-01-07T12:17:03.000" UserId="15197" />
  <row Id="9484" PostId="9677" Score="0" Text="@Luke The problem is that the question &quot;what are the most basic skills&quot; are based on personal opinion. People might disagree on the answer; there is not a single right answer. This kind of question is probably more suitable on reddit.com" CreationDate="2016-01-07T13:26:04.827" UserId="8820" />
  <row Id="9485" PostId="9677" Score="0" Text="@moose I wanted to ask for the essential knowledge. Feel free to edit my question." CreationDate="2016-01-07T13:31:39.363" UserId="15197" />
  <row Id="9486" PostId="9677" Score="1" Text="@Luke I understand what you asked and I've answered to that question. If you think I understood it wrong, you have to clarify. If I got it right, then this is opinion based. (I'm not saying this is a bad question; I only say that it is not suitable for the StackExchange format)." CreationDate="2016-01-07T13:36:11.140" UserId="8820" />
  <row Id="9487" PostId="9664" Score="1" Text="&quot;all articles&quot; - can you name some? Because that statement does not make sense, as you do not apply k-means to a &quot;(non-) convex function&quot; but to points from R^d" CreationDate="2016-01-07T14:08:16.330" UserId="924" />
  <row Id="9488" PostId="9662" Score="0" Text="So for example we want to be able to try and do some discovery queries using a SQL like language (aggregation with sum, avg, group by etc). Hive, Impala, Drill, Presto, Sparksql all are SQL on Hadoop type tools but don't implement the full SQL standard. Fro example putting this data into a Hive table I would consider it being in a &quot;relational store&quot;. I've got a feeling that doing this type of work is going to be difficult anyways even for any of those tools. So any other tips or suggestions on doing discover on this data in a timely manner would be awesome." CreationDate="2016-01-07T14:28:26.267" UserId="14829" />
  <row Id="9489" PostId="6778" Score="0" Text="I am still interested in this question." CreationDate="2016-01-07T17:49:22.903" UserId="843" />
  <row Id="9490" PostId="6778" Score="0" Text="So any answer is welcome :)" CreationDate="2016-01-07T17:49:28.247" UserId="843" />
  <row Id="9491" PostId="9668" Score="0" Text="Slightly off-topic, but how did you decide to use an 11th-degree polynomial? That's a very complex function for what appears to be a small amount of data!" CreationDate="2016-01-07T19:05:11.737" UserId="13413" />
  <row Id="9492" PostId="9686" Score="0" Text="A major and two minors is too thin.   Why not Computer Science or Data Science undergrad?  If math then statistics." CreationDate="2016-01-07T20:03:54.723" UserId="13285" />
  <row Id="9493" PostId="9686" Score="0" Text="Oh no, it's not two minors. I can choose what to focus on in my minor of data science so I'll be taking courses in economics and statistics." CreationDate="2016-01-07T20:06:24.707" UserId="15253" />
  <row Id="9494" PostId="9686" Score="0" Text="I'm not too good in computer science taught by professors. I was weak in computer science last year thus causing me to change my major to computer science. Major in data science isn't offered in my school. I'm still gonna do computer science through moocs and online courses." CreationDate="2016-01-07T20:08:01.330" UserId="15253" />
  <row Id="9495" PostId="9686" Score="0" Text="@user6214 - I think being &quot;weak&quot; in computer science is going to be a huge deal once you're done your undergrad and want to work. Learning programming is hard. It doesn't get any easier if you ignore it." CreationDate="2016-01-07T20:24:35.557" UserId="6529" />
  <row Id="9496" PostId="9659" Score="0" Text="Excellent point. I'm sure there are shortcomings to TF-IDF, just as there are to any model. Hopefully sharing different ways of tackling this problem will help readers find the best solution for their document set." CreationDate="2016-01-07T20:28:05.633" UserId="3466" />
  <row Id="9497" PostId="9686" Score="0" Text="I don't mean to argue here but I'm not ignoring programming, I just decided to go from the straightforward track of computer science to Mathematics. I can code. One of the best things I'm good at coding is the logic, I just can't turn it into code. I can't learn computer science in a 3 month setting. That's why I'm going to learn code on my own pace, individually." CreationDate="2016-01-07T20:32:36.137" UserId="15253" />
  <row Id="9498" PostId="9691" Score="0" Text="it looks great, I will try it out." CreationDate="2016-01-07T22:17:31.180" UserId="13023" />
  <row Id="9499" PostId="9556" Score="0" Text="Have you considered joining your efforts with one of the many other similar projects? Say, ELKI, Weka, JSAT, Smile, Hubminer, ... have you benchmarked? From a quick look, I saw a few constructs that look very expensive to me (though probably not as bad as R)." CreationDate="2016-01-07T22:27:00.350" UserId="924" />
  <row Id="9500" PostId="9692" Score="3" Text="You would make FAR more money consulting." CreationDate="2016-01-07T23:17:16.953" UserId="947" />
  <row Id="9501" PostId="9692" Score="0" Text="Isn't this waaay off topic for the site?" CreationDate="2016-01-07T23:29:59.340" UserId="6529" />
  <row Id="9503" PostId="9692" Score="0" Text="@rocinante Yes, you are right. This might be on-topic in the Startups SE, but still, it isn't framed well." CreationDate="2016-01-08T03:00:15.393" UserId="11097" />
  <row Id="9504" PostId="9668" Score="0" Text="@kyle in Matlab I tried with many polynomial degrees and then finally found that degree 11 gives desirable results so I used it." CreationDate="2016-01-08T03:52:10.267" UserId="14789" />
  <row Id="9506" PostId="9662" Score="0" Text="I think you should edit your question to expand greatly on the shape of your data (rows/columns/types/structures) and the sort of queries you want to make, without reference to any tool that you may think is the one for the job. We have to know if you have a nail or a screw before recommending a hammer or a screwdriver." CreationDate="2016-01-08T08:21:59.610" UserId="471" />
  <row Id="9507" PostId="9556" Score="0" Text="My goal was to have a nice programmer friendly set of tools, performance was not a goal. But I started to re-evaluate that. Regarding other projects: due to legal matter, I can't commit to distributed stuff and I can contribute only to projects with some type of open source licences. Anyway, thank you for taking a look, I would be very happy if you can give me more details regarding the constructs you mentioned. I expect your time is limited, and I don't dare to have a full discussion" CreationDate="2016-01-08T09:43:07.867" UserId="108" />
  <row Id="9508" PostId="9652" Score="0" Text="Thank you a lot -)" CreationDate="2016-01-08T10:02:13.960" UserId="14487" />
  <row Id="9509" PostId="9703" Score="0" Text="That is a good point, thank you. My intuition behind this is that this is mostly an issue around the borders (not just of the whole data set but also  of dense parts of the space). I'll just implement it on my set and see how big the buckets become. Thanks" CreationDate="2016-01-08T12:35:26.673" UserId="14904" />
  <row Id="9512" PostId="9702" Score="0" Text="A random forest regressor is a random forest of `decision trees`, so you won't get one equation like you do with linear regression.  Instead you will get a bunch of `if, then, else` logic and many final equations to turn the final leaves into numerical values.  Even if you can visualize the tree and pull out all of the logic, this all seems like a big mess.  If you are working in excel, maybe think about just training your model in excel using Azure.  However, I would probably just call the python from within excel." CreationDate="2016-01-08T17:08:55.240" UserId="9420" />
  <row Id="9513" PostId="9702" Score="0" Text="Taking the average of each leaf will not work? I tried also a linear regression model and the difference is inside the limits. So, if there isn't a reasonable and efficient way to export the random forest, I may need to step back to linear regression." CreationDate="2016-01-08T17:11:34.247" UserId="201" />
  <row Id="9514" PostId="9702" Score="0" Text="http://stackoverflow.com/questions/33732875/how-to-export-a-linear-regression-formula-out-of-sklearn-linearregression" CreationDate="2016-01-08T17:13:28.817" UserId="9420" />
  <row Id="9515" PostId="9702" Score="1" Text="Thank you but I was aware of this way in LR. Can you please join your comments on an answer so I could mark it as answered?" CreationDate="2016-01-08T17:15:09.193" UserId="201" />
  <row Id="9516" PostId="9702" Score="0" Text="Its probably worth leaving up/unanswered for a couple days to see if someone else has useful insight.  Data science stack exchange is much much smaller than stack overflow, so it takes 2-3 days at times to get good insightful answers." CreationDate="2016-01-08T17:22:01.170" UserId="9420" />
  <row Id="9517" PostId="9669" Score="0" Text="thanks for the suggestion. But initially i had tried with NGrams(3 words) with tf-idf approach. But the label generated were jot that meaningful. Can you suggest any NLP approach which will be more helpful." CreationDate="2016-01-08T19:13:45.183" UserId="15192" />
  <row Id="9518" PostId="9684" Score="0" Text="thanks... i will check the papers (in particular the first one)" CreationDate="2016-01-08T19:14:47.580" UserId="15192" />
  <row Id="9519" PostId="9664" Score="0" Text="@Anony-Mousse Correct me if I'm wrong tanawatl, but I believe OP is asking why the K-means optimization problem is non-convex." CreationDate="2016-01-08T19:57:05.157" UserId="14779" />
  <row Id="9520" PostId="9664" Score="0" Text="That question is discussed here: http://math.stackexchange.com/questions/463453/how-to-see-that-k-means-objective-is-convex" CreationDate="2016-01-08T20:45:19.507" UserId="924" />
  <row Id="9521" PostId="9706" Score="0" Text="Hi , sure  the math   behind   will be so good  if you  are kind  enough . The only thing I dont  understand  is  how  cand I  make the kernel  density  estimare  for  all the feature  spaces  between   the   2  sets?" CreationDate="2016-01-08T21:12:58.330" UserId="14946" />
  <row Id="9522" PostId="8338" Score="0" Text="You look at frequency in the corpus and discount.  Log of count in corpus is a common discount." CreationDate="2016-01-08T21:44:11.077" UserId="13285" />
  <row Id="9523" PostId="9617" Score="0" Text="See also: [What's the difference between a Markov Random Field and a Conditional Random Field?](http://stats.stackexchange.com/q/156697/25741)" CreationDate="2016-01-08T21:52:14.980" UserId="8820" />
  <row Id="9524" PostId="9692" Score="0" Text="Thank you for the comment. It will be actually off-line data science learning schools taking apprenticeship approach just like others, but I would like to create a brand identity of the company like a sport team as my initial statement. We can honestly try to win the competition but the main revenue source will come from the tuition and I can think of a few other small revenue sources. I just wanted to know if there is such an idea in the industry." CreationDate="2016-01-09T00:24:43.680" UserId="15258" />
  <row Id="9525" PostId="9693" Score="0" Text="Thank you for the comment. I just wanted to spit my idea to see what responses I would get. That's why I just asked &quot;have you heard...&quot;. It seems like you haven't and it is a good news. The company will be one of those offline coding schools in big cities, but my idea is to brand it to look distinct for the publicity. I would expect to get a stable stream of consulting works and I think our students will do better on the job placement." CreationDate="2016-01-09T00:34:45.993" UserId="15258" />
  <row Id="9526" PostId="9298" Score="0" Text="Just to give a little more info about n_jobs, it determines whether you parallelize the model fitting and predicting: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" CreationDate="2016-01-09T04:07:49.970" UserId="14715" />
  <row Id="9527" PostId="9708" Score="0" Text="Bayesian regression would give you these uncertainties through the posterior. Look up [Gaussian process regression](https://en.wikipedia.org/wiki/Kriging) too." CreationDate="2016-01-09T06:14:46.213" UserId="381" />
  <row Id="9528" PostId="9692" Score="1" Text="I'm voting to close this question as off-topic because this is just inviting open ended conversation." CreationDate="2016-01-09T09:04:13.350" UserId="21" />
  <row Id="9529" PostId="9706" Score="0" Text="@gm1 Did the edits to my answer, answer your question?" CreationDate="2016-01-09T09:05:18.647" UserId="14779" />
  <row Id="9530" PostId="9650" Score="0" Text="I'm a newbie to the subject of NLP, so I couldn't really understand the answer completely. Can you please guide me to some resource or any link where I could learn more about Sequence Label Modelling and also how or where can i leverage the regex features / dictionaries using NLTK. Also, you had mentioned about marking up the data? Can you please elaborate on this on how to approach? Also, is NLTK right choice for this problem or any other nlp tools? Thanks!" CreationDate="2016-01-08T02:21:50.947" UserId="15260" />
  <row Id="9531" PostId="9706" Score="0" Text="Yes  great :) I  selected an upvoted!" CreationDate="2016-01-09T10:41:40.603" UserId="14946" />
  <row Id="9532" PostId="9717" Score="0" Text="thanks a lot Russell. Very helpful response - will definitely check the book." CreationDate="2016-01-09T16:50:40.060" UserId="9197" />
  <row Id="9534" PostId="9692" Score="0" Text="I don't think this question is off-topic but I agree that it invites pen ended conversation. I think my question is the best addressed in the data science community rather than startup community in stackexchange." CreationDate="2016-01-09T19:28:36.870" UserId="15258" />
  <row Id="9535" PostId="9693" Score="0" Text="@Ikuyasu: If you do this, please check you understand Kaggle's rules on private sharing and make sure the submissions comply, because the branding would backfire badly if the end result was disqualification. In short, the school should either enter as a single class/team, or as individuals who must work separately (no shared materials, which would rule out some types of lesson - probably not suitable) or make the shared effort 100% public (which may defeat the purpose of trying to win)." CreationDate="2016-01-09T21:34:56.400" UserId="836" />
  <row Id="9536" PostId="9662" Score="0" Text="I can add a lot of detail but comment section probably isn't long enough to give all details so I'll edit my post. Thanks." CreationDate="2016-01-09T22:52:11.397" UserId="14829" />
  <row Id="9537" PostId="9718" Score="0" Text="Thanks for the information. Would it ever be feasible to run a single query at atime against this whole dataset in &lt; 5 minutes per query with Hive or SparkSQL with 6 machines with similar specs of the DS2.8XL machines you quoted? Currently using the optimizations found [here](http://hortonworks.com/blog/5-ways-make-hive-queries-run-faster/) and querying only on ~450GB of this CSV data I'm getting 30-60 second response times." CreationDate="2016-01-10T07:42:50.347" UserId="14829" />
  <row Id="9538" PostId="9719" Score="0" Text="This is a bit unclear. How exactly does the ML model interact with the web user? Are they supplying data for training and prediction, or is that done already and the interface is just doing predictions from new data from the web user? Do you want to run an ML model in Javascript on the web client or on the server? How does all this relate to slack? Please clarify." CreationDate="2016-01-10T13:14:20.463" UserId="471" />
  <row Id="9539" PostId="9724" Score="0" Text="Is every person already marked as abnormal or normal? If not, then you can't use K-NN, because that is a supervised learning technique which requires every observation have a value for a dependent variable (like normal/abnormal, in your case). Supposing you did have every person marked as normal or abnormal, depending on how many people you have in your dataset, you very well may run into serious curse of dimensionality with 100 features, so yes, some dimensionality reduction may be helpful." CreationDate="2016-01-10T16:46:30.710" UserId="14715" />
  <row Id="9540" PostId="6784" Score="0" Text="I've often wondered why they retained the ipython notebook command in splitting jupyter from ipython.  This is precisely why it gets really confusing. Thanks to you and kau zsh for pointing out the profile mismatch.  Its almost like they should use the ipython profile when using the ipython notebook command and the jupyter-notebook profile when using that command." CreationDate="2016-01-10T16:47:33.310" UserId="9420" />
  <row Id="9541" PostId="9724" Score="0" Text="So, @RussellRichie can I use SOM for dimensionality reduction and then K-Means for clustering?Is that a good approach?" CreationDate="2016-01-10T17:14:23.680" UserId="15308" />
  <row Id="9542" PostId="9725" Score="0" Text="Welcome to the site Anahita :)" CreationDate="2016-01-10T17:58:34.820" UserId="11097" />
  <row Id="9543" PostId="9718" Score="0" Text="Welcome to the site @Guy :)" CreationDate="2016-01-10T17:59:10.917" UserId="11097" />
  <row Id="9544" PostId="9727" Score="0" Text="Can you include a link which explain what is a ZeroR classifier, please?  And, welcome to the site :)" CreationDate="2016-01-10T18:18:50.290" UserId="11097" />
  <row Id="9545" PostId="9724" Score="0" Text="No. it's not. Clustering is not good for classification of normal vs. anomalous. It does not know what you are looking for. It may cluster as &quot;male&quot; vs. &quot;female&quot; and that would actually be a much better clustering! Get labels, and use a decision treenfor such data." CreationDate="2016-01-10T21:01:42.123" UserId="924" />
  <row Id="9546" PostId="9693" Score="0" Text="Thank you for your comment. I will check out the rule carefully." CreationDate="2016-01-10T22:18:43.873" UserId="15258" />
  <row Id="9547" PostId="9708" Score="0" Text="Thanks, I'll check that out. The specific implementation requirements strongly favour a simple linear regression, but it is possible I could do something more complex." CreationDate="2016-01-10T22:38:30.720" UserId="15281" />
  <row Id="9549" PostId="9734" Score="0" Text="Welcome to the site :)" CreationDate="2016-01-11T06:53:34.933" UserId="11097" />
  <row Id="9550" PostId="9734" Score="0" Text="@Dawny33 Thank you :)" CreationDate="2016-01-11T07:18:55.143" UserId="15324" />
  <row Id="9551" PostId="9735" Score="0" Text="Welcome to the site :)  Made some minor edits to the question, do revert them back if it conflicts with your intention!" CreationDate="2016-01-11T07:46:14.660" UserId="11097" />
  <row Id="9552" PostId="9736" Score="0" Text="Removed the `supervised learning` tag, as SVM's can also be used in [unsupervised learning problems](https://www.quora.com/Support-Vector-Machines/Is-it-possible-to-use-SVMs-for-unsupervised-learning-density-estimation)." CreationDate="2016-01-11T07:48:21.257" UserId="11097" />
  <row Id="9553" PostId="9741" Score="0" Text="If possible, it would be better if you can rewrite the Obj. func. in Latex :)  [I don't know Latex, so couldn't make the edit]" CreationDate="2016-01-11T10:41:33.323" UserId="11097" />
  <row Id="9554" PostId="9724" Score="0" Text="If I get labels should I use dimensionality reduction and then decision tree or just decision tree? And would the decision tree be useful to find why I have an abnormality? Thank you @Anony-Mousse" CreationDate="2016-01-11T10:42:14.137" UserId="15308" />
  <row Id="9555" PostId="9740" Score="0" Text="Alright~ `batch` is high dimensional. I should have noticed that. Thanks~" CreationDate="2016-01-11T11:08:16.477" UserId="3167" />
  <row Id="9556" PostId="9743" Score="0" Text="Tryied  stacking  before  , we  talked  on  a   previous thread about this, hence  it  is  not  always  improving   ,  I  arrived at this question" CreationDate="2016-01-11T11:52:54.080" UserId="14946" />
  <row Id="9557" PostId="9739" Score="0" Text="do we need to remove stopwords as a data pre processing step?" CreationDate="2016-01-11T12:38:52.210" UserId="15303" />
  <row Id="9558" PostId="9724" Score="0" Text="As said before, a decision tree does not need dimensionality reduction. On contrary, it will likely become worse. You need *labels* of what is &quot;abnormal&quot;!" CreationDate="2016-01-11T13:23:31.717" UserId="924" />
  <row Id="9559" PostId="9749" Score="1" Text="Thank you very much Dawny for your comprehensive answer. It gave some insight but still too technical for me. E.g. &quot; Extraction step of the ETL pipeline&quot; is a bit heavy for me! I know my knowldege of BigData structures is not enough but this post (along with two others about Flume &amp; ZooKeeper) is supposed to give me a starting point. a VERY starting point! I'll accept your answer for sure if I do not get better (simpler) ones and I'd like to ask you to dig a bit deeper for me IF POSSIBLE! Thanks in advance!" CreationDate="2016-01-11T14:54:59.187" UserId="14672" />
  <row Id="9560" PostId="9749" Score="0" Text="@DanielWelke Welcome to the site :) Yeah, please wait for more better answers.   For ETL, please have a look at this [Wikipedia](https://en.wikipedia.org/wiki/Extract,_transform,_load) article, and for understanding Big Data tools, a clear understanding of what ETL means; is important.  And that link would help you with that!" CreationDate="2016-01-11T14:58:36.740" UserId="11097" />
  <row Id="9561" PostId="9749" Score="1" Text="Thank you so much Dawny for answers and warm welcome :)" CreationDate="2016-01-11T15:00:12.003" UserId="14672" />
  <row Id="9563" PostId="9739" Score="1" Text="No, in the word2vec approach you don't need to do that, since the algorithm itself relies on a broad context to find similarities in words, so stop words (most of which are prepositions, pronouns and such) are an important asses for algorithm." CreationDate="2016-01-11T17:09:41.637" UserId="2573" />
  <row Id="9564" PostId="9737" Score="0" Text="Thanks for your answer @rapaio.  I've long thought we need a good example of bagging vs. boosting on this site and you have eloquently explained aspects of the two methods.  I would love to see this example developed still further if you happen to feel like geeking out a little and adding some graphics for a specific case.  Cheers!" CreationDate="2016-01-11T20:05:19.370" UserId="9420" />
  <row Id="9565" PostId="9737" Score="0" Text="Thank you, I have some misunderstanding and  updated the question ." CreationDate="2016-01-12T01:38:28.403" UserId="15325" />
  <row Id="9566" PostId="9751" Score="1" Text="@HoapHumaboid I know about SVC and SVR and that we can use Kernels to apply SVMs to NLPs and I'm aware of papers that compare SVMs with other learning algorithms...I was wondering about the characteristics of a prospective learning problem that would make it evident that it should be tackled with an SVM. In other words, what is it that when you see a learning problem makes you go &quot;oh I should definitely use SVMs for this'' rather than NNs or Decision trees or anything else" CreationDate="2016-01-12T02:59:04.730" UserId="11044" />
  <row Id="9567" PostId="9685" Score="0" Text="Thanks alot @rapaio I also found the same while reading it.&#xA;But at least if using same configuration they must give almost similar result.&#xA;&#xA;And moreover also found that If I scale my points rather from [0,1] to [0,100] the in scikit-learn SVM changes, it was not so in Matlab, The SVM scales it accordingly." CreationDate="2016-01-12T04:11:41.383" UserId="14789" />
  <row Id="9568" PostId="9736" Score="0" Text="Can explain how svm can be used for unsupervised problem and which package implements it?" CreationDate="2016-01-12T05:37:59.170" UserId="1151" />
  <row Id="9569" PostId="9736" Score="0" Text="@ML_Pro Please go through the link I have included in my comment." CreationDate="2016-01-12T05:59:31.453" UserId="11097" />
  <row Id="9570" PostId="9736" Score="0" Text="@ML_Pro He is referring to Support Vector Clustering" CreationDate="2016-01-12T06:01:52.897" UserId="11044" />
  <row Id="9571" PostId="9755" Score="1" Text="How does this answer the question?" CreationDate="2016-01-12T06:12:18.530" UserId="11097" />
  <row Id="9572" PostId="9736" Score="2" Text="@Dawny33, the application of SVMs to unsupervised learning is the exception and not the rule.  SVMs are a supervised learning method." CreationDate="2016-01-12T06:54:10.583" UserId="9420" />
  <row Id="9573" PostId="9736" Score="1" Text="@AN6U5 Thanks for informing :)   By `exception`, you mean that it is just a tweak and not the convention, right?" CreationDate="2016-01-12T08:40:33.037" UserId="11097" />
  <row Id="9574" PostId="9751" Score="0" Text="@Ragnar please see my edited response" CreationDate="2016-01-12T09:26:31.217" UserId="14994" />
  <row Id="9575" PostId="9750" Score="0" Text="Thank you for your nice answer Kevin! I'll wait 1-2 more days for other answers and in case I do not get better ones I'll accept yours. Cheers :)" CreationDate="2016-01-12T09:52:48.093" UserId="14672" />
  <row Id="9576" PostId="9758" Score="0" Text="What are red and green lines?" CreationDate="2016-01-12T11:28:35.053" UserId="8878" />
  <row Id="9577" PostId="9758" Score="1" Text="@KasraManshaei: I have updated the question." CreationDate="2016-01-12T11:34:39.950" UserId="15115" />
  <row Id="9578" PostId="9758" Score="1" Text="If possible, pl add the link to the tutorial.  Would help us understanding the answer and context better :)" CreationDate="2016-01-12T11:39:21.457" UserId="11097" />
  <row Id="9579" PostId="9758" Score="0" Text="@Dawny33: It's a video tutorial and uploading it would violate copyright issues I guess. :)" CreationDate="2016-01-12T13:37:46.613" UserId="15115" />
  <row Id="9580" PostId="9755" Score="0" Text="Why did you ask this question? Are you writing an encyclopedia or trying to solve a problem? Kinesis and Kafka solve similar problems. In the context of data science (I think this is the site...), they can be used interchangeably." CreationDate="2016-01-12T14:43:18.120" UserId="15300" />
  <row Id="9581" PostId="9762" Score="0" Text="What do you mean by taking into account the shipping time? Also it is better to randomly select the held-out data as you might have seasonality effect in such &quot;last month&quot; selection." CreationDate="2016-01-12T14:47:20.090" UserId="15300" />
  <row Id="9582" PostId="9736" Score="0" Text="Yes, that's what I mean. The `supervised learning` label is very much appropriate for a post on SVM and the `unsupervised learning` label is an outlier (I guess we could data mine 1000 publications and test this rigorously :-). Thanks for adding it back and thanks for your contributions to the site!" CreationDate="2016-01-12T14:49:20.740" UserId="9420" />
  <row Id="9583" PostId="9762" Score="0" Text="@Guy thank you for pointing out the random selection. What I mean by taking into account shipping time is using it as an independent variable." CreationDate="2016-01-12T15:14:29.647" UserId="15356" />
  <row Id="9584" PostId="9762" Score="0" Text="@Guy I edited the question to reflect the random selection of data." CreationDate="2016-01-12T15:16:12.137" UserId="15356" />
  <row Id="9585" PostId="9755" Score="0" Text="Yes, your answer is correct for `What are some nice alternatives to Kafka?`, but not for this question :)" CreationDate="2016-01-12T15:45:24.250" UserId="11097" />
  <row Id="9587" PostId="9729" Score="0" Text="If you want a really nice way of comparing images, I suggest reading [Supervised Learning of Semantics-Preserving Hashing via Deep Neural Networks for Large-Scale Image Search](http://arxiv.org/abs/1507.00101). There is also some [example code](https://github.com/kevinlin311tw/Caffe-DeepBinaryCode)" CreationDate="2016-01-12T17:56:58.253" UserId="8820" />
  <row Id="9588" PostId="8493" Score="0" Text="Actually ANN is parametric and SVM in non-parametric, see https://en.wikipedia.org/wiki/Nonparametric_statistics#Non-parametric_models" CreationDate="2016-01-12T21:26:25.180" UserId="15368" />
  <row Id="9589" PostId="9760" Score="0" Text="Thank you for the answer. I have few ambiguities. &#xA;&#xA;- At the end of the graph, green line and the red line converged. Doesn't it mean that we have enough data for our model?&#xA;- Is it possible to get a better accuracy from the test set than the training set?&#xA;- Lets say that we got a better model and how should that graph look like?" CreationDate="2016-01-13T01:50:28.517" UserId="15115" />
  <row Id="9590" PostId="9737" Score="0" Text="Thanks again for your awesome answer!" CreationDate="2016-01-13T01:54:26.170" UserId="15325" />
  <row Id="9591" PostId="9766" Score="0" Text="(+1)  Time series modelling should be the approach which needs to be followed here!" CreationDate="2016-01-13T03:29:14.257" UserId="11097" />
  <row Id="9592" PostId="9714" Score="0" Text="I have tried many of the options for C and hence arrived for the value of it. Moreover its like my human mind see a polynomial running on the surface which separates the two regions.&#xA;&#xA;Lately I found the Solution using RBF with low gamma and high C value" CreationDate="2016-01-13T05:53:41.287" UserId="14789" />
  <row Id="9593" PostId="9714" Score="0" Text="My guess is that you've reached some sort of limit in SKL treatment of polynomial basis functions at order 11.  All I'm saying is back off and retune your model from the default parameters.  Maybe try finding the best SKL model, then try porting those params over to Matlab.  Take a look at what RBFs look like by comparison (actually visualize the decision surface) and I bet you'll use those by default next time!" CreationDate="2016-01-13T06:04:10.013" UserId="9420" />
  <row Id="9594" PostId="9761" Score="0" Text="That feature *may* be a unique identifier (user number). **If** so, then you should ignore it, obviously." CreationDate="2016-01-13T09:45:00.893" UserId="924" />
  <row Id="9595" PostId="9768" Score="0" Text="I agree this is the original data model. I was wondering however how I should model my data to feed it to a clustering algorithm.. I was thinking of one table with the following elements: Term, Polarity, Frequency, Person, Age, Gender, Country. But I'm afraid that feeding this to an clustering algorithm wouldn't work as terms occur multiple times (as do persons)" CreationDate="2016-01-13T10:04:03.050" UserId="15366" />
  <row Id="9597" PostId="9761" Score="0" Text="@Anony-Mousse good point. At my case I know its not." CreationDate="2016-01-13T12:20:22.673" UserId="14560" />
  <row Id="9600" PostId="9760" Score="1" Text="&quot;Doesn't it mean that we have enough data for our model?&quot; That is exactly what I wrote. Yes, you have enough data so if you want to improve you should try more complexity. Data is enough. &quot;Is it possible to get a better accuracy from the test set than the training set?&quot; I never saw such a thing. That might happen in a single experiment but not in general. This question can be translated to &quot;Can I know more than what I know?&quot; and the answer is &quot;Of course not!&quot;" CreationDate="2016-01-13T13:03:58.080" UserId="8878" />
  <row Id="9601" PostId="9760" Score="1" Text="&quot;Lets say that we got a better model and how should that graph look like?&quot; I assume (you try and let me know if I'm right :) ) that either both training and test improve or non of them. It is possible that training improves and test falls but not the vice versa and also it's possible that both improve for a while then test falls down which is called Overfitting. You should stop training at the point test line is starting to fall" CreationDate="2016-01-13T13:07:00.153" UserId="8878" />
  <row Id="9602" PostId="9765" Score="1" Text="Very nice @moose! (+1) for comprehension of the explanation" CreationDate="2016-01-13T13:08:23.817" UserId="8878" />
  <row Id="9603" PostId="9760" Score="1" Text="Thank you. Your understanding helped a lot." CreationDate="2016-01-13T13:27:15.940" UserId="15115" />
  <row Id="9604" PostId="9767" Score="0" Text="The data model I'm referring to in my question, is how I should model my data to feed it to a clustering algorithm. Not how my data model of the 'raw data' should look like." CreationDate="2016-01-13T14:28:36.780" UserId="15366" />
  <row Id="9605" PostId="9778" Score="1" Text="Yes, that makes sense. So how are these features handled in real life? One-hot vector would increase number of features which could affect the performance (due to overfitting  and all)." CreationDate="2016-01-13T16:02:50.373" UserId="13518" />
  <row Id="9606" PostId="9778" Score="1" Text="But the alternative would create a feature that adds misleading information. Using a one-hot encoding and then performing some form of unsupervised feature selection (based on frequency) to keep your number of features lower would probably be a better solution. If you are worried about overfitting, use a regularized learner.&#xA;&#xA;You may also be interested in the accepted answer of [this SO question](http://stackoverflow.com/questions/17469835/one-hot-encoding-for-machine-learning)." CreationDate="2016-01-13T16:32:49.973" UserId="12909" />
  <row Id="9607" PostId="9778" Score="0" Text="Thanks Jeremie, that helped." CreationDate="2016-01-13T16:40:05.977" UserId="13518" />
  <row Id="9608" PostId="9780" Score="0" Text="The bozo solution might be to run one algorithm then create an additional feature &quot;cluster_label&quot; and feed it into the next algorithm.  You could then scale the &quot;cluster_label&quot; feature up or down until you get the result you want.  Possibly experiment with the ordering of the two." CreationDate="2016-01-13T19:59:42.673" UserId="9420" />
  <row Id="9609" PostId="9750" Score="0" Text="Nice explanation :) is it conceptually similar to Spark Streaming?" CreationDate="2016-01-13T20:12:52.647" UserId="13891" />
  <row Id="9610" PostId="9780" Score="1" Text="https://en.wikipedia.org/wiki/Consensus_clustering" CreationDate="2016-01-13T20:14:01.467" UserId="381" />
  <row Id="9611" PostId="9750" Score="0" Text="Not quite. Spark Streaming would be analogous to Apache Storm or Apache Flink. While there might be some overlap in the same concept of ingesting data into HDFS, Spark Streaming, Storm, Flink all are more related to streaming datasets (think Twitter streams). Flume/Sqoop would be more for &quot;static&quot; type datasets (e.g. extracting data from log files or SQL databases). This is a good article that discusses the [performance comparison of SS, Flink, Storm](http://yahooeng.tumblr.com/post/135321837876/benchmarking-streaming-computation-engines-at). Might give you a better idea of possible utilities." CreationDate="2016-01-13T20:31:48.560" UserId="14829" />
  <row Id="9612" PostId="9776" Score="0" Text="Thanks! However, the $w' = w^T$ seems to be common practice, as it is, e.g., used in the very basic tutorial for denoising autoencoders of deeplearning.net: (http://deeplearning.net/tutorial/dA.html#daa) I do not find it so reasonable to use the same activation function for reconstruction, could you elaborate on this? Its true that it is the simplest choice, but it seems much more natural to me to use the $z'_i$ with the $arctanh$, because this yields actually the mathematical inverse of the activation." CreationDate="2016-01-13T21:20:47.370" UserId="15372" />
  <row Id="9613" PostId="9776" Score="0" Text="You can if you want. E.g from http://deeplearning.net/tutorial/dA.html &quot;**Optionally**, the weight matrix $W'$ of the reverse mapping may be constrained to be the transpose of the forward mapping: $W' = W^T$. This is referred to as tied weights.&quot; (Emphasis mine). The point of my answer is that if you do this, it is not in order to provide automatic reversal of the encoding, it is just a constraint which will regularise the training." CreationDate="2016-01-13T21:26:59.860" UserId="836" />
  <row Id="9615" PostId="9783" Score="0" Text="Welcome to the site, Matt :)" CreationDate="2016-01-14T05:34:31.003" UserId="11097" />
  <row Id="9619" PostId="9786" Score="0" Text="Pls do not cross post. Remove either of thetwo posts :)" CreationDate="2016-01-14T10:17:13.017" UserId="11097" />
  <row Id="9625" PostId="9795" Score="0" Text="installed already" CreationDate="2016-01-14T18:12:23.087" UserId="15412" />
  <row Id="9626" PostId="9795" Score="0" Text="Got more than one python install, an older one not in path?" CreationDate="2016-01-14T18:13:42.513" UserId="1244" />
  <row Id="9627" PostId="9776" Score="0" Text="Thanks Neil. Your comment about the $w' = w^T$ issue helped me to generalize my question and make it more precise, so I edited the question accordingly. In fact, I actually don't understand why it is useful to have separate $w'$ at all, instead of always using the transposed matrix $w^T$. The answer might be &quot;because it gives better results&quot;, but then I am wondering __why__ it gives better results. It looks unintuitive to me." CreationDate="2016-01-14T19:59:10.610" UserId="15372" />
  <row Id="9628" PostId="9776" Score="0" Text="@ManfredEppe: Perhaps instead you should be thinking carefully about why you think the transposed weight matrix and inverse function would be useful? There is no specific reason to use them - what exactly is your intuition behind thinking that they would be useful? If it is for &quot;symmetry&quot; then take another look at the order in which they are applied - it is not a symmetric reversal of the input-to-hidden layer (if it were, the inverse activation function should be first)" CreationDate="2016-01-14T20:40:27.087" UserId="836" />
  <row Id="9629" PostId="9787" Score="0" Text="Max, thanks for the guidance. `plot_patrial_dependence` is really helpful, not just for this, but for future feature selection. Cheers." CreationDate="2016-01-15T06:02:50.533" UserId="15398" />
  <row Id="9630" PostId="9797" Score="0" Text="I couldn't add pybrain as tag (too little rep.)" CreationDate="2016-01-15T07:44:27.620" UserId="15197" />
  <row Id="9631" PostId="9797" Score="1" Text="Done it for you :)" CreationDate="2016-01-15T10:47:33.390" UserId="11097" />
  <row Id="9633" PostId="9796" Score="1" Text="Maybe try the Anaconda Python distribution (https://www.continuum.io/downloads). It's got a great package manager that takes care of all dependency problems." CreationDate="2016-01-15T13:28:31.973" UserId="676" />
  <row Id="9638" PostId="9770" Score="0" Text="K-means segmentation can also give local minima, because you still need to chose initial medoids\centroids. My current solution is good enough for me, but I can't claim it is the best one. I can share details of my solution if you interested in it." CreationDate="2016-01-16T18:08:55.007" UserId="9637" />
  <row Id="9639" PostId="9807" Score="0" Text="Have you tried adding regularization?" CreationDate="2016-01-16T07:56:47.340" UserId="14101" />
  <row Id="9640" PostId="9770" Score="0" Text="It depends how you implement it. In the meantime, I have found out in some literature that for segmentation, it's feasible to find the global minimum, in polynomial time." CreationDate="2016-01-17T09:30:26.803" UserId="15373" />
  <row Id="9641" PostId="9817" Score="0" Text="if you could elaborate your answer a little please" CreationDate="2016-01-17T12:19:33.760" UserId="13953" />
  <row Id="9642" PostId="9813" Score="0" Text="This is a cross-posting: http://stackoverflow.com/q/34835159/562769" CreationDate="2016-01-17T12:36:29.727" UserId="8820" />
  <row Id="9643" PostId="9528" Score="0" Text="See also: Alexander Ihler: [Support Vector Machines (1): Linear SVMs, primal form](https://www.youtube.com/watch?v=IOetFPgsMUc&amp;index=159) on YouTube. 25.01.2015." CreationDate="2016-01-17T13:43:34.770" UserId="8820" />
  <row Id="9645" PostId="9818" Score="0" Text="It's not a question of domain. It's a question of how much data you have, how good your priors are, and whether you want posteriors." CreationDate="2016-01-17T18:22:27.547" UserId="381" />
  <row Id="9646" PostId="9818" Score="0" Text="@Emre Which is a question of domain... (and, of course, of money when you have the possibility to not only use existing datasets but can also hire people to create / label new data)." CreationDate="2016-01-17T18:26:05.697" UserId="8820" />
  <row Id="9647" PostId="9818" Score="0" Text="It would be a question of domain if there were some property of the data, some structure, that one algorithm took advantage of better than the other, but that is not what I am suggesting." CreationDate="2016-01-17T19:16:20.560" UserId="381" />
  <row Id="9648" PostId="9729" Score="0" Text="@moose, thanks for your help. I will definitely explore the neural networks for image clustering. Meanwhile, please see the latest update to the original post - I've shared my project on GitHub about this topic." CreationDate="2016-01-17T23:53:44.540" UserId="15315" />
  <row Id="9649" PostId="9778" Score="0" Text="I suggest  one-hot followed by some dimensionality reduction like PCA.  PCA will find linear correlations in the binary features and eliminate them. Don't worry too much about overfitting apriori.  Instead wait until you actually see some overfitting and then reduce the dimensionality or add some regularization." CreationDate="2016-01-18T00:59:14.667" UserId="9420" />
  <row Id="9651" PostId="9704" Score="0" Text="It was really helpful." CreationDate="2016-01-18T08:34:48.257" UserId="837" />
  <row Id="9653" PostId="9804" Score="1" Text="I agree. My boosted trees almost always outperform very painstakingly crafted and optimized GLMs. Of course, you lose interpretability when you gain predictive power." CreationDate="2016-01-18T12:44:54.797" UserId="1264" />
  <row Id="9654" PostId="9823" Score="0" Text="How much data do you have? What is length of your sequences? LSTM's are only really useful for problems with lots of data and long term dependencies." CreationDate="2016-01-18T12:52:00.547" UserId="3044" />
  <row Id="9655" PostId="9823" Score="0" Text="Random search or Bayesian optimization are standard ways of finding hyperparameters :)" CreationDate="2016-01-18T12:52:37.343" UserId="3044" />
  <row Id="9656" PostId="9823" Score="1" Text="Are you sure you need the embedding layer? Many time series datasets would not need it." CreationDate="2016-01-18T12:54:15.233" UserId="3044" />
  <row Id="9657" PostId="9821" Score="2" Text="Why are they preferred?" CreationDate="2016-01-18T14:13:44.307" UserId="1272" />
  <row Id="9663" PostId="9823" Score="0" Text="I have nearly 100k data points and twice as many features as the IMDB example so I don't think that's the problem. As for the embedding layer, how exactly would you connect the LSTM layer to the input? According to the documentation http://keras.io/layers/recurrent/#lstm Keras' LSTM only takes initializations, activations and output_dim as arguments. If that is the source of the error, code describing how to eliminate the embedding layer will be greatly appreciated." CreationDate="2016-01-18T16:58:32.903" UserId="13023" />
  <row Id="9664" PostId="9837" Score="3" Text="This is too vague and conversational to make a good answer. Some specifics, facts and editing would improve it." CreationDate="2016-01-18T17:54:56.320" UserId="21" />
  <row Id="9665" PostId="9290" Score="0" Text="You could think as an alternative solution such as using random forest over h2o :[http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shannon/2/docs-website/h2o-docs/index.html](http://h2o-release.s3.amazonaws.com/h2o-dev/rel-shannon/2/docs-website/h2o-docs/index.html)" CreationDate="2016-01-10T08:05:09.040" UserId="14946" />
  <row Id="9666" PostId="9837" Score="0" Text=",,Specific  facts'' should be specified   by people  that post  messages   like  that  say that neural  nets  are the best  , you  cant say neural nets are doing  fine just   because   they sound  fancy ,  there  are  also   data sets  in wich   neural networks  probably  do  so  bad  in such a manner  that  knn are getting from   far  better   result." CreationDate="2016-01-18T19:30:29.407" UserId="14946" />
  <row Id="9667" PostId="9823" Score="0" Text="Please see my answer. It seems you don't need the embeding layer." CreationDate="2016-01-18T23:08:20.360" UserId="3044" />
  <row Id="9668" PostId="6374" Score="0" Text="Orange 3 now seem to feature FP-growth in its Orange3-Associate add-on. Please see the other answer. :)" CreationDate="2016-01-19T00:04:20.707" UserId="15527" />
  <row Id="9669" PostId="6202" Score="0" Text="Did you mean like a _scatter plot_?" CreationDate="2016-01-19T00:07:44.567" UserId="15527" />
  <row Id="9670" PostId="9837" Score="1" Text="While I don't deny your views, you should also not that your answer doesn't really answer the question. So, pl consider adding it as a comment. And, please add any concrete proofs and theory supporting your answer, else it might be looked at, as a rant, by future viewers :)" CreationDate="2016-01-19T03:12:36.487" UserId="11097" />
  <row Id="9671" PostId="9837" Score="1" Text="@gm1 I guess you meant me with &quot;,,Specific facts'' should be specified by people that post messages like that say that neural nets are the best&quot;. Please note that I did not write a statement which was that general. I wrote that NN win in many competitions / CV tasks. And I've added a couple of challenges in which neural networks approaches won." CreationDate="2016-01-19T06:02:09.383" UserId="8820" />
  <row Id="9672" PostId="9846" Score="0" Text="You can try `RandomForestRegressor`" CreationDate="2016-01-19T06:28:40.533" UserId="13406" />
  <row Id="9673" PostId="9846" Score="0" Text="Welcome to the site :)" CreationDate="2016-01-19T06:33:07.497" UserId="11097" />
  <row Id="9674" PostId="9667" Score="0" Text="Just a suggestion :) ... I'm not sure if you get any proper answer as long as the question is that long. For example your tags are exactly my research direction but I really have no time &amp; energy to read it all! If you can update a shorter version, it would be better for you Q and also for yourself as in scientific reporting you need to talk things out briefly." CreationDate="2016-01-19T07:03:26.493" UserId="8878" />
  <row Id="9675" PostId="9667" Score="0" Text="I will try to reduce the question. Is just that usually if I don't explain myself people confuse what I intended. Anyway, as soon as I get time on my job Ill try to reduce the size of the question, thank you for the recommendation" CreationDate="2016-01-19T07:30:42.690" UserId="15230" />
  <row Id="9676" PostId="9850" Score="0" Text="Good question.  Welcome to the site :)" CreationDate="2016-01-19T11:49:57.497" UserId="11097" />
  <row Id="9677" PostId="6202" Score="0" Text="No, i meant custom tables where you can customize columns and rows according to your needs." CreationDate="2016-01-19T14:16:01.053" UserId="8999" />
  <row Id="9678" PostId="9851" Score="0" Text="Thank you, I have read about FAMD before, which unfortunately seems to have only R support - hence my question. The least I can do now is to treat results of both methods (PCA &amp; MCA) in separation. However, if there is a way to 'mix' them together to yield a monolithic dataset then this is the answer I am looking for." CreationDate="2016-01-19T15:22:05.497" UserId="15148" />
  <row Id="9679" PostId="6202" Score="0" Text="Sorry, you mean customize columns and rows as in add new columns/rows and the data within?" CreationDate="2016-01-19T15:59:12.373" UserId="15527" />
  <row Id="9680" PostId="8427" Score="0" Text="Are you fine with just using an API / service?" CreationDate="2016-01-19T16:16:03.797" UserId="12363" />
  <row Id="9681" PostId="9851" Score="0" Text="Ok, it turned out that my dataset is big enough as to make the MCA implementation at hand to run out of memory." CreationDate="2016-01-19T17:00:03.760" UserId="15148" />
  <row Id="9682" PostId="9851" Score="0" Text="This is a recurrent issue I get with the R implementation. If you have a limited set of variable, try to weight your observations. Or use only a subset of your observation. Otherwise, you could try to transform your numeric variable into ordinal category or to transform your qualitative variables into flags." CreationDate="2016-01-19T17:03:19.673" UserId="15543" />
  <row Id="9684" PostId="9495" Score="0" Text="For people finding this and looking for function approximation: I've wrote a short blog post how to do function approximation with gaussian processes and sklearn: https://martin-thoma.com/function-approximation/" CreationDate="2016-01-19T19:16:40.867" UserId="8820" />
  <row Id="9685" PostId="9855" Score="0" Text="can it be done the other way round so a Bayes network matches any given neural network?" CreationDate="2016-01-19T19:20:12.613" UserId="1272" />
  <row Id="9686" PostId="9849" Score="0" Text="Also posted on CS.SE (where it got some comments): http://cs.stackexchange.com/q/52042/755" CreationDate="2016-01-19T19:59:03.677" UserId="8560" />
  <row Id="9688" PostId="9860" Score="0" Text="Which version of pandas are you using? With 0.17.1 I was able to read a csv consisting only of &quot;©1990-2016 AAR,All rights reserved&quot; with and without the `encoding` flag." CreationDate="2016-01-19T21:44:33.853" UserId="381" />
  <row Id="9689" PostId="9837" Score="0" Text="Hi there , there are of course some  Kaggle competitions in which neural nets    did well (supposing  they didn't used neural nets   combined with other    models) , but this  is  a small proportion  of  all  kaggle competitions, could you use a neural network  to  go  TOP 3  in  kaggle  TFI?I think  I am able  to do  for both public  and private   LB   with model that is   non non-linear." CreationDate="2016-01-19T22:00:07.107" UserId="14946" />
  <row Id="9690" PostId="9837" Score="0" Text="Anyway , I think  kaggle should  not  be treated  with so  much seriosity." CreationDate="2016-01-19T22:01:14.297" UserId="14946" />
  <row Id="9691" PostId="9858" Score="0" Text="What *is* a &quot;bucket&quot;?" CreationDate="2016-01-19T22:20:30.333" UserId="924" />
  <row Id="9692" PostId="9858" Score="0" Text="@Anony-Mousse &quot;bucket&quot; is a &quot;cluster&quot;. For ex: k-means is used to divide the data into 'k' buckets/clusters" CreationDate="2016-01-19T22:25:31.773" UserId="15552" />
  <row Id="9693" PostId="9858" Score="0" Text="But then it sounds like MacQueens algorithm." CreationDate="2016-01-19T22:27:43.287" UserId="924" />
  <row Id="9694" PostId="9858" Score="0" Text="@Anony-Mousse. yes apart from the first step Hartigan-Wong seems just like MacQueens algorithm. But I am not sure if this is the correct understanding. There might be some concept I am missing for iterations and convergence." CreationDate="2016-01-19T22:31:17.860" UserId="15552" />
  <row Id="9695" PostId="9858" Score="0" Text="@Anony-Mousse I added one more possible method. Hope that is clear." CreationDate="2016-01-19T22:56:37.887" UserId="15552" />
  <row Id="9696" PostId="9856" Score="1" Text="Just a small suggestion; for the sake of clarity you should include in your code that &quot;T&quot; stands for Theano operations. The same for your question text. A toy dataset with the same characteristics as your real data would facilitate reproducibility." CreationDate="2016-01-19T23:19:47.563" UserId="13023" />
  <row Id="9697" PostId="9855" Score="0" Text="Note that this says nothing about the size of the neural network required; it is not a practical result." CreationDate="2016-01-20T03:53:52.767" UserId="381" />
  <row Id="9698" PostId="9862" Score="0" Text="Welcome to the site :)  Random forests is one of the many ways XGBoost does classification. Can you please make your question more clear?" CreationDate="2016-01-20T05:45:30.490" UserId="11097" />
  <row Id="9700" PostId="9867" Score="0" Text="Thanks for answering.  Yeah, NLP seems more like taking out the latent features of the text, and text-ming is more like _Exploratory Analytics_ on the text :)   And would you agree with the explanation in my answer here?" CreationDate="2016-01-20T06:59:02.013" UserId="11097" />
  <row Id="9701" PostId="9862" Score="0" Text="OK, suppose I have xgboost classifier (objective=&quot;binary:logistic&quot;, metric = &quot;auc&quot;) based on 50 trees and new observation based on which I want to make a prediction. How does it work in practice? I see 2 options: Each tree &quot;says&quot; what's the predicted y (depend on the leaf structure where my observation fell into) and the final y is chosen based on majority votes. Or maybe each tree give you predicted probability and final results is an average (not sum like in xgboost regression model) of them.&#xA;Or maybe there are some more alternatives?" CreationDate="2016-01-20T09:23:22.570" UserId="13384" />
  <row Id="9702" PostId="9849" Score="0" Text="Already answered at http://stackoverflow.com/questions/34661818/building-speech-dataset-for-lstm-binary-classification/" CreationDate="2016-01-20T10:24:26.607" UserId="15557" />
  <row Id="9703" PostId="9868" Score="0" Text="Thanks for the response. Can you elaborate little more on what you wrote? How exactly the intercept is being calculated? I'm referring to tutorial written by the author of xgboost package (https://xgboost.readthedocs.org/en/latest/model.html - section Tree boosting - additive training) and there is no intercept - just a linear combination of single trees' prediction (I know that this formula refers to regression, but still there is some inconsistency). Last but not least, are the temporary predicted variables 0-1 or there are predicted probabilities stemmed from single trees?" CreationDate="2016-01-20T10:57:46.307" UserId="13384" />
  <row Id="9704" PostId="9861" Score="0" Text="Thanks @Emre for your quick answer. Yes I ended up by using sklearn.preprocessing.MultiLabelBinarizer to Hot encode the categorical variables. What do you think ?" CreationDate="2016-01-20T11:16:51.697" UserId="3433" />
  <row Id="9705" PostId="9856" Score="1" Text="I don't see any sampling in your code. E.g. for negative phase of hidden variables you should have something like `hid_means = T.nnet.sigmoid(T.dot(x, w) + bh); hid = sample(hid_means)`, where `sample(means) = int(rand() &lt;= means)` for binary hidden variables.  Also how do you conclude that it doesn't work? How many epochs did you use and how was `costs` changing with each epoch?" CreationDate="2016-01-20T11:38:23.883" UserId="1279" />
  <row Id="9707" PostId="9850" Score="1" Text="Usually, MSE is taken for regression and Cross-Entropy for classification. Classification Figure of Merit (CFM) was introduced in &quot;A novel objective function for improved phoneme recognition using time delay neural networks&quot; by Hampshire and Waibel. If I remember it correctly, they also explain why they designed CFM like they did." CreationDate="2016-01-20T13:26:34.447" UserId="8820" />
  <row Id="9711" PostId="9856" Score="0" Text="I slightly changed my code. Is that what you mean by sampling?" CreationDate="2016-01-20T13:58:30.140" UserId="13736" />
  <row Id="9712" PostId="9856" Score="0" Text="I used 100 epochs. Cost initially decreases, then it stabilizes (goes up and down a little but it does not decrease)" CreationDate="2016-01-20T13:59:37.630" UserId="13736" />
  <row Id="9713" PostId="9868" Score="0" Text="I have some doubts on what you said. For bernoulli and trees that give you some small probability (like 0.05 for all), the more weak learners you have the final predictions converge to 1 (since you add &gt;0 elements in denominator). In that case you would rather guess that 0 is a good prediction. Apart from that, I will definetely peruse documentation you mentioned but is it somehow connected with xgboost package (which is similar to gbm but not exactly)." CreationDate="2016-01-20T14:18:52.190" UserId="13384" />
  <row Id="9716" PostId="9856" Score="0" Text="You aren't updating the biases." CreationDate="2016-01-20T14:43:53.943" UserId="14906" />
  <row Id="9717" PostId="9868" Score="0" Text="Not sure to understand, but each time you create a model, all the set of tree is modified. So in the end, there is no similarities between a model with n trees and a model with n+1 trees. Yes, the first one is for gbm. But the concept is equivalent for xgboost." CreationDate="2016-01-20T15:16:14.463" UserId="15543" />
  <row Id="9718" PostId="9856" Score="0" Text="You are right, would that cause the problem I am seeing? I am going to try that to see if I get a better result." CreationDate="2016-01-20T15:30:03.330" UserId="13736" />
  <row Id="9719" PostId="9868" Score="0" Text="Actually not. Look at slides 20-21 of https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf. In consecutive steps, new tree is built to &quot;upgrade&quot; current prediction which is constant (once i-th tree is built its form never changes till the end of algorithm). I'm pretty sure that is, by and large, the principle of boosting, isn't it?" CreationDate="2016-01-20T15:33:22.213" UserId="13384" />
  <row Id="9720" PostId="9868" Score="0" Text="At each step, of the creation of one model, yes, the previous tree is kept. But when you create a whole new model, modifying the parameter &quot;number of tree&quot;, the two resulting tree may be very different, if only due to the seed used. And other point, the temporary &quot;predicted variable&quot; could be negative." CreationDate="2016-01-20T15:46:58.497" UserId="15543" />
  <row Id="9721" PostId="9868" Score="0" Text="Cerainly, these models will be different, but having one particular model which has many (like &gt;100000) trees, final prediction will be 1, even if all the trees give value close to 0. Of course, this is true assuming predictive probabilities are \in [0,1]. Thus, what does it mean that it might be negative (if so, what's their definition because it's very counter-intuitive)" CreationDate="2016-01-20T16:14:06.697" UserId="13384" />
  <row Id="9722" PostId="9861" Score="0" Text="Sounds good, but [the documentation says](http://scikit-learn.org/stable/modules/multiclass.html) *All classifiers in scikit-learn do multiclass classification out-of-the-box*, so you should be able to let sklearn do the encoding. If it does not work for you let us know." CreationDate="2016-01-20T17:01:42.927" UserId="381" />
  <row Id="9723" PostId="9861" Score="0" Text="Yes I ended up by using sklearn.preprocessing.MultiLabelBinarizer before using sklearn.naive_bayes.BernoulliNB and gave pretty nice results (not satisfied though) :)" CreationDate="2016-01-20T21:49:05.820" UserId="3433" />
  <row Id="9724" PostId="9856" Score="1" Text="Yes, that's what I meant by sampling. Note, that even though hidden variables are almost always binary, for visible variables you can (and in context of images even recommended) to use Gaussian or even use mean values directly (i.e. binary sampling for hiddens, no sampling at all for visible) . Anyway, more important is how you evaluate the result. For MNIST it's very convenient to visualize learned weights after each epoch, i.e. reshaping every each wait to original image's size and showing it. If you had some decrease in cost for some time, you could just learn good representation." CreationDate="2016-01-20T22:56:42.443" UserId="1279" />
  <row Id="9725" PostId="9856" Score="0" Text="In this case your weights may look something like [this](http://people.idsia.ch/~masci/software/filters_at_epoch_14.png) (maybe a little bit worse, but not just random noise). Also it's important to choose good number of hidden variables - for 20x20 MNIST images number between 40 and 100 should be suitable." CreationDate="2016-01-20T23:00:00.903" UserId="1279" />
  <row Id="9726" PostId="5404" Score="0" Text="We're asking the same question. It appears R can already read/write from.to SQL, So I don't understand what SQL 2016 will offer. Maybe you can run R packages within SQL?" CreationDate="2016-01-21T01:51:40.617" UserId="15584" />
  <row Id="9727" PostId="9877" Score="1" Text="What kind of data is it, what are you trying to optimize, how big is it, etc.?" CreationDate="2016-01-21T03:31:12.843" UserId="381" />
  <row Id="9728" PostId="8569" Score="2" Text="I feel like @Kyle answer is true for kernel type, degree and possibly even gamma, but isn't true for C.  C is the regularization parameter which is used to tweak models when high variance becomes an issue.  Another well-known tweak to adjust for high variance is adding more data.  So you should expect that you will be able to turn down C a bit as you add the extra data.  I would suggest following Kyle's recommendation then try a couple more tweaks with backing off on C with closer to the full set." CreationDate="2016-01-21T04:26:35.670" UserId="9420" />
  <row Id="9729" PostId="8569" Score="1" Text="@AN6U5 good correction! I agree." CreationDate="2016-01-21T04:27:55.597" UserId="13413" />
  <row Id="9730" PostId="9856" Score="0" Text="I added updates for the biases. I still don't think I am getting correct results. I can post the whole code if anyone is interested, but all I am doing is loading the mnist dataset and that is correct because I can visualise it. I posted the results, so people can see what I mean. I understand @ffriend point about no sampling for the visible, but this still looks suspicious even the way I do it." CreationDate="2016-01-21T08:08:51.123" UserId="13736" />
  <row Id="9731" PostId="9850" Score="1" Text="I think reduce_sum(y_train*tf.log(y_output)) is used a lot because its a fairly common &quot;simple case&quot; example. It'll run sum each batch's error, which means your error's double the cost (and the magnitude of the gradient) if your batch_sizes double. Making the simple change to reduce_mean will at the very least make debugging and playing with settings more understandable in my opinion." CreationDate="2016-01-21T13:24:45.390" UserId="15599" />
  <row Id="9732" PostId="9856" Score="0" Text="Could you please post the whole code (say, to [pastebin](http://pastebin.com/))? Current code has a lot of missing details like number of hidden units, weight initialization, training rate and so on, and the issue may be in any of them." CreationDate="2016-01-21T14:04:35.907" UserId="1279" />
  <row Id="9735" PostId="9856" Score="0" Text="Actually, if I corrected the code and if I take the average for positive_association and negative_association (by dividing by training_examples) and instead of T.sum I take T.mean for the biases, things work better, though the cost goes down very slowly." CreationDate="2016-01-21T14:35:55.860" UserId="13736" />
  <row Id="9736" PostId="9856" Score="0" Text="Ok, I pasted it at http://pastebin.com/g6AibNTY" CreationDate="2016-01-21T14:42:01.507" UserId="13736" />
  <row Id="9737" PostId="9885" Score="0" Text="Welcome to the site! Did you try `igraph` or D3?" CreationDate="2016-01-21T14:49:47.613" UserId="11097" />
  <row Id="9738" PostId="9885" Score="0" Text="Thanks! I reviewed what igraph can do for me and I didn't find it inline with the data that I have. As far as I understood, igraph visualizes a network based on the data of connected vertices (or similar data about edges). Based on some algorithms and priorities, igraph locates the vertices on the plot compatible to the given data. But I want the vertices to be exactly in their corresponding coordinates that I have in my dataset. I don't want the package choose their layout and location." CreationDate="2016-01-21T14:54:52.507" UserId="15604" />
  <row Id="9739" PostId="9278" Score="1" Text="You should look and see how words that have similar vectors are related to one another. There has been some work done on the size of the context window and type of context that suggests that smaller windows (and perhaps smaller document sizes, like sentences), might make words that are functionally similar (like US states) rather than topically similar (like US states and government-related words) have more similar vectors. I'm mostly citing Omer Levy and Yoav Goldberg's Dependency-Based Word Embeddings from 2014. I could be mistaken though and would love to be corrected if so." CreationDate="2016-01-21T16:33:57.220" UserId="11136" />
  <row Id="9740" PostId="9883" Score="0" Text="Please edit your post to link to the article." CreationDate="2016-01-21T17:41:50.533" UserId="381" />
  <row Id="9741" PostId="9888" Score="0" Text="Thanks for your reply. It's a two part question. The question is of this nature: Part 1: are you happy with the service? Part 2: if you answered no, what can we do to improve our service? And for Part 2 the answer is open-ended. We are interested in sorting the part 2 answers into the 60 categories. Doing so, we would like to match each answer to the top 3 most likely categories. Does that make sense?" CreationDate="2016-01-21T18:29:09.143" UserId="15609" />
  <row Id="9742" PostId="9888" Score="0" Text="That does make sense. Could you provide examples of the categories you'd like to sort them into?" CreationDate="2016-01-21T18:30:28.007" UserId="12575" />
  <row Id="9743" PostId="9888" Score="0" Text="You mean aside from the 3 I mentioned in the original question?" CreationDate="2016-01-21T18:37:44.257" UserId="15609" />
  <row Id="9744" PostId="9888" Score="0" Text="Oops, my bad.&#xA;&#xA;Ok so I think what may be more instructive for you is to perform some NLP algorithms (LDA &amp; K-means strike me as good ideas) to cluster the responses to this question. Then, you can examine the qualities of these clusters of text to come up with some understanding of what similarities they share. You can also take new data points and assign them to clusters, which is similar to classification.&#xA;&#xA;In either case, if you have 60 labels your prospects are very dim in any classification scheme. It is hard to coerce data into classifying well in 2 dimensions, much less 60." CreationDate="2016-01-21T18:48:11.147" UserId="12575" />
  <row Id="9745" PostId="9888" Score="0" Text="The main problem I'm having with this is how you'd accurately turn open-ended text into a finite space of features without falling back on an identity function. For instance, you could just search the response for &quot;customer service&quot; and if you find it, label it as such, but that's not really machine learning." CreationDate="2016-01-21T18:53:19.040" UserId="12575" />
  <row Id="9746" PostId="9888" Score="0" Text="I hear you. Do you think it's best to just search for key words in each response?" CreationDate="2016-01-21T19:05:28.017" UserId="15609" />
  <row Id="9747" PostId="9888" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/34673/discussion-between-econstat-and-derek-janni)." CreationDate="2016-01-21T19:13:29.137" UserId="15609" />
  <row Id="9748" PostId="9892" Score="0" Text="Not full proof  but if it start with a 0 it is typically not *numeric*" CreationDate="2016-01-21T21:34:20.323" UserId="13285" />
  <row Id="9749" PostId="8427" Score="0" Text="If you are using phrases then keyword extraction is maybe not the best title" CreationDate="2016-01-21T21:38:22.983" UserId="13285" />
  <row Id="9750" PostId="9856" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/34680/discussion-between-ffriend-and-user)." CreationDate="2016-01-21T21:56:12.927" UserId="1279" />
  <row Id="9751" PostId="9898" Score="0" Text="I would be interested to see what you are talking about.  Could you include a screen grab or two?  I'm particularly interested in whether k has been appropriately chosen or is perhaps too high. Did you use a silhouette score or the elbow method to select k?  Thanks!" CreationDate="2016-01-22T08:25:44.087" UserId="9420" />
  <row Id="9752" PostId="9556" Score="0" Text="ELKI is AGPL-3. It is not distributed, but has some low-level optimizations for high &quot;local&quot; performance (no Java 8 yet). JSAT, Smile, Hubminer - all of them are open-source, probably mostly apache, not distributed, and not as optimized if I'm not mistaken (I mostly use ELKI)." CreationDate="2016-01-22T08:43:49.783" UserId="924" />
  <row Id="9753" PostId="9893" Score="0" Text="This answer is interesting. Instead of giving another answer, I'd like to suggest a modification to this one.&#xA;&#xA;What about treating this problem as a regular classification problem? You could define a set of features for each column, based on the attributes in this answer (integer, float, normally distributed, relative number of unique values, etc.) After that, it is just a matter of using a classification algorithm." CreationDate="2016-01-22T10:12:51.287" UserId="2576" />
  <row Id="9754" PostId="9892" Score="0" Text="You should probably also ask yourself what is the consequence of getting it wrong? That should determine how much effort you need to put into the process. Do you want 90% accurate, or 99% accurate, or 99.9999% accurate?" CreationDate="2016-01-22T12:13:33.953" UserId="471" />
  <row Id="9755" PostId="9893" Score="0" Text="@PabloSuau, I like that idea. Classification algorithms might be a great method here. Thanks to jncraton for some characteristics to look at." CreationDate="2016-01-22T15:12:43.223" UserId="15617" />
  <row Id="9756" PostId="9906" Score="0" Text="I have deleted the question on Cross Validated. For reference, @NickCox stated: &quot;It's worse than you think, even if you think it's worse than you think. Decimal points could lurk within categorical variables, as part of coded classifications, e.g. of industries or diseases. Small integers could mean counts rather than categories: 3, meaning 3 cars or cats, is not the same as 3, meaning &quot;person owns a car&quot; or &quot;person is owned by a cat&quot;. Measurements could just be integers by convention, e.g. heights of people may just be reported as integer cm or inches, blood pressures as integer mm Hg.&quot;" CreationDate="2016-01-22T15:32:06.170" UserId="15617" />
  <row Id="9757" PostId="9906" Score="0" Text="@NickCox second comment: &quot;The number of distinct (a better term than &quot;unique&quot;, which still has the primary meaning of occurring just once) values is not a good guide either. The number of different heights of people possible in moderate samples is probably much less than the number of different religious affiliations or ethnic origins.&quot;" CreationDate="2016-01-22T15:32:19.433" UserId="15617" />
  <row Id="9758" PostId="9905" Score="0" Text="Thanks for the explanation." CreationDate="2016-01-22T16:25:02.820" UserId="3314" />
  <row Id="9759" PostId="9900" Score="0" Text="Thanks a lot @Emre" CreationDate="2016-01-22T16:25:13.827" UserId="3314" />
  <row Id="9761" PostId="9302" Score="0" Text="See also: http://stats.stackexchange.com/questions/80967/qualitively-what-is-cross-entropy" CreationDate="2016-01-22T19:04:06.880" UserId="289" />
  <row Id="9762" PostId="9892" Score="0" Text="I would take all the heuristics suggested in the answers as features and train a binary classifier." CreationDate="2016-01-22T20:22:43.750" UserId="381" />
  <row Id="9763" PostId="8539" Score="0" Text="This looks like multi armed bandit (https://en.wikipedia.org/wiki/Multi-armed_bandit) problem to me. At different time moments you would like to make a max reward decision given some context, policy. It is implemented in Vowpal Wabbit which scales very well with large datasets." CreationDate="2016-01-22T21:48:43.490" UserId="7848" />
  <row Id="9764" PostId="9877" Score="0" Text="It's basically the 0.0-1.0 similarity between every sample in a data set. The x and y axes have the ID of every sample in the same order so it ends up being a symmetrical matrix. I still feel like there's got to be a better way." CreationDate="2016-01-22T22:38:47.103" UserId="13165" />
  <row Id="9765" PostId="9917" Score="0" Text="I think you've got the idea." CreationDate="2016-01-23T08:57:40.127" UserId="381" />
  <row Id="9766" PostId="9912" Score="0" Text="Thank you @padura. So what is exactly the function optimized by the RBM?" CreationDate="2016-01-23T09:48:09.807" UserId="13736" />
  <row Id="9768" PostId="8950" Score="0" Text="Our mod, @Seanowen is a speaker at WrangleConf :)" CreationDate="2016-01-23T13:31:08.917" UserId="11097" />
  <row Id="9769" PostId="9912" Score="1" Text="@user I edited my answer and changed my recommendation to this lecture: https://www.youtube.com/watch?v=AyzOUbkUf3M" CreationDate="2016-01-23T16:45:43.153" UserId="15656" />
  <row Id="9771" PostId="9858" Score="1" Text="[Hartigan's method is *way* more complicated](http://www.labri.fr/perso/bpinaud/userfiles/downloads/hartigan_1979_kmeans.pdf)" CreationDate="2016-01-23T22:10:23.067" UserId="924" />
  <row Id="9772" PostId="9916" Score="0" Text="I'm slightly confused; if you don't think this is an appropriate metric, why are you using it?" CreationDate="2016-01-24T06:37:42.213" UserId="381" />
  <row Id="9773" PostId="9926" Score="0" Text="Thank you, I know what the parameters are, isn't that the main factor to for choosing an algorithm ?  do you have any ideas for pre-processing stage, how to split this parameters into sub-parameters ?" CreationDate="2016-01-24T08:53:42.847" UserId="15686" />
  <row Id="9774" PostId="9926" Score="0" Text="@huji1991 Glad you liked the answer(you can also upvote it if it helped :)). Generally, the parameters/threshold depends on the problem domain. For example, a particular stretch of internet time might be okay for US kids, and might not go well with Indian kids.  So, the **context is the key**, when dealing with fraud detection problems!" CreationDate="2016-01-24T09:01:33.730" UserId="11097" />
  <row Id="9775" PostId="9926" Score="0" Text="I need reputation to upvote (15points+)" CreationDate="2016-01-24T09:10:08.527" UserId="15686" />
  <row Id="9776" PostId="9926" Score="0" Text="It's okay :) And welcome to the site!" CreationDate="2016-01-24T09:10:56.203" UserId="11097" />
  <row Id="9777" PostId="9917" Score="0" Text="@KylerBrown everything I mentioned, I'm looking for new ideas" CreationDate="2016-01-24T09:39:23.527" UserId="15664" />
  <row Id="9778" PostId="9916" Score="0" Text="@Emre because the challenge organizers defined it to be their evaluation metric" CreationDate="2016-01-24T09:40:34.477" UserId="15664" />
  <row Id="9779" PostId="9928" Score="0" Text="I have added the [tag:reference-request] tag, just in case people would think it's a broad question.  And, welcome to the site :)" CreationDate="2016-01-24T11:53:27.430" UserId="11097" />
  <row Id="9780" PostId="9917" Score="1" Text="@spore234 Neural network models on fixed-classes and with a softmax output layer can inherently do something similar. When you know that the data is one of n given classes, then the network will output a probability distribution for all classes, given the data when you have a softmax output layer." CreationDate="2016-01-24T13:05:38.873" UserId="8820" />
  <row Id="9781" PostId="9930" Score="0" Text="This might be a language problem. If you know the German translation of &quot;residual&quot; in this context, I would be happy about it, too." CreationDate="2016-01-24T16:49:39.203" UserId="8820" />
  <row Id="9784" PostId="8851" Score="0" Text="@Luke, did you manage to solve the problem? See update, I visualized the data (sorry for the delay)." CreationDate="2016-01-24T19:37:36.800" UserId="12469" />
  <row Id="9785" PostId="9917" Score="1" Text="No @moose, [that's a misconception](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html). A distribution over the classes says nothing about the model uncertainty; the distribution has to add up to unity, after all. What if the model is not confident about any of the classes?" CreationDate="2016-01-24T20:48:08.380" UserId="381" />
  <row Id="9786" PostId="9917" Score="0" Text="@Emre Then you should be close to a uniform distribution / the distribution of classes in you training set. I agree it is not the same as confidence, but for some contexts probably close enough." CreationDate="2016-01-24T20:52:48.953" UserId="8820" />
  <row Id="9787" PostId="9917" Score="1" Text="That is incorrect. Softmax will give you the confidence of one class relative to another, but not their absolute confidences, so they could all be uncertain and you would not know." CreationDate="2016-01-24T21:16:08.507" UserId="381" />
  <row Id="9788" PostId="5393" Score="0" Text="I tried many times running mahout ssvd, but it all failed.&#xA;Someone can provide me the step by step instruction how to run a successful ssvd? thanks a lot" CreationDate="2016-01-24T18:26:35.710" UserId="15699" />
  <row Id="9789" PostId="1073" Score="0" Text="Not online learning, but take a look at this post, this may help you to get a start http://francescopochetti.com/stock-market-prediction-part-introduction/" CreationDate="2014-10-09T10:22:47.857" UserId="3596" />
  <row Id="9790" PostId="9917" Score="1" Text="@spore234 You could also use a [Gaussian Process classifier](http://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html)" CreationDate="2016-01-24T23:14:39.330" UserId="8820" />
  <row Id="9791" PostId="9922" Score="0" Text="Thanks Emre. I'll try these options." CreationDate="2016-01-25T05:57:13.880" UserId="13518" />
  <row Id="9794" PostId="9935" Score="0" Text="I did a CV gridsearch for the hyperparameters of a random forest and my results are very unstable. Correlation ranges between 0.15 and 0.4, while RMSE is between 0.45 and 0.41 for the hyperparameters. When I predict on a hold-out set the correlation is far off the CV values. Hyperparameters that perform well in gridsearch perform badly on the holdout set and vice versa." CreationDate="2016-01-25T07:44:18.103" UserId="15664" />
  <row Id="9798" PostId="9828" Score="1" Text="I'd add that [stratification](https://en.wikipedia.org/wiki/Stratified_sampling) could be a viable tool to deal with smaller sets once acknowledge that the distance is above your threshold." CreationDate="2016-01-25T13:59:27.357" UserId="1244" />
  <row Id="9799" PostId="9902" Score="0" Text="Thanks for the reply @K3--rnc. What package in R do you recommend for the classification? Do give you an idea of how much I know about these things, I know and understand all the concepts in your post and I've done some sentiment analysis before. Thank you" CreationDate="2016-01-25T14:06:32.760" UserId="15609" />
  <row Id="9800" PostId="9935" Score="0" Text="Indeed, please see the updated answer." CreationDate="2016-01-25T17:40:20.333" UserId="15527" />
  <row Id="9802" PostId="9950" Score="1" Text="I wouldn't call it cheating per se...unless you were specifically designed to use a non-dictionary look-up solution. I also wouldn't call it traditional NLP, though you should never overlook the substantial mileage you can get out of a simple solution to a problem." CreationDate="2016-01-25T18:49:36.020" UserId="13413" />
  <row Id="9803" PostId="9883" Score="0" Text="Are we looking at [the same paper](http://cnel.ufl.edu/files/1429900003.pdf)? Eq. (1) uses a logistic, not a log nonlinearity, and there is no **convolution** after the $w$ matrix. And just to check, you are working on EEG classification, right?" CreationDate="2016-01-25T19:06:53.960" UserId="381" />
  <row Id="9804" PostId="9950" Score="1" Text="You could perhaps use it to train a named-entity recognizer. What's your gazetteer going to do about entities not in its corpus?" CreationDate="2016-01-25T19:19:42.710" UserId="381" />
  <row Id="9805" PostId="9950" Score="0" Text="i was hoping that by correctly using pos tags and noun chunking i wont ever need a gazetteer. is that possible?" CreationDate="2016-01-25T20:18:07.270" UserId="15735" />
  <row Id="9806" PostId="9955" Score="1" Text="You may do cleaning/substitution with grep-like tools in R, Python, or GNU Linux. No need in ML/AI tools." CreationDate="2016-01-25T21:23:42.197" UserId="13406" />
  <row Id="9807" PostId="9955" Score="0" Text="@SergeyBushmanov thanks, I will try that. but  I also want to understand the algorithms that are used for this purpose." CreationDate="2016-01-25T21:32:08.597" UserId="15739" />
  <row Id="9808" PostId="9956" Score="0" Text="Can you flesh it out a little bit more; I'm trying to connect the polynomial business with the prime factorization? Forgetting about SVMs for the moment, what kind of information do you need to better predict which numbers to look at and why?" CreationDate="2016-01-25T22:18:57.900" UserId="381" />
  <row Id="9809" PostId="9954" Score="0" Text="If you are asking how to use the library you linked to, [here is an example](https://networkx.github.io/documentation/latest/examples/drawing/atlas.html)." CreationDate="2016-01-25T22:42:31.667" UserId="381" />
  <row Id="9810" PostId="9957" Score="0" Text="thanks, what other topics should I look into other than stop word removal?" CreationDate="2016-01-25T22:51:50.233" UserId="15739" />
  <row Id="9811" PostId="9957" Score="2" Text="Look up &quot;preprocessing&quot;. Tokenization, stemming, lemmatization." CreationDate="2016-01-25T22:55:31.150" UserId="381" />
  <row Id="9812" PostId="9954" Score="0" Text="@Emre Unless ready parser, each multiplication can be described as an edge and each summation as thinner edge. Same variables could have the same colouring. There are many ways to describe a group of polynomials, it is surprising if none has researched this visualisation issue earlier. Added simple example [here](http://chat.stackexchange.com/transcript/message/27096175#27096175) about polynomials from Mathematica -- Networkx Sage may be the best option for basis polynomials and then graph infographics." CreationDate="2016-01-25T23:17:35.420" UserId="15738" />
  <row Id="9813" PostId="9958" Score="0" Text="You could certainly use a relational database, but what is the goal; what do you want to know?" CreationDate="2016-01-26T01:28:52.477" UserId="381" />
  <row Id="9814" PostId="9958" Score="0" Text="There's no real goal other than detect patterns. For instance, which character combos produce the best episodes? Ultimately I'd have another table with episode data like imdb rating, episode writers, topic tags, etc. Also stuff contained within this dataset alone like what is each characters &quot;affinity&quot; to each other character calculated as a summation of each time they're paired, whether or not they won, etc&#xA;&#xA;This is ultimately for fun and practice. Also any ideas on the best way to visually represent this data? Something in Tableau perhaps?" CreationDate="2016-01-26T01:41:00.107" UserId="13165" />
  <row Id="9815" PostId="9940" Score="0" Text="Thanks for your answer. In Conclusion, My work is to balance churn rate and churn period by confidence level" CreationDate="2016-01-26T01:55:25.437" UserId="15715" />
  <row Id="9816" PostId="9953" Score="0" Text="Thanks for your answer" CreationDate="2016-01-26T01:58:19.603" UserId="15715" />
  <row Id="9817" PostId="9964" Score="1" Text="Right this way: http://quant.stackexchange.com/" CreationDate="2016-01-26T03:03:46.763" UserId="381" />
  <row Id="9818" PostId="9964" Score="0" Text="@Emre I think it would get closed there too, as it's very broad and opinion-based. So, they'd reject the migration most probably :)&#xA;&#xA;It would be a fit, if the question is improved and narrowed down!" CreationDate="2016-01-26T05:37:52.530" UserId="11097" />
  <row Id="9819" PostId="9964" Score="1" Text="I suggest asking for a survey paper outlining the latest research challenges. That's more concrete." CreationDate="2016-01-26T05:40:23.940" UserId="381" />
  <row Id="9820" PostId="9953" Score="1" Text="Great way to look at it." CreationDate="2016-01-26T06:23:00.510" UserId="13727" />
  <row Id="9821" PostId="9940" Score="0" Text="Yes. Do note that in such cases using &quot;common&quot; periods is of benefit. If your analysis show that 99% of the users inactive for 13 days never return, round the period to two weeks. Otherwise, anytime you'll present churn analysis you'll have to spend some time explaining why you choose 13 days." CreationDate="2016-01-26T06:25:42.737" UserId="13727" />
  <row Id="9822" PostId="9962" Score="0" Text="Why not define a probability distribution and sample from it using [scipy.stats](http://docs.scipy.org/doc/scipy/reference/stats.html)?" CreationDate="2016-01-26T08:59:37.410" UserId="381" />
  <row Id="9825" PostId="9883" Score="0" Text="Yep! BUT authors of this paper wrote one  thing in paper, and completely different in python implementation. Eq.(1) in their implementation become something  like&#xA;$Yprob = softmax(ln(W*(X*V)^{2})*U^{T}+B)$" CreationDate="2016-01-26T09:54:11.403" UserId="15595" />
  <row Id="9826" PostId="9919" Score="0" Text="OK, so during both training and testing we have a label for each input, and we obtain an error for every input, is that right? If, in your example, input 1 wil be (1,4), it has a label (e.g. 1), then input 2 (3,3) with label 0 and so on. At the end of the sequence we somehow average out the errors to get the argmax class of the input, is this right?" CreationDate="2016-01-26T10:57:10.927" UserId="1426" />
  <row Id="9827" PostId="9967" Score="0" Text="thanks man! that clears up a lot of things." CreationDate="2016-01-26T12:53:10.603" UserId="15735" />
  <row Id="9828" PostId="9962" Score="0" Text="Are you looking for [`sklearn.datasets.make_moons`](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html)?" CreationDate="2016-01-26T14:07:01.693" UserId="8820" />
  <row Id="9829" PostId="9955" Score="0" Text="Why do you need to clean the table? (By the way: Your table gets the cleanest with `TRUNCATE TABLE` ;-))" CreationDate="2016-01-26T14:15:14.710" UserId="8820" />
  <row Id="9830" PostId="9970" Score="0" Text="Are you using the activations from the 784 units as features? It seems from looking at various sources this is the case (but as will most things, there are different versions). I was also looking at the autoencoder for Keras and there they use the last hidden layer activations http://keras.io/layers/core/#autoencoder" CreationDate="2016-01-26T14:23:53.247" UserId="1138" />
  <row Id="9833" PostId="9935" Score="0" Text="The problem is that regression minimizes a cost function (presumably using least squares in your case) and the cost function is **not** based on the correlation, but also includes adjustments to the y intercept. Your grid search is effectively interfering with your cost function minimization.  If you are coding this yourself, I suggest choosing the Pearson correlation as the cost function within the algorithm.  Thus the y intercept (w0 or bias weight) won't be fit and will reduce the bias of the problem." CreationDate="2016-01-26T14:40:21.640" UserId="9420" />
  <row Id="9834" PostId="9970" Score="0" Text="I am not sure what your question is. Are you asking if the pre-activation of the last layer or post-activation values are taken? Then the answer is that it probably doesn't matter, as long as you do it consistantly. - And yes, you take those values as features. Just like you take the values of a first hidden layer as features for a second hidden layer. It is the same idea." CreationDate="2016-01-26T15:31:38.547" UserId="8820" />
  <row Id="9835" PostId="9970" Score="0" Text="Your answer mentioned using the &quot;weights&quot; and I was looking for clarification if the nodes (activation) or the weights were used typically as features. The other part was if the last hidden layer (e.g. Keras) was used typically or the middle hidden layer as you mentioned. Thanks!" CreationDate="2016-01-26T16:43:48.187" UserId="1138" />
  <row Id="9836" PostId="9970" Score="0" Text="@B_Miner You use the activation, not the weights themselves, as features. The idea why AEs work is that they learn what is important in images. They learn a more compact representation of an image. The learning is done by gradually adjusting the weights. - In case you are looking for an introduction to neural networks, I can recommend [neuralnetworksanddeeplearning.com](http://neuralnetworksanddeeplearning.com/chap1.html)" CreationDate="2016-01-26T16:55:08.600" UserId="8820" />
  <row Id="9837" PostId="9952" Score="0" Text="Thanks for a great answer , in fact i have more attributes and i am aslo including both behaviour and demographic attributes. So according to your answer i should  do principal component analysis then a non-supervised classification. &#xA;Do you recommend any particular tools ? &#xA;Thanks again" CreationDate="2016-01-26T17:20:37.147" UserId="15695" />
  <row Id="9838" PostId="9952" Score="0" Text="Sure, If you already know R, then R for the data management, profiling, first analysis and the h2o package/software for your pca (the R implementation of PCA are a little bit slow and may not handle millions of rows)." CreationDate="2016-01-26T17:26:18.087" UserId="15543" />
  <row Id="9839" PostId="9883" Score="0" Text="Ask the authors." CreationDate="2016-01-26T17:37:15.293" UserId="381" />
  <row Id="9840" PostId="9972" Score="1" Text="http://stackoverflow.com/questions/3330227/free-tagged-corpus-for-named-entity-recognition" CreationDate="2016-01-26T18:14:51.477" UserId="381" />
  <row Id="9841" PostId="9972" Score="0" Text="thanks Emre, but i want to build my own rules" CreationDate="2016-01-26T18:31:05.920" UserId="15735" />
  <row Id="9842" PostId="9969" Score="0" Text="It seems as if you have cited the first part from &quot;An Introduction to Statistical Learning: with Applications&quot;. Please mark it as a citation (prefix it with `&gt;`) and add the reference." CreationDate="2016-01-26T18:33:05.520" UserId="8820" />
  <row Id="9843" PostId="9972" Score="2" Text="That's what tagged corpuses are for. How are you going to know if your algorithm works without something to train on and test against?" CreationDate="2016-01-26T18:34:24.040" UserId="381" />
  <row Id="9844" PostId="5696" Score="0" Text="&quot;In comparison, with SVM, every single record in the data base will require a computation of the distance function&quot; - What do you mean with &quot;distance function&quot;? The kernel?" CreationDate="2016-01-26T18:40:54.327" UserId="8820" />
  <row Id="9845" PostId="9919" Score="0" Text="Not quite, it's simpler than that. In test time these predictions are completely irrelevant for us- we just sample it after forwarding the full sequence. During training we are forwarding whole sequence and only at the end we calculate loss function between the final prediction and the label for the whole sequence. And we backpropagate the error through time but then we are not samling predictions." CreationDate="2016-01-26T19:01:43.503" UserId="15656" />
  <row Id="9846" PostId="9919" Score="0" Text="Of course this applies only to &quot;Many to one&quot; model (I'm refering to karpathy). You said &quot;..Label '1' applies to the full sequence, not just a single input.&quot; so that's your answer. You literally cannot compute error after each datapoint becouse your labels describe whole sequences. But BPTT propagates that final error backwards in time so that the error is &quot;evenly distributed&quot; between your consequtives datapoints." CreationDate="2016-01-26T19:36:44.610" UserId="15656" />
  <row Id="9847" PostId="9972" Score="0" Text="oh, so you mean that look at the verbs around `ORG` tags for example. Now there can be a problem as i am now relying on the fact that the corpus has been tagged perfectly. What if i want to create `ORG` tags from scratch? POS tags can be my starting point. The reason i want to do this is that sometime StanfordNLP tools do not tag entities as expected" CreationDate="2016-01-26T20:01:27.993" UserId="15735" />
  <row Id="9848" PostId="9919" Score="0" Text="'only at the end we calculate loss function between the final prediction and the label for the whole sequence' but how? Look, we've fed forward a data point, ok. How do we get an error out of it? Do we have the same label for all data points?  For example, in gesture classification it will be '1' for 'snap', so every data point will be labelled '1'. Do I miss something?" CreationDate="2016-01-26T21:27:11.437" UserId="1426" />
  <row Id="9849" PostId="9919" Score="0" Text="If we backprop the error signal after every data point we are connecting these single data points with a label. Suppose that we have a dataset of phrases and we want to assing some class to them. Our input vector will encode one single character. Does 'c' or 'd' belong to a specified class? No. But RNN allows us to model very complex and even longterm dependencies between our letters so we can assign labels to sequences. We can calculate the error only once because RNN have the memory cells which remember the history of a sequence. How do we calculate that? I will update my answer in a moment." CreationDate="2016-01-26T22:09:34.660" UserId="15656" />
  <row Id="9850" PostId="9976" Score="0" Text="I could not relate these sentences: &quot;I would like to cluster groups of channels together, so that it is easier to deal with similar channels in similar ways.&quot; and &quot;However, if I aggregate the user data into averages for each channel...&quot;" CreationDate="2016-01-26T22:22:04.940" UserId="381" />
  <row Id="9851" PostId="9976" Score="0" Text="So I have a ton of individual user data, and each user comes from a certain channel. I want to perform the clustering on the channel that they came from. To do this, I've been just aggregating the data from the users to come up with summary statistics for each channel. However, I would like to still use the individual user data but perform the clustering on the channel." CreationDate="2016-01-26T22:32:12.927" UserId="15768" />
  <row Id="9852" PostId="9960" Score="0" Text="Social network analysis does make sense. Without even considering winners/losers, I would tally each time there's an interaction and set those values equal to their corresponding connection in the network, right? Could I give them different weights based on whether or not it was a pair or a large group? Also because there's only 5 nodes and a hundreds of repeated interactions, how do I represent the change in time? Does that even matter?" CreationDate="2016-01-26T22:49:37.683" UserId="13165" />
  <row Id="9853" PostId="9960" Score="0" Text="I don't quite understand your suggestion for a fancy graph database. Are the users the 5 characters? If so would I need to make columns for every possible combination of 5 characters?" CreationDate="2016-01-26T22:51:47.737" UserId="13165" />
  <row Id="9854" PostId="9960" Score="0" Text="Yes, the users/characters are the nodes of the graph/network. I assumed interactions were in pairs, but if one person can compete with several you'll have to create an entry for every pair. Weights could be used for quantifying the magnitude of the win/loss, if applicable. If you want to analyze the evolution of graph, you can tag each edge with the episode number, then use that as an input for temporal graph analysis. Here are [some](https://www.youtube.com/watch?v=ehlFqkyre3k) [leads](http://research.microsoft.com/en-us/projects/graph/) in case you are interested in temporal analysis." CreationDate="2016-01-26T23:17:15.567" UserId="381" />
  <row Id="9857" PostId="5696" Score="0" Text="@moose yes, the distance function is basically the kernel applied on two instances, which usually is linear in number of features" CreationDate="2016-01-27T06:43:12.330" UserId="108" />
  <row Id="9858" PostId="5696" Score="0" Text="Additionally to the answer, we can mention the time for prediction which is given by the number of instances predicted x number of support vectors x kernel evaluation (which is usually the number of features). Prediction time is important because if for training one can employ some caching, for prediction the caching is useless" CreationDate="2016-01-27T06:45:23.600" UserId="108" />
  <row Id="9859" PostId="9420" Score="1" Text="I'm voting to close this question because it is a cross-posting: http://stats.stackexchange.com/q/187186/25741" CreationDate="2016-01-27T08:10:47.430" UserId="8820" />
  <row Id="9860" PostId="9954" Score="0" Text="On reflection I think this is a very interesting question but you should direct it to mathematicians, not data scientists: what quantities of interest do polyomials have? I suppose they will say it depends on what are you are trying to understand about it.&#xA;&#xA;Sure, we can help you draw this graph, but does it really afford any insight?" CreationDate="2016-01-27T08:50:11.917" UserId="381" />
  <row Id="9861" PostId="9980" Score="1" Text="Opposite of churn is customer retention AFAIK :)" CreationDate="2016-01-27T10:12:37.403" UserId="11097" />
  <row Id="9862" PostId="9919" Score="0" Text="I'm still not 100% clear. Could you have a look at the edit in the question I wrote?" CreationDate="2016-01-27T10:52:33.657" UserId="1426" />
  <row Id="9863" PostId="9980" Score="0" Text="In some contexts I've also seen using &quot;recurrent clients&quot; and &quot;active users&quot;" CreationDate="2016-01-27T12:49:45.707" UserId="1244" />
  <row Id="9864" PostId="9713" Score="0" Text="It's not clear what your question actually is.  Are you trying to evaluate models during training (i.e. cross-validation) or are you trying to evaluate a single model?" CreationDate="2016-01-27T13:46:48.177" UserId="6385" />
  <row Id="9865" PostId="9979" Score="0" Text="Thanks for sharing your idea. I was able to accomplish my task by using Row Sampling and I sampled the entire 77k into 10k data and was able to plot it. I am testing my data with several BI tools and will next try Orange Data mining." CreationDate="2016-01-27T15:32:34.617" UserId="13034" />
  <row Id="9866" PostId="9952" Score="0" Text="Thanks again for your valuable help , I have implemented PCA and reduced my multidimensional data to lower key dimensions. But now how I can i proceed to find out the pattern that i have to follow to define this group of users , The result i want to have is a table of attributes with a number range that defines the group pattern." CreationDate="2016-01-27T15:37:30.063" UserId="15695" />
  <row Id="9867" PostId="9991" Score="1" Text="A really nice question :)  Updated it with the appropriate tag.  Welcome to the site!" CreationDate="2016-01-27T16:02:48.667" UserId="11097" />
  <row Id="9868" PostId="9698" Score="0" Text="I have tried to use your solution putting the grep function instead of the apply function (properly) but it doesn't work. I've also tried to order the patternST such to substitute before the longer statements. It still doesn't work." CreationDate="2016-01-27T16:03:59.047" UserId="10024" />
  <row Id="9869" PostId="9990" Score="0" Text="Welcome to the site :)" CreationDate="2016-01-27T16:34:51.417" UserId="11097" />
  <row Id="9870" PostId="9993" Score="0" Text="Great answer @cdeterman! many things seem to be more clear now. last thing... according to you, how can I backtest the model ability to forecast the output basing on the input value. Let me explain better; in the case I use a simple logistic model, I can use the $\beta$ vector and the independent variables to compute a forecast of the dependent variable. In the neural network model, how can I do this?" CreationDate="2016-01-27T18:36:09.697" UserId="9225" />
  <row Id="9871" PostId="9993" Score="0" Text="@Quantopic I think you are referring to the `compute` function in the `neuralnet` package.  The source isn't terribly complex if you wish to do it by hand.  Essentially you are applying the weights and activation function at each layer to the final result." CreationDate="2016-01-27T18:48:39.753" UserId="6385" />
  <row Id="9872" PostId="9950" Score="1" Text="In industry there's no such thing as cheating. :-)  But anyway, if you want a &quot;general approach&quot;, you need to start with some hand-annotated data, ie seed, to learn contexts in which NEs appear, to learn more NEs." CreationDate="2016-01-27T18:52:41.297" UserId="12363" />
  <row Id="9873" PostId="6446" Score="0" Text="@Arun, confusionMatrix is related to data science but your problem is regarding troubleshooting code.  As such it is best to be moved to stackOverflow." CreationDate="2016-01-27T18:53:59.583" UserId="6385" />
  <row Id="9874" PostId="9950" Score="0" Text="correct :) thanks" CreationDate="2016-01-27T20:17:59.383" UserId="15735" />
  <row Id="9875" PostId="9990" Score="0" Text="Thank you! Any suggestions related to the topic? :)" CreationDate="2016-01-27T20:38:58.793" UserId="15690" />
  <row Id="9877" PostId="9475" Score="0" Text="Thank you! I have read up on Autoencoders and it is indeed what I was looking for." CreationDate="2016-01-28T08:33:27.927" UserId="13392" />
  <row Id="9878" PostId="675" Score="0" Text="@ffriend, How do we get that keyword list ?" CreationDate="2016-01-28T09:53:32.480" UserId="15817" />
  <row Id="9879" PostId="675" Score="0" Text="@NG_21 the easiest way is just to collect them manually. If you have too much jobs to handle it yourself, you can use crowdsourcing platform like Amazon MTurk to do it quickly and accurately. This is not very technical solution, but will cost much less than implementing algorithm for keyword extraction." CreationDate="2016-01-28T10:24:39.217" UserId="1279" />
  <row Id="9880" PostId="10003" Score="0" Text="Thanks for your answer. Indeed, the historical state would be a great improvement in my model, that's a very good idea. Unfortunately, I don't have time dependent information about the companies. Also, I don't have leakers in my model...The &quot;perfect model&quot; was just an example of how inconsistent this approach seems to be. @sds, do you know any reference (books, papers) that used this approach?" CreationDate="2016-01-28T10:56:10.800" UserId="15215" />
  <row Id="9881" PostId="3793" Score="0" Text="Thumbs up for nice tool you introduced but there is actually a fundamental difference between the question and your answer. The question is about the network of roads (a graph) in which the ditances are not Euclidean but Geodesic and whole the concept is totally different than geometric calculations. If your answer is right then the question is wrong for including the term Graph. Then I'll edit the question. Thanks for answer again!" CreationDate="2016-01-28T12:50:00.480" UserId="8878" />
  <row Id="9883" PostId="9981" Score="0" Text="Can you shed more light on, what is &quot;no. of properties&quot; ? I believe this is a regresion problem with numerical and categorical variables." CreationDate="2016-01-27T17:07:31.407" UserId="15790" />
  <row Id="9884" PostId="5255" Score="0" Text="I would further add [Quant](https://www.mathworks.com/company/events/conferences/matlab-computational-finance-conference-nyc/2015/proceedings/is-data-science-new-quant.pdf) in this list!" CreationDate="2016-01-27T19:01:28.743" UserId="15215" />
  <row Id="9885" PostId="6446" Score="0" Text="This is a tough call. It could well be posted on SO. This site is also about data science in practice, including tools. While SO is a somewhat better site I'd stop short of calling it off-topic here. The answer here is related to what the operation means and why the constraint in question here is needed and why it may have been violated" CreationDate="2016-01-28T13:58:27.880" UserId="21" />
  <row Id="9886" PostId="9999" Score="0" Text="I'm not in this business but I imagine they would want error bars on their estimates to quantify the risk, so they would gravitate towards Bayesian methods. The best place to ask as @wacax says is Quant.SE." CreationDate="2016-01-28T18:30:46.277" UserId="381" />
  <row Id="9887" PostId="9993" Score="0" Text="Thanks for the further explanation @cdeterman! :)" CreationDate="2016-01-28T21:23:21.110" UserId="9225" />
  <row Id="9888" PostId="10011" Score="0" Text="[Confidence intervals for cross-validated statistics](http://stats.stackexchange.com/questions/69831/confidence-intervals-for-cross-validated-statistics)" CreationDate="2016-01-28T23:14:53.180" UserId="381" />
  <row Id="9889" PostId="9524" Score="1" Text="If you have two points in a Manhattan space (0, 0) and (1, 1), what point would you want to use as the centre? The two points of minimum distance from them are (0, 1) and (1, 0). If you want a unique point, maybe you should decide a scheme for preferring a particular point over another.&#xA;On the other hand, since Manhattan space is a subset of Euclidean space, maybe you can use the Euclidean centre (0.5, 0.5)." CreationDate="2016-01-29T02:30:18.310" UserId="15002" />
  <row Id="9890" PostId="9981" Score="0" Text="@ML_Passion &quot;No. of properties&quot; here is the total subtasks, resources have to work on. And yes, earlier we were trying with Regression algorithm when the entities were fixed. Now there can be multiple sets of the entity values which can vary." CreationDate="2016-01-29T02:53:54.280" UserId="8412" />
  <row Id="9891" PostId="10007" Score="0" Text="It may be a country-dependent thing, but in North America, the creators of databases are called &quot;business analysts&quot;. Visualization and reporting is most certainly NEVER done by MBAs or &quot;business analysts&quot; in how you use the term." CreationDate="2016-01-29T04:54:52.323" UserId="6529" />
  <row Id="9892" PostId="10015" Score="1" Text="There are several mistakes stemming from your application of the binary logistic regression model to the [multinomial case](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) (remember that MNIST has ten classes). Please follow the [tensorflow MNIST guide for beginners](https://www.tensorflow.org/versions/master/tutorials/mnist/beginners/index.html)." CreationDate="2016-01-29T05:06:08.043" UserId="381" />
  <row Id="9893" PostId="10007" Score="0" Text="@rocinante Yeah, must be country dependant :)" CreationDate="2016-01-29T05:31:55.083" UserId="11097" />
  <row Id="9894" PostId="10015" Score="1" Text="@Emre, it would be very appreciate if you can point out where I got wrong. Thanks~" CreationDate="2016-01-29T06:17:47.777" UserId="3167" />
  <row Id="9895" PostId="10015" Score="0" Text="Your loss function and its arguments are both wrong. I don't think it would work as is even if it were binary; the argument of the loss function is not normalized. Please follow the links." CreationDate="2016-01-29T06:28:22.597" UserId="381" />
  <row Id="9898" PostId="10012" Score="0" Text="Can you please explain more about what you mean by tree depth for random forest? It's not a problem for me to run with 50K &quot;number of trees&quot; - but the error doesn't change. My main question is are there any ML algorithms that have lower errors than that?" CreationDate="2016-01-29T18:37:51.457" UserId="15797" />
  <row Id="9899" PostId="10012" Score="1" Text="tree depth (max.depth) and interaction depth only apply to xgboost and GBM respectively, both tree based methods and both more accurate than random forests. And as for better algorithms, from a purely classification accuracy (as well as other metrics) point of view the ones included in the answer almost always outperform Random Forests. IF the error doesn't change even after switching to a better algorithm then you should most likely do some feature engineering which unfortunately is mostly kept secret in the quant world." CreationDate="2016-01-29T19:33:20.380" UserId="13023" />
  <row Id="9900" PostId="10025" Score="0" Text="The simplest way is to use a fixed-length FFT instead of an STFT (spectrogram). That will eliminate your variable-length problem. Why don't you just apply a recurrent neural network? Do you just need a worked example? If so, are you flexible about which software to use?" CreationDate="2016-01-29T20:29:19.070" UserId="381" />
  <row Id="9901" PostId="10025" Score="0" Text="I think I would lose a lot of information with a fixed-length FFT. If I would do that I think I would have to do a segmentation first, where I look for 'interesting' parts. I might do that or go to the recurrent neural networks (an example is nice but not super necessary, I wanted to use Lasagne). The reason I tried to avoid it is that the output of a recurrent neural network is more difficult to deal with (at each time step but I only have the labels for the whole file). So I wanted to start with the simplest model and gradually make it more complex." CreationDate="2016-01-30T00:54:01.350" UserId="15847" />
  <row Id="9902" PostId="9999" Score="0" Text="What do you mean by 'copy some of the top traders'? It isn' t very clear what you are trying to predict with what features." CreationDate="2016-01-30T01:41:54.320" UserId="3470" />
  <row Id="9903" PostId="10030" Score="1" Text="I'm voting to close this question as off-topic because it shows a lack of research!" CreationDate="2016-01-30T12:07:38.673" UserId="11097" />
  <row Id="9904" PostId="10030" Score="2" Text="I'm not sure &quot;lack of research&quot; needs to be as much a thing here as on Stack Overflow (maybe a good discussion topic for Data Science meta?). However, I also think there is a problem. The question is too broad, someone trying to answer it would need to write a relatively long review of text processing in general. The question also does not give enough information about the data (&quot;textual data&quot; could many things). Please add more description covering the starting data - e.g. is there labeled data to use supervised learning with, does the text appear in any context, or is it purely a word list?" CreationDate="2016-01-30T16:37:03.540" UserId="836" />
  <row Id="9905" PostId="10032" Score="1" Text="Go playing programs had been quietly progressing into new territory with machine learning techniques for a few years. The Google team have pushed it further, but the improvement is not IMO as radical as it seems (many people will be comparing it with the &quot;common knowledge&quot; from 10 years ago that Go was too hard for computers). For example, some Go playing programs did beat Google's player in testing.  Also, look at progress on the wikipedia page: https://en.wikipedia.org/wiki/Computer_Go#2000s . . ." CreationDate="2016-01-30T20:47:36.867" UserId="836" />
  <row Id="9906" PostId="9151" Score="0" Text="I think you nailed it: it's not a classification problem but a case of anomaly detection." CreationDate="2016-01-31T09:36:46.330" UserId="15878" />
  <row Id="9907" PostId="10037" Score="0" Text="Why do you want to throw away information? Can't you cluster users as is; e.g., by modeling each user as a vector of points per category?" CreationDate="2016-01-31T10:31:37.573" UserId="381" />
  <row Id="9908" PostId="10037" Score="0" Text="I have 3 others tables with customer and I have to join them together.&#xA;If I have for the table 1 5 times customer 1500 and for the table 2 26 times the customer 1400 I can&quot;t join my 2 tables ?" CreationDate="2016-01-31T17:58:24.407" UserId="15879" />
  <row Id="9909" PostId="10037" Score="0" Text="From a machine learning perspective, that is no problem. Is it feasible to do this join even once?" CreationDate="2016-01-31T19:31:26.077" UserId="381" />
  <row Id="9910" PostId="10037" Score="0" Text="Yes only once. But maybe I should do : 1 Column per category for example.  I would know if a customer use more points in category 1 than 5. Then when i will do the clustering process an customers who spend a lot of points in &quot;Gift card&quot; will be together ?This is why I should use pivot table ?" CreationDate="2016-01-31T20:47:36.440" UserId="15879" />
  <row Id="9912" PostId="9138" Score="0" Text="Please can @YuwenYan clarify whether the provided solutions have helped by rating them or marking the problem as solved?" CreationDate="2016-01-31T21:07:07.727" UserId="381" />
  <row Id="9913" PostId="10042" Score="0" Text="thanks a lot Emre, will go ahead and try. Any particular library you recommend? thanks again." CreationDate="2016-01-31T23:02:25.627" UserId="9197" />
  <row Id="9914" PostId="9138" Score="0" Text="@Emre no, I'm still looking for the solution..." CreationDate="2016-02-01T00:34:14.550" UserId="14445" />
  <row Id="9915" PostId="10035" Score="0" Text="Thanks a ton! I really appreciate all of the thought and time put into your response. This is incredibly helpful!" CreationDate="2016-02-01T02:45:50.237" UserId="15768" />
  <row Id="9916" PostId="10042" Score="0" Text="http://keras.io/layers/core/#autoencoder" CreationDate="2016-02-01T04:05:25.323" UserId="381" />
  <row Id="9918" PostId="9138" Score="0" Text="Nobody seems to clearly understand what you want, so please can you rephrase the question &quot;how to distinguish abcd with abcdef and abc, since abcdef just appear once, and abc is the sub-string of 'abcd'&quot;, and complete the output for your given input list?" CreationDate="2016-02-01T04:17:49.600" UserId="381" />
  <row Id="9919" PostId="10042" Score="0" Text="awesome! I have already used keras, will try out the autoencoder" CreationDate="2016-02-01T10:36:22.600" UserId="9197" />
  <row Id="9920" PostId="10038" Score="0" Text="You might be interested in this question http://datascience.stackexchange.com/questions/9950/nlp-is-gazetteer-a-cheat/" CreationDate="2016-02-01T12:49:36.510" UserId="13727" />
  <row Id="9921" PostId="6634" Score="0" Text="I'm voting to close this question as off-topic because this is a statistics question. Try stats.stackexchange.com for help in formulating a statistical model for your data." CreationDate="2016-02-01T15:32:28.947" UserId="471" />
  <row Id="9922" PostId="10048" Score="1" Text="Do you mean &quot;model&quot;, or just referring to choice of last layer's `activation='softmax'` and compile choice of `loss='categorical_crossentropy'`? IMO, your choices for those are good for a model to predict multiple mutually-exclusive classes. If you want advice on the whole model, that is quite different, and you should explain more about what your concerns are, otherwise there is too much to explain in a single answer." CreationDate="2016-02-01T16:09:41.737" UserId="836" />
  <row Id="9924" PostId="10048" Score="0" Text="I mean about `architecture` of layers mostly. Any advise for my question #2?" CreationDate="2016-02-01T16:29:10.250" UserId="14684" />
  <row Id="9925" PostId="10048" Score="1" Text="There is rarely a &quot;right&quot; way to construct the architecture, that should be something you test with different meta-params, and should be results-driven (including any limits you might have on resource use for training time/memory use etc). For #2, you can either just have two outputs with softmax similar to now, or you can have output layer with one output, `activation='sigmoid'` and `loss='binary_crossentropy'`" CreationDate="2016-02-01T16:33:59.260" UserId="836" />
  <row Id="9926" PostId="10048" Score="0" Text="`activation='sigmoid'` in the *output* layer. The hidden layer can stay as `'relu'` if you like (although I would probably start with `'tanh'` for this problem, that is personal preference with very little support from theory)" CreationDate="2016-02-01T16:38:24.197" UserId="836" />
  <row Id="9927" PostId="10051" Score="0" Text="Now I see the English subtitle is incorrect here, Chinese one is correct." CreationDate="2016-02-01T17:22:44.593" UserId="15908" />
  <row Id="9929" PostId="10047" Score="0" Text="For the second question, Andrew says: &quot;If d equals 1, we're going to have a relatively high training error&quot;. But please, don't change your question, if you have an other question you should ask a new one, see: [how to ask](http://datascience.stackexchange.com/help/how-to-ask)." CreationDate="2016-02-01T18:38:27.887" UserId="13123" />
  <row Id="9930" PostId="10050" Score="0" Text="That first L column shouldn't be there.  Your input goes directly to the first hidden layer." CreationDate="2016-02-01T19:45:43.693" UserId="14913" />
  <row Id="9931" PostId="10050" Score="0" Text="Editted it, is that what the nntools network represents?" CreationDate="2016-02-01T19:51:12.940" UserId="14617" />
  <row Id="9932" PostId="9999" Score="0" Text="I'm trying to train an algorithm using the &quot;buy and sell decisions&quot; of real people so that I can then predict when I should buy or sell in the future." CreationDate="2016-02-01T20:30:03.063" UserId="15797" />
  <row Id="9934" PostId="10054" Score="0" Text="So you recommend the approach of modeling the schema using a document and then inserting the columns (whose number if variable) as subdocuments?" CreationDate="2016-02-01T20:48:41.213" UserId="13688" />
  <row Id="9935" PostId="10035" Score="0" Text="Helpfull and inspirative!" CreationDate="2016-02-01T20:53:23.170" UserId="10620" />
  <row Id="9936" PostId="10047" Score="0" Text="@agold, I am sorry.  I hope you and other users can get my apology. I am glad here is a forum and users kind to discuss machine learning." CreationDate="2016-02-01T21:02:56.027" UserId="15908" />
  <row Id="9937" PostId="10054" Score="0" Text="Not necessarily. In this case I was thinking more specifically in a mongoDB world view, wherein you can use any javascript data type. So, in this case, you could have a document with a field that is a list or list of lists, of varying length. Does that make sense?" CreationDate="2016-02-01T21:38:38.460" UserId="13413" />
  <row Id="9938" PostId="10054" Score="0" Text="In the generic sense of the list yes. However I have no knowledge of Javascript. Do I need to use necessarily JS datatypes or is it possible to use dataypes from another language? (in concrete C++ or Haskell)" CreationDate="2016-02-01T21:47:08.777" UserId="13688" />
  <row Id="9939" PostId="10054" Score="0" Text="I don't think knowledge of javascript is required for mongo, though it does help if you use the mongo shell. Here's a list of supported types from their docs: https://docs.mongodb.org/manual/reference/bson-types/" CreationDate="2016-02-01T21:57:22.260" UserId="13413" />
  <row Id="9940" PostId="10054" Score="0" Text="You don't necessarily have to use mongoDB, if that's not your preference, though I do strongly recommend some sort of NoSQL approach for what you're doing!" CreationDate="2016-02-01T21:57:54.047" UserId="13413" />
  <row Id="9941" PostId="10054" Score="0" Text="Yes, I am almost sure to choose NoSQL approach. Can you recommend another mature NoSQL database solutions? (I would prefer open-source ones)." CreationDate="2016-02-01T22:00:01.307" UserId="13688" />
  <row Id="9942" PostId="10054" Score="0" Text="We've used Apache Cassandra here for my work, but it might be overkill for what you're doing. This table might be helpful in making an informed choice based on your requirements: https://en.wikipedia.org/wiki/NoSQL#Performance" CreationDate="2016-02-01T22:08:44.797" UserId="13413" />
  <row Id="9943" PostId="10057" Score="2" Text="I do not vote for closing because your potential question sounds interesting but at the moment it's pretty unclear and fuzzy so please revise it and care about the structure and clarity. What do u mean by Network? What do u mean by &quot;network slows significantly after about 40 pages&quot;? Do u want to cluster readers based on their interest or what? What is &quot;reader understanding relational connectivity&quot;? and please also check and modify the structure of question again. Proper usage of dots and question marks help people to help you." CreationDate="2016-02-01T22:31:36.480" UserId="8878" />
  <row Id="9944" PostId="10054" Score="0" Text="I see. Let me ask you something. Is it possible to create the table in advance so that the size of the datatype or container extends itself to fit all the data entered by the user? Or do I need to create the table through a script just after the dataset is submitted?" CreationDate="2016-02-01T23:41:23.730" UserId="13688" />
  <row Id="9945" PostId="10054" Score="0" Text="I'm not sure about that...I imagine you can find information on that somewhere on stackoverflow!  My gut feeling is that you shouldn't need to pre-create a table at all. I'd image space is dynamically allocated, but I'm more of a data science person, rather than a DBadmin :)" CreationDate="2016-02-02T00:27:25.157" UserId="13413" />
  <row Id="9946" PostId="10060" Score="1" Text="Welcome to the site :)" CreationDate="2016-02-02T01:58:47.343" UserId="11097" />
  <row Id="9947" PostId="10060" Score="1" Text="Hello @Dawny33 (Pikachu), Lapras here. :) Thank you!" CreationDate="2016-02-02T01:59:34.587" UserId="15927" />
  <row Id="9948" PostId="10055" Score="4" Text="I'm voting to close this question as off-topic because it belongs on StackOverflow." CreationDate="2016-02-02T08:17:38.507" UserId="471" />
  <row Id="9949" PostId="10055" Score="3" Text="What map - can you add it to the post? Maybe it's freely available as a shahepfile already? In case you have to get your hands dirty, better use a dedicated tool to polygonize your raster image to a vector file, such as QGIS. The result might - after some cleanup - be fine for a simple choropleth map. If you also need geographical locations, you might want to georeference it. You already find plenty of information on http://gis.stackexchange.com on all that." CreationDate="2016-02-02T09:05:21.153" UserId="15202" />
  <row Id="9950" PostId="10062" Score="0" Text="Why not just make the test set larger? That has to be the simplest and most efficient way of improving the accuracy of the estimate. Also, are you using a separate cross-validation set (or k-fold cross-validation) for tuning your model, or using results on the test set for tuning?" CreationDate="2016-02-02T10:07:02.647" UserId="836" />
  <row Id="9951" PostId="10015" Score="0" Text="I implemented the model using *numpy* and *scipy* and it works. When I try to print the `y_` during the training loop, the elements are all **nan**. I think there's some arithmetic error happened in tensorflow, but I could not figure out how to fix it." CreationDate="2016-02-02T13:32:37.677" UserId="3167" />
  <row Id="9952" PostId="10055" Score="0" Text="Who upvoted this? Note the mouseover note says &quot;This questions shows research effort; it is useful and clear&quot;. I don't see any of those apply here. This should be moved to SO or GIS and still needs a lot of work with some basic details." CreationDate="2016-02-02T16:50:36.637" UserId="471" />
  <row Id="9953" PostId="10067" Score="0" Text="Hey Kyle, thanks! Stop words, yes! How didn't I thought of that? Vectorizing the data in *tf-idf* format was just a function call (see the linked question in my question), is there anything equally easy for binary representation that you are aware of? And finally, if the question is whether all categories have the same number of articles, the answer is no. The distance function seems to need research and my exams started, so I do not think I will go into this for now (first time in these fields for me) :). (oops I know saw that I was using stop words, I will try with a different set though)..!" CreationDate="2016-02-02T17:35:49.477" UserId="15927" />
  <row Id="9954" PostId="10067" Score="1" Text="That's sounds reasonable! I'm not sure what library you're using, but I'd imagine that if you go to the documentation for the parent class of your tf-idf function, it likely has a variety of vectorization functions. If not, binary vectorization is fairly easy to write, and it's good practice! I'd keep the class imbalance in the back of your mind for whenever you have the time; my gut feeling is that is holding your performance back a bit. Good luck on your exams!" CreationDate="2016-02-02T17:42:44.440" UserId="13413" />
  <row Id="9955" PostId="10067" Score="0" Text="Thanks Kyle! For future readers, I am using this: http://stackoverflow.com/questions/35109424/how-to-make-tf-idf-matrix-dense" CreationDate="2016-02-02T18:22:02.537" UserId="15927" />
  <row Id="9956" PostId="10006" Score="0" Text="Somehow I am not able to get a clear understanding of what you are trying to achieve. I mean can you specify what data does Google API provide you and what results you get after first iteration of clustering and so on." CreationDate="2016-02-02T18:47:59.187" UserId="75" />
  <row Id="9957" PostId="10006" Score="0" Text="I edited the question" CreationDate="2016-02-02T19:18:58.753" UserId="15613" />
  <row Id="9958" PostId="10062" Score="0" Text="Thanks Neil. It makes sense to increase the size. How large is sufficient enough to represent the population so I have a proper representation of the skew? To answer your question, I am using a separate data set to tune the model." CreationDate="2016-02-03T03:59:58.340" UserId="15930" />
  <row Id="9959" PostId="10063" Score="1" Text="DBSCAN is finding connected components. Objects in images tend to be connected and not necessarily sphere like, so DBSCAN is good for them." CreationDate="2016-02-03T07:48:00.117" UserId="13727" />
  <row Id="9960" PostId="10000" Score="0" Text="1. You can also learn the topology of an HMM.&#xA;2. When doing inference with BNs, besides asking for maximum likelihood estimates, you can also sample from the distributions, estimate the probabilities, or do whatever else probability theory lets you.&#xA;3. A DBN is just a BN copied over time, with some (not necessarily all) nodes chained from past to the future.&#xA;&#xA;In this sense, a HMM is a simple DBN with just two nodes in each time-slice and one of the nodes chained over time." CreationDate="2016-02-03T09:25:34.987" UserId="14519" />
  <row Id="9961" PostId="10070" Score="0" Text="You seem to be asking about &quot;Sentiment Analysis&quot;." CreationDate="2016-02-03T09:53:24.237" UserId="471" />
  <row Id="9962" PostId="10076" Score="0" Text="I think the point of the question was to ask about a real-life data set that exhibits properties under which dbscan is likely to work well. I fully agree with you observations but it does not seem to solve the problem." CreationDate="2016-02-03T16:30:02.943" UserId="75" />
  <row Id="9964" PostId="10062" Score="0" Text="I don't think anyone could answer &quot;how large&quot; without knowing a lot more about your data. Personally I would not know how to calculate the optimum, just that more test data equate to better error bars on your accuracy estimate. It is typical to reserve e.g. 20% of total data set as a final test set, as a compromise between having more data to train with vs understanding the metrics of your model accurately. If you have a single CV set, you might split train/cv/test as 60/20/20." CreationDate="2016-02-03T17:47:15.300" UserId="836" />
  <row Id="9965" PostId="10076" Score="0" Text="@Shagun see that last case study example. K-means probably won't work well on that data because of the noise." CreationDate="2016-02-03T17:54:03.343" UserId="924" />
  <row Id="9966" PostId="10076" Score="0" Text="I agree that astronomical dataset could be a good fit for dbscan but could you please add say the link to one such dataset? Your answer is definitely correct though." CreationDate="2016-02-03T18:00:53.943" UserId="75" />
  <row Id="9967" PostId="10035" Score="0" Text="So I'm wondering how a k-means algorithm would work for the distance metric you defined. Isn't k-means supposed to minimize the within-cluster sum of squares from points to centroids? But the centroids wouldn't have a channel designation. The distance metric is specified for pairwise distance between points, not between points and centroids, unless there's a way to accommodate this? Thanks again for the help!" CreationDate="2016-02-03T20:52:51.217" UserId="15768" />
  <row Id="9968" PostId="10035" Score="0" Text="Obviously I wouldn't have to use k-means, I was just wondering, since you said it could work even with k-means." CreationDate="2016-02-03T20:54:39.410" UserId="15768" />
  <row Id="9969" PostId="10078" Score="1" Text="Looks like a map of Hungarian postal codes, I bet you'll be fine from a copyright perspective." CreationDate="2016-02-04T00:14:03.930" UserId="160" />
  <row Id="9970" PostId="10057" Score="0" Text="Thanks Kasra, I will modify and try to clarify what I am working on, more to come" CreationDate="2016-02-04T02:52:50.523" UserId="15921" />
  <row Id="9971" PostId="10035" Score="0" Text="it will work with kmeans, since if the distance is 0 between points from the same channel than all those points will stay to a single centroid. the only problem would be that you have to avoid assigning initially centroids to points from the same channel. One can try using an initialization like kmeans++ or better by selecting randomly points as initial centroids in such a way that no two points are part of the same channel. If time will allow me I will try a simulation on some artificial data." CreationDate="2016-02-04T06:42:44.313" UserId="108" />
  <row Id="9972" PostId="10083" Score="2" Text="Welcome to the site!  Can you pl expand your answer with a detailed explanation of what you aim to do, and what you already know. It'd help the answerer write a better quality answer :)" CreationDate="2016-02-04T07:03:07.830" UserId="11097" />
  <row Id="9973" PostId="10085" Score="1" Text="What do you mean by _then preprocess the remaining if needed_?  Is it data cleaning?" CreationDate="2016-02-04T10:09:45.527" UserId="11097" />
  <row Id="9974" PostId="10085" Score="1" Text="I meant to preprocess the remaining features that you think are useful. By preprocessing I mean, do scaling, or transformations like log, or others if and as needed." CreationDate="2016-02-04T11:20:09.767" UserId="15984" />
  <row Id="9975" PostId="10085" Score="0" Text="Ahh, as I expected :)  Anyways, I have written the answer with the workflow which me and my team generally follow!" CreationDate="2016-02-04T11:22:05.947" UserId="11097" />
  <row Id="9976" PostId="10087" Score="0" Text="I think your steps match my option 2. My understanding is that as part of the Feature Selection step, we can run a recursive feature elimination function (RFE) using random forests for example with cross-validation to determine the best number of predictors and what they are and then use those predictors to train several algorithms with cross-validation and compare accuracy to get the best model that uses those best predictors. What do you think?" CreationDate="2016-02-04T11:24:59.203" UserId="15984" />
  <row Id="9977" PostId="10087" Score="0" Text="@AndrewKostandy Yeah, the subset selection algorithm for feature selection almost works the same way :)" CreationDate="2016-02-04T11:30:25.043" UserId="11097" />
  <row Id="9978" PostId="10072" Score="0" Text="Thanks for the valuable info! I will definitely check this out." CreationDate="2016-02-04T14:05:05.547" UserId="1406" />
  <row Id="9981" PostId="10087" Score="0" Text="You're welcome. I'm currently learning for an exam where one of the standard questions of the professor is &quot;what is the first think you do after obtaining and cleaning the data?&quot; :-)" CreationDate="2016-02-04T15:12:33.380" UserId="8820" />
  <row Id="9982" PostId="10079" Score="0" Text="The comments are not marked through anything so far. But after reading all your answers, I'll try marking them. So far I'm not sure about the scale, I thought about sth like [1;10], but if I do so, I also need a scientific source. Which I have only for [1;5],[0;1] and [0,1]." CreationDate="2016-02-04T19:02:18.603" UserId="15946" />
  <row Id="9983" PostId="10073" Score="0" Text="Seems like a good idea. I'll get a neutral resource and afterwards I'll try to get your idea running on my data. Btw.: I've already removed the stopwords with the nltk stopword corpus. [nltk](http://www.nltk.org/nltk_data/)" CreationDate="2016-02-04T19:05:58.947" UserId="15946" />
  <row Id="9984" PostId="10035" Score="1" Text="Aren't centroids points in the d(i,j) space though and therefore have no channel? For example, if my distance metric was just euclidean distance on R2, then pairwise distance between points takes into account the channel, and points from the same channel have 0 distance among themselves, but distance from any data point to a centroid will just be the euclidean distance, since centroids have no channel. Thus, since kmeans iterates on distance from points to centroids, it would just behave like a normal kmeans not taking channel into account. Let me know if I'm misunderstanding kmeans though!" CreationDate="2016-02-04T21:14:11.140" UserId="15768" />
  <row Id="9985" PostId="8294" Score="0" Text="What algorithm do you use? SGD can process hundreds of thousands of features on hundreds of thousands data rows in a few minutes on a laptop." CreationDate="2016-02-04T22:08:24.227" UserId="15361" />
  <row Id="9986" PostId="10070" Score="0" Text="My apologies since this is not an answer but a suggestion but I don't have the points to comment yet. Have you looked into wordnet synsets or a similar capability? You might start with a set of words and extend it to include the related words perhaps?" CreationDate="2016-02-03T05:40:22.847" UserId="15930" />
  <row Id="9987" PostId="5255" Score="0" Text="The first idea come up to my head is `Data Analyst` or `Business Intelligence Analyst`." CreationDate="2016-01-29T01:33:57.403" UserId="4825" />
  <row Id="9989" PostId="10091" Score="0" Text="[Helpful Reference](http://datascience.stackexchange.com/q/8847/11097) for feature extraction in images" CreationDate="2016-02-05T08:05:42.273" UserId="11097" />
  <row Id="9990" PostId="10092" Score="0" Text="I have quite some idea about image processing, done some projects using MATLAB. Its the first time i am using Python for image. So didn't had idea about the libraries." CreationDate="2016-02-05T09:14:23.823" UserId="13046" />
  <row Id="9991" PostId="10086" Score="0" Text="I see the bias now, I didn't register that in the matlab diagram. This definitely helps me clarify what the matlab diagram means with its various symbols. Thank you!" CreationDate="2016-02-05T11:20:14.577" UserId="14617" />
  <row Id="9992" PostId="10086" Score="1" Text="No problem. I find it is sometimes useful (especially when coding your own) to simply think of the bias as an additional input which is equal to 1." CreationDate="2016-02-05T11:21:55.363" UserId="5144" />
  <row Id="9994" PostId="10084" Score="0" Text="Validation on a recommender can be tough, what context is it in? sales?" CreationDate="2016-02-05T17:49:23.040" UserId="14913" />
  <row Id="9996" PostId="10084" Score="0" Text="@MarkHeiler Yes." CreationDate="2016-02-05T18:20:10.560" UserId="13046" />
  <row Id="9997" PostId="10099" Score="0" Text="Couple questions: 1) When you say you have a normal distribution, do you mean you have data that seems to be normally distributed or you actually have a normal distribution? 2) For the &quot;confidence level&quot;, do you mean the confidence interval? Or do you want to know the equivalent confidence level of the interval mu +/- .1*sigma? An individual value doesn't really have a confidence level." CreationDate="2016-02-05T22:48:56.663" UserId="10884" />
  <row Id="9998" PostId="10099" Score="0" Text="my data seems to be normally distributed. So I would like to get the confidence level of mu +/- .1*sigma ( for example mu +/- 1*sigma is about 68%) How do we get those percentage(confidence level)?" CreationDate="2016-02-06T01:11:50.043" UserId="10240" />
  <row Id="9999" PostId="10101" Score="0" Text="Bob sorry, I want to learn, not just call a function, I shall update my question, but you deserve an upvote for throwing that info for future use! ;)" CreationDate="2016-02-06T03:15:07.857" UserId="15927" />
  <row Id="10000" PostId="10101" Score="0" Text="No problem, thanks for the vote." CreationDate="2016-02-06T03:15:56.340" UserId="10384" />
  <row Id="10001" PostId="10103" Score="0" Text="Did you try map-reduc'ing in a framework like Spark?" CreationDate="2016-02-06T14:23:05.187" UserId="11097" />
  <row Id="10002" PostId="10103" Score="0" Text="Nope.. how does it work and can you please direct me.." CreationDate="2016-02-06T14:46:32.263" UserId="16024" />
  <row Id="10003" PostId="10103" Score="0" Text="Pl go through [Spark's documentation](http://spark.apache.org/) for understanding it :)" CreationDate="2016-02-06T14:48:23.540" UserId="11097" />
  <row Id="10005" PostId="10076" Score="1" Text="Try this: http://skyserver.sdss.org/dr12/en/help/download/downloadhome.aspx maybe this is interesting data." CreationDate="2016-02-06T21:44:47.553" UserId="924" />
  <row Id="10006" PostId="10108" Score="2" Text="The short answer to your question is &quot;no&quot;. I found it too hard to write a good answer though, because there are some things that might chip away at the problem or related parts (e.g. a neural network to classify a format so you know which parsers to try first is feasible). They would probably be more work than they are worth, unless your goal was to learn some ML. My recommendation to start would be to just use normal development techniques to add abstraction and code re-use to your parsers." CreationDate="2016-02-06T22:18:38.210" UserId="836" />
  <row Id="10007" PostId="10108" Score="0" Text="It's good to get some confirmation of that. I had been coming to the same conclusion. Thanks!" CreationDate="2016-02-07T01:50:30.470" UserId="16032" />
  <row Id="10008" PostId="10076" Score="0" Text="Thanks @Anony-Mousse. Add it to your original answer itself :)" CreationDate="2016-02-07T04:06:03.867" UserId="75" />
  <row Id="10009" PostId="5435" Score="0" Text="@whuber you should post an answer!" CreationDate="2016-02-07T05:21:52.067" UserId="15927" />
  <row Id="10010" PostId="10073" Score="0" Text="I'm glad to here that. If you can update as you advance it will be very interesting." CreationDate="2016-02-07T06:37:26.163" UserId="13727" />
  <row Id="10011" PostId="10113" Score="0" Text="You might be interested in this answer: http://stats.stackexchange.com/questions/193887/classification-with-a-neural-network-when-one-class-has-disproportionately-many/193960#193960" CreationDate="2016-02-07T06:48:48.813" UserId="13727" />
  <row Id="10012" PostId="10083" Score="0" Text="Thank you! I wanted to start with the Higgs Boson challenge to explore machine learning methods for improving discovery significance of scientific data. The experiment and data are available at https://www.kaggle.com/c/higgs-boson . The code used by the winner was released here: https://www.kaggle.com/c/higgs-boson/forums/t/10425/code-release . I know the basic theory of supervised and unsupervised learning, and have studied and used Linear Regression and SVM. I've read about neural networks also but on a basic approach level. I'm just learning about deep learning coming from the natural scien" CreationDate="2016-02-05T05:03:43.703" UserDisplayName="user16002" />
  <row Id="10013" PostId="10108" Score="0" Text="I can not really answer to the above (reputation) but we will need some additional information to help you. How do the csv files look (before and after changes). Without knowing there is a few techniques you could try out. First one that comes to mind are regular expressions. If you want to solve this with machine learning, than this might or might not be possible most likekly using NLP with somewhat deep networks. I can direct you to some videos on the topic but first off some examples would help." CreationDate="2016-02-06T20:52:11.910" UserId="16041" />
  <row Id="10014" PostId="10114" Score="0" Text="Did you try Google? There are so many blogs/articles around this problem." CreationDate="2016-02-07T09:38:50.457" UserId="75" />
  <row Id="10016" PostId="10113" Score="0" Text="Thanks Dan...the link is very helpful. Any thoughts on the above steps that I am taking...do you see anything flawed with it?" CreationDate="2016-02-07T15:01:25.760" UserId="16046" />
  <row Id="10017" PostId="10112" Score="0" Text="&quot;correctness&quot; depends on only on data in nodes but also on position of the node in the graph. There is a &quot;correctness&quot; flag assigned externally humans could figure it out based on the data but with a VERY large amount of effort. Again, there is an external process which tells if data is correct for the whole path or if it is incorrect. Data about correctness is available for the whole graph (all paths) or not available at all. I don't know if it can be used to predict correctly, I'd like to play and find out. :)" CreationDate="2016-02-07T17:48:25.407" UserId="16042" />
  <row Id="10018" PostId="10112" Score="0" Text="Based on correctness of some paths in graph you might not be able to predict correctness of others. it is more about the whole graph" CreationDate="2016-02-07T17:51:04.547" UserId="16042" />
  <row Id="10020" PostId="10112" Score="0" Text="It is extremely hard, it would be like automating image recognition. It is not just logical, it is good amount of heuristics. When I said possible, humans can work with graphical representation of this data, but to figure out from those data structures what is going on might take months and still need lots of human heuristics on how it should work. I don't know, it might be related to search, in this case I'm completely lost and don't understand what do I need to do here." CreationDate="2016-02-07T18:28:20.033" UserId="16042" />
  <row Id="10021" PostId="10112" Score="0" Text="I hope to convert human heuristics to some kind of &quot;correctness&quot; recognition. Does it sound like neural networks or something like that?" CreationDate="2016-02-07T18:29:56.280" UserId="16042" />
  <row Id="10023" PostId="10112" Score="1" Text="If the humans are using something more like intuition, then yes an ML technique might help. Often it's a good indicator to use ML when a human expert can do something, but it is hard to identify the logic. I suggest putting some of these salient points from your comments into the question, to help someone answering." CreationDate="2016-02-07T18:48:03.143" UserId="836" />
  <row Id="10024" PostId="10098" Score="0" Text="https://msdn.microsoft.com/en-us/library/dn282355.aspx" CreationDate="2016-02-07T19:24:26.003" UserId="381" />
  <row Id="10025" PostId="10138" Score="0" Text="Are you thinking visual representation, or mathematical? If it's the former, look at [sparklines](https://en.wikipedia.org/wiki/Sparkline). If it's the latter, look at PCA; the PCA coefficients will give you a succinct summary." CreationDate="2016-02-08T06:05:59.117" UserId="381" />
  <row Id="10026" PostId="10138" Score="0" Text="Thanks @Emre for pointing to sparklines,  but sparklines still take enough space on paper,i.e, thirty rows (for each day). I want something more compact." CreationDate="2016-02-08T06:17:41.287" UserId="13291" />
  <row Id="10027" PostId="10113" Score="0" Text="It is hard to answer about a specific case in remote. In general, I think you should try with simple models and advance to more complex as needed. Even if the dataset is unbalanced, try working with all of it first. Then, move to balancing. If that doesn't work, generate synthetic samples. Working like that enable estimating where the core difficulty is. You wrote that you already got to sample generation, and I guess that is justified. Try to identify why the models didn't work on previous steps (e.g., no strong features, to big feature space) and maybe it will give you more directions." CreationDate="2016-02-08T07:00:51.147" UserId="13727" />
  <row Id="10028" PostId="6440" Score="0" Text="Could you please add a wiki entry for &quot;multitask-learning&quot;?" CreationDate="2016-02-08T07:23:03.283" UserId="8820" />
  <row Id="10029" PostId="10138" Score="0" Text="How about 2D PCA? Each time series will be represented as a point on one chart." CreationDate="2016-02-08T07:40:53.193" UserId="381" />
  <row Id="10032" PostId="10138" Score="0" Text="Sparklines is a good idea, maybe just comma-seperated next to each other (depending on what you wanna show) - just like flowing text, Or plot several lines in one chart - maybe 4 to 5 lines per 7 charts if you aggregate by weekday. But I guess you already tried that..." CreationDate="2016-02-08T09:08:09.770" UserId="15202" />
  <row Id="10034" PostId="10079" Score="0" Text="You may try marking comments up with plain-text categories (positive, neutral, negative) - that will result in more simple mark up process. Then, when your dictionary of negative words will be in place, you would have a way to rank negativity/positivity of a comment using any measure you'd like based on more simple mark up." CreationDate="2016-02-08T09:37:56.747" UserId="2573" />
  <row Id="10035" PostId="10107" Score="0" Text="We don't have historical data about user. We have set of features of hotels on which we want to show the best recommended hotels so that our sales can increase at a good speed and user doesn't have to dig deep into the filters to select the best hotel. I am using the following approach - http://datascience.stackexchange.com/questions/10090/modified-voting-algorithm-to-find-the-best-recommendation" CreationDate="2016-02-08T12:04:44.740" UserId="13046" />
  <row Id="10036" PostId="10113" Score="0" Text="Thanks Dan for your thoughts." CreationDate="2016-02-08T13:41:45.580" UserId="16046" />
  <row Id="10037" PostId="10138" Score="0" Text="Difficult to say but if you might have expected periodicity then Panel of 4 (and 1 of 3), with 7 colours for 7 days would be compact and show/not show the relation? You would also want to do something like ARIMA plot?" CreationDate="2016-02-08T14:28:48.807" UserId="1066" />
  <row Id="10038" PostId="10081" Score="0" Text="Comment: It looks like the mlr package in R allows you to perform multi-label classification for anyone who is interested. My personal challenge is in data preparation and if anyone has any tips it would be very useful. Thanks" CreationDate="2016-02-08T19:13:36.200" UserId="15609" />
  <row Id="10039" PostId="10107" Score="0" Text="Hmmm, so the idea is to have a dynamic top-10 that changes for each filter they select?" CreationDate="2016-02-08T23:43:23.623" UserId="14913" />
  <row Id="10040" PostId="6202" Score="0" Text="Oh, I know! Do you mean like _Select Rows_ and _Select Columns_ widgets? Or perhaps the _Feature Constructor_ widget? There's also the _Data Table_ widget and its selection is passed further down the data flow ... Is this is? :D" CreationDate="2016-02-09T01:36:18.343" UserId="15527" />
  <row Id="10041" PostId="10131" Score="0" Text="So how do I know whether to use a .25 discount factor or a .75 one? When do I want to use a greedy gamma? Is there a formula to get precise value or do I just &quot;use whatever feels right&quot;?" CreationDate="2016-02-09T04:51:39.530" UserId="13165" />
  <row Id="10042" PostId="10107" Score="0" Text="Its one of the steps. But I am not stuck at that. The problem is to show the best 25 hotels on initial page load. We don't know which will be the best, we have to make a algo. that gives us the best possible guess. Filters and similarity comes when a user starts interacting. Its the second phase of the requirement." CreationDate="2016-02-09T06:46:51.280" UserId="13046" />
  <row Id="10043" PostId="9990" Score="0" Text="Although this question is absolutely suited to this site, I suspect you might have more luck over at http://electronics.stackexchange.com" CreationDate="2016-02-09T10:45:58.477" UserId="5144" />
  <row Id="10044" PostId="6813" Score="2" Text="But the question is about underfitting, not overfitting." CreationDate="2016-02-09T12:42:39.707" UserId="6550" />
  <row Id="10045" PostId="9469" Score="0" Text="Note some architectures will take *current* output from a neuron as input to neurons connected to it, when &quot;current&quot; can mean &quot;whatever has already been calculated in order&quot; - potentially from a previous input or a learned start value. RNNs do this, and they can be used in control systems similar to NEAT, but I'm not at all sure what NEAT does. So if you are only interested in NEAT's approach, you should clarify because there is likely more than one way to resolve the issue." CreationDate="2016-02-09T12:50:45.943" UserId="836" />
  <row Id="10046" PostId="10107" Score="0" Text="Can you see what hotel a customer would end up booking with, then go back and see where that hotel was on their top-25 list in order to validate it?" CreationDate="2016-02-09T14:05:40.853" UserId="14913" />
  <row Id="10047" PostId="10157" Score="0" Text="Good starting point for CTC could be: &#xA;&#xA;Yajie Miao, Mohammad Gowayyed, and Florian Metze,: [EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding](http://arxiv.org/abs/1507.08240) in Proc. ASRU 2015." CreationDate="2016-02-09T14:24:19.807" UserId="16124" />
  <row Id="10048" PostId="10156" Score="0" Text="Is that a homework question?" CreationDate="2016-02-09T14:25:47.777" UserId="11097" />
  <row Id="10049" PostId="10035" Score="0" Text="You are right, of course, this is my mistake. It will work with k-medoids, but not with k-means, for reasons you mentioned." CreationDate="2016-02-09T15:48:00.573" UserId="108" />
  <row Id="10050" PostId="10107" Score="0" Text="Its again a customer data. Think of it as a cold start problem, with a difference that all users are new. We cannot show some random hotels to them, so we need to show in such a way that it fulfils all my features and their priorities. Thanks for your effort. If you need any further explanation please let me know." CreationDate="2016-02-09T16:48:57.767" UserId="13046" />
  <row Id="10052" PostId="10154" Score="0" Text="I think you should clarify what pincodes means, what graph and trajectory mean in this context. I'm not sure what the problem is" CreationDate="2016-02-09T19:16:40.473" UserId="21" />
  <row Id="10053" PostId="10167" Score="0" Text="On the same plot, I do this for different k? or only one k for one plot as in the example? and what do you mean by &quot;index&quot;" CreationDate="2016-02-09T20:53:20.010" UserId="13915" />
  <row Id="10054" PostId="10167" Score="0" Text="Using the 6NN when you only have 6 points is of course nonsense. Do it for an appropriate k. Index as in &quot;array index&quot;. because you need 2d to plot." CreationDate="2016-02-09T20:57:03.203" UserId="924" />
  <row Id="10055" PostId="10167" Score="0" Text="And i only use the last column of the distance matrix. Because in the example they talk about averaging distances.." CreationDate="2016-02-09T22:26:03.143" UserId="13915" />
  <row Id="10056" PostId="10167" Score="0" Text="That post is incorrect there *and* in at least another place (you don't need to set a seed)" CreationDate="2016-02-09T22:46:04.180" UserId="924" />
  <row Id="10057" PostId="10167" Score="0" Text="Okey. Just one last thing, you seem to tell me that I have to make a plot for each k right? Cause here: https://github.com/alitouka/spark_dbscan/wiki/Choosing-parameters-of-DBSCAN-algorithm it seems different to what you say no? Thx" CreationDate="2016-02-09T22:51:54.793" UserId="13915" />
  <row Id="10058" PostId="10167" Score="1" Text="You only have one k. Why don't you use the DBSCAN paper. but mash-up various low-quality websites?" CreationDate="2016-02-09T22:53:32.090" UserId="924" />
  <row Id="10059" PostId="10167" Score="0" Text="Great idea, thank you very much." CreationDate="2016-02-09T23:47:40.740" UserId="13915" />
  <row Id="10060" PostId="10156" Score="0" Text="@Dawny33 yep..." CreationDate="2016-02-10T08:26:35.570" UserId="16123" />
  <row Id="10061" PostId="10156" Score="0" Text="Then, please add the [tag:self-study] tag" CreationDate="2016-02-10T08:29:20.963" UserId="11097" />
  <row Id="10062" PostId="10156" Score="1" Text="@Dawny33 okay, done!" CreationDate="2016-02-10T08:32:25.950" UserId="16123" />
  <row Id="10063" PostId="10164" Score="2" Text="Truly sad story, bro in machine learning. What is the question though?" CreationDate="2016-02-10T10:05:57.333" UserId="2573" />
  <row Id="10064" PostId="10156" Score="3" Text="Questions that are simply cut and paste of homework assignments should be voted down instantly as &quot;the question does not show any research effort&quot;. At least show some working, or thoughts, or ideas, or where you've looked, or some other indication that you don't just want us to do your homework for you." CreationDate="2016-02-10T10:11:01.737" UserId="471" />
  <row Id="10066" PostId="10109" Score="0" Text="50K rows is not big data and 1 per hour is not high frequency. What's wrong with doing it with SAS Base? You can even chart your data flows in SAS GUI Enterprise Guide. Ab Initio and Informatica are other commercial solutions for ETL to name a few." CreationDate="2016-02-10T12:10:52.257" UserId="15361" />
  <row Id="10069" PostId="10172" Score="0" Text="Hi, we are trying to keep content quality high on this site. Please clean up your grammar (missing capitalization). Thanks and welcome!" CreationDate="2016-02-10T21:18:08.727" UserId="3466" />
  <row Id="10070" PostId="10154" Score="0" Text="Hi Sean, I have edited the question with details you asked for. I am new to the forum so kindly tell me if something more is expected in question details, so you and other users can help me out. Thanks." CreationDate="2016-02-10T21:45:57.010" UserId="15906" />
  <row Id="10071" PostId="10190" Score="0" Text="Kyle yes I have seen that, but in my question I said: &quot;*I want to implement K-means algorithm in Spark*&quot;." CreationDate="2016-02-10T23:07:36.590" UserId="15927" />
  <row Id="10072" PostId="10190" Score="0" Text="Ah. Sorry, I misread!" CreationDate="2016-02-10T23:09:28.683" UserId="13413" />
  <row Id="10075" PostId="10190" Score="0" Text="It's OK Kyle, it's good info, thus my +1. I may explore the source code of mllib and compare it with the one I linked! I also updated my question, if that matters." CreationDate="2016-02-10T23:17:12.003" UserId="15927" />
  <row Id="10076" PostId="10190" Score="0" Text="No problem! That's a great place to start!" CreationDate="2016-02-10T23:17:51.723" UserId="13413" />
  <row Id="10077" PostId="10190" Score="0" Text="And a big one too. I found this, looking into it now...https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/clustering" CreationDate="2016-02-10T23:22:55.867" UserId="15927" />
  <row Id="10079" PostId="10194" Score="1" Text="Great explanation, @Harsh!" CreationDate="2016-02-11T02:04:53.413" UserId="13413" />
  <row Id="10080" PostId="10193" Score="1" Text="Welcome to the site. That is a very nice question (+1) :)" CreationDate="2016-02-11T02:23:14.383" UserId="11097" />
  <row Id="10081" PostId="10184" Score="1" Text="Not between 9 and 10. The structures are different." CreationDate="2016-02-11T02:33:35.410" UserId="16145" />
  <row Id="10082" PostId="10191" Score="1" Text="In addition to differentiability, the $L^2$ norm is unique in the $L^p$ norms in that it is a Hilbert space.  The fact that the norm arises from an inner product makes a huge amount of machinery available for $L^2$ which is not available for other norms." CreationDate="2016-02-11T05:59:41.023" UserId="16189" />
  <row Id="10084" PostId="963" Score="0" Text="Erich Schubert, i am looking for the exact same thing! Did you have any chance to find a useful resource for it?" CreationDate="2016-02-10T18:27:01.027" UserId="16168" />
  <row Id="10086" PostId="10196" Score="1" Text="To calculate the probability of observation X and Y's credit ratings using the formula found on the link you gave:  &#xA;  &#xA;#(NodeCondition &amp; Attr=Value) / #(NodeCondition)  &#xA;  &#xA;**Observation X**  &#xA;P(Good) = 272/483 = 0.56  &#xA;P(Bad) = 211/483 = 0.44  &#xA;  &#xA;**Observation Y**  &#xA;P(Good) = 99/553 = 0.18  &#xA;P(Bad) = 454/553= 0.82  &#xA;  &#xA;Have I done this correctly?" CreationDate="2016-02-11T14:55:46.370" UserId="16179" />
  <row Id="10087" PostId="10196" Score="0" Text="Sorry, I can't seem to use line breaks :(" CreationDate="2016-02-11T14:58:42.103" UserId="16179" />
  <row Id="10089" PostId="10196" Score="0" Text="@DataNewb Yeah, you're on the right way :)" CreationDate="2016-02-11T15:03:13.350" UserId="11097" />
  <row Id="10090" PostId="10103" Score="0" Text="See if [this](https://github.com/saurfang/spark-tsne) Spark implementation works." CreationDate="2016-02-11T17:55:31.833" UserId="381" />
  <row Id="10091" PostId="10205" Score="0" Text="I edited my question. please take a look." CreationDate="2016-02-11T18:21:50.367" UserId="3151" />
  <row Id="10092" PostId="10103" Score="0" Text="@Emre: Which language is used in that implementation ? It seems there is bit of R and scala.. I haven't worked in any of these.. I was looking for python implementation." CreationDate="2016-02-11T19:02:59.953" UserId="16024" />
  <row Id="10093" PostId="10103" Score="1" Text="It's Scala for Spark. If you want a python implementation you might be able to translate it; Spark runs on python too." CreationDate="2016-02-11T19:07:20.390" UserId="381" />
  <row Id="10094" PostId="10103" Score="0" Text="@Emre So does it mean I should install `Spark` and compile this `spark-tsne` and then import as a module in python." CreationDate="2016-02-11T19:20:53.810" UserId="16024" />
  <row Id="10097" PostId="10205" Score="1" Text="I don't understand what you need from the edit. You can query edges like this: `E(g)[from(&quot;2&quot;) &amp; to(&quot;1&quot;)]`. Please add a full reproducible example with a sample graph, your function and the reason, why the output of the function is not what you want." CreationDate="2016-02-11T19:36:45.710" UserId="15202" />
  <row Id="10098" PostId="10103" Score="0" Text="No, it means you should read the Scala package and write one in python based on it. Personally I would advise trying to use the Scala package as is. Although I have no personal experience with, [Beaker](http://beakernotebook.com/features) might help you use Scala and python concurrently. As an alternative to t-SNE, you could one of many python neural network libraries to find a autoencoded 2D embedding." CreationDate="2016-02-11T20:45:01.863" UserId="381" />
  <row Id="10099" PostId="10198" Score="0" Text="The problem is what string? I don't see any strings except the file name. Create a test graph dataset we can all use and give an example and show the error message." CreationDate="2016-02-11T22:40:18.187" UserId="471" />
  <row Id="10100" PostId="10194" Score="0" Text="Tnx! So: We have our initial dataset, which is distributed (every node takes a subset of the dataset S). Then, every node computes distances (serially) and finds the new centroids for their respective S. Then what is happening? Every node continues running its local kmeans in its subset of the dataset or we split again the dataset to the nodes, but now we use the centroids computed in the previous iteration?" CreationDate="2016-02-12T00:53:44.777" UserId="15927" />
  <row Id="10101" PostId="10205" Score="0" Text="I need two functions called f_source and f_target in my code and I want to subset on specific edges. How can I do that?" CreationDate="2016-02-12T07:31:52.500" UserId="3151" />
  <row Id="10102" PostId="6169" Score="0" Text="[Helpful Post](http://stackoverflow.com/a/33513655/4993513)" CreationDate="2016-02-12T07:40:08.547" UserId="11097" />
  <row Id="10106" PostId="10207" Score="1" Text="I would like to add it may be beneficial to reshuffle the data after each full iteration of the dataset to generate new min-batches on further iterations." CreationDate="2016-02-12T13:10:41.693" UserId="5144" />
  <row Id="10111" PostId="10223" Score="0" Text="Again: rpart **requires labels**. Don't just type something in. but make sure to understand A) what the method does/can/cannot B) what the parameters are Here, you obviously have no idea what `x ~ .` means. It will not work if you just do things at random." CreationDate="2016-02-12T22:16:47.360" UserId="924" />
  <row Id="10113" PostId="10230" Score="0" Text="That's great thanks very much! As an aside as I am starting to play around with genim - I am curious if it is possible to calculate the similarity between a doc (regardless if unique or shared tags are used ) and a word - can that be done in your experience?" CreationDate="2016-02-13T02:26:30.150" UserId="1138" />
  <row Id="10114" PostId="10227" Score="0" Text="+1 for helping with the intuition of the choice between the options!" CreationDate="2016-02-13T02:35:45.967" UserId="1138" />
  <row Id="10115" PostId="10225" Score="0" Text="Not really, but SORTing by each column and looking at the minimum and maximum values, and setting alerts for ridiculous values might be helpful." CreationDate="2016-02-13T03:41:18.130" UserId="4710" />
  <row Id="10117" PostId="10228" Score="0" Text="Sometimes one is better, sometimes the other." CreationDate="2016-02-13T08:41:37.360" UserId="924" />
  <row Id="10118" PostId="10230" Score="1" Text="Some training modes create word and document vectors inside the 'same space' and so similarities may be meaningful. See for example this paper – http://arxiv.org/abs/1507.07998 – which even does a sort of 'analogy arithmetic' involving doc-vectors (Wikipedia articles) and word-vectors. The training in that paper is like gensim's DBOW mode with concurrent skip-gram word-training: `dm=0, dbow_words=1`." CreationDate="2016-02-13T10:01:27.350" UserId="8278" />
  <row Id="10119" PostId="10225" Score="0" Text="In case it doesn't have to be super sophisticated you could use mlinreg moving window linear regression and thus identify large deviations." CreationDate="2016-02-13T10:36:37.930" UserId="15361" />
  <row Id="10120" PostId="10230" Score="0" Text="Thanks @gojomo! I added a query about how to do this on the gensim list: https://groups.google.com/forum/#!topic/gensim/RLRfY6k3ulw" CreationDate="2016-02-13T14:03:20.037" UserId="1138" />
  <row Id="10121" PostId="10211" Score="0" Text="Why should it be any different from, say, English? Python can transparently process UTF8. Do you foresee any specific problem with grammar or anything?" CreationDate="2016-02-13T14:55:50.920" UserId="15361" />
  <row Id="10122" PostId="10232" Score="2" Text="Do you mean the encoding *beyond* the hexadecimal system of encoding bytes, or have you never seen hexadecimal before?" CreationDate="2016-02-13T16:15:36.193" UserId="471" />
  <row Id="10123" PostId="10211" Score="0" Text="@Diego, as I understand, I need a dictionary of words of the Russian language with the weight of each word (something like [this](http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) for English). Which library could you advise my?" CreationDate="2016-02-13T19:51:36.853" UserId="9992" />
  <row Id="10124" PostId="10234" Score="0" Text="Are you looking for databases where the data is stored differently between the two databases? If not, you could theoretically split a single database into 2 smaller ones randomly." CreationDate="2016-02-13T20:14:31.223" UserId="4710" />
  <row Id="10125" PostId="10234" Score="0" Text="Thanks a lot  Barry. I already thought of your great idea. However, I'm looking for 2 or more different sources of databases since their origin plays an important role in the processing. If for instance I'll have same structure of data from different banks for instance it would be helpful. I looked in UCI datasets repository and didn't find. If you can help me I'll highly appreciate it." CreationDate="2016-02-13T20:20:17.823" UserId="16251" />
  <row Id="10126" PostId="10211" Score="0" Text="May be you should replace the word library with the word dictionary in your question. Python library is usually referring to a bunch of functions and classes, not just a collection of words." CreationDate="2016-02-14T01:25:15.097" UserId="15361" />
  <row Id="10127" PostId="9793" Score="0" Text="Can you add the code you're trying to run ?" CreationDate="2016-02-14T07:52:19.520" UserId="16260" />
  <row Id="10128" PostId="10228" Score="4" Text="@Anony-Mousse I guess that was obvious before your comment. The question is not if both have their advantages, but in which scenarios one is better than the other." CreationDate="2016-02-14T10:34:55.057" UserId="8820" />
  <row Id="10129" PostId="10232" Score="0" Text="I have seen hexadecimal before but I am not sure if I understand your question completely. Can you elaborate?" CreationDate="2016-02-14T11:15:13.780" UserId="16249" />
  <row Id="10130" PostId="6084" Score="0" Text="Kyler's solution is awesome, but incomplete. The code above only plots points on 6 axes... The value of 20 for the &quot;Inverted 3%&quot; axis does not plot when I run this." CreationDate="2016-02-13T00:05:59.130" UserDisplayName="user16242" />
  <row Id="10131" PostId="10238" Score="0" Text="Welcome to DS. #1 It's better to provide a full working example, with which others can play and reproduce your error. #2  _&quot;need finite 'ylim' values:&quot;_ basically speaks for itself: you need to specify finite y limits, not sth like `plot(0, ylim=c(-Inf, 2))`." CreationDate="2016-02-14T15:29:18.680" UserId="15202" />
  <row Id="10132" PostId="10238" Score="0" Text="I tried using plotly which worked without any problem,  but i wasn't able to save it..  So i had to find a workaround for the problem.." CreationDate="2016-02-14T15:38:38.867" UserId="16253" />
  <row Id="10133" PostId="10232" Score="1" Text="Convert the data to binary. If the data's not private, post it and I can show you what I mean. The first few bytes you provided don't suggest a file type, but emacs thinks it might be Japanese/JIS text." CreationDate="2016-02-14T15:49:07.197" UserId="4710" />
  <row Id="10134" PostId="10234" Score="0" Text="Sorry, that was my only thought, hopefully others can help." CreationDate="2016-02-14T15:55:23.640" UserId="4710" />
  <row Id="10135" PostId="10232" Score="1" Text="Its not clear from your Q whether you understand that &quot;9c4e79cb&quot; means anything to you (ie you understand its hexadecimal). So there's (at least) two &quot;encodings&quot; going on - first the representation of the data as hex, and then however the force plate measurements are arranged into that data. It could just be 64x64xN bytes. There's not really enough context in the Q to figure it out..." CreationDate="2016-02-14T23:22:52.130" UserId="471" />
  <row Id="10137" PostId="10251" Score="0" Text="So what you reckon is that with that kind of data you cannot do that much unless of plain statistics that pretty much will give you just some insight of the current data?" CreationDate="2016-02-15T09:52:38.823" UserId="16280" />
  <row Id="10138" PostId="10251" Score="0" Text="You can use the data you have to predict the price of a house which is not on the current data set, but I would be careful with the time dimension. First of all you have seasonality (houses are more expensive during summer), and then there's the problem of price prediction that falls into the realm of economic forecasting." CreationDate="2016-02-15T09:57:59.250" UserId="16279" />
  <row Id="10139" PostId="10251" Score="0" Text="You could naively assume that, if house prices have been rising for the past 2 years, that they will continue rising in the future, and a regression model with the correct number of parameters will give you that answer, but that will most certainly turn out to be incorrect, as the future price diverges from the current price in a way that cannot be predicted from your current data alone." CreationDate="2016-02-15T10:00:24.780" UserId="16279" />
  <row Id="10140" PostId="10251" Score="0" Text="Thank you very much. It sounds like (of course) there is no magic formula about that. What it is required is a specific &quot;domain knowledge&quot; for the topic. Any good link you can suggest to go more in depth with regression-model?" CreationDate="2016-02-15T10:19:45.773" UserId="16280" />
  <row Id="10141" PostId="10251" Score="1" Text="There are quite a few good online resources on this now. [Andrew Ng's course](https://www.coursera.org/learn/machine-learning) is pretty good, and popular." CreationDate="2016-02-15T10:33:06.760" UserId="16279" />
  <row Id="10143" PostId="10232" Score="1" Text="I think its a bit tricky ... blobs can be of any structure inside. It helps that you expect a certain array of data (64x64) sampled at 300Hz, though how long is the data stored for? If you know how many seconds it is stored, and you know the blob size, you can then estimate the individual point resolution." CreationDate="2016-02-15T13:22:40.260" UserId="16284" />
  <row Id="10144" PostId="10239" Score="0" Text="This question is completely unanswerable unless you tell us what you want to do with the data." CreationDate="2016-02-15T14:18:12.237" UserId="471" />
  <row Id="10145" PostId="10211" Score="0" Text="@Diego, thank you for recommendation, but I think my question correctly shows that I'm &#xA;interested in dictionary or library. Both for me will be ok." CreationDate="2016-02-15T16:22:03.790" UserId="9992" />
  <row Id="10146" PostId="10250" Score="2" Text="An answer provided on [SE](http://stats.stackexchange.com/questions/179026/objective-function-cost-function-loss-function-are-they-the-same-thing) explains some differences between these similar terms.  I believe criterion, error, &amp; cost are synonyms however." CreationDate="2016-02-15T19:41:59.623" UserId="6385" />
  <row Id="10147" PostId="10232" Score="0" Text="Thanks for all you comments so far! I understand that some information is lacking: I will post more data and some more details I found about the data tomorrow." CreationDate="2016-02-15T20:44:50.553" UserId="16249" />
  <row Id="10148" PostId="10239" Score="0" Text="@Spacedman. updated question." CreationDate="2016-02-15T21:02:54.460" UserId="16266" />
  <row Id="10149" PostId="10234" Score="0" Text="Are these SQL or NoSQL databases? Is the question on collecting the data, or on the analysis of the data?" CreationDate="2016-02-15T21:12:24.083" UserId="16284" />
  <row Id="10150" PostId="10234" Score="0" Text="Thanks Marcus. I am looking for SQL databases (with columns). I would like to use them for analysis but the question is about the data itself. I need about 6 databases (each can be at least 2000 tuples) of different companies such as banks with same columns. No matter the column names only they have to be the same. The databases should be with target column. So that I'll be able to create a decision tree for each." CreationDate="2016-02-15T21:20:58.410" UserId="16251" />
  <row Id="10151" PostId="10234" Score="0" Text="It's probably the terminology of my background (database programming) that is giving me difficulty in understanding what you are after. &#xA;&#xA;I can give you the SQL statements to find out MS SQL Server schema structure (e.g.), but it sounds like you actually want a number of database schemas for similar subject areas to test your decision trees on. I guess this might mean approaching a number of similar companies and ask them to run the same code on a similar. I would suggest Lloyds syndicates as there are hundreds of them that will be storing quite similar data." CreationDate="2016-02-15T22:25:33.750" UserId="16284" />
  <row Id="10153" PostId="10234" Score="0" Text="Thanks Marcus, What I actually need is the data not the schema nor the structure. Pure data of 6 different companies in the same branch. That the data has same columns. Something like datasets in UCI repository. Lloyds syndicates is a great source if they can send me a data for number of different brokers that can fit a classification model such as decision tree. Can you help me with it?" CreationDate="2016-02-15T22:39:52.703" UserId="16251" />
  <row Id="10155" PostId="10228" Score="0" Text="I have proposed &quot;Information gain&quot; instead of &quot;Entropy&quot;, since it is quite closer (IMHO), as marked in the related links. Then, the question was asked in a different form in [When to use Gini impurity and when to use information gain?](http://stats.stackexchange.com/questions/130155/when-to-use-gini-impurity-and-when-to-use-information-gain)" CreationDate="2016-02-16T07:23:36.827" UserId="12527" />
  <row Id="10156" PostId="10261" Score="0" Text="Welcome to the site, Mary!" CreationDate="2016-02-16T08:53:06.463" UserId="11097" />
  <row Id="10157" PostId="10261" Score="0" Text="Thanks Dawny :)" CreationDate="2016-02-16T08:59:49.480" UserId="16312" />
  <row Id="10158" PostId="10234" Score="0" Text="I think you would be hard pressed to get companies to give you their data, unless it was anonymised, but even then, they might be concerned about pricing and confidential information that could be gleaned from it." CreationDate="2016-02-16T10:26:40.607" UserId="16284" />
  <row Id="10159" PostId="10234" Score="0" Text="How about data from the same source but different years?" CreationDate="2016-02-16T13:20:13.317" UserId="7738" />
  <row Id="10160" PostId="10250" Score="0" Text="You can add &quot;loss function&quot; to your list" CreationDate="2016-02-16T13:57:22.640" UserId="12527" />
  <row Id="10161" PostId="10266" Score="0" Text="But as per the mathematical formula, both denominator and numerator becomes 0,making the estimate not defined. Pls correct me if wrong." CreationDate="2016-02-16T13:59:57.713" UserId="14688" />
  <row Id="10162" PostId="10234" Score="0" Text="Thanks Kyler, the point in the process I use is the source of data. Time is has less importance." CreationDate="2016-02-16T14:09:21.187" UserId="16251" />
  <row Id="10163" PostId="10266" Score="0" Text="0/0 is un undetermined form. This does not mean that the correlation does not exist. I think that there's a way to solve the indetermination." CreationDate="2016-02-16T14:19:33.343" UserId="10024" />
  <row Id="10164" PostId="10239" Score="0" Text="So you don't care at all about the speed or acceleration? Only the line travelled. If the person loitered in one spot for ten minutes, you don't care, you can drop all those points?" CreationDate="2016-02-16T14:24:44.573" UserId="471" />
  <row Id="10165" PostId="10266" Score="0" Text="@ Andrea yes, my question is precisely the one that you thought :)" CreationDate="2016-02-16T14:49:09.953" UserId="14688" />
  <row Id="10166" PostId="10250" Score="0" Text="@LaurentDuval Thank you. I've added it." CreationDate="2016-02-16T15:01:19.530" UserId="8820" />
  <row Id="10167" PostId="5011" Score="0" Text="It's probably ok to merging training and test data for the purpose of scaling though. There's no risk of a &quot;leak&quot; from the test data in this because scaling doesn't use the target values, only feature values. But you actually might make your results *worse* by merging because most of the benefit of scaling is to make convergence faster, and to help with regularization. But it only helps when you train on the data that you scaled. If test data features have a very different means, you kinda make your scaling worse." CreationDate="2016-02-16T18:51:46.933" UserId="13901" />
  <row Id="10168" PostId="10232" Score="0" Text="I updated my question with extra information, actual data and new insights. Any help is appreciated. I will keep you updated on my progress. I might also post my progress as a separate answer to not pollute the question too much." CreationDate="2016-02-16T20:44:49.300" UserId="16249" />
  <row Id="10169" PostId="10262" Score="0" Text="There are at least two indeterminations. The &quot;mean&quot; on the $x$-axis is not defined either" CreationDate="2016-02-17T04:46:09.927" UserId="12527" />
  <row Id="10170" PostId="10262" Score="0" Text="Possible duplicate of http://stats.stackexchange.com/questions/18333/what-is-the-correlation-if-the-standard-deviation-of-one-variable-is-0" CreationDate="2016-02-17T04:48:12.643" UserId="12527" />
  <row Id="10171" PostId="10239" Score="0" Text="no i need all the 5 metrics, but I want to optimize the size. douglas-peucker is perfect. I am asking if I should separate the components before optim." CreationDate="2016-02-17T06:34:07.287" UserId="16266" />
  <row Id="10172" PostId="10275" Score="0" Text="Thanks a lot Dan. Can you specify directly the link of data itself in  US census 2000?" CreationDate="2016-02-17T08:05:04.743" UserId="16251" />
  <row Id="10173" PostId="10239" Score="1" Text="DP is only appropriate for simplifying lines defined by points by thinning out certain points. It doesn't care what other data you have defined at each point. What you can do is recompute the speed and acceleration from the simplified points (noting that speed is only computable between two points, and acceleration only computable between three points)" CreationDate="2016-02-17T08:13:21.183" UserId="471" />
  <row Id="10174" PostId="10275" Score="0" Text="There are many aspect of the census you can get. An example to the demographic data is in here http://www2.census.gov/census_2000/datasets/100_and_sample_profile/ . I didn't work with this dataset myself before. The files I saw now are pdf summaries and I cannot download the zip files (hopefully raw data) so this specific case might not be good. Was I right about the goal? Is such type of dataset fit your needs?" CreationDate="2016-02-17T08:23:21.683" UserId="13727" />
  <row Id="10178" PostId="10275" Score="0" Text="Thanks a lot. About my goal - is there a way I can discuss it with you personally?" CreationDate="2016-02-17T08:34:56.560" UserId="16251" />
  <row Id="10179" PostId="10262" Score="0" Text="@LaurentDuval yes the question is similar, but there isn't a convincing answer. The answers suggest that the correlation is 0. Yes the correlation must be 0, but the mathematical equation i guess should include this. I mean as two values, one is the usual computation and the other 0 when the standard deviation of one of the variable is 0." CreationDate="2016-02-17T09:29:25.533" UserId="14688" />
  <row Id="10180" PostId="10262" Score="0" Text="To me, the answer is in the limit. If you take $y=c+ \epsilon  x$, compute the correlation, and take $\epsilon\to 0$, you reach $0$, because a term on the numerator, and the counterpart on the denominator converge to $0$ with the same speed." CreationDate="2016-02-17T09:33:13.670" UserId="12527" />
  <row Id="10181" PostId="10275" Score="0" Text="levind4 at gmail" CreationDate="2016-02-17T09:37:07.817" UserId="13727" />
  <row Id="10182" PostId="10262" Score="0" Text="@LaurentDuval yes I understand it. But the thing is, I was carrying out correlation analysis between variables and I observed many values returning NaN. Then I closely checked the inputs, they were constant functions. I was wondering why they were returning NaN when it should be returning 0." CreationDate="2016-02-17T09:43:27.827" UserId="14688" />
  <row Id="10183" PostId="10232" Score="0" Text="The blobs are assorted lengths which aren't multiples of 64. The blob data looks random, with no obvious pattern or repeat or structure. That makes me think the blob data is compressed or encoded in some unknown manner, which makes sense since there's likely to be a lot of redundancy in the original measurements. Its possibly a raw chunk of gzipped data, but any of a number of compression algorithms might have been used. Have you asked the vendor?" CreationDate="2016-02-17T12:59:05.813" UserId="471" />
  <row Id="10184" PostId="10283" Score="0" Text="Thanks Samuel, could you give some more details when you're saying &quot;one of the regression methods&quot; ? It would be helpfull for later search." CreationDate="2016-02-17T16:17:58.347" UserId="16337" />
  <row Id="10185" PostId="10274" Score="0" Text="Try computing histograms for each cluster... good luck:  I don't think k-means works well on such data." CreationDate="2016-02-17T16:19:55.710" UserId="924" />
  <row Id="10186" PostId="10261" Score="0" Text="I would start with Box-Jenkins seasonal modeling (an appropriate stationarity transformation, which you can do in a Spark dataframe, and an ARIMA model)." CreationDate="2016-02-17T16:37:04.600" UserId="1077" />
  <row Id="10187" PostId="10232" Score="0" Text="Hi @Spacedman thanks for your reply. I emailed the vendor but didn't get a response yet. I am still hoping that they just 'cropped' the data to the sensor area where the foot actually was. In the table 'Foot' there are columns AreaWidth, AreaHeight and FrameCount but these values multiplied give &gt;4 times the expected number of data points." CreationDate="2016-02-17T16:57:12.357" UserId="16249" />
  <row Id="10188" PostId="8792" Score="0" Text="See also http://stats.stackexchange.com/questions/52104/multinomial-logistic-regression-vs-one-vs-rest-binary-logistic-regression" CreationDate="2016-02-17T20:17:13.963" UserId="21" />
  <row Id="10189" PostId="5715" Score="0" Text="I think this is better on Stack Overflow, but it's too old to migrate" CreationDate="2016-02-17T20:18:09.473" UserId="21" />
  <row Id="10190" PostId="1069" Score="1" Text="@MartinThoma https://en.wikipedia.org/wiki/Cohen%27s_kappa" CreationDate="2016-02-17T20:20:33.760" UserId="21" />
  <row Id="10191" PostId="10277" Score="0" Text="Welcome to the site! What does Y represent, what are some salient p's and q's, and how many are there of each? Give us a flavor of the data." CreationDate="2016-02-17T21:09:12.390" UserId="381" />
  <row Id="10194" PostId="10255" Score="0" Text="Is the work item type a categorical variable with a fixed number of categories? Does the list of valid categories vary over time?" CreationDate="2016-02-18T01:49:32.800" UserId="381" />
  <row Id="10195" PostId="10296" Score="0" Text="Look at Watson analytics. Bring data and does supervised learning. Is this what you looking for?" CreationDate="2016-02-18T02:20:19.743" UserId="8752" />
  <row Id="10196" PostId="10255" Score="0" Text="Hi Emre,Thanks for the query,the Work_item_Type is a numerical value which signifies the count of Work_items received for that particular day.I am sorry for putting up a misleading variable name!" CreationDate="2016-02-18T03:49:46.537" UserId="13515" />
  <row Id="10197" PostId="10255" Score="0" Text="If you believe the response variable really depends on nothing but the date, then you have no choice but to create derived variables (extracted features) and use them with an appropriate regression model: Poisson. I would suggest using one variable to denote &quot;days since start&quot;, one binary variable to indicate weekday/weekend, one categorical variable for the day of the week, another for week of the month, and another for the month of the year (if the time series is long enough)." CreationDate="2016-02-18T04:47:36.970" UserId="381" />
  <row Id="10198" PostId="10299" Score="0" Text="Do you understand matrix completion through factorization _without_ the non-negative part?" CreationDate="2016-02-18T05:02:03.187" UserId="381" />
  <row Id="10199" PostId="1027" Score="0" Text="This doesn't make any sense." CreationDate="2016-02-18T05:20:26.060" UserId="9123" />
  <row Id="10200" PostId="10299" Score="1" Text="There are a quantity of ressources, more or less complicated. Please tell us what you have found, what lacks for your understanding. In other words, can you factorize &quot;useful&quot; in a non-negative way?" CreationDate="2016-02-18T06:39:52.403" UserId="12527" />
  <row Id="10202" PostId="10159" Score="0" Text="From my experience, different people tend to mean a broad range of different things when they use the term &quot;data analysis&quot;. You already hinted at your particular interest (&quot;I'm very much focused on tabular data.&quot;), and you might benefit from further narrowing down which particular meaning of &quot;data analysis&quot; you have in mind." CreationDate="2016-02-18T07:57:06.280" UserId="13213" />
  <row Id="10203" PostId="10285" Score="1" Text="Thanks for mind stimulating answer, I will go through your suggestions." CreationDate="2016-02-18T09:23:40.577" UserId="16195" />
  <row Id="10204" PostId="8441" Score="0" Text="Hi, I love your answer but could you please fix the link to the paper?" CreationDate="2016-02-18T09:53:44.823" UserId="15656" />
  <row Id="10205" PostId="10290" Score="0" Text="Thank you very much! It's a nice idea, but I don't think that could be a general solution to the problem. The output in this case should follow a Poisson distribution and I don't think that in this sense account transactions always respect [all the assumptions of the Poisson distribution](https://en.wikipedia.org/wiki/Poisson_distribution#Assumptions:_When_is_the_Poisson_distribution_an_appropriate_model.3F)" CreationDate="2016-02-18T10:16:51.560" UserId="848" />
  <row Id="10206" PostId="8441" Score="0" Text="@padura Fixed. Please take a read. It was a good read for data science." CreationDate="2016-02-18T10:28:14.037" UserId="9123" />
  <row Id="10207" PostId="8441" Score="0" Text="(+1) Great answer.  Loved the paper too :)" CreationDate="2016-02-18T10:32:43.490" UserId="11097" />
  <row Id="10208" PostId="2315" Score="1" Text="That's definitely what Leo Breiman and the theory says, but empirically it seems like they definitely do overfit. For example I currently have a model with 10-fold CV MSE of 0.02 but when measured against the ground truth the CV MSE is .4. OTOH if I reduce tree depth and tree number the model performance improves significantly." CreationDate="2016-02-18T14:41:48.437" UserId="2723" />
  <row Id="10209" PostId="8786" Score="0" Text="Are you sure scikit-learn is a DN package?" CreationDate="2016-02-18T15:02:10.850" UserId="9123" />
  <row Id="10210" PostId="10290" Score="0" Text="All models are wrong; some are useful. [Household size and the poisson distribution](http://www.jstor.org/stable/41110478)" CreationDate="2016-02-18T16:13:06.663" UserId="381" />
  <row Id="10211" PostId="9208" Score="1" Text="The comparisons in that link you provided were very informative" CreationDate="2016-02-18T16:58:30.867" UserId="12515" />
  <row Id="10212" PostId="8441" Score="0" Text="Thats a great article, shame that I didn't know this at the time I was working on my own chess engine ;)&#xA;I wonder if we can improve by combining with this technique: http://www.bjmc.lu.lv/fileadmin/user_upload/lu_portal/projekti/bjmc/Contents/770_7.pdf" CreationDate="2016-02-18T17:39:21.373" UserId="15656" />
  <row Id="10213" PostId="10275" Score="0" Text="I didn't find the required datasets in US census." CreationDate="2016-02-18T17:52:25.383" UserId="16251" />
  <row Id="10214" PostId="10310" Score="1" Text="By &quot;heavily overlapped data points of different labels&quot;, do you mean &quot;data sets with linearly correlated features&quot;?" CreationDate="2016-02-18T22:37:05.057" UserId="9420" />
  <row Id="10215" PostId="10310" Score="0" Text="What about tackling this problem from a feature engineering point of view? Can you design/create new features?" CreationDate="2016-02-19T09:04:39.560" UserId="2576" />
  <row Id="10217" PostId="10317" Score="0" Text="Welcome to the site!  Which system are you using?  Maybe the size of the data is the reason. But, I might be wrong too :)  (Let's wait for a good answer!)" CreationDate="2016-02-19T12:33:54.623" UserId="11097" />
  <row Id="10218" PostId="10317" Score="0" Text="PC with 8 GB RAM. Size of the data 230 kb CSV file for 3498 rows and 17 columns. I do not think size is the reason but could be. Let me see if someone has a solution here." CreationDate="2016-02-19T12:35:35.003" UserId="16401" />
  <row Id="10219" PostId="9345" Score="0" Text="You could probability at most $\alpha / 2$ from both sides. It should be $\arg \max$ instead of $\sup$." CreationDate="2016-02-19T12:44:14.857" UserId="6550" />
  <row Id="10220" PostId="2315" Score="2" Text="If you reduce the tree depth is a different case because you are adding regularisation, which will decrease the overfitting. &#xA;Try to plot the MSE when you increase the number of trees while keeping the rest of parameters unchanged. So, you have MSE in the y-axis and num_tress in the x-axis. You will see that when adding more trees, the error decreases fast, and then it has a plateau; but it will never increase." CreationDate="2016-02-19T13:43:12.200" UserId="4719" />
  <row Id="10223" PostId="10304" Score="0" Text="Thanks for the answer. However, I still have a doubt.&#xA;So if I give _origin (latitude-longitude)_ I get _TimestampA_ and give _destination (latitude-longitude)_ I get _TimestampB_. I just have to take difference between _TimestampB_ and _TimestampA_ which will be predicted ETA right ?&#xA;And I also want to add third parameter which is time when the current data was recorded e.g 16.00 hrs or 18.00 hrs (for considering traffic conditions). Will this model solve the problem of considering traffic conditions ?" CreationDate="2016-02-19T15:00:48.473" UserId="15906" />
  <row Id="10224" PostId="8834" Score="0" Text="In this and most other cases the Markov chain obtained by Gibbs sampling has a stationary distribution which is P. Please see the Wikipedia article about Gibbs sampling." CreationDate="2016-02-19T15:15:42.950" UserId="6550" />
  <row Id="10225" PostId="10313" Score="0" Text="Many thanks, Rapaio. I tried this with different types of models, however I had the same problem with MSE. RF seems to work best for this particular use case. The reasons that I'm doing stacking are 2-fold -- one is that there's features which I can only create in R or Python and the is that while these learners are both in the RF family, they are very different implementations and have different parameters. I can afford to run a much bigger forest in R than in Python, for instance. Thus they are different learners, just in the same general family of models." CreationDate="2016-02-19T15:28:28.313" UserId="2723" />
  <row Id="10226" PostId="10317" Score="0" Text="The same question was also asked on [stackoverflow](http://stackoverflow.com/questions/35505040/matlab-code-crashing-for-higher-number-of-csv-records)" CreationDate="2016-02-19T15:33:25.647" UserId="14906" />
  <row Id="10227" PostId="10319" Score="0" Text="That makes sense. Thanks, Mark!" CreationDate="2016-02-19T15:37:52.357" UserId="16404" />
  <row Id="10228" PostId="10319" Score="1" Text="No Problem!  On a side note if you want to get a test error, you should create the PCA rotations on the x_original_training, then use those same rotations on x_original_test.  If you split up your data into train/test after you transform everything, then you're using data from your training set to influence your test set before the prediction occurs, which will make your test error overly-optimistic." CreationDate="2016-02-19T15:48:16.657" UserId="14913" />
  <row Id="10229" PostId="10317" Score="0" Text="Pl don't cross-post.  Thanks for pointing it out @hbaderts. Loved your comments underneath both the qns  :D" CreationDate="2016-02-19T16:16:23.953" UserId="11097" />
  <row Id="10230" PostId="10308" Score="0" Text="Thanks, looks like API for model buildling (`sklearn.py`) is a little incomplete for the Python package." CreationDate="2016-02-19T16:26:30.003" UserId="588" />
  <row Id="10231" PostId="10320" Score="1" Text="[This](http://stackoverflow.com/a/28232401/4993513) might also help you if you want to plot in matplotlib. Else, @Emre's answer should be good to go :)" CreationDate="2016-02-19T16:39:16.240" UserId="11097" />
  <row Id="10232" PostId="10317" Score="0" Text="Sure, @Dawny33. I really don't mind cross-posts - it is sometimes difficult to find good answers to your questions, so exposing it to two communities might just help to get the right answer. But please link the cross-posts in both questions, so one can first check the other question and does not do the work twice. Maybe even comments from one site can help a user from the other site spot the problem." CreationDate="2016-02-19T18:00:38.940" UserId="14906" />
  <row Id="10233" PostId="10304" Score="0" Text="For the origin, you don't have to compute anything. You can indeed compute how long it will take to reach destination by subtracting the timestamps. Given that you have sufficient data for high-traffic times, the model might be able to capture that." CreationDate="2016-02-19T23:21:00.003" UserId="16366" />
  <row Id="10234" PostId="10294" Score="0" Text="Ah, rank-aggregation. I knew there was a term for it" CreationDate="2016-02-20T01:01:17.277" UserId="13165" />
  <row Id="10235" PostId="10303" Score="0" Text="That's def interesting but it doesn't give me the rank order I'm seeking." CreationDate="2016-02-20T01:01:54.247" UserId="13165" />
  <row Id="10236" PostId="10259" Score="0" Text="Thank you so much!!" CreationDate="2016-02-20T01:12:33.347" UserId="9663" />
  <row Id="10239" PostId="10329" Score="2" Text="What tool? what laptop? what problem? what data size? what's reasonable? This is far too underspecified" CreationDate="2016-02-20T09:28:25.197" UserId="21" />
  <row Id="10241" PostId="10328" Score="1" Text="I don't think it &quot;requires&quot; fuzzy c-means. But as there is &quot;noise&quot; in such data, it may still work better than ordinary k-means." CreationDate="2016-02-20T09:53:59.043" UserId="924" />
  <row Id="10242" PostId="10330" Score="0" Text="Welcome to the site. What do you mean by content quality? Emails, logs and irc chats are different in their own ways!" CreationDate="2016-02-20T12:01:31.463" UserId="11097" />
  <row Id="10244" PostId="10330" Score="0" Text="@Dawny33 content quality - i mean to say that the preprocessed corpus is more suitable for mining with topic models than the raw one." CreationDate="2016-02-20T17:10:27.177" UserId="16427" />
  <row Id="10245" PostId="10330" Score="0" Text="i am aware of perplexity. i dont want to use this metric as it only evaluates the topic model performance and not the corpus." CreationDate="2016-02-20T17:12:39.200" UserId="16427" />
  <row Id="10246" PostId="10331" Score="0" Text="Please clarify. Are you trying to cluster the regions according to this data? So for each region you have a 52-dimensional vector describing it. &#xA;&#xA;What function did you try in R?" CreationDate="2016-02-20T17:15:50.843" UserId="13505" />
  <row Id="10247" PostId="10330" Score="0" Text="Thanks for clarifying. I have edited your question to reflect the intent better. Let's wait for a good answer!  :)" CreationDate="2016-02-20T17:16:19.320" UserId="11097" />
  <row Id="10248" PostId="10275" Score="1" Text="Take a look at the question http://stats.stackexchange.com/questions/197238/synthetic-datasets-for-concept-drifting-data It is about concept drifting but the datasets (partitioned by time) might be helpful for you too." CreationDate="2016-02-21T07:15:39.723" UserId="13727" />
  <row Id="10249" PostId="10275" Score="0" Text="Thanks Dan, The datasets that are in the question you suggested are not numeric data and are not in column format." CreationDate="2016-02-21T07:42:06.083" UserId="16251" />
  <row Id="10250" PostId="313" Score="0" Text="I can't add a comment yet, but just FYI ESL is [available for free online as a pdf](http://statweb.stanford.edu/~tibs/ElemStatLearn/)" CreationDate="2014-06-20T20:44:39.280" UserId="426" />
  <row Id="10253" PostId="10177" Score="1" Text="I expect that you get [bread, butter, milk, cereals, eggs and tomatoes] or something relatively similar depending on a country and a region, am I right?" CreationDate="2016-02-21T13:14:20.257" UserId="12632" />
  <row Id="10254" PostId="10339" Score="1" Text="For understanding that, you must understand what a drop out mean.  [This](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf) would help you with that :)" CreationDate="2016-02-21T15:51:27.047" UserId="11097" />
  <row Id="10255" PostId="10329" Score="0" Text="I think the edit has improved the question. One thing: There is no standard &quot;professional in the field&quot; that uses deep learning techniques. What a professional *should know* is therefore not well defined." CreationDate="2016-02-21T18:23:00.323" UserId="836" />
  <row Id="10256" PostId="10341" Score="1" Text="If your R has density, https://en.wikipedia.org/wiki/Order_statistic tells you how to get the density of the k-th order statistic. You can then integrate and sum the first c values. k-th order statistic is simply the k-th smallest element in the sample, so you can work with -R instead." CreationDate="2016-02-21T18:28:30.917" UserId="6550" />
  <row Id="10257" PostId="10329" Score="1" Text="I know the question isn't well defined in the sense that it has a specific correct answer, but I think it is phrased well enough so that experienced people can provide their insights and &quot;sketch&quot; the boundaries of what I can expect to achieve with the above mentioned laptop" CreationDate="2016-02-21T19:40:43.500" UserId="16424" />
  <row Id="10258" PostId="10343" Score="0" Text="No harm in trying both and seeing what it looks like and how it affects your modeling." CreationDate="2016-02-21T21:18:03.590" UserId="14913" />
  <row Id="10259" PostId="10343" Score="0" Text="Would an extreme like -9999 skew the scaling or would it still retain some evidence for the odel to know it is a missing value?" CreationDate="2016-02-21T21:28:08.060" UserId="16452" />
  <row Id="10260" PostId="10341" Score="0" Text="What would the c-th smallest element in the R distribution tell me? I think the problem is that c is referring to work units/tasks, where r from R is referring to rewards so the scales are different? Maybe I'm unsure of what the comment is about. Sorry." CreationDate="2016-02-21T22:38:13.520" UserId="16449" />
  <row Id="10262" PostId="9748" Score="0" Text="HI, can I use the regression model to predict the upvotes only with the comments (such as the interarrival time of the time series data)? But I have no idea about the clearly steps to build the model? Woud you mind give me some suggesion, please? Thanks a lot." CreationDate="2016-02-22T14:18:12.963" UserId="15261" />
  <row Id="10263" PostId="10255" Score="0" Text="hi @Emre ,I have tried your suggestions and have added a new question on the model performance here [link](http://datascience.stackexchange.com/questions/10349/analyze-performance-poisson-regression-model-on-a-time-seriescount-forecasting).Thanks for the suggestions." CreationDate="2016-02-22T17:07:27.610" UserId="13515" />
  <row Id="10264" PostId="10301" Score="0" Text="This is not very clear, and I don't understand how anyone can try and answer it. Where's the origin coordinates? Where's the destination coordinates? What do the timestamps mean? Why are there three lines in your sample data and only `_1` and `_2` in your changed form? What happened to the third line?" CreationDate="2016-02-22T18:20:47.187" UserId="471" />
  <row Id="10265" PostId="10304" Score="0" Text="I don't see how that helps. You can be at a specific lat long at different timestamps. How does learning the relation between the coordinates and timestamp help in any way?" CreationDate="2016-02-22T18:33:15.790" UserId="8113" />
  <row Id="10266" PostId="10347" Score="0" Text="If you think that making your time series regular (fixed time intervals) will make things easier, maybe a solution would be to discretize time (e.g., number of passes in an intersection during a 1 minut window). Then you'll be able to create a new time series with a sample every second. Now you can start experimenting different methods such as ARIMA and such. Another option is some sort of a dynamic bayesian network where neighbors traffic at time $t-1$ affect a specific intersection at time $t$" CreationDate="2016-02-22T18:44:16.283" UserId="8113" />
  <row Id="10267" PostId="10341" Score="0" Text="How about tackling this with discrete event simulation? You could simulate the entire process, run it and calculate different statistics based on the output. It's very common to use discrete events simulation for operation research problems such as this one." CreationDate="2016-02-22T18:48:02.760" UserId="8113" />
  <row Id="10268" PostId="10319" Score="0" Text="You can also check out supervised methods, such as LDA (Linear Discriminant Analysis)" CreationDate="2016-02-22T18:50:41.767" UserId="8113" />
  <row Id="10269" PostId="10339" Score="0" Text="Care to help explain drop out" CreationDate="2016-02-22T19:06:55.373" UserId="15408" />
  <row Id="10270" PostId="9710" Score="0" Text="If you still need an answer, could you provide some update and describe your data?" CreationDate="2016-02-22T20:54:43.280" UserId="15429" />
  <row Id="10271" PostId="10351" Score="0" Text="Thanks a lot @Miller. Can you please explain how can I find the relevant 'banks' in the dataset? I mean different 'organizations' that behaves differently (i.e. has different models). Can you please explain how can I find the criteria (1) to (4) mentioned in my question (what is the target - price?) Did someone build a different decision tree with this database for each 'orgaznization'?" CreationDate="2016-02-22T21:31:25.407" UserId="16251" />
  <row Id="10272" PostId="10349" Score="0" Text="How did you create your variables? Is it treating your categorical variables (day of week, for example) as a multi-level factor or as a numeric linear term? The fact you only get one coefficient for day_of_week makes me think the latter. Do you really expect the count to be linear during a week? You should probably do some basic reading of GLMs..." CreationDate="2016-02-22T22:52:11.793" UserId="471" />
  <row Id="10273" PostId="10341" Score="0" Text="Well,  I've already created a simulation, but  I'm currently trying to create an analytical model to create a stronger understanding of the overall system." CreationDate="2016-02-23T00:08:54.543" UserId="16449" />
  <row Id="10274" PostId="10353" Score="0" Text="If there are no interactions, you could choose the top n arms. Otherwise you have you define the quality of the set. And then you run into a combinatorial optimization problem." CreationDate="2016-02-23T03:03:11.523" UserId="381" />
  <row Id="10275" PostId="10301" Score="0" Text="Instead of trying to &quot;learn&quot; the relation between lat1,lat2,long1 and long2, just calculate the distance between them. Use Haversine instead of Euclidean as our world is not flat..." CreationDate="2016-02-23T05:13:53.060" UserId="8113" />
  <row Id="10277" PostId="10353" Score="0" Text="Thanks Emre, taking top n arms will probably work to some extent. What I am after is how to design efficient learning algorithm when you don't show just one ad but many at the same time. There is obviously a very strong interaction because click on one ad steals it from the others. It appears to me to be a very common problem for online shops etc. so I thought there will be some work done around exactly this type of the problem already. It is more of a practical concern where you don't want to build yet another web shop just to test the ctr of your ads one by one." CreationDate="2016-02-23T10:00:23.310" UserId="15361" />
  <row Id="10279" PostId="10347" Score="0" Text="You mean moving my 1 min summation by 1 second steps? I guess that'd preserve most of the information and work with ARIMA. I don't really get the DBN option yet, but I'll look into that further. Thanks" CreationDate="2016-02-23T11:14:39.357" UserId="16199" />
  <row Id="10280" PostId="10330" Score="0" Text="You asked exactly the same question here: http://stats.stackexchange.com/questions/197852/what-metrics-must-i-use-in-my-dataunstructured-preprocessing-research" CreationDate="2016-02-23T13:33:14.863" UserId="10169" />
  <row Id="10281" PostId="8181" Score="0" Text="I believe you meant &quot;syntactic&quot; not &quot;synthetic." CreationDate="2016-02-23T16:52:40.447" UserId="7848" />
  <row Id="10282" PostId="10353" Score="1" Text="If n is fixed: [Multi-armed bandit problems with multiple plays and switching cost](http://web.eecs.umich.edu/~teneket/pubs/MABWithSwitchingCosts-1990.pdf)" CreationDate="2016-02-23T16:58:29.943" UserId="381" />
  <row Id="10283" PostId="10330" Score="0" Text="@jknappen the question was not answered in data science.... which forced me to requestion it in cross validated....still i couldn get a better answer...." CreationDate="2016-02-23T17:31:11.023" UserId="16427" />
  <row Id="10284" PostId="10359" Score="0" Text="I use simple Q-learning but I guess articles about deep Q-learning would also be useful, not for my current work however." CreationDate="2016-02-23T17:42:28.990" UserId="15197" />
  <row Id="10285" PostId="10359" Score="0" Text="In my opinion, although the papers are about deep q-learning the concepts are applicable to what you are doing: different agents collecting trajectories in parallel and sharing and exchanging the network parameters. Definitely can serve as inspiration." CreationDate="2016-02-23T18:28:08.527" UserId="5041" />
  <row Id="10286" PostId="10364" Score="0" Text="But , for large data sets this would be considerably slow , wouldn't? There is no built-in function on pandas for doing it?" CreationDate="2016-02-23T19:05:31.123" UserId="16096" />
  <row Id="10287" PostId="10364" Score="1" Text="These are built in functions :) It should be as fast as anything else you can do in pandas? If the data is in a file you could preprocess it with a command line utility. You should specify what your input is; file, db, etc." CreationDate="2016-02-23T19:07:47.983" UserId="381" />
  <row Id="10288" PostId="10364" Score="0" Text="Oh sorry , Itoght it was RE. And it seems that some of data the cents are separated with , rather then . and is returning the fallowing error 'ValueError: invalid literal for float(): 300,00 ' , how to handle it?&#xA;&#xA;btw, the input is a file" CreationDate="2016-02-23T19:12:20.147" UserId="16096" />
  <row Id="10289" PostId="10360" Score="0" Text="Could you provide some dummy data and the expected result?" CreationDate="2016-02-23T19:28:54.767" UserId="8113" />
  <row Id="10290" PostId="10359" Score="0" Text="Ok, I'll check them. Thanks." CreationDate="2016-02-23T20:52:53.783" UserId="15197" />
  <row Id="10291" PostId="10365" Score="0" Text="Yes, however I don't get `b.key` as part of my resultant. I understand my query has a where clause, however if I want to filter for other keys on Table2, I would not be able to. Basically the df should have key from table 1, key from table 2, other fields. If Key from table 2 (key2) does not exist in Key from table 1, that row could be `NA` for key2" CreationDate="2016-02-23T21:06:26.323" UserId="10345" />
  <row Id="10292" PostId="10360" Score="0" Text="@omri374  example added" CreationDate="2016-02-23T21:39:57.370" UserId="10345" />
  <row Id="10293" PostId="10365" Score="0" Text="You don't need to add a WHERE clause, since using a LEFT JOIN instead of an INNER JOIN adds all the rows in df1 that don't have matching keys in df2." CreationDate="2016-02-23T21:43:57.430" UserId="8113" />
  <row Id="10294" PostId="10360" Score="0" Text="see my updated answer" CreationDate="2016-02-23T21:44:20.240" UserId="8113" />
  <row Id="10295" PostId="10365" Score="0" Text="Correct! That would give me the all records in Table1 and matching records in Table2. Your edit brings me closer to what I am expecting. Is there a way to get the key from table2 in the resultant df?" CreationDate="2016-02-23T21:46:26.307" UserId="10345" />
  <row Id="10296" PostId="10365" Score="0" Text="on second thought, I can still work through with this result. Even though this is not ideally what would help me, having `BV2 as NA` can be used as a proxy for `Key2 being NA`." CreationDate="2016-02-23T21:56:18.730" UserId="10345" />
  <row Id="10297" PostId="10364" Score="0" Text="Something like `str.replace('^[(^\d]*(\d+)[.,](\d+)', '\1.\2')`" CreationDate="2016-02-23T22:59:47.607" UserId="381" />
  <row Id="10298" PostId="5443" Score="0" Text="Regardless of years specified, I'd expect a Pro/Con list of at least three tools from a data scientist. They need to show capacity to investigate, weight options, and communicate resolutions. Even, or especially, in an interview, I'd expect to see real engagement and a capacity to expand past a potentially great, but currently lacking, interview question." CreationDate="2016-02-23T23:07:38.293" UserId="13578" />
  <row Id="10302" PostId="10368" Score="0" Text="Pl include a link to the documentation in the qn, please :)  Would be helpful for the users!" CreationDate="2016-02-24T04:50:29.443" UserId="11097" />
  <row Id="10303" PostId="10368" Score="1" Text="@Dawny33 Link added." CreationDate="2016-02-24T04:57:09.440" UserId="13518" />
  <row Id="10304" PostId="10369" Score="0" Text="See https://en.wikipedia.org/wiki/Feature_selection" CreationDate="2016-02-24T07:04:51.750" UserId="13727" />
  <row Id="10306" PostId="10369" Score="0" Text="I assume you are looking to apply unsupervised learning, right? Do you have any labels in your data set?" CreationDate="2016-02-24T09:18:51.400" UserId="5041" />
  <row Id="10307" PostId="10362" Score="0" Text="Thank you this is very useful, that makes a lot of sense! Just to clarify, once the system has been trained, would you simply feed each possible action with the current input states to the neural network and select the action which estimates the largest reward (q-value)?" CreationDate="2016-02-24T10:42:27.223" UserId="5144" />
  <row Id="10308" PostId="10354" Score="0" Text="Thanks a lot for your reply. You do seem be talking about sentence-level sentiment analysis here, while I need it at  aspect-level, am I correct in this assessment? I'm talking about extraction of individual topics or aspects of topics from a sentence and their sentiments, and not just whether a sentence is positive or negative-sounding." CreationDate="2016-02-24T10:58:54.807" UserId="16443" />
  <row Id="10309" PostId="10362" Score="1" Text="Correct. However, keep in mind that if you environment is not stationary you may want to keep some exploration going on. In general, Q-learning runs online and you continuously train." CreationDate="2016-02-24T11:09:24.927" UserId="5041" />
  <row Id="10310" PostId="10362" Score="0" Text="Great, are you aware of any literature surrounding your answer? I guess an alternative to online training would be to retrain on newly observed data every n time periods in order to not let failed exploration affect performance in a production environment." CreationDate="2016-02-24T13:04:55.177" UserId="5144" />
  <row Id="10311" PostId="10362" Score="1" Text="I am basically referring to the exploration-exploitation dilemma. There many approaches.. An easy to apply one is here: http://www.tokic.com/www/tokicm/publikationen/papers/AdaptiveEpsilonGreedyExploration.pdf where you adaptively adjust epsilon based on the error of your value function" CreationDate="2016-02-24T13:21:21.350" UserId="5041" />
  <row Id="10312" PostId="10362" Score="0" Text="That is a good reference paper thanks, but i was referring to your answer more generally wrt using neural networks to approximate the q-values. I am wondering how it might be best to represent state in the network - should this just be a set of inputs for each state with 0-1 representation?" CreationDate="2016-02-24T14:17:34.143" UserId="5144" />
  <row Id="10313" PostId="1189" Score="0" Text="If the answer was useful, would you mind accepting it?" CreationDate="2016-02-24T14:18:25.993" UserId="5041" />
  <row Id="10314" PostId="10362" Score="1" Text="A classic are this paper / book (1996): http://web.mit.edu/people/dimitrib/NDP_Encycl.pdf. http://www.athenasc.com/ndpbook.html Nowadays there is a lot of work combining Deep Neural Networks and RL. Deepmind papers are a good example: https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf" CreationDate="2016-02-24T14:29:20.320" UserId="5041" />
  <row Id="10316" PostId="10362" Score="0" Text="Excellent, thank you @mtk99" CreationDate="2016-02-24T15:47:49.950" UserId="5144" />
  <row Id="10317" PostId="10354" Score="0" Text="@SimonGray: I guess by aspect-level, you mean the semantic meaning in the text. if that is the case, then the model which I have suggested is the right one for it. The results are amazing.&#xA;PS: im working on emotion detection in sentences using that model." CreationDate="2016-02-24T16:05:20.473" UserId="16024" />
  <row Id="10318" PostId="10372" Score="0" Text="Thanks a ton!! I got the answer for it. I was so stupid, It worked when i created  unique Date and time values and then full outer Join showed its magic :).." CreationDate="2016-02-24T16:41:22.873" UserId="13034" />
  <row Id="10319" PostId="10370" Score="0" Text="I like the idea of using T-SNE to visualise the data set and draw conclusions. T-SNE main purpose is visualization and in particular with respect to the crowding problem. However, it is not good for general dimensionality reduction. Have a look at van der Maaten FAQ when he discusses embedding in more than 2D https://lvdmaaten.github.io/tsne/" CreationDate="2016-02-24T17:28:05.163" UserId="5041" />
  <row Id="10320" PostId="10370" Score="0" Text="@mtk99: Yes, that is true. Since the question is towards finding the important features, the best way is visualisation. when the dimensionality is reduced to more than 2D, it is hard to plot them. So, if you just want to reduce the dimensionality I cannot say TSNE is a good approach. But if you ask me to visualise the primary features, better to reduce it to 2D via TSNE and plot them." CreationDate="2016-02-24T17:37:31.620" UserId="16024" />
  <row Id="10321" PostId="10372" Score="0" Text="Why don't you include your solution for the benefit of others?" CreationDate="2016-02-24T17:57:56.563" UserId="16284" />
  <row Id="10324" PostId="10377" Score="0" Text="So, what is the question here?" CreationDate="2016-02-25T02:08:47.400" UserId="11097" />
  <row Id="10325" PostId="10377" Score="0" Text="@ Dawny, I'm thinking something is wrong. How come the validation root MSE could always beat the training?" CreationDate="2016-02-25T02:22:29.327" UserId="12867" />
  <row Id="10326" PostId="10385" Score="0" Text="Welcome to the site!  Do you want a solution on in the clustering approach?  As I see it, a basic collaborative filtering algo. would do better on this data :)" CreationDate="2016-02-25T05:55:41.117" UserId="11097" />
  <row Id="10327" PostId="10385" Score="0" Text="@Dawny33 you are right collaborative filtering will work better. But, is there any knowledge I can extract using cluster.  I am asking because as part of the project, I am applying various Machine learning techniques and determine their applications." CreationDate="2016-02-25T06:23:11.810" UserId="16540" />
  <row Id="10328" PostId="10387" Score="0" Text="I disagree: trying k-means is a waste of time if it does not help your objective..." CreationDate="2016-02-25T10:15:30.167" UserId="924" />
  <row Id="10329" PostId="10387" Score="0" Text="Ok, just edited the comment. What I mean is that it can be helpful to visualize and understand the data a bit more. Definitely it won't help directly to a retrieval task; however, it is always useful to try to understand if the data is giving you hints of clear groupings or  not." CreationDate="2016-02-25T10:52:13.790" UserId="5143" />
  <row Id="10330" PostId="10389" Score="0" Text="There's a great intuitive + mathematical answer available here (see &quot;1. Achieving perfect separation&quot;): http://stats.stackexchange.com/questions/80398/how-can-svm-find-an-infinite-feature-space-where-linear-separation-is-always-p" CreationDate="2016-02-25T11:34:26.957" UserId="676" />
  <row Id="10331" PostId="10384" Score="0" Text="Yes, thanks! I'm wondering what it is one looks for to know know whether data has AR or MA terms, other than just model selection metrics like AIC." CreationDate="2016-02-25T11:37:20.977" UserId="13413" />
  <row Id="10332" PostId="10354" Score="0" Text="what I need is sentiment expressed towards entities in the sentences, not whether or not the sentences themselves are positive or negative, is that what you mean by semantic meaning in the text? I need to be able to trace the sentiment to a specific target." CreationDate="2016-02-25T13:02:41.363" UserId="16443" />
  <row Id="10333" PostId="10384" Score="1" Text="I tried to more fully answer your question. If I succeeded remember to accept it. :)" CreationDate="2016-02-25T16:23:06.503" UserId="16538" />
  <row Id="10334" PostId="10354" Score="0" Text="@SimonGray: No, semantic meaning doesn't mean just positive or negative. It means the contextual information which the sentence conveys, for example, emotion detection model will consider which emotion is embedded in the sentence." CreationDate="2016-02-25T16:43:23.087" UserId="16024" />
  <row Id="10335" PostId="10390" Score="0" Text="I think if you want to claim answers won't be opinion-based you should probably have stopped after your second sentence rather than gone on with your opinions for your specific case." CreationDate="2016-02-25T16:43:37.447" UserId="471" />
  <row Id="10336" PostId="10388" Score="0" Text="So should I run nearest neighbour search for each player to find similar players? I have over 2000 players." CreationDate="2016-02-25T17:15:28.163" UserId="16540" />
  <row Id="10337" PostId="10392" Score="1" Text="Can you formalize your notion of completeness a little bit?" CreationDate="2016-02-25T17:16:23.857" UserId="381" />
  <row Id="10338" PostId="10388" Score="0" Text="Yes, that is the canonical approach. 2000 is tiny." CreationDate="2016-02-25T17:23:56.087" UserId="924" />
  <row Id="10339" PostId="10388" Score="0" Text="Okay Thanks. Could you elaborate on your comment &quot;k-means is based on similar object&quot;?" CreationDate="2016-02-25T17:30:59.970" UserId="16540" />
  <row Id="10340" PostId="10388" Score="0" Text="k-means computes a lot of similarities, too. If you have 20 clusters and 100 iterations until converge, it is as expensive as comparing every player to every other player in your case. Except that k-means only work woth squared deviations, but you may need to choose a more advanced similarity." CreationDate="2016-02-25T17:36:04.170" UserId="924" />
  <row Id="10341" PostId="10384" Score="0" Text="That's what I was looking for! Thanks!" CreationDate="2016-02-25T18:39:03.827" UserId="13413" />
  <row Id="10342" PostId="5734" Score="2" Text="Good comment and it's also worth mentioning of Exponential Linear Units (ELUs) which can help to addres that issue in a better way: http://arxiv.org/abs/1511.07289" CreationDate="2016-02-25T19:56:10.017" UserId="15656" />
  <row Id="10343" PostId="5158" Score="0" Text="In SageMathCloud at http://cloud.sagemath.com, you can also start a Jupyter Notebook and select &quot;change kernel &gt; R&quot;." CreationDate="2016-02-25T20:55:01.080" UserId="16562" />
  <row Id="10344" PostId="10393" Score="0" Text="@Anony-Mouse **But then if there is no guarantee, does that mean that we can't use hard-margin and kernels on linearly inseparable data?** I know using soft-margin generalizes better and that in that case an imperfect separation is fine...I'm just wondering about the case when it is hard-margin." CreationDate="2016-02-26T00:29:09.257" UserId="11044" />
  <row Id="10345" PostId="10393" Score="0" Text="There exists data that is *impossible* to separate. Example: (0,0,classA), (0,0,classB). No kernel *ever* can separate these classes, because the data is identical. Thus: no guarantee of making data linearly separable is possible on such data sets." CreationDate="2016-02-26T00:36:19.300" UserId="924" />
  <row Id="10346" PostId="10400" Score="0" Text="You helped me get more clear about the idea. Any recommendation on real tutorials, examples or codes?" CreationDate="2016-02-26T00:58:50.187" UserId="16310" />
  <row Id="10347" PostId="10393" Score="0" Text="@Anony-Mouse Withstanding such examples (which is reasonable because it is a contradiction):  **But then if there is no guarantee, does that mean that we can't use hard-margin and kernels on linearly inseparable data?**" CreationDate="2016-02-26T06:48:59.130" UserId="11044" />
  <row Id="10348" PostId="10379" Score="0" Text="Hey @Ping, it would be great if you could add a link to the book. Thanks." CreationDate="2016-02-26T08:49:46.730" UserId="75" />
  <row Id="10349" PostId="10354" Score="0" Text="ok, but what about tracing sentiment to specific targets in the sentence? Are we still only at sentence-level here since you say &quot;embedded in the sentence&quot;? :-)" CreationDate="2016-02-26T09:26:37.480" UserId="16443" />
  <row Id="10350" PostId="10397" Score="0" Text="What do you mean when you say that the input variables are encoded? Are encoding structure or the specific values?" CreationDate="2016-02-26T10:32:25.427" UserId="5041" />
  <row Id="10351" PostId="10392" Score="0" Text="@emre I agree, what are you trying to measure exactly? What is your purpose? This paper discusses different definitions of completeness. http://www.sciencedirect.com/science/article/pii/S1532046413000853" CreationDate="2016-02-26T10:48:21.507" UserId="5041" />
  <row Id="10352" PostId="10409" Score="0" Text="When iterating, you can take advantage of symmetry: $d(V_1,V_2)=d(V_2,V_1)$. That results in half the calculations. You calculate $n*(n-1)$ distances, accumulate them in a vector" CreationDate="2016-02-26T16:31:13.437" UserId="5041" />
  <row Id="10353" PostId="10403" Score="0" Text="I would say the results of your experiment and invalid and you'll simply have to redo it. As you know, you must randomly choose a control/experiment group from a larger group, not use a specific criteria. There's no real way to get around the results of a badly designed experiment, sorry." CreationDate="2016-02-26T18:09:45.530" UserId="4710" />
  <row Id="10354" PostId="10411" Score="0" Text="Actually, I was looking for some methods which can ensure randomization on future data in A/B testing. Say if I am launching an experiment for a offer on a product, and I created two groups, control(A) and target(B). How do I make sure(or maybe, just ensure at best) that B does not consist of all non-buyers (or by random chance, all purchasers go into A). While preparing training/test data I can ensure this since its an offline process and I already have class distributions from each of groups." CreationDate="2016-02-26T18:26:28.810" UserId="16570" />
  <row Id="10355" PostId="10411" Score="1" Text="It seems like you are asking &quot;how does one ensure comparability between conditions/groups&quot;? You want to ensure that the same type of people are in both your control and test groups. The standard approach is random selection and random assignment to groups. Before running your experiment, you can always do a statistical test to ensure no differences between groups. On a experiment testing a diet drug, you would want to ensure that your test/control groups have the same starting weight, similar age ranges, etc. This gives you an opportunity to re-random assign before your actual experiment." CreationDate="2016-02-26T19:13:42.400" UserId="6478" />
  <row Id="10356" PostId="10411" Score="0" Text="When you eval a model you feed it existing labelled data (e.g.,  person did/didn't buy a product) &amp; have the model make its own predictions (for each person, predict the likelihood they will buy product). During eval you just compare how well the model's predictions compare to the &quot;ground truth&quot; established by labelled data. The idea of eval is to get an approx idea of how the model will perform when it is deployed and used to predict in-the-wild data. When the model actually get's deployed, the data in-the-wild is by definition un-labelled... its the job of the model to give this data labels." CreationDate="2016-02-26T19:16:20.910" UserId="6478" />
  <row Id="10357" PostId="10414" Score="0" Text="This is not correct for every possible metric" CreationDate="2016-02-26T20:36:31.273" UserId="5041" />
  <row Id="10358" PostId="10354" Score="0" Text="it depends on your recognition unit. If you wanna detect phrases, or even paragraph it works too, go with doc2vec model. [gensim](https://radimrehurek.com/gensim/models/doc2vec.html) offers nice python module for it. check it out." CreationDate="2016-02-26T21:51:23.353" UserId="16024" />
  <row Id="10360" PostId="6201" Score="2" Text="And where have you already searched? What projects have you already found? What do *you* mean by &quot;data science project&quot;, because I know loads and loads of **statistical** projects that use soccer data, but maybe they aren't whatever *you* call &quot;data science&quot;. Expand." CreationDate="2016-02-27T15:10:38.000" UserId="471" />
  <row Id="10361" PostId="10425" Score="0" Text="Interesting approach, particularly because its fast, however since its dependent on initialization it requires to be ran multiple times. I have found in literature references on Affinity Propagation. Do you know this method? Would you recommend it for this task? http://www.vision.jhu.edu/assets/ElhamifarNIPS12.pdf" CreationDate="2016-02-27T17:59:55.940" UserId="5143" />
  <row Id="10364" PostId="10429" Score="0" Text="How do you calculate the loss? On the training set?" CreationDate="2016-02-27T19:17:33.753" UserId="8820" />
  <row Id="10365" PostId="10425" Score="0" Text="I tried it (in ELKI, I think I tried all of them multiple times) but it did not work well for me. But every data is different." CreationDate="2016-02-27T19:35:17.377" UserId="924" />
  <row Id="10366" PostId="10429" Score="0" Text="@MartinThoma On the test set (not used to train)." CreationDate="2016-02-27T19:47:27.347" UserId="12377" />
  <row Id="10367" PostId="10431" Score="0" Text="I don't quite grasp yet how this deals with points that are being evaluated, but I'll have a look at the code, thanks!" CreationDate="2016-02-28T00:42:24.513" UserId="10907" />
  <row Id="10368" PostId="8119" Score="0" Text="In addition http://deepart.io/ seems to be a commercial venture based on the technique from your first link." CreationDate="2016-02-28T11:41:05.853" UserId="836" />
  <row Id="10369" PostId="10434" Score="0" Text="Yeah, makes sense. Thanks, that answers my question. I'll try both approaches." CreationDate="2016-02-28T11:54:45.437" UserId="13518" />
  <row Id="10370" PostId="10434" Score="0" Text="I don't have a lot of experience with this but I could imagine you could even do some kind of hybrid where you weigh the words depending on if they appear in the same sentence or in another sentence still within the window" CreationDate="2016-02-28T11:57:09.993" UserId="14904" />
  <row Id="10372" PostId="10394" Score="0" Text="What do you mean &quot;have to choose&quot;? Are you talking about your learning focus, what platform you're going to be adopting for your company, what software you're going to use for self-study or classes, or what? Makes a big difference." CreationDate="2016-02-28T18:16:50.993" UserId="16613" />
  <row Id="10373" PostId="10390" Score="2" Text="_Data Science_ is a super-broad field. I'd say that the top 3 in your list are essential, and wouldn't consider hiring someone without them. On the other hand, there are some companies where _Data Scientist_ means Computer Scientist who has specialized in cluster computing (Hadoop, etc), and knows basic statistics. A Data Scientist is going to be good at only a small subset of what Data Science includes. (Statistics, Programming, Consulting, Databases, Scientific Methods, etc.)" CreationDate="2016-02-28T18:41:01.300" UserId="16613" />
  <row Id="10374" PostId="10333" Score="1" Text="If you want to compare packages with similar functionality, compare `caret` and `mlr`. It's my impression that `mlr` is a second-generation answer that's been influenced by Python's SciKit-Learn. For whatever reason, I'veI never had much luck with `caret`, but haven't gotten to try `mlr`." CreationDate="2016-02-28T18:44:01.800" UserId="16613" />
  <row Id="10375" PostId="10369" Score="0" Text="@mtk99, by labels you mean headers, correct? I do have headers but they are very vague headers." CreationDate="2016-02-28T21:11:39.763" UserId="16511" />
  <row Id="10377" PostId="10370" Score="0" Text="@blueSerpent Thanks for the reply! Quick follow up question on the PCA part, so I understand the concept of using eigenvectors and eigenvalues to determine which dimension could be reduced with minimum information loss - but what is the next step after that? I'm reading up on a lot of PCA and I haven't found a source with clear steps to attribute selection." CreationDate="2016-02-29T07:30:12.330" UserId="16511" />
  <row Id="10378" PostId="10275" Score="1" Text="See this site on data sets http://opendata.stackexchange.com/" CreationDate="2016-02-29T07:38:45.277" UserId="13727" />
  <row Id="10379" PostId="10437" Score="0" Text="It still seems very slow, but what do you mean by regular linear regression? Comparing regular linear regression with L2 loss function to SGD methods is unfair, L2 loss function linear regression does not have to iterate, it has a closed form solution." CreationDate="2016-02-29T10:17:57.563" UserId="14904" />
  <row Id="10380" PostId="10430" Score="0" Text="A producer consumer model may be worth searching on" CreationDate="2016-02-29T11:07:16.150" UserId="13285" />
  <row Id="10387" PostId="10449" Score="1" Text="This not only depends on your RAM and CPU but also on the goal. What will you be doing with your sample? Are there rare events? How high is the dimension?" CreationDate="2016-02-29T16:55:26.100" UserId="14904" />
  <row Id="10390" PostId="10437" Score="0" Text="Have you excluded all other computational tasks like scaler etc. from your pipeline measurement loop?" CreationDate="2016-03-01T00:07:39.607" UserId="15361" />
  <row Id="10391" PostId="10456" Score="3" Text="I do not understand how this question related to data science." CreationDate="2016-03-01T04:11:32.887" UserId="75" />
  <row Id="10392" PostId="10456" Score="0" Text="well it is computational science and it is dealing with data" CreationDate="2016-03-01T04:18:13.057" UserId="16646" />
  <row Id="10394" PostId="10458" Score="0" Text="is this a machine vision approach? i need to build a algorithm not use a app" CreationDate="2016-03-01T05:14:18.030" UserId="16646" />
  <row Id="10395" PostId="10458" Score="0" Text="this links are of no help" CreationDate="2016-03-01T05:33:09.540" UserId="16646" />
  <row Id="10396" PostId="10456" Score="4" Text="Cross-posted on many other SE sites: http://programmers.stackexchange.com/q/311442/34181, http://dsp.stackexchange.com/q/29115/5874, http://datascience.stackexchange.com/q/10456/8560, http://cs.stackexchange.com/q/53834/755.&#xA;Please [do not post the same question on multiple sites](http://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted. If you don't get a satisfying answer after a week or so, feel free to flag for migration." CreationDate="2016-03-01T05:33:37.103" UserId="8560" />
  <row Id="10398" PostId="10458" Score="1" Text="Why is that? Please go into detail." CreationDate="2016-03-01T05:40:59.217" UserId="381" />
  <row Id="10399" PostId="10455" Score="1" Text="This sounds like a clustering or collaborative filtering problem. Which model to choose depends on your data. Think of it as a recommendation system problem: Which users are the most similar to user x?" CreationDate="2016-03-01T07:44:30.493" UserId="8113" />
  <row Id="10400" PostId="10451" Score="0" Text="When someone asks a question which you admit is tough to answer without more information, add a comment asking for more information rather than speculating and wasting your time." CreationDate="2016-03-01T07:44:37.293" UserId="471" />
  <row Id="10401" PostId="10459" Score="0" Text="https://stanford.edu/~mwaskom/software/seaborn/examples/many_pairwise_correlations.html" CreationDate="2016-03-01T08:27:03.413" UserId="381" />
  <row Id="10402" PostId="10437" Score="0" Text="Yes, I have excluded all other computational tasks. Also, it seems to work faster when I use only one split for flie reading - I am reading data from CSV.&#xA;&#xA;@JanvanderVegt I was comparing it with linear regression that I have written myself. It is using simple gradient descent. I honestly don't know how SGD works so maybe it is supposed to be this slow - but I feel like it is unlikely since 70,000k entries should be processed in 5 minutes for 100 steps." CreationDate="2016-03-01T11:25:29.143" UserId="16612" />
  <row Id="10403" PostId="10456" Score="1" Text="@jai Pl do not abuse anyone. I agree with the points put forward by d.w, and the  question is also off-topic. No, it does not really come under the domain of `computer vision`" CreationDate="2016-03-01T11:31:58.897" UserId="11097" />
  <row Id="10404" PostId="10458" Score="0" Text="@jai  As Emre has taken the efforts to refine his answer to fit your request (in your comment), pl explain him in detail why the papers have not help you. This would explain him in refining his answer further, if he can!" CreationDate="2016-03-01T11:48:35.200" UserId="11097" />
  <row Id="10405" PostId="10462" Score="0" Text="Welcome to the site. Do include what you learnt about the approach, and what is causing you confusion :)" CreationDate="2016-03-01T16:00:38.670" UserId="11097" />
  <row Id="10406" PostId="10458" Score="0" Text="@Dawny33 what papers? he directed me to a book store, to buy information, if i have to spend money to get a answer he might as well had directed me to a local university to pay for a degree or perhaps to a online freelancing company.  This is stackExchange you ask a question you get the answer with supporting evidence &quot;this is where the links comes into play&quot;  that was from one link while other link is to a site that talks about current applications that do what im looking todo and this apps aren't even for sale 'not that im looking to buy a app'" CreationDate="2016-03-01T16:08:34.340" UserId="16646" />
  <row Id="10407" PostId="10458" Score="0" Text="i need to implement the algorithm that makes up the job of stitching and im not even sure stitching is what i actually need" CreationDate="2016-03-01T16:08:38.933" UserId="16646" />
  <row Id="10408" PostId="10462" Score="1" Text="Thanks, so I basically know that a visual bag words is essentially a &quot;count&quot; of how many of each feature are in an image. So if we had an image of a face the features would be the eyes, the hair, the nose etc. and the BoVW would be how many of each we have. The confusion's coming from how this applies in a CS context when we use more technical features such as colour histograms." CreationDate="2016-03-01T16:08:40.793" UserId="16661" />
  <row Id="10409" PostId="10455" Score="0" Text="@Omri374  That makes sense and I agree.  I was hoping not to have to build a tool/interface myself if something was already out there." CreationDate="2016-03-01T16:42:28.943" UserId="16640" />
  <row Id="10410" PostId="10451" Score="0" Text="Appreciated - I figured I should attempt to answer - I also, only now, have enough reputation to comment." CreationDate="2016-03-01T16:45:07.430" UserId="16558" />
  <row Id="10411" PostId="10462" Score="0" Text="This is something I'm interested in learning fairly soon and would attempt to provide an answer if I knew more in detail, but http://programmingcomputervision.com/ (there's a free PDF on the site) has a great deal of information about computer vision. There's a small section on how visual words work along with some Python code to test it out." CreationDate="2016-03-01T17:22:19.357" UserId="16558" />
  <row Id="10412" PostId="10437" Score="1" Text="Why are you using spark for 70k rows? That really doesn't make any sense and will likely require more time and result in a worse model than using a simple in-memory algorithm. To address your &quot;edit&quot;, try using fewer partitions on your training data, you can do this via `coalesce`." CreationDate="2016-03-01T17:39:49.670" UserId="947" />
  <row Id="10413" PostId="10465" Score="0" Text="How is that a nice description of a dataset? It looks like a description of a project that wants to &quot;mine large, distributed, heterogeneous data systems&quot;. Why do you think that dataset would be available as a CSV, or even available at all?" CreationDate="2016-03-01T18:03:15.020" UserId="471" />
  <row Id="10414" PostId="10460" Score="0" Text="I'm amazed you can see clear seasonal periodicity in data with only four time points. You need to see at least two full seasons to figure that out, so how long are the &quot;seasons&quot; here? Two time points? In which case there's clearly no seasonality." CreationDate="2016-03-01T18:08:28.817" UserId="471" />
  <row Id="10415" PostId="10461" Score="0" Text="The second part was really very helpful, but I still have the first problem and I need to solve it before going to the second part" CreationDate="2016-03-01T18:10:33.360" UserId="16096" />
  <row Id="10416" PostId="10466" Score="0" Text="Seaborn uses [bootstrapped confidence intervals](http://stackoverflow.com/questions/29481134/how-are-the-error-bands-in-seaborn-tsplot-calculated). Alternative estimators are [currently not supported](http://stackoverflow.com/questions/32771520/how-to-use-a-weighted-mean-estimator-in-seaborn-factor-plot-incl-bootstrapping)." CreationDate="2016-03-01T18:26:26.490" UserId="381" />
  <row Id="10417" PostId="10464" Score="0" Text="While I understand that this is the common practice in such cases, I don't think it affects the prediction as I would like. It will 'ignore', or allow more freedom in the space where weights are small, instead of actually correcting the skew." CreationDate="2016-03-01T19:06:57.770" UserId="14044" />
  <row Id="10418" PostId="10468" Score="0" Text="Do you have these words by UserID or just the strings of the total words?" CreationDate="2016-03-01T22:07:05.780" UserId="16558" />
  <row Id="10419" PostId="10468" Score="0" Text="@ImperativelyAblative just strings of total words" CreationDate="2016-03-01T22:47:02.803" UserId="237" />
  <row Id="10420" PostId="10468" Score="0" Text="Ah that's too bad - with the UserID, it would have enabled some analysis around possibly similarities between users/sessions/searches which may allow for some classification." CreationDate="2016-03-01T22:52:59.980" UserId="16558" />
  <row Id="10421" PostId="10461" Score="0" Text="Its very hard to understand what you want in the first part without some data.  Can you add some data to illustrate the other piece that you have a question about.  I believe this is trivially solved based on what you've mentioned.  Just write 10 rows of the dataframe and the before and after of what you have and want." CreationDate="2016-03-02T00:16:38.317" UserId="9420" />
  <row Id="10422" PostId="10461" Score="0" Text="There are to much to post here. I have a table with 20 columns (20 labels) and each row is an entry.&#xA;&#xA; There are 2 columns that I am interested in, 'Activity' and 'income'. For each activity I want to create a column on a new dataframe, all the incomes from stores of the same type will be on the same column" CreationDate="2016-03-02T03:01:13.477" UserId="16096" />
  <row Id="10423" PostId="10468" Score="0" Text="You can look at distribution of the [topics](https://en.wikipedia.org/wiki/Topic_model). What's your goal?" CreationDate="2016-03-02T04:45:14.940" UserId="381" />
  <row Id="10425" PostId="10474" Score="0" Text="Thank you for this detailed answer: One question to the definition of the `acost` function though. Does it matter that you calculate `y_model-Y` twice?" CreationDate="2016-03-02T06:15:02.163" UserId="16683" />
  <row Id="10426" PostId="10474" Score="0" Text="You mean in terms of speed? I don't know; you'll have to time it yourself to see if tensorflow avoids recalculation. It is fine otherwise." CreationDate="2016-03-02T06:26:53.797" UserId="381" />
  <row Id="10427" PostId="10468" Score="0" Text="@Emre I don't have a specific goal, this is discern what's possible with this dataset. I could categorize each word using Wikipedia, bottom of wikipedia article displays categories for given word. This is not exhaustive as some Wikipedia terms have multiple topics." CreationDate="2016-03-02T08:23:28.413" UserId="237" />
  <row Id="10428" PostId="10471" Score="0" Text="Probably, the most simple solution is to use different weights, based on whether the prediction is positive or negative. I should have thought of that earlier." CreationDate="2016-03-02T02:23:45.133" UserId="16683" />
  <row Id="10430" PostId="10481" Score="0" Text="What you describe sounds like the &quot;boosting&quot; model ensembling technique where you repeatedly take a model and improve it by training another model on the previous model's errors. You could take your initial model, train a simple regression tree on the initial model's errors, add the results of both models together and repeat. Figure out the best number of iterations using a validation set (or cross validation). This might result in better predictions than you have right now." CreationDate="2016-03-02T13:32:13.727" UserId="676" />
  <row Id="10431" PostId="6768" Score="0" Text="Can you post the data somewhere? Either &quot;allData&quot; or &quot;features_all&quot; (without normalization and PCA)." CreationDate="2016-03-02T14:10:13.860" UserId="676" />
  <row Id="10432" PostId="10481" Score="0" Text="Thanks @stmax, that will definitely help when this is part of a machine learning algorithm. Currently, though, it isn't, as the data set is still fairly small and my question is probably a more basic statistical inference question." CreationDate="2016-03-02T14:10:23.030" UserId="16693" />
  <row Id="10433" PostId="10486" Score="0" Text="Thank you very much for the correction. It made me realized that the problem is not really the quantization of the color space, but which transform is used. I still obtain very much different results than matplotlib (I cannot see the blocks on the diagonal). So, I think I have to look for which transform matplotlib is doing on the input matrix..." CreationDate="2016-03-02T14:28:18.563" UserId="7966" />
  <row Id="10434" PostId="10477" Score="0" Text="What is going to change if my ANN is not already trained?" CreationDate="2016-03-02T14:36:49.193" UserId="14310" />
  <row Id="10435" PostId="10464" Score="0" Text="Ah I see what you mean, added some more stuff that maybe someone else can add on to.  I'm not sure what else to try, but let us know what you end up doing!" CreationDate="2016-03-02T15:11:55.673" UserId="14913" />
  <row Id="10436" PostId="10477" Score="1" Text="You'd have to add some code to the fit method that trains the ANN using X and y." CreationDate="2016-03-02T15:15:24.617" UserId="676" />
  <row Id="10438" PostId="10437" Score="0" Text="Thanks David. I will try coalesce and see how it goes. That might be the parameter I was looking for :)&#xA;&#xA;The reason I am using Spark is just to learn how it works. At the moment this whole work is prototyping, but I thought it might be good to write all in Spark from the beginning, and can scale it easily if needed." CreationDate="2016-03-02T16:23:59.663" UserId="16612" />
  <row Id="10440" PostId="10478" Score="1" Text="https://github.com/pydata/pandas/issues/10109" CreationDate="2016-03-02T16:29:05.597" UserId="381" />
  <row Id="10442" PostId="10464" Score="0" Text="My simulations showed that it mellows out at ~ 50,000 trials. This problem stems from the fact that for small p: prob(S&lt;pT) &gt; prob(S&gt;pT), so now I'm thinking perhaps I should normalize the distribution as a nonlinear function of T. I have loads of data, but still I'd rather compensate the lacking conditions than drop data from the abundant ones." CreationDate="2016-03-02T17:53:32.297" UserId="14044" />
  <row Id="10444" PostId="10432" Score="1" Text="Have a look at the Examples | Neat Examples section near the bottom of this link: [Classify](http://reference.wolfram.com/language/ref/Classify.html)." CreationDate="2016-03-02T21:26:50.450" UserId="10814" />
  <row Id="10446" PostId="10490" Score="1" Text="http://sacred.readthedocs.org/ See also https://www.reddit.com/r/MachineLearning/comments/3npg0d" CreationDate="2016-03-02T22:35:05.053" UserId="381" />
  <row Id="10447" PostId="10490" Score="0" Text="@Emre exactly what I was looking for. Please convert it to an answer, and I will happily accept it." CreationDate="2016-03-02T22:47:40.717" UserId="16680" />
  <row Id="10448" PostId="10490" Score="0" Text="Sacred is looking great, though even more valuable can be the featureforge app (see https://github.com/machinalis/featureforge), mentioned in reddit link, especially if you use scikit-learn." CreationDate="2016-03-02T22:50:16.060" UserId="16680" />
  <row Id="10449" PostId="10489" Score="1" Text="I see that questions like these are a good fit here, as they deal with feature selection, which is a very important part of a data science pipeline! :)" CreationDate="2016-03-03T00:36:33.210" UserId="11097" />
  <row Id="10453" PostId="10498" Score="0" Text="For sure your implementation is correct. But I wanted to follow the idea from the course strictly. I omitted `bias` terms for I am lazy :) I think the steps to compute `softmax` and `cross entropy` is where I did wrong." CreationDate="2016-03-03T03:33:11.490" UserId="3167" />
  <row Id="10454" PostId="10498" Score="0" Text="You are right; the bias is &quot;optional&quot;, but the softmax is essential." CreationDate="2016-03-03T03:37:46.733" UserId="381" />
  <row Id="10455" PostId="10497" Score="0" Text="Are all these variables time series? IF so, you seem to have a problem of [optimal control](https://en.wikipedia.org/wiki/Optimal_control). Have you tried applying [a sequential model](https://en.wikipedia.org/wiki/Online_machine_learning)?" CreationDate="2016-03-03T03:49:42.540" UserId="381" />
  <row Id="10456" PostId="10497" Score="0" Text="@Emre Well, kind of. E.g. Let's say I want to keep my room at a &quot;fixed&quot; lightning. At this situation my variables X1 and X2 are my lightbulbs, Y1 is a light sensor, F2 is the time of the day and F3 the percentage of clouds in the sky." CreationDate="2016-03-03T03:55:34.407" UserId="16712" />
  <row Id="10457" PostId="10497" Score="2" Text="In that case you might not even need machine learning. I'd try using a [PID controller](https://en.wikipedia.org/wiki/PID_controller) first. [Here](https://www.youtube.com/watch?v=txftR4TqKYA) [are](https://www.youtube.com/watch?v=7BDjZYGHupE) two videos for inspiration. (I'm assuming you really want a physical system, not a software simulation.)" CreationDate="2016-03-03T04:42:13.933" UserId="381" />
  <row Id="10458" PostId="10497" Score="0" Text="@Emre The thing is, I don't want it to try and fail and fix, I want it to learn how it behaves, because not all rooms are the same, not all rooms have the same ammount of windows, or the same ammount of lightbulbs, or what if I can achieve the desired lightning in this specific room by just pulling up the curtain, instead of turning on the lightbulbs, but this won't work in another room. So I'm betting the idea of learning the behavior of anyroom. In this scenario." CreationDate="2016-03-03T04:45:45.907" UserId="16712" />
  <row Id="10461" PostId="10015" Score="0" Text="I don't have tensorflow to try it, but I think you forgot to apply the sigmoid function on y_. Try y_ = tf.nn.sigmoid(tf.matmul(h1, W_out)) instead of y_ = tf.matmul(h1, W_out)." CreationDate="2016-03-03T07:52:14.820" UserId="676" />
  <row Id="10462" PostId="10497" Score="0" Text="If you're running a control loop (PID) it would detect the light level in reach room individually and adjust accordingly. No need to learn parameters. This design also makes it easy to smoothly transition when the environment changes." CreationDate="2016-03-03T09:33:19.130" UserId="5144" />
  <row Id="10464" PostId="10497" Score="0" Text="Sounds like regression is what you're need. More specifically GLM." CreationDate="2016-03-03T09:51:56.077" UserId="15361" />
  <row Id="10466" PostId="10015" Score="0" Text="@stmax, thanks, but I tried that already." CreationDate="2016-03-03T15:05:16.390" UserId="3167" />
  <row Id="10469" PostId="10495" Score="0" Text="Thank you for your help!" CreationDate="2016-03-03T17:35:38.770" UserId="16677" />
  <row Id="10471" PostId="10495" Score="0" Text="you're welcome!" CreationDate="2016-03-03T18:01:52.000" UserId="14313" />
  <row Id="10472" PostId="10509" Score="0" Text="Do you need the granularity of zip codes? Could you calculate/infer the distances from them?&#xA;&#xA;The reason I ask is that you may not have enough observations for some zip codes depending on the density of the data" CreationDate="2016-03-03T20:31:46.470" UserId="16558" />
  <row Id="10473" PostId="10509" Score="0" Text="No, the distance between zip codes is irrelevant in my case." CreationDate="2016-03-03T20:32:58.437" UserId="13891" />
  <row Id="10475" PostId="10510" Score="0" Text="thanks, so you mean treating a binary vector of million (even billions) dimensions is not a problem for the actual tools (spark for example)?" CreationDate="2016-03-03T21:12:44.187" UserId="13891" />
  <row Id="10483" PostId="10510" Score="0" Text="Not in terms of the raw capacity to process it, but you have to be mindful of overfitting and the curse of dimensionality in such situations. If your training error significantly differs from your test error, use more regularization." CreationDate="2016-03-03T22:04:08.027" UserId="381" />
  <row Id="10484" PostId="10494" Score="0" Text="For each of the three tables, what do the x and y dimensions represent? Does its shape change over time?" CreationDate="2016-03-03T22:18:14.480" UserId="381" />
  <row Id="10485" PostId="10507" Score="0" Text="What do you consider data, and how do you decide if it is polluted? Could you give some examples?" CreationDate="2016-03-03T22:21:58.230" UserId="381" />
  <row Id="10487" PostId="10494" Score="0" Text="@Emre x is time, and y is the value i'm trying to analyse and predict from the earlier samples." CreationDate="2016-03-03T22:25:57.280" UserId="2861" />
  <row Id="10488" PostId="10494" Score="0" Text="What does each bucket in the y dimension contain; a real number? Do empty buckets mean there were no event/observation at that time? Do events &quot;update&quot; the state for the corresponding row (e.g., increment it)?" CreationDate="2016-03-03T22:27:26.060" UserId="381" />
  <row Id="10489" PostId="10494" Score="0" Text="@Emre that's correct, the blue squares are an empty bucket, at that moment in time, and the white is the observation that the event took place at that moment for that value and so placed in that bucket. Each column represents a new sample with bucket values being reset to 0" CreationDate="2016-03-03T23:09:30.143" UserId="2861" />
  <row Id="10490" PostId="10494" Score="0" Text="Can the three tables be likened to three users, and you want to find event patterns across users and time? If so, I would try using a Markov model since the state space is well defined, and there is a temporal dimension. This would allow you predict the distribution (column) at the next moment." CreationDate="2016-03-03T23:11:49.333" UserId="381" />
  <row Id="10491" PostId="10494" Score="0" Text="Not three users but the same system at four different points in time, days apart. I'm assuming I need many more samples before being able to build a markov model for every possible state?" CreationDate="2016-03-03T23:44:29.673" UserId="2861" />
  <row Id="10492" PostId="10517" Score="0" Text="its really hard to see this data... can you jitter your plots? Or zoom in towards x=0... what's the scale on the x-axis? Or better yet, do you have a sample of the actual data?" CreationDate="2016-03-03T23:49:06.233" UserId="6478" />
  <row Id="10493" PostId="10517" Score="0" Text="added samples of the data in those urls" CreationDate="2016-03-04T00:03:32.663" UserId="16744" />
  <row Id="10494" PostId="10494" Score="0" Text="That's fine, but you will want more data than this to build a good model, and not just Markovian ones." CreationDate="2016-03-04T00:04:27.257" UserId="381" />
  <row Id="10495" PostId="10494" Score="0" Text="what do you mean by &quot;not just Markovian ones&quot;?" CreationDate="2016-03-04T00:05:19.607" UserId="2861" />
  <row Id="10496" PostId="10494" Score="0" Text="Not only Markovian models." CreationDate="2016-03-04T00:07:27.820" UserId="381" />
  <row Id="10497" PostId="10517" Score="0" Text="It seems some of your inputs have exaggerated values. Try ignoring them, or excluding them from the plots." CreationDate="2016-03-04T00:10:22.427" UserId="381" />
  <row Id="10498" PostId="10517" Score="0" Text="That might possibly help. In general, are there times where linear regression is just a waste and clearly not gonna work - like if you see data like it is here? Or is there always something that you can do to get the data into a form where linear regression might work - like removing outliners, etc." CreationDate="2016-03-04T00:17:35.853" UserId="16744" />
  <row Id="10499" PostId="10517" Score="0" Text="It works when there is a linear relationship between the features and the response. If you picked the features yourself, you should have some idea about whether such a relationship holds, otherwise you find out when you try. It helps to know something about the process that relates the input to the output." CreationDate="2016-03-04T00:22:41.867" UserId="381" />
  <row Id="10500" PostId="10517" Score="0" Text="The lengths of these files are different; there are more labels than features. Can you verify that they match; that it is safe to truncate the longer file?" CreationDate="2016-03-04T01:14:22.820" UserId="381" />
  <row Id="10501" PostId="10517" Score="0" Text="yeah. They are both just trimmed to match the requirments of heypasteit. Perhaps another paste tool would be better. Im not sure if there is a standard for sharing data files on this site?" CreationDate="2016-03-04T01:33:06.110" UserId="16744" />
  <row Id="10502" PostId="10495" Score="0" Text="what does the `d` mean in `for index, (group_name, d) in enumerate(grouped)`?" CreationDate="2016-03-04T01:59:39.210" UserId="16677" />
  <row Id="10503" PostId="10495" Score="0" Text="d is the mini-dataframe that contains `Feature` and `amount` as columns. For each combination of `ID, Location`, you have one d" CreationDate="2016-03-04T03:53:33.323" UserId="14313" />
  <row Id="10506" PostId="10515" Score="0" Text="+1, If you can easily map out if/thens to tackle a problem there is no way that ANN is going to outperform pure inferential logic. Even a statistical argument (service A is down on 5/6 servers, thus service A is down) is better than dealing with the overhead that a Neural Net requires." CreationDate="2016-03-04T07:09:48.977" UserId="12575" />
  <row Id="10507" PostId="10494" Score="1" Text="I don't see any clusters in your examples. If you want to answer this question, you will need to *formalize it*. I.e. *what* is a cluster, *what is not a cluster*. What would a *random* process generate? Then you can estimate or simulate random data to get thresholds." CreationDate="2016-03-03T22:09:48.340" UserId="924" />
  <row Id="10508" PostId="10509" Score="0" Text="could you please elaborate the problem of your system and how the zip codes hold importance in the data? Are they helping in any inference that does not have a dead end? M" CreationDate="2016-03-03T21:12:00.747" UserId="16740" />
  <row Id="10509" PostId="10509" Score="0" Text="yes, the zip code is an information that wight help in the classification task that I am trying to solve." CreationDate="2016-03-03T21:16:31.297" UserId="13891" />
  <row Id="10510" PostId="10509" Score="0" Text="what technology are you using for the problem?" CreationDate="2016-03-03T21:23:57.173" UserId="16740" />
  <row Id="10511" PostId="10509" Score="0" Text="I am using Spark 1.6" CreationDate="2016-03-03T21:25:36.233" UserId="13891" />
  <row Id="10512" PostId="10509" Score="0" Text="Did you vectorize the column containing the zipcodes?" CreationDate="2016-03-03T21:30:12.437" UserId="16740" />
  <row Id="10513" PostId="10509" Score="0" Text="this is my question actually, what is the best way to deal with this column." CreationDate="2016-03-03T21:31:39.890" UserId="13891" />
  <row Id="10514" PostId="10509" Score="0" Text="Ok, so you are talking about the hashing trick? and the TF-IDF vector will be a very sparse vector with one non-zero dimension that corresponds to the IDF only in this case. right?" CreationDate="2016-03-03T21:40:04.640" UserId="13891" />
  <row Id="10515" PostId="10509" Score="1" Text="Welcome to the site @RahulS. Please leave comments such as this below the initial post, not as an &quot;answer&quot;." CreationDate="2016-03-03T22:23:12.137" UserId="381" />
  <row Id="10517" PostId="10510" Score="0" Text="thanks Emre, ok so One-hot-encoder + dimensionality reduction is reasonable choice then. I also like what @RahulS suggested in applying TF-IDF to the column. I will try both and compare if I have time. thanks again" CreationDate="2016-03-04T09:16:59.240" UserId="13891" />
  <row Id="10518" PostId="10525" Score="0" Text="assuming actions are non-repeatable independent events couldn't you just compute the conversion rate for each action, reverse sort these resuts by conv rate, and then your if your most recent action was i, the next best is i+1." CreationDate="2016-03-04T10:42:36.113" UserId="6478" />
  <row Id="10520" PostId="10512" Score="0" Text="Yes, the composition is crucial,i was thinking if i.e. citrus fruits dont mix well with nuts, but nuts mix well with berries and berries and citrus makes an awesome composition. Otherwise just a regular weighted measure would work. Is it possible to analyze things like this with machine learning?" CreationDate="2016-03-04T15:55:26.080" UserId="16735" />
  <row Id="10522" PostId="10528" Score="0" Text="What do the variables represent?" CreationDate="2016-03-04T17:18:19.047" UserId="381" />
  <row Id="10525" PostId="10528" Score="1" Text="If you're just interested in curve fitting $y=a(1-x)^b$ (with $x$ rescaled) works well, but if you want a statistically justifiable choice, it would help to know more about the data." CreationDate="2016-03-04T18:21:08.593" UserId="381" />
  <row Id="10526" PostId="10531" Score="0" Text="can you provide a sample of what your labelled training data looks like?" CreationDate="2016-03-04T19:01:18.330" UserId="6478" />
  <row Id="10527" PostId="10531" Score="0" Text="Edited question." CreationDate="2016-03-04T19:10:19.413" UserId="15613" />
  <row Id="10528" PostId="10531" Score="0" Text="what is the set of labels you are tying to classify texts into?" CreationDate="2016-03-04T19:18:26.227" UserId="6478" />
  <row Id="10529" PostId="10528" Score="0" Text="Do you have the option to share the numerical data?" CreationDate="2016-03-04T19:43:32.110" UserId="12527" />
  <row Id="10532" PostId="10528" Score="0" Text="Here are a couple of valuable resources I discovered in a previous post in reference to a question I had on tools and IDE for both Oracle packages and SQL Server stored procedures. http://www.databaseanswers.org/data_models (overall a total resource)&#xA;http://www.red-gate.com/products/sql-development/sql-toolbelt/ (a company with a great set of database tools) HTH ~" CreationDate="2016-03-04T17:16:17.503" UserId="16761" />
  <row Id="10533" PostId="10521" Score="0" Text="Recurrent Neural Networks if you have lots of training examples." CreationDate="2016-03-04T23:25:12.047" UserId="7848" />
  <row Id="10534" PostId="10521" Score="0" Text="@Xeon is there a rule for the definition of &quot;lots&quot;?" CreationDate="2016-03-04T23:37:08.687" UserId="2861" />
  <row Id="10535" PostId="10521" Score="0" Text="@user3791372 unfortunately no definition. For text classification task, for RNN to outperform simple Logistic Regression, the number of training examples should be in hundreds of thousands or in millions and more." CreationDate="2016-03-04T23:40:29.767" UserId="7848" />
  <row Id="10536" PostId="10535" Score="0" Text="You've already noted that feature 4 is less correlated to the other features. You've already &quot;learned&quot; that difference by computing correlations. What else do you mean?" CreationDate="2016-03-05T09:31:13.523" UserId="21" />
  <row Id="10537" PostId="10535" Score="0" Text="The original data that I'm handling is not that small ,so instead of manually noticing the difference can we have a function that returns feature 4 in this case by understanding the variation. Thanks for answering , i would appreciate any help to start with this work,.&#xA;Finally I want to have a function that returns the index of least correlated data like this index_least_correlation = func(Matrix)" CreationDate="2016-03-05T09:38:57.690" UserId="16749" />
  <row Id="10538" PostId="10393" Score="0" Text="@Anony-Mouse Let me simplify the question. Suppose we are working with a linear kernel SVM but our data is linearly inseparable. Assume all points are distinct. How will the SVM behave? Will it converge and produce a decision surface at all?" CreationDate="2016-03-05T09:49:36.307" UserId="11044" />
  <row Id="10539" PostId="10389" Score="0" Text="@stmax Let me simplify the question. Suppose we are working with a linear kernel SVM but our data is linearly inseparable. Assume all points are distinct. How will the SVM behave? Will it converge and produce a decision surface at all?" CreationDate="2016-03-05T09:49:46.760" UserId="11044" />
  <row Id="10541" PostId="10437" Score="1" Text="Number of jobs is related to the number of iterations you set. It can be lower if SGD converges before reaching `NumIterations` but it cannot be lowered manually.  Regarding performance - it takes less than a fives seconds (~160 iterations) on a few years old box with random data of the same shape as yours. It would be useful if you show how you load and prepare the data." CreationDate="2016-03-05T11:27:36.467" UserId="12632" />
  <row Id="10542" PostId="10528" Score="0" Text="I have the option to share the data, although we are talking about two data series of `1000` observations each. But it would made no sense to discuss the nature of these data, as an extensive explanation of how they are obtained should come in the first place." CreationDate="2016-03-05T11:32:03.403" UserId="13481" />
  <row Id="10544" PostId="10393" Score="0" Text="Convergence and result depend on your optimization procedure, this cannot be answered for the abstract SVM *model*. If e.g. your data is (0,0,+), (0,1,-), (1,0,-), (1,1,+) and you use the linear kernel, then there exists no SVM that has 100% accuracy. But what the program does that is responsible for finding the &quot;optimal&quot; SVM is an entirely different question. This is a nontrivial optimization problem." CreationDate="2016-03-05T11:53:24.393" UserId="924" />
  <row Id="10547" PostId="10529" Score="0" Text="Try scaling your input data (feature-wise) to $[-1, 1]$." CreationDate="2016-03-05T15:51:06.757" UserId="15527" />
  <row Id="10548" PostId="10538" Score="0" Text="How is this different from your previous question? You do want to rank a set of movies, right?" CreationDate="2016-03-05T18:14:33.087" UserId="381" />
  <row Id="10549" PostId="10538" Score="0" Text="The previous question concerned discrete yes/no votes. This concerns a continuous 0-10 rating. To be perfectly clear, I don't want to actually &quot;rank&quot; anything. I just want to take into account the total number of votes somehow for each episode to produce a fairer rating. Like give 0-10 ratings with more votes more weight. I just used the term &quot;rank aggregate&quot; cause I thought that's what the official term for this is." CreationDate="2016-03-05T18:42:31.403" UserId="13165" />
  <row Id="10550" PostId="10539" Score="0" Text="Processing any significant amount of video should get the &quot;big data&quot; label that the OP is looking for. E.g. http://cs.stanford.edu/people/karpathy/deepvideo/" CreationDate="2016-03-05T19:09:44.263" UserId="836" />
  <row Id="10551" PostId="10507" Score="0" Text="I think data represents facts and information. I believe that collectively, data becomes polluted once some of those facts and information become obsolete or redundant. For example, a stock market analyst creates a spreadsheet for stock predictions and spreads it across several stock-related websites. His data has many typos, inaccurate data, and duplicate data. This information is misleading and contributes to data pollution." CreationDate="2016-03-06T00:26:32.150" UserId="16738" />
  <row Id="10552" PostId="10542" Score="0" Text="How about [Domino](https://www.dominodatalab.com/benefits)?" CreationDate="2016-03-06T01:43:43.387" UserId="381" />
  <row Id="10554" PostId="10525" Score="0" Text="Do want to take into consideration physician attributes or do they all look equal to you?" CreationDate="2016-03-06T14:35:19.200" UserId="15361" />
  <row Id="10555" PostId="10539" Score="0" Text="How difficult is it to find examples in &quot;traditional&quot;, solid industry(car making, construction, chemistry, etc.)?" CreationDate="2016-03-06T15:04:18.273" UserId="12527" />
  <row Id="10556" PostId="10558" Score="0" Text="I wrote an echo app a few months ago so my knowledge is rudimentary, but believe the actual speech-to-text is done at the device level (i.e. the physical echo/alexa unit). Text is then sent to a remote process, where it attempts to match the utterances in the skill set. Obviously placeholders like {Sign} function as arguments to functions whose values are supplied by the end-user/speaker." CreationDate="2016-03-06T18:41:48.147" UserId="6478" />
  <row Id="10558" PostId="10550" Score="1" Text="[Also posted on CS.SE](http://cs.stackexchange.com/q/54074/755).&#xA;Please [do not post the same question on multiple sites](http://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted. If you don't get a satisfying answer after a week or so, feel free to flag for migration." CreationDate="2016-03-06T18:58:49.450" UserId="8560" />
  <row Id="10559" PostId="10560" Score="0" Text="Unsupervised for the most part. Little input can be given by the user as a correction, but not during the calculation if that makes sense. The number of different emails is not known." CreationDate="2016-03-06T14:06:09.433" UserId="16808" />
  <row Id="10560" PostId="10560" Score="1" Text="@RicardoCruz Seems reasonably on-topic to me...  It's kinda broad and kinda vague but requests for algorithms look like computer science to me." CreationDate="2016-03-06T17:30:27.287" UserDisplayName="David Richerby" />
  <row Id="10561" PostId="10560" Score="0" Text="Requests for software implementations, tools, or libraries are off-topic here, so I've edited that part out of your question.  Requests for algorithms or techniques are on-topic here, so with this edit, the question seems on-topic to me.  (Cc: @RicardoCruz)  Note to hakunin: please don't leave clarifications in the comments.  Instead, edit your question to add the missing information.  We want questions to be self-contained: Readers shouldn't have to read the comments to understand your question." CreationDate="2016-03-06T18:55:24.810" UserId="8560" />
  <row Id="10562" PostId="10560" Score="0" Text="@RicardoCruz, in the future, if you're going to mention another site, please remind the person not to cross-post on multiple SE sites, and tell them how they can migrate their question.  That violates SE rules, so by suggesting another site, you're basically encouraging the poster to do something that will get them in trouble, which isn't a great user experience for them.  I know this isn't intuitive/obvious, so I thought I'd mention it for your future consideration.  Thank you!" CreationDate="2016-03-06T18:57:53.420" UserId="8560" />
  <row Id="10563" PostId="10558" Score="0" Text="Thanks @BrandonLoudermilk. I updated the question to clarify that I am trying to understand the natural language parsing aspects." CreationDate="2016-03-06T19:10:43.233" UserId="16816" />
  <row Id="10564" PostId="10539" Score="1" Text="Not very. Anywhere there is prediction involved (yield, sales, etc.), there is a potential job for machine learning. If self-driving cars count you could come up with so many examples ... I will leave enumeration of examples from &quot;traditional&quot; industries to people more familiar with them." CreationDate="2016-03-06T19:26:54.630" UserId="381" />
  <row Id="10565" PostId="10562" Score="0" Text="A data scientist could easily do this; typically in R or python. [Even Excel](http://www.real-statistics.com/logistic-regression/finding-logistic-regression-coefficients-using-excels-solver/) seems to be a possibility. My main concern is that your model might be suboptimal; do you really want to know what to know which items have the potential to be sold, or do you want to know _how many_ of each you are likely to sell? The first is a classification problem; the second is a regression. If your items are from the &quot;long tail&quot; and rarely bought, you might be right, otherwise consider regression." CreationDate="2016-03-06T19:58:50.440" UserId="381" />
  <row Id="10566" PostId="10552" Score="1" Text="There are better ways of dimensionality reduction than clustering, which is an ill-defined problem; it's hard to know if you've clustered well. A more contemporary approach would be to learn a representation with a neural network." CreationDate="2016-03-06T20:40:31.077" UserId="381" />
  <row Id="10568" PostId="10545" Score="0" Text="The gradient check seems to work for the sequence of length 1 but not for 2 or more." CreationDate="2016-03-07T02:29:46.980" UserId="12250" />
  <row Id="10569" PostId="10568" Score="1" Text="A common way to do it is market basket analysis https://en.wikipedia.org/wiki/Affinity_analysis" CreationDate="2016-03-07T07:05:05.280" UserId="13727" />
  <row Id="10572" PostId="9466" Score="0" Text="I tried to do that for &quot;n&quot; no of documents say where topics are t. However for say x no of documents , all t topics prob do not shows up just some ( t- no) topics prob shows up,  where 1&lt;=no&lt; t. It does not happen when I run the experiment on small document size. Is it because it doesn't print at all if the prob is 0 ?" CreationDate="2016-03-07T17:40:21.910" UserId="16795" />
  <row Id="10573" PostId="10560" Score="0" Text="(@D.W. Duly noted!) Poster: If you want something fast, I think something you may want to try is hierarchical clustering (because you don't need to predefine the number of clusters, you can prune it later), together with a text mining measure such as something simple like TF-IDF cosine similarity. For something more elaborate, I would use something based on word embeddings." CreationDate="2016-03-07T17:56:20.973" UserId="16853" />
  <row Id="10574" PostId="10575" Score="0" Text="Don't use k-means on latitude/longitude, and also not on categorical data." CreationDate="2016-03-07T20:13:41.033" UserId="924" />
  <row Id="10578" PostId="10581" Score="0" Text="so given this simple example data:&#xA;&#xA;User 1 rates Product A.&#xA;User 2 rates Product B.&#xA;User 3 rates Product A.&#xA;Product C and Product D exits with no ratings. What would the density be? 50%?" CreationDate="2016-03-07T21:35:00.897" UserId="16854" />
  <row Id="10579" PostId="10581" Score="1" Text="There are three users (1,2,3) and four products (A,B,C,D). There are three ratings (1A, 2B, 3A), hence the density is 3/12 = 25%." CreationDate="2016-03-07T21:48:06.627" UserId="381" />
  <row Id="10580" PostId="10575" Score="0" Text="Why we can not use that? Is there any constraint not to use them." CreationDate="2016-03-08T02:25:38.160" UserId="14310" />
  <row Id="10581" PostId="10572" Score="0" Text="Reminds me of a certain Kaggle contest... ;)" CreationDate="2016-03-08T03:52:33.293" UserId="2723" />
  <row Id="10582" PostId="10545" Score="0" Text="I reduced a learning rate a bit and ran the training step. Got decent accuracy on train and test data. The cost vs iteration graph looks ugly though." CreationDate="2016-03-08T06:00:25.797" UserId="12250" />
  <row Id="10588" PostId="10585" Score="1" Text="Interesting question. I think you have a better chance getting answers for it by asking at http://stats.stackexchange.com/" CreationDate="2016-03-08T07:05:41.497" UserId="13727" />
  <row Id="10593" PostId="10592" Score="0" Text="I'm not trying to gain reputation by answering my own question. Just wanted to document this somewhere since I found it useful and perhaps someone else will find it useful too." CreationDate="2016-03-08T07:17:47.960" UserId="13686" />
  <row Id="10595" PostId="10575" Score="1" Text="Because the mean of -179 and +179 is 0. It will fail to find the optimum center (it is also off in many other places, just not that completely off). It may even fail to converge and go into an infinite loop." CreationDate="2016-03-08T07:26:18.473" UserId="924" />
  <row Id="10602" PostId="10592" Score="0" Text="It is okay, as long as it is helpful to the users.  Please add a more clear and detailed answer :)" CreationDate="2016-03-08T07:50:17.240" UserId="11097" />
  <row Id="10608" PostId="10563" Score="0" Text="Thank you! Yes, an increase in support vectors would give more prediction time. However is there a relation between number of support vectors and size of training dataset?" CreationDate="2016-03-08T14:03:35.657" UserId="16812" />
  <row Id="10611" PostId="10563" Score="0" Text="That's a good question. The answer is yes, but for details I'll defer details to this paper: [Sparseness of Support Vector Machines—Some Asymptotically Sharp Bounds](http://papers.nips.cc/paper/2477-sparseness-of-support-vector-machines-some-asymptotically-sharp-bounds.pdf)." CreationDate="2016-03-08T16:27:13.873" UserId="381" />
  <row Id="10614" PostId="10581" Score="1" Text="@Emre You should put that example into your answer." CreationDate="2016-03-08T18:15:05.167" UserId="8820" />
  <row Id="10621" PostId="8338" Score="0" Text="I'm not sure I understand your question entirely, but are you looking for something like TF-IDF (Term Frequency - Inverse Document Frequency)?" CreationDate="2015-10-09T12:54:11.923" UserId="13176" />
  <row Id="10622" PostId="10588" Score="1" Text="@Ragnar I also don't understand why you antagonize answerers this way. You can vote down; you can clarify your question; you can ask for more detail; you can move on. You're not asking why the tanh function is called &quot;hyperbolic&quot; right? it's a rescaling of the logistic sigmoid function, hence the explanation above. Aren't you just asking about sigmoid kernels?" CreationDate="2016-03-08T22:10:53.907" UserId="21" />
  <row Id="10626" PostId="10512" Score="0" Text="It is possible but from my feeling for your task it is infeasible." CreationDate="2016-03-08T22:58:09.457" UserId="12798" />
  <row Id="10628" PostId="10533" Score="0" Text="hmm. Can you paste your exact code? I tried the code you pasted and got a regression score of -0.277155605577. Is your 0.37 perhaps after clipping some outliers?" CreationDate="2016-03-08T23:35:07.407" UserId="16744" />
  <row Id="10629" PostId="10533" Score="0" Text="This is the actual code, and I still get 0.37. Are you using the data set you posted? Note that sklearn [outputs the negative of the score for its own convenience](http://stackoverflow.com/questions/21443865/scikit-learn-cross-validation-negative-values-with-mean-squared-error), so your score was actually 0.277. Obviously, the MAE/MSE can not be negative." CreationDate="2016-03-08T23:44:32.717" UserId="381" />
  <row Id="10630" PostId="10533" Score="0" Text="ah well that makes me feel better. Still 0.27 or 0.37 are both pretty horrible. I think I need to look for a different algorithm to up the accuracy, perhaps a neural network?" CreationDate="2016-03-09T00:00:19.247" UserId="16744" />
  <row Id="10631" PostId="10533" Score="0" Text="Sure. To get a better sense of what a good score is, I would run the algorithm on a test set, and have it show you the predicted color beside the real one. There are well-known limits to [human sensitivity](https://en.wikipedia.org/wiki/Just-noticeable_difference) to [color difference](https://en.wikipedia.org/wiki/Color_difference)." CreationDate="2016-03-09T00:07:22.890" UserId="381" />
  <row Id="10632" PostId="10606" Score="0" Text="So if I have say 30 categorical features, each one with some hundred thousands different types, each one of them types are supposed to be a new column with 0/1 values?" CreationDate="2016-03-09T00:15:38.947" UserId="16893" />
  <row Id="10633" PostId="10606" Score="1" Text="Yes. If you are worried about it, look into [feature hashing](https://en.wikipedia.org/wiki/Feature_hashing)." CreationDate="2016-03-09T00:22:16.893" UserId="381" />
  <row Id="10634" PostId="10609" Score="1" Text="Welcome to DataScience.SE! Please can you tell us a little bit about your goal?" CreationDate="2016-03-09T00:26:56.300" UserId="381" />
  <row Id="10635" PostId="10609" Score="0" Text="Hello @Emre. For example, given data about file changes, I'd like to use changed file path as one of the inputs for clustering to find changes which were roughly in the same area." CreationDate="2016-03-09T00:36:52.147" UserId="16897" />
  <row Id="10636" PostId="10533" Score="0" Text="in your code sample, it seems to predict labels using all 3 features ('r','g','b') through the linear regression. I figured that this was impossible and that linear regression can only predict 1 feature. Is this indeed what your code does, or does it specify a particular color somewhere that I'm not seeing?" CreationDate="2016-03-09T01:20:02.493" UserId="16744" />
  <row Id="10637" PostId="10583" Score="0" Text="Could you please elaborate on this? If I assign a number to each crime, what would the clustering be based on? How will I be able to include both the number as well as the lat-long information while clustering?" CreationDate="2016-03-09T03:12:49.297" UserId="2481" />
  <row Id="10638" PostId="10583" Score="0" Text="Also, could you please explain the second approach using KNN?&#xA;Thanks in advance :)" CreationDate="2016-03-09T03:13:36.100" UserId="2481" />
  <row Id="10639" PostId="10582" Score="0" Text="This seems like a possibility. Thanks! :)" CreationDate="2016-03-09T03:14:24.790" UserId="2481" />
  <row Id="10640" PostId="10533" Score="0" Text="Yes! It is multivariate (inputs) and multivariable or multiple (outputs). The color is the response, not the feature, by the way." CreationDate="2016-03-09T05:52:59.020" UserId="381" />
  <row Id="10641" PostId="10613" Score="0" Text="I think you answered yourself by &quot;values of B are not comparable&quot;. Learning for predictions is based on a fundamental assumption which is the data for prediction has the same joint distribution as the data for learning. This is the link between those processes." CreationDate="2016-03-09T10:27:24.900" UserId="108" />
  <row Id="10642" PostId="10347" Score="0" Text="The moving average approach works quite well during busy periods, which is when it's most relevant. For the quiet periods the time since the last time someone passed the intersection seems to work adequately (but has a strong bias towards long times between passes, some log scale could help)" CreationDate="2016-03-09T11:04:52.817" UserId="16199" />
  <row Id="10643" PostId="10616" Score="0" Text="I faced this question myself when taking practical decisions on estimating hardware requirements and project planning for a deep learning project.  &#xA;PS: I didn't answer my own question to just gain reputation points. I want to **know if my answer is right** from the community." CreationDate="2016-03-09T11:17:03.137" UserId="13686" />
  <row Id="10645" PostId="10583" Score="0" Text="If you assign a number, i.e., make a dict with the crime type, you have a 3 dimension vector and now you can do clustering with the data &quot;itself&quot;. I recommend you K nearest neighbors because its very easy to understand, but it could be any classification algorithm. [KNN on Wikipedia](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm)" CreationDate="2016-03-09T12:09:21.583" UserId="11007" />
  <row Id="10646" PostId="10611" Score="0" Text="Thank you @Emre. I do use sklearn and didn't realise it can take similarity function for clustering." CreationDate="2016-03-09T12:33:11.607" UserId="16897" />
  <row Id="10647" PostId="10611" Score="0" Text="More generally, I was looking for encoding which can be used with different algorithms not only clustering. Something like [Huffman coding](https://en.wikipedia.org/wiki/Huffman_coding) but usable with machine learning." CreationDate="2016-03-09T12:47:06.890" UserId="16897" />
  <row Id="10648" PostId="10611" Score="0" Text="Currently I can think of two options:&#xA; - expand tree to features corresponding to each possible path with 0 or 1 value. This should work but can be expensive if the tree is large.&#xA; - expand tree to features corresponding to each depth level with unique values for nodes at particular depth level. This might work but given many child nodes for one parent, some children will be encoded as if they are closer. I can write an example if it can be useful.&#xA;&#xA;I was hoping this has been already researched and there is article/paper with trade-offs." CreationDate="2016-03-09T12:47:30.890" UserId="16897" />
  <row Id="10650" PostId="10613" Score="0" Text="OKAY. In that case how do I combine all the models into a single model? I am using time series fr prediction and there are chances that the data from the same device are not always same." CreationDate="2016-03-09T13:17:35.147" UserId="14332" />
  <row Id="10651" PostId="10611" Score="0" Text="Detailed examples are always welcome; you can [edit your question](http://datascience.stackexchange.com/posts/10609/edit)." CreationDate="2016-03-09T16:09:45.447" UserId="381" />
  <row Id="10652" PostId="8561" Score="0" Text="@Hooked What type of optimizer are you using? e.g. SGD, RMSProp, Adam?" CreationDate="2016-03-09T17:17:08.183" UserId="14779" />
  <row Id="10653" PostId="10533" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/36762/discussion-between-bigboy1337-and-emre)." CreationDate="2016-03-09T19:31:04.503" UserId="16744" />
  <row Id="10655" PostId="10479" Score="1" Text="This was cross-posted to http://stats.stackexchange.com/questions/199620/on-the-properties-of-hyperbolic-tangent-kernel . Either site may be fine for this question. Since the other post got an answer that @Ragnar liked more, I propose this be closed as a duplicate. (Generally, we don't cross post.)" CreationDate="2016-03-09T20:59:38.510" UserId="21" />
  <row Id="10658" PostId="10528" Score="0" Text="That's part of the discussion. Feel free to edit your post to include details, and a link to data." CreationDate="2016-03-10T03:27:46.593" UserId="381" />
  <row Id="10659" PostId="10621" Score="1" Text="You have to start with a question. What you are actually doing is to find answers to unknown questions. Think about some good questions which you want to answer and then for each one you can think of a question if you are in doubt." CreationDate="2016-03-10T08:06:45.110" UserId="108" />
  <row Id="10661" PostId="10581" Score="0" Text="I really like the answer here, but. To be more precise, sparsity and density are fraction of non-filled ratings and filled ratings respectively. They can not be treated interchangeably. Though, density + sparsity should result in 1.0. I know that due to definition from the article @Emre gave the good answer. But the definition of sparsity as actual density is misleading." CreationDate="2016-03-10T10:48:07.223" UserId="9314" />
  <row Id="10662" PostId="10479" Score="0" Text="I'm voting to close this question because it's a cross post and seems the OP found it better suited to stats SE" CreationDate="2016-03-10T11:04:20.293" UserId="21" />
  <row Id="10663" PostId="10630" Score="1" Text="Looks like you need to do a serious feature design first in order to be able to formulate this as a classification problem." CreationDate="2016-03-10T13:08:07.393" UserId="15361" />
  <row Id="10664" PostId="10581" Score="0" Text="I agree, but that's how they defined it, and it's common." CreationDate="2016-03-10T16:17:19.860" UserId="381" />
  <row Id="10668" PostId="10646" Score="0" Text="Thanks, I'll look into that. A lot of the tutorials I've found online deal with the process in an abstract manner; I'm trying to get into the finer details, will this text help?" CreationDate="2016-03-11T00:28:25.780" UserId="14586" />
  <row Id="10670" PostId="10649" Score="0" Text="Don't cluster with the Euclidean distance if you're operating in very high dimensions (typical of word2vec). Use cosine similarity instead. The reason is a bit technical; cf. [this thread](http://stats.stackexchange.com/questions/120350/)." CreationDate="2016-03-11T02:35:04.850" UserId="381" />
  <row Id="10672" PostId="10643" Score="0" Text="If the purpose of the dataset is for descriptive analysis, why do you need to impute anything? Can't you just describe the data as they exist? But I presume one could compute accuracy of imputation methods by randomly removing data from a dataset, running imputation on partial data set, and then comparing the actual to computed values." CreationDate="2016-03-11T04:59:09.283" UserId="6478" />
  <row Id="10673" PostId="10652" Score="1" Text="Welcome to DataScience SE. Can you pl elaborate your answer? Currently, it is just throwing in options instead of explaining what to use when and why?" CreationDate="2016-03-11T05:52:37.000" UserId="11097" />
  <row Id="10674" PostId="37" Score="0" Text="I think this is a good business perspective of what big data is but does not answer the specific question which is quite pointed &quot;how big is big data?&quot;" CreationDate="2016-03-11T06:17:05.157" UserId="13686" />
  <row Id="10675" PostId="8708" Score="0" Text="Note that the $O(n)$ barrier also has been breached now in some domains of ML. See [http://grigory.us/mpc-workshop-dimacs.html] for the Workshop on Sublinear Algorithms for ML&#xA;&#xA;[1]:http://grigory.us/mpc-workshop-dimacs.html" CreationDate="2016-03-11T06:35:14.090" UserId="13686" />
  <row Id="10676" PostId="10639" Score="0" Text="Thanks. If I use KNN, then first I will have to form clusters of the training data set based on some characteristics or features. You gave some examples on the type of features like average speed, left and right turns, etc. How many features do I need to get an above average performance? I don't need it to be highly accurate." CreationDate="2016-03-11T06:53:46.417" UserId="16943" />
  <row Id="10677" PostId="10639" Score="0" Text="There is no formula for the number of features you need, you will have to try them out and check against some quality metric. One possible option is to use PCA against a large set of features and have the algorithm generate a new set of recombined features for you. If you want to explain the classification using the original set of features, then at least check for correlations between them so you can weed out correlated features that add little information." CreationDate="2016-03-11T09:14:08.303" UserId="3439" />
  <row Id="10679" PostId="10656" Score="0" Text="The attributes in the OP's case have been anonymised to the point where domain knowledge cannot be used. This is quite common in Kaggle competitions, yet limited forms of feature engineering are still possible. Typically new features are created in bulk then tested, as opposed to using subject knowledge to guide their generation. An example: https://www.kaggle.com/c/bnp-paribas-cardif-claims-management/data" CreationDate="2016-03-11T10:04:15.863" UserId="836" />
  <row Id="10680" PostId="10656" Score="0" Text="as Neil said, the features are anonymized, that is what i mean from my that statement." CreationDate="2016-03-11T10:07:48.443" UserId="12377" />
  <row Id="10681" PostId="10643" Score="0" Text="I need a complete data set in order to make better and more precise estimation. But the latter, I think it might be interesting to do it in such way. Thanks!" CreationDate="2016-03-11T12:43:46.773" UserId="16966" />
  <row Id="10683" PostId="10667" Score="0" Text="Can you clarify what you mean by &quot;relate the variables to their number&quot;? Each plot has thirteen series, corresponding to thirteen variables, presumably numbered in order of definition. What command did you run to create these plots?" CreationDate="2016-03-11T21:51:30.860" UserId="381" />
  <row Id="10684" PostId="10667" Score="0" Text="I used  fit.ridge=glmnet(x,y,alpha=0,standardize=TRUE) and plot(fit.ridge,xvar=&quot;norm&quot;,label=TRUE) for ridge and similar thing for lasso. I'm not sure if the first number is my first variable etc. Can I the variables' name in the graph instead of their number. Thanks." CreationDate="2016-03-11T22:40:48.753" UserId="12867" />
  <row Id="10686" PostId="2537" Score="2" Text="What GBM implementations have you looked at? sklearn's GradientBoostingClassifier/Regressor have a max_features parameter and XGBoost has colsample_bylevel and colsample_bytree parameters that control how many features are sampled for each tree / split." CreationDate="2016-03-11T23:20:59.390" UserId="676" />
  <row Id="10687" PostId="10626" Score="1" Text="If you tell us the use case for the model then the community can suggest ways in which it can still be useful." CreationDate="2016-03-12T05:30:09.197" UserId="13686" />
  <row Id="10688" PostId="10672" Score="2" Text="Sounds like a straightforward combinatorial optimization problem. These are usually intractable to solve optimally but I have had success with genetic algorithms. It seems applicable here." CreationDate="2016-03-12T06:30:35.260" UserId="381" />
  <row Id="10689" PostId="10672" Score="0" Text="k-means minimizes *variance*. It would put people with similar &quot;productivity&quot; in the same team. No, this will not help you. You need to treat this as an **optimization problem**, formalize your objective, and then try the usual optimization strategies: greedy, evolutionary, particle swarm, exhaustive search (how many million people do you have?). It's not machine learning nor clustering." CreationDate="2016-03-12T09:07:28.143" UserId="924" />
  <row Id="10690" PostId="10672" Score="0" Text="Thank you both! Exhaustive search is no feasible, so I will explore the other 4 suggestions." CreationDate="2016-03-12T14:09:00.577" UserId="16992" />
  <row Id="10691" PostId="10675" Score="0" Text="Thanks for answering! What if there is a strong inter-attributes correlation as high as 1, does this mean that these features hold redundant information for the learner? can I safely remove one of them without risking to lose information?" CreationDate="2016-03-12T15:58:25.160" UserId="9774" />
  <row Id="10692" PostId="10651" Score="0" Text="I was also looking at examples where people had taken an average output of the prediction." CreationDate="2016-03-12T19:03:04.090" UserId="16971" />
  <row Id="10693" PostId="10650" Score="0" Text="but, can we further train those pre-trained models? or we can initialize weights of pre-trained models and train them for our purposes ?" CreationDate="2016-03-12T19:42:53.393" UserId="16964" />
  <row Id="10694" PostId="10658" Score="0" Text="I have already group similar book together.But what I need is add some function to make it more correctly. `xxxxx 1` and `xxxxx vol.one` can be accept as similar , but `xxxxx 1` with `xxxxx 2` or `xxxxx vol.II` not.That's the problem." CreationDate="2016-03-13T02:49:42.470" UserId="15325" />
  <row Id="10695" PostId="10658" Score="0" Text="As I said in the question, My step now is  1. group similar book(what you mention correlation clustering, this can not solve step 2)  2. tokenize book name again, extract the book number , regroup them. I need a better step 2, or merge step 1,2 together." CreationDate="2016-03-13T02:53:13.797" UserId="15325" />
  <row Id="10696" PostId="10683" Score="0" Text="A home team advantage may apply, and be a learnable thing, so this need for symmetry may not be so important in sporting fixtures. But otherwise very interesting question, could apply in many A vs B predictions." CreationDate="2016-03-13T09:05:32.440" UserId="836" />
  <row Id="10697" PostId="10685" Score="0" Text="Thanks for answering! In such case, is there a systematic way to identify when a combination of features have more &quot;predictive power&quot; when put together? there are few tips here, http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/ which other methods are there to accomplish this?" CreationDate="2016-03-13T18:41:42.103" UserId="9774" />
  <row Id="10698" PostId="10673" Score="0" Text="Bandit package takes 2 inputs trials and successes and based on that gives me a probability of success of a arm. I couldn't figure how to modify the code in the book to this. ( i am a newbie, a little help would be useful)" CreationDate="2016-03-14T06:45:03.673" UserId="15905" />
  <row Id="10699" PostId="10673" Score="0" Text="@karan please specifiy exactly what portion of the code you did not follow and an example of what you have tried out" CreationDate="2016-03-14T06:59:13.747" UserId="13686" />
  <row Id="10700" PostId="10685" Score="0" Text="What you are asking for is https://en.wikipedia.org/wiki/Feature_selection . Solving it accurately is NP-Complete. however, there are plenty useful methods that can help in practice." CreationDate="2016-03-14T07:03:41.747" UserId="13727" />
  <row Id="10701" PostId="10673" Score="0" Text="The current implementation involves randomly choosing an arm and recording the reward. But my problem statement is  - n ads are running with each of budget B. Now i want to re -distribute budget nB among n ads based on the success probability of ad. (prob can be calculated with revenue and trial metric of each ad)" CreationDate="2016-03-14T07:17:20.953" UserId="15905" />
  <row Id="10702" PostId="10693" Score="0" Text="I'm not quite sure about whether this answer is right (since I've answered my own question). Please correct it if it's wrong." CreationDate="2016-03-14T08:33:56.397" UserId="13686" />
  <row Id="10704" PostId="10206" Score="0" Text="I used the `--packages` options. It seems like the addition to `spark-defaults.conf` is crucial. Without it, as suggested by @DamianWilbur it doesn't work." CreationDate="2016-03-14T11:22:44.267" UserId="3591" />
  <row Id="10705" PostId="10685" Score="0" Text="Thanks for answering!" CreationDate="2016-03-14T11:27:31.377" UserId="9774" />
  <row Id="10706" PostId="10644" Score="0" Text="`which(rowSums(coef(fit.lasso))!=0)`?" CreationDate="2016-03-14T13:10:21.253" UserId="15202" />
  <row Id="10707" PostId="10699" Score="0" Text="What is the architecture of your neural network? How many layers, what type of activation's, number of nodes, etc..." CreationDate="2016-03-14T15:03:30.207" UserId="14779" />
  <row Id="10708" PostId="10699" Score="0" Text="I used input layer of 1 unit, one hidden layer of 15 units (tried up to 25 units) and output layer of 1 unit. For activation I used the sigmoid function." CreationDate="2016-03-14T15:43:00.570" UserId="17047" />
  <row Id="10709" PostId="10670" Score="0" Text="A follow-up question: I was thinking that in situations where there might be a huge number of predictor variables, one of the uses of Data Mining could be to find a short-list of predictors to then do regressions (or some such) with. What's your view on doing something like that? Legit or non-legit?" CreationDate="2016-03-14T16:32:16.493" UserId="16928" />
  <row Id="10710" PostId="10626" Score="0" Text="@Wabbit It's a very general question. I've run a few Decision Trees, and in each case, the ROC has never been as great as I see in tutorials. The last one was to do with the [Stack Overflow Developer Survey](http://stackoverflow.com/research/developer-survey-2015) data. I was trying to predict &quot;Job Satisfaction&quot; using Age, Experience, Compensation, Employment Status and Remote Status (i.e. telecommuting)." CreationDate="2016-03-14T16:37:49.150" UserId="16928" />
  <row Id="10711" PostId="10670" Score="1" Text="Very legit. There's a whole class of algorithms for &quot;dimensionality reduction&quot; and it sounds like that's what you're looking for. These algos help you take your &quot;too many predictors&quot; and turn them into &quot;just enough predictors&quot; by isolating the ones that contain the most signal to predictions. Give principle component analysis a look." CreationDate="2016-03-14T16:37:52.187" UserId="12575" />
  <row Id="10712" PostId="10670" Score="0" Text="I'm familiar with traditional principal components analysis. Is there a flavor of that for Decision Tree analysis too? You mentioned the importance of prediction. When using Decision Trees to essentially shortlist the most important predictors, does the ROC become less important?" CreationDate="2016-03-14T16:41:49.637" UserId="16928" />
  <row Id="10713" PostId="10696" Score="0" Text="Yes, I had a basic understanding and I know about supervised and unsupervised learning methods in a general sense. I was stumped at this problem because it felt like a Supervised learning problem but there was no labels.&#xA;Can you shed more light on what algorithms can be used in this case? &#xA;Thank you for the link to the tutorials, I'd definitely go through them and try to figure this out." CreationDate="2016-03-14T17:23:29.697" UserId="17034" />
  <row Id="10714" PostId="10670" Score="1" Text="Decisions Trees do not do what you want them to in that regard - by default the algorithm considers every feature you give it, even if there's not a lot of signal it will still be used. The only way to get a &quot;shortlist&quot; is with a technique that explicitly reduces the dimensionality of the feature space." CreationDate="2016-03-14T17:27:10.477" UserId="12575" />
  <row Id="10715" PostId="10646" Score="0" Text="The text above has pretty good steps for doing back-propagation. It's spelled out procedure-wise to the point where I got it, which means you should be more than fine. :)" CreationDate="2016-03-14T19:08:10.323" UserId="16969" />
  <row Id="10716" PostId="10699" Score="0" Text="Is the sigmoid activation function applied to the output layer as well?" CreationDate="2016-03-15T00:20:33.047" UserId="14779" />
  <row Id="10717" PostId="10699" Score="0" Text="Yes for the output layer as well" CreationDate="2016-03-15T00:45:45.770" UserId="17047" />
  <row Id="10718" PostId="10699" Score="0" Text="Thanks Armen. I will try without the sigmoid today evening. However in my training data set the output y is not linear, I have converted y to 0 or 1 based on whether the linear output is greater than or less than 12.5; don't you think it should work for such a case with sigmoid function?" CreationDate="2016-03-15T02:10:03.867" UserId="17047" />
  <row Id="10719" PostId="10699" Score="1" Text="Can you post the code somewhere? Have you scaled the input data to -1, 1? How did you initialize the weights and what learning rates did you try? Can you plot the learning curves - if they don't decrease, learning rate might be too low, if they jump around a lot, learning rate might be too high. You should definitely use the sigmoid function also on the output, don't remove it." CreationDate="2016-03-15T07:59:12.837" UserId="676" />
  <row Id="10720" PostId="10678" Score="0" Text="Can you write your definition and derivation of $error^2$?" CreationDate="2016-03-15T08:17:33.927" UserId="381" />
  <row Id="10722" PostId="10714" Score="0" Text="Thanks Armen! I implemented my code in octave and i am not aware of keras and epochs, i will try and explore these concepts." CreationDate="2016-03-15T08:26:18.363" UserId="17047" />
  <row Id="10724" PostId="10690" Score="0" Text="Would you be interested in an audio fingerprinting technique like the one used in Shazam ? https://www.ee.columbia.edu/~dpwe/papers/Wang03-shazam.pdf" CreationDate="2016-03-15T12:25:52.667" UserId="12527" />
  <row Id="10725" PostId="10690" Score="1" Text="@LaurentDuval I've read that fantastic paper, but no, that's not what we're looking for.  We're hoping to be able to have our system &quot;listen&quot; to arbitrary sound recordings (not studio recordings) and analyze them." CreationDate="2016-03-15T15:22:31.333" UserId="17022" />
  <row Id="10726" PostId="10690" Score="0" Text="In passing, this post talked about bird sound classification, just for the record http://dsp.stackexchange.com/questions/28612/best-similarity-measure-for-audio-classification/28614#28614" CreationDate="2016-03-15T15:31:35.673" UserId="12527" />
  <row Id="10727" PostId="10721" Score="0" Text="Did you train a custom NER on your data?" CreationDate="2016-03-15T16:18:59.707" UserId="381" />
  <row Id="10730" PostId="10716" Score="0" Text="Okay, that is great piece of information good work. But, can i use pre-trained models layer weights to initialize a new model and then further tune that model with my sentences?" CreationDate="2016-03-15T18:04:39.250" UserId="16964" />
  <row Id="10731" PostId="10700" Score="0" Text="But, i want to initialize my new word2vec model with pre-trained model weights. Is it possible to use already pre-trained model layer weights to initialize new model. After initialization i want to train that model with new sentences. is it possible?" CreationDate="2016-03-15T18:07:05.093" UserId="16964" />
  <row Id="10732" PostId="10714" Score="0" Text="@NeilSlater Done" CreationDate="2016-03-15T20:30:36.730" UserId="14779" />
  <row Id="10733" PostId="10725" Score="0" Text="In the original paper [https://cs.stanford.edu/~quocle/paragraph_vector.pdf] they write about PV-DBOW: &quot;We only need to store the softmax weights as opposed to both softmax weights and word vectors in the previous model (PV-DM).&quot; Does it mean that 'W1' and 'W2' are initialized randomly and afterwards are not updated. Are they only used to identify the respective word?" CreationDate="2016-03-15T23:21:12.863" UserId="17081" />
  <row Id="10734" PostId="10698" Score="0" Text="This is great. I am going to take your third option as my solution because it works better for my actual data set. There could be a situation that a month is missing for a year however I would still need the `c` in that case." CreationDate="2016-03-15T23:35:24.890" UserId="10345" />
  <row Id="10735" PostId="10730" Score="0" Text="Depends on the tolerable data loss, and also on the problem statement too!" CreationDate="2016-03-16T04:50:56.490" UserId="11097" />
  <row Id="10736" PostId="10721" Score="0" Text="No I haven't tried training a custom NER. But there might be issues with that approach, I do not have a large enough training data set to train the NER, and even if I did have a large enough training data set, i'm not sure how the model would react to terms that were not present in the training data set or are from a different domain." CreationDate="2016-03-16T05:51:45.043" UserId="14393" />
  <row Id="10741" PostId="10725" Score="0" Text="Nope. There is no word weight matrices `W1` and `W2` in DBOW model. The model only learns paragraph vectors. Each paragraph vectors tries to predict the words in context directly. Thus finally only the document matrix is learned. But in gensim, there is a parameter `dbow_words`, when it is set to 1, it learns word weight matrices `W1` and `W2` just as the skip-gram model." CreationDate="2016-03-16T13:03:52.350" UserId="16024" />
  <row Id="10742" PostId="10740" Score="0" Text="How can I fit only salary ? km.fit(data['salary']) ?" CreationDate="2016-03-16T13:56:26.467" UserId="5091" />
  <row Id="10743" PostId="10731" Score="0" Text="(+1) Yeah, as simple as that :)" CreationDate="2016-03-16T14:00:19.263" UserId="11097" />
  <row Id="10744" PostId="10740" Score="1" Text="You don't have any other attribute? http://stats.stackexchange.com/questions/40454/determine-different-clusters-of-1d-data-from-database" CreationDate="2016-03-16T14:00:24.320" UserId="11007" />
  <row Id="10745" PostId="10740" Score="0" Text="no other attribures" CreationDate="2016-03-16T14:01:31.833" UserId="5091" />
  <row Id="10746" PostId="10740" Score="1" Text="You should try another technique instead of clustering, read the link above" CreationDate="2016-03-16T14:14:28.510" UserId="11007" />
  <row Id="10747" PostId="10736" Score="2" Text="Please be more precise. If your scores are really &quot;scores&quot; they are not categories but numerals. If your problem is about categorical data please tell me what are scores and then I'll post the answer." CreationDate="2016-03-16T14:48:10.140" UserId="8878" />
  <row Id="10748" PostId="10714" Score="0" Text="Please find my code here, still not been able to solve in Octave.&#xA;&#xA;Main program @ http://pastebin.com/v1LCqYqT&#xA;Cost function @ http://pastebin.com/TNQJ59dm&#xA;Predict function @ http://pastebin.com/T51WABvk&#xA;Sigmoid function @ http://pastebin.com/gvp2SPH9&#xA;&#xA;Please see where I could have gone wrong" CreationDate="2016-03-16T16:13:02.040" UserId="17047" />
  <row Id="10750" PostId="10745" Score="0" Text="Totally depends on your data. What are trying to do, how much data you've got and what does it look like?" CreationDate="2016-03-16T22:38:15.337" UserId="676" />
  <row Id="10751" PostId="10745" Score="0" Text="@stmax This is true. It definitely does depend in part on the specific data. But it's also somewhat generalizable, which is why they do ML benchmarking. I'm really just looking for some general benchmarks. At any one time I've got 4 - 5 different projects I'm working on and I'm asking this more for general / future reference than for a specific analysis. I typically deal with 40,000 - 2,000,000 rows and usually about 100 predictors. Most commonly multiclass dependent variables." CreationDate="2016-03-16T22:46:09.020" UserId="16787" />
  <row Id="10752" PostId="10725" Score="0" Text="And how exactly are the words predicted from a paragraph vector? Or with which vector I have to multiply a paragraph vector to estimate the probability of a word? And which parameters are updated during gradient descent apart from the paragraph vector?" CreationDate="2016-03-17T01:22:29.333" UserId="17081" />
  <row Id="10753" PostId="10725" Score="0" Text="In the first paper, a softmax function is used to convert the dense vector to log-probability vector but then hierarchical softmax function is used. later, negative sampling method is applied to determine it. I suppose only the document matrix which has the paragraph vectors is updated. By mentioning softmax weights, I guess in the paper they have mentioned about the parameters used in the function which is intrinsic." CreationDate="2016-03-17T01:28:11.557" UserId="16024" />
  <row Id="10754" PostId="10747" Score="0" Text="do you mind explaining what's &quot;translation invariant&quot;? Do you have proof or any reference in literature to claim that k-means does not work on skewed data?" CreationDate="2016-03-17T03:31:59.650" UserId="14313" />
  <row Id="10755" PostId="10747" Score="0" Text="yes I got your point, so this is not a typical clustering problem. right ? Then the best way is to sort salaries based on score and divide them to 'K'  categories right ?  Also is there a way to calculate the best number for 'K' ? suggestions please." CreationDate="2016-03-17T06:36:54.117" UserId="5091" />
  <row Id="10756" PostId="10747" Score="0" Text="@TuN. I already gave an example... beware, there are two kinds of skewedness. Some are fine, e.g. if your data is 1000 objects from N(1, 1) and 100 from N(10,1) then this is will be considered a skewed variable, but probably fine for k-means because the clusters are well separated and not skewed themselves." CreationDate="2016-03-17T06:51:18.837" UserId="924" />
  <row Id="10757" PostId="10747" Score="0" Text="@Sreejithc321 I don't know if you have a &quot;typical clusteeing problem&quot; because you have not stated the problem. What are *valid* answers to your problem, and what makes one answer *better* than another. Until you specify this, random assignment is a valid solution to your problem." CreationDate="2016-03-17T06:54:31.293" UserId="924" />
  <row Id="10758" PostId="10747" Score="0" Text="For 1 dimensional and 2 dimensional data, I suggest you also visualize the data (and any result) and discuss what is interesting, bad, difficult, desired on the *plot*. Datascience is about telling a story, and images help a lot there." CreationDate="2016-03-17T06:56:45.720" UserId="924" />
  <row Id="10759" PostId="10745" Score="1" Text="read [this research](http://jmlr.org/papers/v15/delgado14a.html) where they compare 179 different models on 121 data sets. It talks about the accuracy of the models across the data sets, but not so much about the speed." CreationDate="2016-03-17T06:57:13.630" UserId="10517" />
  <row Id="10760" PostId="10658" Score="0" Text="I added a portion to describe how I would tacke it, which is basically merging the two steps" CreationDate="2016-03-17T07:28:56.517" UserId="14904" />
  <row Id="10761" PostId="10750" Score="0" Text="I am in a fix whether this belongs here or SoftwareRecs.  Looks like it's more at home here :)" CreationDate="2016-03-17T07:44:58.067" UserId="11097" />
  <row Id="10762" PostId="10746" Score="0" Text="Thank you bogatron, so it is not a good idea to compare two topic models based on how well they 'reconstruct' the original documents, right? can you please explain why? or point me to good (technical) reads?" CreationDate="2016-03-17T07:46:08.033" UserId="17101" />
  <row Id="10763" PostId="10658" Score="0" Text="That is what I try to avoid.You must extract the number from book title(as I do now), rather than add a weight to number at step 1.  Extract number have a lot work to do , such as when title like `there are 2 man vol.1`.I wouldn't fair such problem when using a similarity weight." CreationDate="2016-03-17T07:47:01.207" UserId="15325" />
  <row Id="10764" PostId="10750" Score="0" Text="I believe so, because of the interplay with dimension-reduction techniques. By the way, I feel that a tag for `high-dimensional data` is lacking here" CreationDate="2016-03-17T07:56:27.100" UserId="12527" />
  <row Id="10767" PostId="6797" Score="0" Text="How do you define/calculate accuracy?" CreationDate="2016-03-17T11:24:01.780" UserId="15361" />
  <row Id="10768" PostId="10760" Score="0" Text="Hi Ethan, welcome to the site. Can you pl add more details to the question?&#xA;&#xA;Maybe a more detailed account on what you did, and what you need help in?" CreationDate="2016-03-17T14:40:21.300" UserId="11097" />
  <row Id="10769" PostId="10765" Score="0" Text="Are they really that unstructured? Everybody has a name, contact information, and a list of jobs and skills. You can leave unfilled fields blank. Seems rather structured to me. The question is: what kind of machine learning do you want to do with the information? I'd stick with SQL." CreationDate="2016-03-17T18:35:02.953" UserId="381" />
  <row Id="10770" PostId="10747" Score="0" Text="@Anony-Mousse I don't think your claim is correct. K-means can be seen as a EM algorithm on Gaussian Mixtures, so skewedness doesn't make K-means work better or worse. Skewedness of data is likely to skew up the initialization process of centroids, which make K-means trap in local optima. That's being said, even if you have balanced data, K-means might not work because of bad initialization." CreationDate="2016-03-17T20:41:45.467" UserId="14313" />
  <row Id="10771" PostId="10725" Score="0" Text="As I understand softmax there are the softmax paramters and an input that is multiplied with the softmax parameters. Like in this tutorial: [http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression ], where the thetas are the softmax parameters and x is the input. In case of PV-DBOW is the input to the softmax function the paragraph vector of interest or is this paragraph vector transformed before?" CreationDate="2016-03-17T20:56:26.957" UserId="17081" />
  <row Id="10772" PostId="10766" Score="0" Text="I thought of that, but since I am comparing this to a set of 1 and 0, my score should be 0 (since most results won't be 0 or 1), but I get 0.3 instead of 0.8, and 0.3 is still much higher than 0" CreationDate="2016-03-17T21:04:16.297" UserId="13736" />
  <row Id="10774" PostId="10766" Score="0" Text="I re-run your script and I only got 0.13 as mean accuracy." CreationDate="2016-03-17T21:28:49.663" UserId="14313" />
  <row Id="10776" PostId="10766" Score="0" Text="From the page I linked I get: 0.787878787879&#xA;[ 0.78451178  0.78787879  0.79124579]" CreationDate="2016-03-17T21:36:13.370" UserId="13736" />
  <row Id="10777" PostId="10766" Score="0" Text="and 0.374682056691&#xA;[ 0.33211124  0.39263029  0.39930464] for linear regression" CreationDate="2016-03-17T21:36:54.500" UserId="13736" />
  <row Id="10778" PostId="10766" Score="0" Text="how do you deal with missing value?" CreationDate="2016-03-17T21:39:35.223" UserId="14313" />
  <row Id="10779" PostId="10766" Score="0" Text="I followed the tutorial. For example, for age, I take the median and then fill that for all missing values." CreationDate="2016-03-17T21:50:18.727" UserId="13736" />
  <row Id="10780" PostId="10766" Score="0" Text="You can check your predictions in the first snippet (before the mapping) to see how many values are actually 0" CreationDate="2016-03-17T22:31:24.787" UserId="14313" />
  <row Id="10782" PostId="10747" Score="0" Text="No, k-means is not assuming Gaussian mixtures. GMM is the usual EM approach; and it is just as much affected by skewed distributions, because it assumes Gaussians which are non-skewed. Apply exp() to all values, and your clustering result will become substantially worse." CreationDate="2016-03-17T22:55:58.827" UserId="924" />
  <row Id="10783" PostId="10730" Score="0" Text="I agree with the two answers below. However, do you know there is a simple way to quantify the information loss, i.e., using the diagonal of SVD of the covariance matrix?" CreationDate="2016-03-17T23:24:17.773" UserId="14463" />
  <row Id="10784" PostId="10725" Score="0" Text="If you mean the dense embedded paragraph vector or the sparse one-hot paragraph vector, it is the embedded vector which is the input. In all the four models, only the embedded vectors are the input. It is just the case that to pick the respective input word embedded vectors in word2vec, the one-hot vectors are used. But basically only the embedded vectors are input for the softmax function." CreationDate="2016-03-18T09:16:00.367" UserId="16024" />
  <row Id="10786" PostId="10774" Score="0" Text="Thank you for your helpful comments. I didn't mention I also have information on things like area of the household and basic income of the owner. Could these be maybe used for more detailed clustering and then creating energy consumption baselines ?" CreationDate="2016-03-18T11:43:21.410" UserId="17149" />
  <row Id="10787" PostId="10725" Score="0" Text="So the one-hot paragraph vector is multiplied with the paragraph matrix. The result is the embedded paragraph vector that is the input to the softmax, where it is multiplied with the softmax parameters. During training softmax weights and the respective embedded paragraph vector can be updated by gradient descent." CreationDate="2016-03-18T11:57:49.333" UserId="17081" />
  <row Id="10788" PostId="10699" Score="0" Text="I'm voting to close this question as off-topic because we generally close questions as not useful to future readers if they were ultimately due to a typo or other local error" CreationDate="2016-03-18T13:20:58.333" UserId="21" />
  <row Id="10789" PostId="10746" Score="0" Text="Are the two topic models both trained from the same &quot;original&quot; corpus?" CreationDate="2016-03-18T13:25:44.227" UserId="964" />
  <row Id="10791" PostId="6930" Score="0" Text="Interesting...do you have a reference that uses this technique?" CreationDate="2016-03-18T15:08:08.517" UserId="13232" />
  <row Id="10792" PostId="10777" Score="0" Text="Thank you for the reply. So you are taking date as 1 for each month. But for the present data, its said that there are 4 services .first service started somewhere in Jan, second service somewhere in May and so on..Then I cannot take 1st as date. Can we just include month and year?" CreationDate="2016-03-18T16:58:49.240" UserId="13417" />
  <row Id="10793" PostId="10777" Score="0" Text="I would highly recommend setting the day as a place holder even if its not exact, but if you use the `zoo` package you can use `as.yearmon`. Here is an example. https://stackoverflow.com/questions/10446833/how-to-convert-a-character-string-date-to-date-class-if-day-value-is-missing" CreationDate="2016-03-18T17:21:46.140" UserId="10135" />
  <row Id="10794" PostId="10725" Score="0" Text="Exactly. I'm not sure whether the softmax parameters are multiplied there, but involves some computation and yes the update occurs by gradient descent via back-propagation." CreationDate="2016-03-18T21:08:15.160" UserId="16024" />
  <row Id="10795" PostId="10780" Score="0" Text="Have you tried sample_weight=balanced or auto in your model? This takes care of unbalanced datasets rather nicely." CreationDate="2016-03-19T00:01:49.203" UserId="15361" />
  <row Id="10796" PostId="10780" Score="0" Text="@Diego I'm pretty new to sklearn, so I did not know about this feature! Will definitely look into it, cause it seems promising, thank you" CreationDate="2016-03-19T00:16:46.167" UserId="17159" />
  <row Id="10797" PostId="10780" Score="0" Text="Yes, go ahead then. It was a blessing for what I was trying to do. I have added this as an answer so in case this works for you don't forget to accept it." CreationDate="2016-03-19T00:27:24.073" UserId="15361" />
  <row Id="10798" PostId="10700" Score="0" Text="Yes you can. However I don't think the weight matrix is available publicly" CreationDate="2016-03-19T03:38:50.097" UserId="13686" />
  <row Id="10799" PostId="10772" Score="0" Text="&quot;Seasonality&quot; not only refers to yearly patterns, but also e.g. the typical day-night baseline change. Look into classic statistical literature for this." CreationDate="2016-03-19T06:49:02.630" UserId="924" />
  <row Id="10800" PostId="10700" Score="0" Text="Yup, right...? If we train a model ourselves and try to access the access the   trained model weights using Gensim library. Is it possible" CreationDate="2016-03-19T07:49:20.993" UserId="16964" />
  <row Id="10801" PostId="10746" Score="0" Text="Yes! I have 1 input, 2 variations of LDA, each model learns the two parameters (theta and phi). Is is reasonable to assume that the best model in terms of inferring latent patterns, is the model with the best reconstruction error?" CreationDate="2016-03-19T09:31:47.840" UserId="17101" />
  <row Id="10802" PostId="10700" Score="0" Text="Not sure about gensim but because it's a parameter to be optimized most software should allow it" CreationDate="2016-03-19T11:04:43.933" UserId="13686" />
  <row Id="10803" PostId="10776" Score="0" Text="thanks Pablo. Could you give me some links to publications for the 1st and 3rd examples." CreationDate="2016-03-19T12:10:32.913" UserId="1426" />
  <row Id="10805" PostId="10716" Score="0" Text="@Nomi Yes. From [gensim documentation] (https://radimrehurek.com/gensim/models/word2vec.html) , once you load the model, `model = Word2Vec.load(fname)  # you can continue training with the loaded model!`" CreationDate="2016-03-20T14:25:08.733" UserId="13957" />
  <row Id="10806" PostId="10798" Score="0" Text="What do you mean &quot;trust&quot;? the key here is defining what videos you are looking for and not looking for. Then you can ask how to get them. What is &quot;relevant&quot; here?" CreationDate="2016-03-20T22:10:07.963" UserId="21" />
  <row Id="10807" PostId="10798" Score="0" Text="@SeanOwen it was a rather general question: the reliability of &quot;Related videos&quot; overall. To put it into another context: are &quot;related videos&quot; more...well, related then search results to a certain keyword?" CreationDate="2016-03-20T22:13:59.247" UserId="14622" />
  <row Id="10808" PostId="10798" Score="0" Text="Presumably related videos relate to the current video; search results relate to a query. You'd have to define what your goal or metric of goodness is to talk about whether this approach meets that goal." CreationDate="2016-03-20T22:30:32.950" UserId="21" />
  <row Id="10809" PostId="10798" Score="0" Text="@SeanOwen *Presumably related videos relate to the current video; search results relate to a query.* I think this is what I was looking for. I'd spare goals and metric to another question, because before all of that, I wanted to know, whether this method is better than sticking to queries, since this is the motor of my app." CreationDate="2016-03-20T22:35:47.450" UserId="14622" />
  <row Id="10810" PostId="10804" Score="0" Text="What does your data look like? Sales per day for a number of days before and after the decision event, and the decision event day, all repeated for a number of companies and some number (&gt;=1) decision events per company?" CreationDate="2016-03-21T08:45:44.633" UserId="471" />
  <row Id="10811" PostId="10804" Score="0" Text="yes, sales per day per article only for one company before and after the decision (only one decision). I edited it also in the question" CreationDate="2016-03-21T09:00:22.913" UserId="15933" />
  <row Id="10812" PostId="10804" Score="1" Text="Any reason you can't just use a t-test?" CreationDate="2016-03-21T09:09:06.413" UserId="471" />
  <row Id="10813" PostId="10776" Score="0" Text="Take a look to the following slides. There you will see more details about spectral theory and very brief examples of application to shape recognition and webpage ranking (PageRank):&#xA;&#xA;https://www.cs.york.ac.uk/cvpr/talks/PRGraphsFinal.pdf" CreationDate="2016-03-21T09:31:32.497" UserId="2576" />
  <row Id="10814" PostId="10808" Score="2" Text="Why not use statistics! Assume the sales before and after the event have a different mean and possibly different variance, then you can test the hypothesis of different means with a Student's t-test. Basic stats 101. Now, if those assumptions aren't valid, we should find out why." CreationDate="2016-03-21T14:32:13.507" UserId="471" />
  <row Id="10815" PostId="10777" Score="0" Text="Thank you so much.. I am working on it." CreationDate="2016-03-21T15:16:41.397" UserId="13417" />
  <row Id="10819" PostId="10812" Score="0" Text="thanks for the resources, I will have a look and come back" CreationDate="2016-03-21T16:42:11.077" UserId="10275" />
  <row Id="10820" PostId="10811" Score="0" Text="thanks for the resources, I will have a look and come back" CreationDate="2016-03-21T16:42:17.987" UserId="10275" />
  <row Id="10821" PostId="10810" Score="0" Text="I'd suggest using a real database that supports fuzzy string matching: [posgres](https://www.rdegges.com/2013/easy-fuzzy-text-searching-with-postgresql/). It'll be efficient." CreationDate="2016-03-21T21:36:05.480" UserId="381" />
  <row Id="10822" PostId="10815" Score="0" Text="I received a reply from google groups stating that, its both distributed and distributional in different perspectives. Distributional in terms of the hypothesis used and distributed in terms of the distributed features in vector space." CreationDate="2016-03-21T22:27:00.423" UserId="16024" />
  <row Id="10823" PostId="10808" Score="0" Text="The mean and variance are in general changing with the  time. The OP needs to isolate the effect of his program from this development." CreationDate="2016-03-21T22:43:06.573" UserId="15361" />
  <row Id="10824" PostId="10801" Score="0" Text="Alas, I've seen this link.  Thank you, though.  The reason it didn't fit for me was it relied on tiling the output for a 2x upscaling.  I could perhaps do as you suggested and produce a feature vector much larger than the convolution output, then resize, but it'll take me a while to figure out how that reshape changes the flow of data." CreationDate="2016-03-21T23:20:41.257" UserId="16135" />
  <row Id="10825" PostId="10802" Score="1" Text="The blue points are the closest to their respective neighbor green points, this is how the embedding was performed. Loosely speaking the similarities (or distance) should be preserved. Going from 25 dimensions to only 2 very likely results in loss of information, but the 2D representation is the closest that can be shown on the screen." CreationDate="2016-03-21T23:50:18.877" UserId="7848" />
  <row Id="10826" PostId="10815" Score="0" Text="yeah, the representation is distributed in the sense that a word vector is capturing multiple concepts, each concept is itself a vector. For example: $v_{king}$ might capture two concepts `male` in gender and `royal`, $v_{queen}$ captures `female` in gender and `royal`. That's why $v_{king} - v_{queen} \sim v_{man} - v_{woman}$" CreationDate="2016-03-22T00:40:02.460" UserId="14313" />
  <row Id="10827" PostId="10821" Score="0" Text="What is the point? What does the residual squared being neither convex nor concave have to do with whether there is only one local minimum, the global minimum?" CreationDate="2016-03-22T02:30:27.997" UserId="12443" />
  <row Id="10830" PostId="10821" Score="0" Text="sorry if it's not clear, the function is convex meaning local minima is global minima, strictly convex means only one global minima. The fact that a function is convex helps a lot in optimization procedure. If it's not convex, then it's difficult to find minima." CreationDate="2016-03-22T04:33:57.867" UserId="14313" />
  <row Id="10831" PostId="10821" Score="0" Text="Ah, so I believe you are saying that we cannot use convexity to prove whether a minimum is unique or not." CreationDate="2016-03-22T06:40:26.430" UserId="12443" />
  <row Id="10833" PostId="10808" Score="0" Text="where's your evidence for that? The OP has provided no data or plots. And given that there's only 15 data points before and 15 after the decision point, a fairly simple statistical model should do the trick. Seeing the data would be useful." CreationDate="2016-03-22T08:07:41.270" UserId="471" />
  <row Id="10834" PostId="10819" Score="0" Text="'what your goal is':  point 2+3; he problem is that the background in hard to extract, as there are a lot of shadows and many cows are of a similar color" CreationDate="2016-03-22T10:42:59.467" UserId="1426" />
  <row Id="10835" PostId="10820" Score="0" Text="I'm not sure if I understood well. In T-sne paper they state &quot;. t-SNE is capable of capturing much of the local structure of the high-dimensional&#xA;data very well, while also revealing global structure such as the presence of clusters at several scales.&quot;. While keeping both local and global structure (as much as possible) would not tend typically to keep global distances? (I'm talking in general sense, I understand that will depend on the data, but probably is less or more likely to happen). In the Swiss-roll I see it working well, In overall I see yellow points similarly close to blue and red." CreationDate="2016-03-22T10:51:24.737" UserId="5143" />
  <row Id="10836" PostId="10819" Score="0" Text="Well, your image still have some good edges. You can try using some edge detection algorithm to accentuate the objects limits: https://en.wikipedia.org/wiki/Edge_detection.&#xA;But once again, I think the current network your working with is just not good enough for this case. Did you try some basic preprocessing like contrast or spectral regularization ?" CreationDate="2016-03-22T11:56:31.230" UserId="17208" />
  <row Id="10837" PostId="10820" Score="1" Text="What I mean is that you can't just use distance in the lower space as a similarity criterion. t-SNE will keep the global structure such as clusters but doesn't necessary keeps distances. This will depend on the shape of the high dimensional data and the perplexity you use." CreationDate="2016-03-22T12:07:30.270" UserId="17208" />
  <row Id="10838" PostId="10808" Score="0" Text="I speak from my experience. The problem with mean before vs mean after is that it discards the time component. Even if there is a difference it might not be due to the treatment but to a trend or seasonality. And trends and seasonality is what rules sales, again _in_general_ . If you can offer a better i.e. more specific answer what prevents you from doing so?" CreationDate="2016-03-22T13:08:11.140" UserId="15361" />
  <row Id="10839" PostId="10820" Score="0" Text="Ok I see. Thanks for clarifying. Yes I agree that distances in lower space would not be accurate. Now, since t-sne is practical for visualization can I use distances in the lower dimensional plot conceptually? For example in my plot can I say with certainty that blue points are closer or more similar to green ones than to red ones, given the obvious separation of the three groups in the 2d space. Or that would be also hard to say?" CreationDate="2016-03-22T13:09:43.673" UserId="5143" />
  <row Id="10843" PostId="10817" Score="0" Text="Thank you for your answer but  I don't have date column in my data set. I just have year and month variables separately." CreationDate="2016-03-22T13:41:12.860" UserId="13417" />
  <row Id="10845" PostId="10819" Score="0" Text="Do you mean like this: http://scikit-image.org/docs/dev/api/skimage.exposure.html#skimage.exposure.equalize_hist" CreationDate="2016-03-22T14:44:43.030" UserId="1426" />
  <row Id="10846" PostId="10808" Score="1" Text="I asked the OP &quot;any reason you can't just use a t-test?&quot; and received no clarification. Anyone who proposes an analysis method without so much as a sniff of a look at the data is likely to be barking up something that isn't even a tree." CreationDate="2016-03-22T15:41:06.927" UserId="471" />
  <row Id="10847" PostId="10834" Score="0" Text="Why would you want to use the number of results within $\pm 1\sigma$ instead of the $\sigma$ of the moving window directly?" CreationDate="2016-03-22T17:06:04.053" UserId="12876" />
  <row Id="10848" PostId="10816" Score="0" Text="Can you post an example of your data set so we can help?" CreationDate="2016-03-22T17:37:47.650" UserId="10135" />
  <row Id="10849" PostId="10834" Score="0" Text="Inherent in Shewhart control techniques is the probability of various events happening. e.g. 68.3% of the population is contained within 1 standard deviation from the mean, so the probably of 8 points within +/i 1 std dev is 4.7% and so it is likely that a process change has occurred." CreationDate="2016-03-22T19:15:45.463" UserId="16284" />
  <row Id="10850" PostId="10317" Score="0" Text="the main problem is that the .mat file contains two variables `X` (the dataset) and `Y` (the labels, I suppose) whereas the .csv file has only the data points but I cannot spot any labels. Any tips about the labels for the .csv file?" CreationDate="2016-03-22T20:08:48.110" UserId="16263" />
  <row Id="10851" PostId="10317" Score="0" Text="Or I had another idea. Maybe the last column from the .csv file since the values are strictly in range 0:9 is the label column. If that's the case you can load the csv file using the `load()` command as you did for the .mat file. Now you have to take the last column and store it in `Y` and all the other columns in `X` (since in your code `X` is the dataset matrix and `Y` is the label vector)" CreationDate="2016-03-22T20:14:22.543" UserId="16263" />
  <row Id="10852" PostId="10834" Score="0" Text="I agree that violating a Shewhart rule is evidence that there's a process change. What's not clear to me is that this is better evidence of the plateau than a direct test on the level of $\sigma$. (Edit: This is the sort of thing we'd want to test by, say, looking at the graph of number of points outside of $\pm 1\sigma$ in a window vs. the graph of $\sigma$ over that window to see how they compare for this dataset.)" CreationDate="2016-03-22T20:14:23.140" UserId="12876" />
  <row Id="10853" PostId="9246" Score="0" Text="can you please attach a little code snippet with the call to `fitcecoc()`?" CreationDate="2016-03-22T20:18:17.900" UserId="16263" />
  <row Id="10854" PostId="10819" Score="0" Text="I think edge-based algorithms are more adapted since neural networks and image recognition are mostly based on edges (and not colors). So you should try amplifying the edges on your image." CreationDate="2016-03-22T20:39:28.393" UserId="17208" />
  <row Id="10855" PostId="10819" Score="0" Text="I've had a bit of success with histogram equalization to increase contrast, but otherwise not sure how to continue. What can I do to maximize the effect of a segmentation algorithm?" CreationDate="2016-03-22T21:06:04.627" UserId="1426" />
  <row Id="10856" PostId="10819" Score="0" Text="Needless to say, I've tried the scikit-image built-in algorithms like pixel-level labeling, but crf-rnn outperforms them" CreationDate="2016-03-22T21:07:55.480" UserId="1426" />
  <row Id="10857" PostId="10819" Score="0" Text="To be even more specific, I'm not sure how to handle the fact that object and background are of (roughly) the same color" CreationDate="2016-03-22T21:30:25.967" UserId="1426" />
  <row Id="10858" PostId="10665" Score="0" Text="It would be good if you uploaded the data used to generate the plot, e.g. to github or http://www.sharecsv.com/" CreationDate="2016-03-22T22:39:51.230" UserId="381" />
  <row Id="10859" PostId="10820" Score="1" Text="It pretty hard to say. The points in the low dimensional space are initialized with a gaussian distribution centered on the origin. They are then iteratively replaced optimizing the KL-divergence. So I would say that in your case blue points are more similar to the green cluster but there is now way to evaluate how closer they are than to the red cluster. t-SNE." CreationDate="2016-03-22T22:52:28.787" UserId="17208" />
  <row Id="10860" PostId="10820" Score="0" Text="_Taken together, t-SNE puts emphasis on (1) modeling dissimilar datapoints by means of large pair-wise distances, and (2) modeling similar datapoints by means of small pairwise distances. Specifically, t-SNE introduces long-range forces in the low-dimensional map that can pull back together two (clusters of) similar points that get separated early on in the optimization._" CreationDate="2016-03-22T22:54:03.840" UserId="17208" />
  <row Id="10861" PostId="10819" Score="0" Text="I don't know any specific algorithm to point you to. But the idea would be to extract edges, and use an algorithm to &quot;close&quot; them (the cow that are not detected have only a few parts of their edge that merge with the background). I work with OpenCV before and remember having working with such algorithms, they usually play with gaussian filters." CreationDate="2016-03-22T23:00:58.003" UserId="17208" />
  <row Id="10862" PostId="10819" Score="0" Text="But anyway, using neural net like the crf-rnn should avoid this kind of exercise since you only need to recognize a part of the cow, not its entire boundary. Try again increasing the contrast (try to see with your own appreciation if this helps differentiating the cows)." CreationDate="2016-03-22T23:03:53.883" UserId="17208" />
  <row Id="10863" PostId="10819" Score="0" Text="'recognize a part of the cow, not its entire boundary' - not exactly, I need the full cow as I'll need in the next step to find what it's doing" CreationDate="2016-03-22T23:38:04.730" UserId="1426" />
  <row Id="10864" PostId="10842" Score="0" Text="So the only variable that you know that influences the position is the number of backlinks?" CreationDate="2016-03-23T08:47:07.523" UserId="14904" />
  <row Id="10865" PostId="10834" Score="0" Text="I'm sure there are better tests for plateau @MatthewGraves." CreationDate="2016-03-23T11:29:52.600" UserId="16284" />
  <row Id="10866" PostId="10841" Score="1" Text="Your question #1 is too generic and seeks **high level advice**. Can you make it more specific and indicate where you are stuck etc.?&#xA;&#xA;For question #2 you can use a feedforward neural network" CreationDate="2016-03-23T11:45:23.783" UserId="13686" />
  <row Id="10867" PostId="10839" Score="0" Text="It's not clear (from just your question) what leave-on-out even is. You should edit this to give a pointer and explain briefly your understanding of the two, and why you think they are the same." CreationDate="2016-03-23T13:31:06.357" UserId="21" />
  <row Id="10868" PostId="10838" Score="0" Text="Thank you so much.. It gave a lot of information." CreationDate="2016-03-23T14:37:13.503" UserId="13417" />
  <row Id="10869" PostId="10838" Score="0" Text="Please let me know if you need clarification." CreationDate="2016-03-23T14:38:49.043" UserId="847" />
  <row Id="10871" PostId="10842" Score="0" Text="Hi @JanvanderVegt! Yes that's correct. Or specifically, I believe there is a negative correlation to it." CreationDate="2016-03-23T15:07:00.877" UserId="17248" />
  <row Id="10873" PostId="10814" Score="0" Text="Thanks @AmanuelNegash! &#xA;Yeah, there's a lot of talk about batch updates getting stuck in local minima but what I figured out was that when training data is less then it's advantageous to stick with online (non-batch) SGD, otherwise batch updates do help for large datasets.  &#xA;&#xA;Moreover, I wanted to implement BATCH NORMALISATION which builds upon batch updates but I was bit confused regarding it's implementation. &#xA;&#xA;Links which I have been following:&#xA;[Practical recommendation for SGD by Y.Bengio](http://arxiv.org/pdf/1206.5533) and &#xA;[Batch Normalization](http://arxiv.org/abs/1502.03167)" CreationDate="2016-03-23T15:31:23.747" UserId="10967" />
  <row Id="10874" PostId="10850" Score="0" Text="Thank you so much for the clarification.&#xA;To confirm, Bias is the difference between the average of all predictions and the average of true values of a single training model. (No multiple samples/training sets here).&#xA;Variance is the difference between each prediction and the average of all predictions over several training models.&#xA;Residual is the difference  between the prediction and true value of a specific data point. The average of residuals against the average of true values can be used to determine Bias?" CreationDate="2016-03-23T15:42:26.560" UserId="2647" />
  <row Id="10875" PostId="10844" Score="1" Text="Did you take a look at Netflix prize dataset? Yes, the competition is long over and it has been pulled from the official website due to some privacy reasons. You can still try to find it in other locations." CreationDate="2016-03-23T15:49:56.960" UserId="7848" />
  <row Id="10876" PostId="10851" Score="0" Text="I don't think a 7 by 7 matrix is a large one. Yes, I agree that the grid-based one with variations in intensity is a good one. But that is just a replica of my confusion matrix. I need a pie chart like representation with each part showing the different areas of observations with color." CreationDate="2016-03-23T16:09:36.280" UserId="16024" />
  <row Id="10877" PostId="10838" Score="0" Text="I tried this with ggtile too. I included facets in it. Do you have any idea if we can REDUCE the gap between facets to ZERO?" CreationDate="2016-03-23T16:58:50.270" UserId="13417" />
  <row Id="10879" PostId="10814" Score="0" Text="@Yashchandak That's right. May I suggest courses? Yes, coursera Geoffrey Hinton, neural net, udacuty Deep learning. They will give u ideas on how u can integrate tweaks to get best results." CreationDate="2016-03-23T17:12:41.683" UserId="11141" />
  <row Id="10880" PostId="10665" Score="0" Text="Please don't [cross post](http://stats.stackexchange.com/questions/201171/online-detection-of-plateaus-in-time-series)." CreationDate="2016-03-23T17:38:23.587" UserId="381" />
  <row Id="10881" PostId="10853" Score="0" Text="Check out Latent Dirichlet Allocation (LDA).  It's a probabilistic model for words that sorts words and responses into a pre-determined number of topics." CreationDate="2016-03-23T19:48:27.550" UserId="14913" />
  <row Id="10882" PostId="10820" Score="0" Text="Very nice explanation. Thank you very much for your effort. I think that you different comments put together a complete answer." CreationDate="2016-03-23T20:33:22.093" UserId="5143" />
  <row Id="10883" PostId="10851" Score="0" Text="I'm not sure why you are so focused on having a pie chart representation. Pie charts are [almost never a good choice](https://en.wikipedia.org/wiki/Pie_chart#Use.2C_effectiveness_and_visual_perception) for visualization. What are you actually trying to communicate with your graph?" CreationDate="2016-03-23T22:27:20.300" UserId="17078" />
  <row Id="10884" PostId="10850" Score="0" Text="You're on the right track. Average residuals are a direct measure of Bias, as a perfect (completely unbiased) model would have an average residual of 0. Bias is not so much about the difference between the average of all predictions and the average of all true values. It's a fuzzier concept, more about how far off your predictions as a whole are from your true values." CreationDate="2016-03-23T22:31:15.757" UserId="17078" />
  <row Id="10885" PostId="10865" Score="0" Text="Interesting, I don't have access to Matlab, but is it possible to do it in octave?" CreationDate="2016-03-24T01:16:33.543" UserId="2861" />
  <row Id="10886" PostId="10865" Score="0" Text="I have not used it, but there does seem to be a signaling package: http://octave.sourceforge.net/signal/" CreationDate="2016-03-24T01:18:19.067" UserId="10135" />
  <row Id="10887" PostId="10865" Score="0" Text="Thanks! As I've achieved a mini-victory spotting this sine wave after modifying the input data, I'll reward myself and go to bed now! I'll experiment with Octave in the morning!" CreationDate="2016-03-24T01:23:13.947" UserId="2861" />
  <row Id="10888" PostId="10857" Score="0" Text="[Create a CSV first](http://stackoverflow.com/questions/1403087/how-can-i-convert-an-html-table-to-csv)." CreationDate="2016-03-24T02:35:50.777" UserId="381" />
  <row Id="10889" PostId="155" Score="0" Text="Cross-link: [A database of open databases?](http://opendata.stackexchange.com/questions/266/a-database-of-open-databases)" CreationDate="2016-03-24T04:22:56.613" UserId="10319" />
  <row Id="10890" PostId="10862" Score="0" Text="This question is too general and not answering anything." CreationDate="2016-03-24T05:09:26.680" UserId="9123" />
  <row Id="10891" PostId="10814" Score="0" Text="yeah, I have seen those, i personally prefer lectures by Hugo Larochelle, Montreal, i find them mathematically more rigorous. Normal batch updates are fine but the thing is they all cover (if at all) 'batch normalisation' very superficially. I had doubts regarding implementing this. I thought someone who has implemented it can pass on their two cents. Meanwhile, I will try looking into popular libraries for it's implementation. Thanks for the help anyway @AmanuelNegash!" CreationDate="2016-03-24T12:06:45.913" UserId="10967" />
  <row Id="10892" PostId="10626" Score="1" Text="I'm not sure if this is really a problem with decision trees. Decision trees have good &quot;capacity&quot; in terms of being able to model complex interactions between features etc. What I mean is that without doing further feature engineering a linear model might have given you even worse ROC.&#xA;You can work on the following to see if your AUC improves:&#xA;1. Adding more features&#xA;2. Features transformations" CreationDate="2016-03-24T14:11:37.843" UserId="13686" />
  <row Id="10893" PostId="10838" Score="0" Text="May be http://stackoverflow.com/questions/3681647/ggplot-how-to-increase-spacing-between-faceted-plots can provide you with how to control for spaces between different facets. Though I am not sure I understand exactly what you're trying to do." CreationDate="2016-03-24T16:58:18.470" UserId="847" />
  <row Id="10898" PostId="10871" Score="0" Text="I meant how is data in training set classified. Is it done manually?" CreationDate="2016-03-24T18:26:03.023" UserId="17302" />
  <row Id="10899" PostId="10871" Score="0" Text="Ah, I see. That wasn't clear. Sometimes it can be done manually - you would have to label Spam/Not Spam yourself if you were the first person trying to make that classifier. Other times it is part of the collection. You could have number of hours of Netflix watched a month and which customers canceled, so your supervised learning algo already has labels. But you might use something unsupervised - maybe using k nearest neighbors or something related." CreationDate="2016-03-24T18:35:27.450" UserId="10135" />
  <row Id="10900" PostId="10870" Score="0" Text="yes, grad students do it for their professors... or more accurately, grad students get their own undergrad research assistants to do it for them ;-)" CreationDate="2016-03-24T18:52:38.833" UserId="6478" />
  <row Id="10901" PostId="10879" Score="0" Text="but you should be careful using crowd sourced labelling such as from MT, as some mturkers will try and game the system by trying to complete the assigned task asap rather than by carefully following the task instructions. You can help prevent this by interspersing easy tasks w known answers and then boot/reject data that does not answer these correctly." CreationDate="2016-03-24T19:08:24.217" UserId="6478" />
  <row Id="10902" PostId="10879" Score="0" Text="Precisely! And that's an active area of research in machine learning as well... the paper I link alludes to that." CreationDate="2016-03-24T20:01:05.553" UserId="847" />
  <row Id="10903" PostId="10877" Score="1" Text="Can you put a snippet of your CSVs, it is very hard to tell what your columns and values are by what you've given." CreationDate="2016-03-24T20:10:31.600" UserId="10135" />
  <row Id="10904" PostId="10865" Score="0" Text="Am I right in assuming that peaks in the positve mean the signal has that particular frequency and negative troughs mean the frequency is present, but out of phase? How exactly do I determine the precise phases of the sines?" CreationDate="2016-03-24T20:26:25.943" UserId="2861" />
  <row Id="10905" PostId="10880" Score="1" Text="sounds reasonable to start with NB using a bag-of-words approach... use this as your baseline, then start improving it via tf-idf, feature engineering while exploring alternative ML algorithms." CreationDate="2016-03-24T20:33:17.840" UserId="6478" />
  <row Id="10906" PostId="10880" Score="0" Text="So, just to be clear, would you train a BoW model for each classification?  Then create a BoW model for an incoming doc and run NB against the trained models to test?" CreationDate="2016-03-24T21:09:03.290" UserId="17309" />
  <row Id="10907" PostId="10880" Score="0" Text="BoW is just the features you use for training and test (a vector of binary values indicating the presence/absence of a word). You can do multiclass with NB (http://stats.stackexchange.com/questions/142505/how-to-use-naive-bayes-for-multi-class-problems). Here is another useful intro link - http://sebastianraschka.com/Articles/2014_naive_bayes_1.html#multinomial-naive-bayes" CreationDate="2016-03-24T21:17:30.720" UserId="6478" />
  <row Id="10908" PostId="10882" Score="0" Text="I just have 40 observations in a sample and 400 observations in another sample.There are many outliers in first sample. If I remove those, I would have very less data. So I conducted t-test with the original dataset.   2)  If boxplots have to be explained to non -technical people, what can we explain them?   3) Can you suggest a better way to represent confidence intervals?" CreationDate="2016-03-24T21:21:24.130" UserId="13417" />
  <row Id="10909" PostId="10875" Score="0" Text="if you're data is not normally distributed, then use a non-parametric test like Mann-Whitney U test: https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test" CreationDate="2016-03-24T21:36:25.520" UserId="10135" />
  <row Id="10910" PostId="10746" Score="0" Text="That may be true but then you would be stuck with the problem of defining &quot;reconstruction error&quot;. I suspect you would be better off using something like [perplexity](https://en.wikipedia.org/wiki/Perplexity) to compare the learned models." CreationDate="2016-03-24T22:44:46.750" UserId="964" />
  <row Id="10911" PostId="10882" Score="0" Text="For this particular study, I have 6 samples of data. 3 of test and 3 of control. Test groups have very less data (40 observations in each) but control has around 400 observations in each sample. I have conducted t-test with each respective test and control groups. Now I need to represent the confidence intervals of these t-test in a chart in R. How can I do that?" CreationDate="2016-03-24T22:47:02.060" UserId="13417" />
  <row Id="10917" PostId="10881" Score="0" Text="The same confusion remains in the answer too. It has properties from both representation. Lets see what it has in common.&#xA;`Distributional`: It has a matrix of size WxC and then its reduced to Wxd, where d is the embedding vector size. It uses window sizes to determine the context.&#xA;`Distributed`: Dense, low-dimensional vectors. It preserves latent features (semantic properties) in those dimensions." CreationDate="2016-03-25T10:48:06.853" UserId="16024" />
  <row Id="10918" PostId="10876" Score="1" Text="How do your training and testing performance compare?" CreationDate="2016-03-25T13:13:06.453" UserId="964" />
  <row Id="10919" PostId="10882" Score="0" Text="What is the difference between the 3 test (/treatment) sets?" CreationDate="2016-03-25T14:02:52.540" UserId="14913" />
  <row Id="10920" PostId="10882" Score="0" Text="You can add error bars to your bar chart,to stand in for your confidence intervals." CreationDate="2016-03-25T14:42:02.163" UserId="16538" />
  <row Id="10921" PostId="10877" Score="0" Text="Thank you Emre. I've done this." CreationDate="2016-03-25T15:44:46.903" UserId="17308" />
  <row Id="10922" PostId="549" Score="0" Text="If the t and t+1 dependency is a trend or seasonality  - consider extracting it and dealing with the rest as with independent variables." CreationDate="2016-03-25T17:20:21.107" UserId="15361" />
  <row Id="10924" PostId="10893" Score="0" Text="You'd have to define what &quot;patterns&quot; mean to you to get a specific answer." CreationDate="2016-03-25T18:08:05.207" UserId="21" />
  <row Id="10926" PostId="10901" Score="0" Text="You can also check this [code](http://scikit-learn.org/stable/auto_examples/text/document_classification_20newsgroups.html#example-text-document-classification-20newsgroups-py)." CreationDate="2016-03-25T20:01:14.770" UserId="17324" />
  <row Id="10927" PostId="10901" Score="0" Text="Till, thank you.  That does answer my question(s).  On another note, what is the best way to prepare training data? The ipython nb uses CSV. I'm thinking I should preprocess text to clean punctuation and store text in a comma delimited way?  Do you have any suggestions?  I have MOUNTAINS of PDFs to extract text from." CreationDate="2016-03-25T20:31:11.147" UserId="17309" />
  <row Id="10928" PostId="10901" Score="0" Text="You can remove stop words for instance by adding the parameter `stop_words='english'` in your Vectorizer. You should also use [stemming](https://github.com/scikit-learn/scikit-learn/issues/1156) to keep the root form of the words. And for the punctuation, the vectorizer can do it for you with attributes like `token_pattern` (that you can customize like that for instance `r&quot;(?u)\b\w\w+\b&quot;`) or `strip_accents='unicode'`." CreationDate="2016-03-25T20:45:33.837" UserId="17324" />
  <row Id="10929" PostId="10876" Score="0" Text="@bogatron During training, I compute the ROC AUC (and an indicative confusion matrix with threshold = 0.5). During testing, I compute the confusion matrix. The ROC AUC's for some trained models get good values (&gt;80%) but the confusion matrices always have too many false positives (FP)." CreationDate="2016-03-25T21:00:52.847" UserId="17305" />
  <row Id="10931" PostId="10900" Score="0" Text="I got the point! Thank you so much!!!" CreationDate="2016-03-26T01:40:14.153" UserId="17332" />
  <row Id="10933" PostId="10907" Score="0" Text="Thank yo so much. I conducted non parametric tests on the data as the data was not that normal and had many outliers.I tried to remove outliers to get normal distribution but I was loosing data and so carried out non parametric test. Regarding the boxplot- what can I explain about each boxplot? What are the important points to list while explaining boxplots?" CreationDate="2016-03-26T18:04:59.500" UserId="13417" />
  <row Id="10936" PostId="10911" Score="0" Text="You're right, of course - I was being stupid.  In other words, don't ever calculate the probability using the Gaussian PDF, but instead take the log of the Gaussian PDF and then compute log probability." CreationDate="2016-03-26T21:11:55.780" UserId="17344" />
  <row Id="10939" PostId="10919" Score="5" Text="I feel you got disappointed too soon. Start by taking data science MOOCs which are free and after getting a little bit into that, decide what to do." CreationDate="2016-03-27T17:10:27.553" UserId="3151" />
  <row Id="10941" PostId="10921" Score="0" Text="Thank you for your reply, will definitly look into that!" CreationDate="2016-03-27T23:08:36.273" UserId="17359" />
  <row Id="10942" PostId="10922" Score="0" Text="Nice explanation!  Are there any papers/tech articles for a read on how AlphaGo works?   I understand that it's Monte-Carlo search with deep learning, but want to know it a bit more clearly!" CreationDate="2016-03-28T06:43:52.043" UserId="11097" />
  <row Id="10943" PostId="10922" Score="0" Text="Thank you ! So the premises of my question are just totally wrong (not sure if it just comes from me or from some badly written and misleading articles), should I somehow edit the question to make clear that the premises are wrong ?" CreationDate="2016-03-28T07:44:56.473" UserId="17360" />
  <row Id="10944" PostId="742" Score="1" Text="I liked your &quot;A data scientist is someone who is better at statistics than any software engineer and better at software engineering than any statistician.&quot;" CreationDate="2016-03-28T08:48:18.593" UserId="13686" />
  <row Id="10945" PostId="10922" Score="0" Text="@agemO: No, in general it is best to leave the question as-is in this respect, because if you correct it with information from an answer, the question/answer pair won't match up." CreationDate="2016-03-28T08:50:19.073" UserId="836" />
  <row Id="10946" PostId="10922" Score="3" Text="@Dawny33: I haven't read any formal papers on AlphaGo, my answer is extrapolated from sources such as  http://googleresearch.blogspot.co.uk/2016/01/alphago-mastering-ancient-game-of-go.html and from what I know about reinforcement learning combined with neural networks (in fact I am studying this in my spare time this year, so only a beginner). I've not seen code for AlphaGo, but you can find code for variations of DeepMind online e.g. https://github.com/spragunr/deep_q_rl - these are related in that they also use a (deep) NN to estimate the value of a move in a grid-like environment." CreationDate="2016-03-28T09:01:30.307" UserId="836" />
  <row Id="10948" PostId="10922" Score="1" Text="Just downloaded the nature paper from scihub.io.  Not sure whether I can share the link here or not :D  (But, the paper helps in making better sense of Neil's answer and the AlphaGo approach)!" CreationDate="2016-03-28T12:01:41.653" UserId="11097" />
  <row Id="10949" PostId="10857" Score="0" Text="As you can  see from the question, creating CSV programmatically is the problem. I need to first get the data by writing a script and that is where I am running into problem." CreationDate="2016-03-28T14:23:20.483" UserId="3314" />
  <row Id="10950" PostId="10922" Score="0" Text="What is not clear for me is this separation between the value and the policy networks : if a position has a high value, then the policy network should tell the program to explore this position ? What is the difference between &quot;this position has a high value&quot; and &quot;this position should be explored further&quot; ? Does one of the two network use the other ?" CreationDate="2016-03-28T14:23:50.180" UserId="17360" />
  <row Id="10951" PostId="10922" Score="0" Text="@agemO: The policy network picks the best possible actions from a given game state. The value network assesses a game state as being a &quot;winning state&quot;. They interact via the Monte Carlo search. I don't know precisely how they are trained, but yes they clearly relate to each other in that choosing a good action should lead towards game states which have a good chance of winning. However, I don't believe they &quot;use&quot; each other during play. Instead the higher level search algorithm uses both - using the policy network to decide where to search and value network to assess the resulting game states." CreationDate="2016-03-28T16:05:38.840" UserId="836" />
  <row Id="10952" PostId="10927" Score="0" Text="Are weights input or output of Stanford NER?" CreationDate="2016-03-28T16:17:38.920" UserId="843" />
  <row Id="10953" PostId="10930" Score="1" Text="Something that you seem to be missing is Batch Normalization. Described here: http://arxiv.org/abs/1502.03167 &#xA;Might be useful." CreationDate="2016-03-28T16:18:35.397" UserId="17355" />
  <row Id="10954" PostId="10930" Score="0" Text="@JoonatanSamuel Thanks!" CreationDate="2016-03-28T16:20:26.590" UserId="843" />
  <row Id="10955" PostId="10876" Score="0" Text="After your ROC AUC analysis, how are you picking your threshold parameter? There has to be a choice of threshold that reduces your FN rate." CreationDate="2016-03-28T19:14:33.177" UserId="12241" />
  <row Id="10956" PostId="10931" Score="0" Text="I can't think of an easy way, but you might try contacting the traffic control department of various cities. Some of them may share this data: http://www.cabq.gov/abq-data However, I'm not sure that even that site has exactly what you need." CreationDate="2016-03-28T19:43:14.203" UserId="4710" />
  <row Id="10957" PostId="10876" Score="0" Text="I'm picking the threshold that minimizes the error FN+FP. Not much changes with threshold = 0.5, because all the predicted values are polarized in [0.0, 0.1] or in [0.9, 1.0], approximately." CreationDate="2016-03-28T19:51:59.940" UserId="17305" />
  <row Id="10958" PostId="10781" Score="0" Text="To clarify, the class_weight value is dependent on the docs for the classifier, the one I was looking at only had &quot;balanced&quot; and &quot;balanced_subsample&quot;, and when digging through the code, I noticed that, at least in this particular instance, 'auto' was being deprecated just through I'd note for anyone else passing this question/answer!" CreationDate="2016-03-28T19:52:08.967" UserId="17159" />
  <row Id="10959" PostId="10894" Score="0" Text="That was extremely helpful, thanks.&#xA;&#xA;Could you elaborate a little bit more about this idea of creating a test? How could this determine causation? Thanks again, sir!" CreationDate="2016-03-28T20:03:29.997" UserId="17330" />
  <row Id="10960" PostId="10933" Score="1" Text="You have labeled training data, but unlabeled test data? How exactly are you testing? Is this a Kaggle competition or similar?" CreationDate="2016-03-28T20:35:16.350" UserId="836" />
  <row Id="10961" PostId="10924" Score="0" Text="Wow great, thanks! I've seen some business analyst job ads and it does seem like a more light version of data science indeed! Thank you for your answers to all my questions" CreationDate="2016-03-28T20:45:23.077" UserId="17359" />
  <row Id="10962" PostId="10919" Score="0" Text="Ok thanks, how do you think employers view moocs though? Can you get a data science job by having just completed several moocs together with an mba?" CreationDate="2016-03-28T20:47:20.977" UserId="17359" />
  <row Id="10963" PostId="10931" Score="0" Text="Can you describe more precisely, the data that you seek? For example, what would you want the fields to be and for what locations?" CreationDate="2016-03-28T20:47:31.227" UserId="847" />
  <row Id="10964" PostId="10931" Score="7" Text="I vote to migrate this to the open data StackExchange: https://opendata.stackexchange.com/" CreationDate="2016-03-28T21:29:35.980" UserId="13413" />
  <row Id="10965" PostId="10936" Score="0" Text="For clustering, use ELKI instead of Weka. It has much better implementations and many more algorithms. Apart from that, handle clustering with *care*. Most of the time, you get a bad/useless/non-interesting result. Whenever you have a clustering result, you need to *study* it." CreationDate="2016-03-28T22:26:39.113" UserId="924" />
  <row Id="10967" PostId="10933" Score="0" Text="You will have to elaborate more on this -- What is your rationale for *adding* records to the training data? What do you intend to do after adding these records to the training data?" CreationDate="2016-03-29T00:22:23.183" UserId="17397" />
  <row Id="10968" PostId="10919" Score="0" Text="I don't even have an MBA, but I lead the data science and growth teams, and all of my education is through MOOCs (as included in my answer, my math education is through OCW) and online tutorials. So, unless and until you have the knowledge and skills to get the work done, the background don't matter **in most cases**. So, I definitely agree with  @HamidehIraj's that MOOC's would defnintely help you get up and running." CreationDate="2016-03-29T06:58:11.717" UserId="11097" />
  <row Id="10972" PostId="10940" Score="2" Text="You're not wrong AFAIK, but I don't think this answers the question. OP seems to be aware of the difference between a value-based or policy-based model in RL. The two approaches do indeed have different outputs. However, it is more common to see one or the other, not both, and the two models are usually redundant in that &quot;Value_maxarg( S' ) == Policy_maxarg( S, A )&quot; where S is current state, A is action to take, and S' is resulting state. I.e. the two networks in a more usual RL setup would simply produce the same result, even though the outputs are different." CreationDate="2016-03-29T09:04:52.643" UserId="836" />
  <row Id="10978" PostId="10912" Score="0" Text="I'd just use Python.. have a look at &quot;30 seconds to Keras&quot;: http://keras.io/#getting-started-30-seconds-to-keras Keras makes building neural networks really simple." CreationDate="2016-03-29T10:46:36.853" UserId="676" />
  <row Id="10980" PostId="10781" Score="0" Text="Yes, I mentioned the &quot;auto&quot; just in case an older version of sklearn has to be used." CreationDate="2016-03-29T11:01:05.453" UserId="15361" />
  <row Id="10981" PostId="10826" Score="0" Text="As I understand OP doesn't have a Control group, that's why I suggest he builds the baseline with a timeseries approach." CreationDate="2016-03-29T11:04:23.900" UserId="15361" />
  <row Id="10982" PostId="773" Score="0" Text="@ffriend You mean 'dissimilarity'. Metric has a precise definition." CreationDate="2016-03-29T12:42:06.673" UserId="154" />
  <row Id="10983" PostId="10876" Score="1" Text="Maybe your cost function doesn't penalize positive outputs? How does the network behave when you use balanced dataset?" CreationDate="2016-03-29T08:16:47.483" UserId="17355" />
  <row Id="10984" PostId="10937" Score="0" Text="Thanks @Emre for the code snippet. I appreciate it." CreationDate="2016-03-29T13:09:07.797" UserId="3314" />
  <row Id="10986" PostId="10885" Score="0" Text="when the `sentiment 140 training set` consists of only two classes, how come the training set consists of three classes. Its unclear." CreationDate="2016-03-29T14:53:57.420" UserId="16024" />
  <row Id="10987" PostId="10885" Score="0" Text="It is indeed so. You can download the corpus from the link and see for yourself." CreationDate="2016-03-29T14:59:13.333" UserId="17313" />
  <row Id="10988" PostId="10876" Score="0" Text="@JoonatanSamuel With balanced dataset (50% positives and 50% negatives) I have similar results. What do you mean for &quot;penalizing positive outputs&quot;? Thanks" CreationDate="2016-03-29T15:27:14.860" UserId="17305" />
  <row Id="10989" PostId="2537" Score="0" Text="@stmax would you mind posting this comment as an answer?" CreationDate="2016-03-29T16:17:44.690" UserId="847" />
  <row Id="10991" PostId="9841" Score="0" Text="What do you propose to replace the embedding layer with? I tried simply removing the embedding layer but that doesn't work." CreationDate="2016-03-29T18:00:07.760" UserId="13023" />
  <row Id="10992" PostId="10944" Score="0" Text="I think you are looking for a [topic model](https://en.wikipedia.org/wiki/Topic_model) or a [summary](https://en.wikipedia.org/wiki/Automatic_summarization). These are fairly advanced models, though; I would recommend beginning with something simpler." CreationDate="2016-03-29T18:21:30.700" UserId="381" />
  <row Id="10993" PostId="9841" Score="1" Text="Look at the other examples - start e.g. directly with the Dense layer. Remember to set the input_shape parameter." CreationDate="2016-03-29T18:27:32.067" UserId="3044" />
  <row Id="10994" PostId="10944" Score="0" Text="@Emre As I mentioned I'm new to the field, can you suggest any simpler ones?" CreationDate="2016-03-29T18:46:40.463" UserId="17415" />
  <row Id="10996" PostId="10948" Score="0" Text="Thank you for the direction, I was really needed some guidance." CreationDate="2016-03-29T19:26:28.747" UserId="17415" />
  <row Id="10997" PostId="10842" Score="0" Text="Anything? Even general thoughts if you have them..." CreationDate="2016-03-29T20:42:53.223" UserId="17248" />
  <row Id="10998" PostId="10944" Score="0" Text="Totally agree with @Emre. I would also check out Introduction to Statistical Learning. The authors have a MOOC too. https://lagunita.stanford.edu/courses/HumanitiesandScience/StatLearning/Winter2015/about" CreationDate="2016-03-29T22:11:54.760" UserId="10135" />
  <row Id="10999" PostId="10919" Score="0" Text="@Ceylon by taking MOOCs, I meant getting yourself exposed to data science topics for free and giving yourself time to decide. You can become a full data scientist but it seems that because of uncertainty, you are assuming that you can't. Start by MOOCs to get into that, you will find the optimal learning method for yourself then." CreationDate="2016-03-30T03:21:34.830" UserId="3151" />
  <row Id="11000" PostId="10944" Score="0" Text="@dmb Thank you I will take a look." CreationDate="2016-03-30T06:27:47.313" UserId="17415" />
  <row Id="11001" PostId="10946" Score="0" Text="Do the rows correspond to each other here? ie row 6 of `data.t1` corresponds to row 6 of `data.t2`, and you are interested in the change in response in that row &quot;caused by&quot; in some sense the change in the `a`, `b` and `c` variables? Could you give a clearer made up example where the &quot;key parameters&quot; are obvious? Otherwise this isn't particularly clear." CreationDate="2016-03-30T07:55:37.027" UserId="471" />
  <row Id="11002" PostId="10954" Score="0" Text="It's not clear what you mean by 'good' or 'bad', but to start, you need historical data on what those things mean so you can learn to identify it. I don't think this is answerable now." CreationDate="2016-03-30T11:42:26.017" UserId="21" />
  <row Id="11003" PostId="10959" Score="0" Text="Could you add a little about your level of understanding of neural networks. For example, if an answer used terms such as weights, transfer functions, hidden layers etc, would you understand it? How about the equations for feed-forward and backpropagation training? Have you heard about Restricted Boltzmann Machines (RBMs)?" CreationDate="2016-03-30T12:47:17.027" UserId="836" />
  <row Id="11004" PostId="10919" Score="0" Text="A bit offtopic but may I ask where you live Dawny? Here in scandinavia its pretty difficult to get more sophisticated jobs without academic degrees, but I guess its becouse education is free and not very difficult to get." CreationDate="2016-03-30T13:25:34.350" UserId="17359" />
  <row Id="11005" PostId="10958" Score="0" Text="If this is your project, please note your affiliation." CreationDate="2016-03-30T13:51:42.640" UserId="21" />
  <row Id="11006" PostId="10916" Score="0" Text="Yes.. I got it using pareto chart." CreationDate="2016-03-30T16:17:14.330" UserId="13417" />
  <row Id="11007" PostId="10960" Score="0" Text="Could you be more specific about what kind of integration you are looking for?" CreationDate="2016-03-30T16:28:13.953" UserId="12384" />
  <row Id="11008" PostId="10946" Score="0" Text="yes that is exactly it. i will add that note to my post. thanks for pointing it out @Spacedman" CreationDate="2016-03-30T16:29:53.300" UserId="17423" />
  <row Id="11009" PostId="10962" Score="0" Text="&quot;AlphaGo's policy network simply replaces this model for biasing unexplored move priors with a Deep Convolutional Neural Network&quot; -- isn't that what the value network is doing though? That's the source of my confusion. If what you and Joonatan are saying is correct, then the policy network and value network are the same exact for maybe the last layer (for example, maybe softmax for the policy network and logistic regression for the value network), but that sounds overly simplistic" CreationDate="2016-03-30T17:59:03.713" UserId="12515" />
  <row Id="11010" PostId="10962" Score="0" Text="No, I don't think so. The policy network takes a position and tries to predict the probability of each move being played. The value network takes a position and tries to predict the probability of each side winning. These are fundamentally different things." CreationDate="2016-03-30T18:08:02.790" UserId="15828" />
  <row Id="11011" PostId="10962" Score="1" Text="But isn't the probability of each move being played primarily a function of the probability of winning with that move? It seems counterintuitive that the policy network would sometimes favor a sup-optimal move" CreationDate="2016-03-30T18:13:39.153" UserId="12515" />
  <row Id="11012" PostId="10962" Score="1" Text="@Imran: They aren't fundamentally different things. If either network was perfect, it could be used to fully derive the other. For instance, the highest probability policy move should always be the one that results in the highest value state win estimate. Part of the answer is likely that these networks are not perfect (for instance the value network is likely very unreliable in the starting game)" CreationDate="2016-03-30T18:15:03.190" UserId="836" />
  <row Id="11013" PostId="10962" Score="0" Text="The probability of each move is more about how that move compares to other available moves. It does not necessarily contain or require any accurate estimate of the absolute probability of winning with that move. If you tried to train a &quot;value&quot; model with the traditional features for move prediction and some sort of aggregration I don't think it would do well. On the other hand, the probability of winning the game might not necessarily depend on any explicit concept of future &quot;moves&quot; in the position. For example it might depend more on material, shape, etc." CreationDate="2016-03-30T18:23:39.363" UserId="15828" />
  <row Id="11014" PostId="10962" Score="0" Text="That is not to say that these two networks are completely unrelated, but they are different enough that you could not really replace one with the other in any meaningful way, in my opinion." CreationDate="2016-03-30T18:24:40.550" UserId="15828" />
  <row Id="11015" PostId="10962" Score="1" Text="@Imran: Using just one model (policy or value) is *precisely* what is done in other reinforcement learning projects. They are normally interchangeable approaches. Something about the setup for AlphaGo makes having both models better. However it is not simply that they predict different things." CreationDate="2016-03-30T18:54:55.613" UserId="836" />
  <row Id="11016" PostId="10962" Score="0" Text="OK, I think I understand the analogy: You could take the value network estimate of every child move of the position, possibly apply some scaling function, then normalize, and that would look like a probability distribution over all moves. However it makes sense that trying to predict the probability of each move directly (and not necessarily depending on the intermediate step of first evaluating the win-probability of each move) as the policy network does would be more accurate." CreationDate="2016-03-30T19:01:47.257" UserId="15828" />
  <row Id="11017" PostId="10960" Score="0" Text="I am trying to decide which language would be preferable for me, considering beautiful data visualization is a priority. I know you can use both languages side by side when needed, but I'm not entirely sure whether it's possible for instance to choose Python and when it comes to data visualization, be able to integrate R vidualization packages into your code." CreationDate="2016-03-30T19:06:31.093" UserId="17450" />
  <row Id="11018" PostId="10962" Score="0" Text="This again gets into the difference between the relative probabilities of winning with each move vs. the relative probabilities of each move being played." CreationDate="2016-03-30T19:28:54.107" UserId="15828" />
  <row Id="11019" PostId="10962" Score="0" Text="@Imran: Yes that could be it, but I think starting to be conjecture rather than an answer from knowing the details. I'm stuck for an answer (or helping with this one further) because I don't know for certain." CreationDate="2016-03-30T19:29:10.800" UserId="836" />
  <row Id="11020" PostId="10962" Score="0" Text="Please don't hesitate to edit my answer or submit your own - I'm sure either would be a good contribution." CreationDate="2016-03-30T20:11:46.617" UserId="15828" />
  <row Id="11022" PostId="10955" Score="0" Text="Thanks a lot. I was thinking on the same line, but wanted to know  the best way possible. We have parsed title of the description using Regex, but for full description it breaks a lot because of different type of description patterns. NER requires manual work and since we are a very small team, its expensive. Can I use a ngram Lookup from the dictionary?" CreationDate="2016-03-31T03:15:45.520" UserId="13046" />
  <row Id="11023" PostId="10876" Score="0" Text="You should include the confusion matrix for the train set. If the results on it are markedly better, your network overfits. For the signature data, many false positives would be not surprising if positive meant &quot;not forgery&quot;." CreationDate="2016-03-31T05:44:08.797" UserId="6550" />
  <row Id="11024" PostId="10954" Score="0" Text="I agree @SeanOwen, and I apologize for being so vague. I edited my original question, I hope it is a bit more clear. If not, please, let me know." CreationDate="2016-03-31T06:22:56.827" UserId="15568" />
  <row Id="11026" PostId="10955" Score="0" Text="Well, regular expressions allow you to do any lookup, literally/ You can mix all the lookup types and mix them in any way you'd want. I would try to manually markup at least 100 documents and fine-tune your regexps test-driven way to increase precision and recall of matching." CreationDate="2016-03-31T08:51:21.160" UserId="2573" />
  <row Id="11027" PostId="10976" Score="0" Text="Do you also have examples of permits that were not granted?" CreationDate="2016-03-31T14:58:27.087" UserId="14904" />
  <row Id="11028" PostId="10976" Score="0" Text="@JanvanderVegt Good question. I'm almost positive that we don't have that data." CreationDate="2016-03-31T15:52:29.377" UserId="17486" />
  <row Id="11029" PostId="10976" Score="0" Text="Your question appears very generic. Try to be more specific. Or add a head(data.frame) to see your actual data and enable people to provide more detailed suggestions." CreationDate="2016-03-31T19:51:33.563" UserId="17269" />
  <row Id="11030" PostId="10827" Score="0" Text="The description of your problem is quite broad. Can you at least show us the code for how you are training your model ?" CreationDate="2016-03-31T21:54:36.463" UserId="5177" />
  <row Id="11031" PostId="10959" Score="0" Text="I have a rectangular neural network that is feed forward. Going one way the inputs are presented with training data, and the specific outputs are trained. The method of learning is by genetic algorithm, not backpropagation. I wish to reverse the algorithm wherein the outputs become the inputs and the inputs become the outputs, the outputs (formerly the inputs) generating a set of training data that it &quot;believes in&quot; (see the video) for a certain fixed input (formerly the output). How would one make this happen?" CreationDate="2016-03-31T22:30:06.837" UserId="17444" />
  <row Id="11034" PostId="10586" Score="0" Text="Many thanks!  And apologies for the delayed accept." CreationDate="2016-04-01T01:33:10.297" UserId="15877" />
  <row Id="11035" PostId="10985" Score="0" Text="I appreciate the efforts. However, this qn is opinion based here, in this format :)  And, welcome to the site!" CreationDate="2016-04-01T05:29:21.487" UserId="11097" />
  <row Id="11036" PostId="10876" Score="0" Text="Could you post some code + data to reproduce the problem?" CreationDate="2016-04-01T06:57:04.457" UserId="676" />
  <row Id="11037" PostId="10959" Score="0" Text="The demo from the video is probably a RBM. https://en.wikipedia.org/wiki/Restricted_Boltzmann_machine You will  need to change your learning algorithm in that case (or just use a library that supports RBMs). Also see https://class.coursera.org/neuralnets-2012-001 - although this doesn't cover code for a generative network in detail, it does describe the maths." CreationDate="2016-04-01T07:53:40.657" UserId="836" />
  <row Id="11044" PostId="10990" Score="0" Text="Thanks Marcus! I will follow your steps, and comment here to how progress. Indeed, the data is quite nice, almost no missing values, which makes it easier to work with. Thanks again!" CreationDate="2016-04-01T11:45:35.917" UserId="15568" />
  <row Id="11045" PostId="9436" Score="0" Text="Coming back to this, [Andrew Ng's course](https://www.coursera.org/learn/machine-learning) is **hard**. I should have mentioned I'm not strong in math. I've heard that this [other Data Science course](https://www.coursera.org/specializations/jhu-data-science) is a bit easier for learning the ropes. What do you think?" CreationDate="2016-04-01T15:19:09.830" UserId="8960" />
  <row Id="11046" PostId="10989" Score="0" Text="YEs.. I have to make ggplot same as yours but I was getting an error while melting." CreationDate="2016-04-01T16:32:40.737" UserId="13417" />
  <row Id="11047" PostId="10464" Score="0" Text="@scf you are wrong about the reason. The median for your p is exactly round(pT), so it fluctuates between being larger or smaller than pT at regular intervals https://en.wikipedia.org/wiki/Binomial_distribution#Median. The distribution of S/T is slightly skewed because for small p, S is closer to Poisson. But what you really see decreasing, is the variance of S/T, so you might want to include a variance term in your model. This is called regularization." CreationDate="2016-04-01T16:46:10.317" UserId="6550" />
  <row Id="11048" PostId="10463" Score="0" Text="I'm voting to close this question as off-topic because its a statistics question and it would belong better on stats.stackexchange.com" CreationDate="2016-04-01T17:32:44.787" UserId="471" />
  <row Id="11049" PostId="10036" Score="0" Text="I'm not really aware of CNN programs that existed before AlphaGo. Do you have examples? CNN's as you describe in (A) are simply a drop in replacement for traditional machine learning models to do the same thing. (B) was also done exactly as you describe in previous engines, so neither of these are novelties. The novelty is simply applying CNN's in place of old machine learning models within the existing framework of Monte Carlo Tree Search." CreationDate="2016-04-01T18:02:33.193" UserId="15828" />
  <row Id="11050" PostId="10998" Score="0" Text="The term you are looking for is [structured learning/prediction](https://en.wikipedia.org/wiki/Structured_prediction).  Also look up &quot;graph classification&quot;." CreationDate="2016-04-01T19:14:12.710" UserId="381" />
  <row Id="11051" PostId="10876" Score="0" Text="@stmax I just added the code and some data, please take a look to them when you've time. Thanks!" CreationDate="2016-04-01T21:19:28.727" UserId="17305" />
  <row Id="11052" PostId="10969" Score="0" Text="Thanks of the reply. I just added some data and my code to the question, could you please take a look to it when you've time? That would save me. Thanks!" CreationDate="2016-04-01T21:20:28.460" UserId="17305" />
  <row Id="11053" PostId="10983" Score="0" Text="A wrong loss function might be the problem. I just added some data and the code, in case you had time to take a look to them. Thanks!" CreationDate="2016-04-01T21:22:00.897" UserId="17305" />
  <row Id="11054" PostId="1006" Score="1" Text="After coming back to this question over a year later, I can certainly echo that knowing your data is key and you need to have in mind what is the &quot;good&quot; data vs the &quot;bad&quot; data. I tried to use magical solutions like neural networks etc, but the data cleanup process wasn't easy. (Hidden markov models seemed to respond the best to dirty input and were able to predict the outputs best).It was infact just pouring over the data for many weeks after the ML fails and after making many graphs (visual representations of the data are very important) that I was able to spot the solutions to my problems!" CreationDate="2016-04-02T00:09:41.903" UserId="2861" />
  <row Id="11055" PostId="1006" Score="0" Text="@user3791372 Glad to hear from you! It clearly seems that year was productive for you in gaining much better understanding of various aspects of data science. I wish I had more opportunities to learn more, but, on the other hand, I can't complain as I learned quite a lot, too (not always related to data science, but, perhaps, it's even better). Keep it up!" CreationDate="2016-04-02T01:46:32.503" UserId="2452" />
  <row Id="11056" PostId="10985" Score="0" Text="You could raise the issue of models that say nothing causal yet are interesting and useful for other reasons. Or models that say something interesting and important about causal issues but are horrible for predictive purposes." CreationDate="2016-04-02T10:15:30.927" UserId="17535" />
  <row Id="11058" PostId="10962" Score="0" Text="@NeilSlater Please check my answer" CreationDate="2016-04-02T10:49:11.393" UserId="9123" />
  <row Id="11061" PostId="10987" Score="0" Text="Mahout k-means is useless. Good luck!" CreationDate="2016-04-02T14:29:46.857" UserId="924" />
  <row Id="11063" PostId="4950" Score="2" Text="Sorry, I forgot to link to UTAH ;) https://github.com/sonalake/utah-parser" CreationDate="2016-04-02T15:51:32.827" UserId="17541" />
  <row Id="11064" PostId="10987" Score="0" Text="Why do you say it is useless?" CreationDate="2016-04-02T17:06:29.667" UserId="8650" />
  <row Id="11065" PostId="11001" Score="0" Text="I think this gives much deeper insight to the internal mechanisms. I am still not sure whether it explains why the two networks. The issue I have is &quot;assume the evaluation network ... is perfect&quot;. If that was the case, then indeed the policy network is redundant. Just look one move ahead (for all possible moves) and pick the one with the best value network assessment. Of course the value network isn't perfect, and I suspect it gets more accurate the further progressed into the game . . . but I don't know how true/useful that is, or whether it completes this answer." CreationDate="2016-04-02T18:06:04.293" UserId="836" />
  <row Id="11067" PostId="10987" Score="0" Text="Because it is the slowest k-means I've ever seen, and it is incredibly hard to use, and it's next to impossible to get a meaningful output in the end. Yet, people expect it to do magic." CreationDate="2016-04-02T19:13:19.973" UserId="924" />
  <row Id="11068" PostId="10945" Score="0" Text="Thanks @Winks, I read the paper and see what you meant by the approximation algorithm for choosing split candidates. However, what did you mean by &quot;XGBoost also uses an approximation on the evaluation of such split points&quot;? as far as I understand, for the evaluation they are using the exact reduction in the optimal objective function, as it appears in eq (7) in the paper." CreationDate="2016-04-02T20:47:52.480" UserId="16050" />
  <row Id="11069" PostId="11001" Score="0" Text="@NeilSlater Ok. The networks aren ot perfect, but the reasons I have here are still good, just that we need more MC simulations." CreationDate="2016-04-02T22:36:23.163" UserId="9123" />
  <row Id="11070" PostId="11002" Score="0" Text="An angle is defined relative to two rays starting at a common point. What's the other vector going through the origin here besides the one you are evaluating?" CreationDate="2016-04-02T22:47:02.403" UserId="17535" />
  <row Id="11071" PostId="11002" Score="0" Text="The origin vector with 0s at all positions. If you could compute the cosine distance between two vectors, why can't be a vector and an origin vector." CreationDate="2016-04-02T23:06:35.367" UserId="16024" />
  <row Id="11072" PostId="11002" Score="0" Text="The problem is that Infinitely many vectors go through the origin. You need a second point for the vector to go through to uniquely identify it." CreationDate="2016-04-02T23:39:26.673" UserId="17535" />
  <row Id="11073" PostId="10508" Score="0" Text="What are you trying to predict?" CreationDate="2016-04-02T23:44:23.423" UserId="15828" />
  <row Id="11074" PostId="10962" Score="0" Text="@NeilSlater I have thought about the question of interchanging the policy and value network some more, and I now am even more confident that they are not interchangeable in any reasonable way. As I mentioned before, the policy network only cares about relative strengths of moves, so it cannot replace the value network, but I also realized that the value network only works later in simulations. So you could not run the value network on every child node of a position earlier in the game, because it would not be able to predict the values with any accuracy." CreationDate="2016-04-03T01:12:13.480" UserId="15828" />
  <row Id="11075" PostId="10987" Score="0" Text="Which library do you recommend? In regards to mahout, I have NOT noticed a method to assign clusters to subsequent files based on previous files. I wrote Python code to decipher the output!" CreationDate="2016-04-03T01:47:43.170" UserId="8650" />
  <row Id="11076" PostId="10987" Score="0" Text="I prefer ELKI, because it has so many methods to chose from, and it was many times faster in my experiments." CreationDate="2016-04-03T08:07:46.730" UserId="924" />
  <row Id="11077" PostId="10945" Score="0" Text="I edited my answer to adress your comment. Check the [this Q/A](http://stats.stackexchange.com/questions/202858/loss-function-approximation-with-taylor-expansion/) for more details on the evaluation of split points." CreationDate="2016-04-03T09:21:01.290" UserId="15501" />
  <row Id="11078" PostId="10945" Score="0" Text="Thanks a lot, @Winks! would be great if you could also answer my more elaborated question here: http://datascience.stackexchange.com/q/10997/16050" CreationDate="2016-04-03T11:04:19.337" UserId="16050" />
  <row Id="11079" PostId="10945" Score="0" Text="This is a great answer. Hat-trick !" CreationDate="2016-04-03T11:43:31.063" UserId="5177" />
  <row Id="11080" PostId="10745" Score="0" Text="@phiver That's highly useful.  I might publish one like that on speed if no one has done so." CreationDate="2016-04-03T15:46:08.780" UserId="2723" />
  <row Id="11081" PostId="10983" Score="0" Text="Your questions looks better now. I'll try to look" CreationDate="2016-04-03T21:55:57.613" UserId="10301" />
  <row Id="11082" PostId="8457" Score="0" Text="This question gives a method for performing a piecewise regression by defining a function and using standard python libraries. http://stackoverflow.com/questions/29382903/how-to-apply-piecewise-linear-fit-in-python" CreationDate="2016-04-02T23:13:41.380" UserId="17550" />
  <row Id="11083" PostId="4950" Score="0" Text="Without a sample it's hard to see what tool could help you. UTAH is a java library that's able to parse semi-structured text. It might be useful for you." CreationDate="2016-04-02T15:48:54.780" UserId="17540" />
  <row Id="11084" PostId="10998" Score="0" Text="Hi Emre, while these links are relevant. I'm more curious about getting from a graph/ relational structure to a flat feature file, while preserving the sub feature correlations with other major features. I am trying to understand what feature engineering strategies are being applied to this end rather than directly applying machine learning algorithms to sub-features. While that would be theoretically desirable, I'm looking for a simpler yet explainable approach towards feature flattening." CreationDate="2016-04-04T01:49:06.233" UserId="17520" />
  <row Id="11085" PostId="11002" Score="0" Text="&quot;when we try to reduce it to 2 dimensions&quot;: are you referring to some kind of PCA on the word vectors? If yes, your &quot;direction information&quot; would be preserved in the principal components." CreationDate="2016-04-04T03:20:20.543" UserId="12241" />
  <row Id="11087" PostId="11021" Score="0" Text="Good Answer. Also thinking that it can be useful to identify the number of clusters to be found, in order to apply unsupervised learning clustering techniques that require the number of clusters to be given initially" CreationDate="2016-04-04T09:16:28.790" UserId="5143" />
  <row Id="11088" PostId="10417" Score="0" Text="Thank you ! Are you aware of any implementation of this ? (as part of gensim for example). Otherwise, it doesn't seem too complicated to compute." CreationDate="2016-04-04T15:53:58.303" UserId="15402" />
  <row Id="11089" PostId="11001" Score="0" Text="@StudentT Nice explanation of some of the key equations. I would make one small change: It doesn't make a lot of sense to say &quot;You need to see ten moves ahead&quot; to understand the position in Monte Carlo Tree Search. MCTS is a depth-first proof number search, and we don't really ever reach fixed depths like we would with iterative deepening in chess. Even with the value network allowing us evaluations of nodes before the end of the game, we are still not reaching these in a breadth first manner, and there is no min-max evaluation of the nodes, etc." CreationDate="2016-04-04T20:14:51.013" UserId="15828" />
  <row Id="11090" PostId="11031" Score="0" Text="Perhaps you can get some ideas here: http://highscalability.com/blog/2010/12/6/what-the-heck-are-you-actually-using-nosql-for.html" CreationDate="2016-04-04T21:44:01.163" UserId="12241" />
  <row Id="11091" PostId="11032" Score="0" Text="More information would be helpful. What does the raw dataset look like? Is it only &quot;person_id&quot;,&quot;date&quot;,&quot;amount&quot;? Do you have any other information on the donations? How many times, on average does each person donate and how many such people you have in your dataset? Some high level information along these lines will help someone answer your question better." CreationDate="2016-04-05T00:11:32.633" UserId="847" />
  <row Id="11092" PostId="11032" Score="1" Text="I have edited the question with a snippet of the data, this is all the data that is available" CreationDate="2016-04-05T00:19:50.423" UserId="16829" />
  <row Id="11093" PostId="11001" Score="0" Text="@Imran Feel free to edit my post." CreationDate="2016-04-05T01:19:04.213" UserId="9123" />
  <row Id="11094" PostId="11036" Score="0" Text="Will the ARCH model predict the amount or the provide the probability of a person donating?" CreationDate="2016-04-05T01:56:13.673" UserId="16829" />
  <row Id="11095" PostId="10876" Score="0" Text="Maybe I missed it, but it looks like you didn't scale your inputs - if you use tanh activations, you should scale them to -1, +1. Also what frankov said, the MSE loss function is for regression problems. One more question since I don't know torch - how are the weights initialized?" CreationDate="2016-04-05T07:00:19.413" UserId="676" />
  <row Id="11098" PostId="11028" Score="0" Text="Wouldn't you have to change your labels to &quot;Normal&quot;, &quot;Normal&quot;, &quot;Attack at P12&quot;, etc." CreationDate="2016-04-05T09:20:32.487" UserId="17618" />
  <row Id="11099" PostId="11028" Score="0" Text="Thank you for comment. Based on the dataset it is not possible to change the labels like the one that you mentioned because the locations and lines are so much and generating attack for each field in training dataset is not possible." CreationDate="2016-04-05T11:40:51.173" UserId="12345" />
  <row Id="11101" PostId="11032" Score="0" Text="The dataset is very small to do any meaningful analysis on it. Its hard to do more than SQL like queries on it for now. Is your goal to create something that is on going and there is a chance that you get more data like this in the future?" CreationDate="2016-04-05T17:38:13.240" UserId="847" />
  <row Id="11102" PostId="11032" Score="1" Text="@Nitesh: I have a sizeable number of records, about 5K. What i dont have is the any other useful data columns other than the donation date and amount." CreationDate="2016-04-05T17:55:31.860" UserId="16829" />
  <row Id="11103" PostId="11035" Score="0" Text="Thanks for your answer. How about Business Intelligence related to NOSQL, is there any topic which i can talk about? For example how business intelligence is used with nosql, can this be a topic? On the other hand, I want to compare SQL with NOSQL in regard to a complementary topic (such as for example a Algorithm implementation or for a certain kind of application use case), can you help me? @SeanOwen" CreationDate="2016-04-05T20:00:06.453" UserId="17596" />
  <row Id="11104" PostId="11057" Score="3" Text="Could you link to, or better briefly explain, the reasoning behind the &quot;clustering is a form of ensembling&quot; part of the question? I've also not seen any example of separately training on different clusters, so some kind of reference or link to that may help. If you do cluster into separate groups, train multiple models, and then re-combine those models into a single one, well that isn't a &quot;form of ensembling&quot;, it just is ensembling, but with a specific approach to creating the component models. The clustering part is still separate though, it doesn't become a &quot;form of ensembling&quot; . . ." CreationDate="2016-04-05T21:02:24.733" UserId="836" />
  <row Id="11105" PostId="11057" Score="0" Text="I think he meant to say it is a form of _dimensionality reduction_." CreationDate="2016-04-05T21:07:14.143" UserId="381" />
  <row Id="11108" PostId="11057" Score="0" Text="The lead machine learning expert in the current company is behind the statement &quot;clustering is a form of ensembling&quot; however, we do not combine, we simply train a different instance of an ML model on each cluster." CreationDate="2016-04-05T21:43:44.297" UserId="9774" />
  <row Id="11109" PostId="11057" Score="0" Text="@MedAli: Did he/she supply any rationale or explanation for that statement? Also, if you train on separate clusters, then presumably you *predict* by assigning new data to a cluster then using the trained model for that cluster?" CreationDate="2016-04-05T21:44:19.970" UserId="836" />
  <row Id="11110" PostId="11057" Score="0" Text="No, not really. But after bring the idea of using ensembling, that was the reply." CreationDate="2016-04-05T21:46:27.467" UserId="9774" />
  <row Id="11112" PostId="11032" Score="1" Text="Have you done some exploratory plots or analysis? Do people seem to donate in clusters - in which case the most probable donors in the next period will be those who just donated - or are donations uniform random in time, in which case it could be anyone. Make some plots, do some basic summary statistics. Do some people make one big donation and then none, and others make several small donations? Would &quot;total donated so far&quot; be a good predictor of &quot;likely to donate again&quot;? etc etc. You've just presented your data and none of your thoughts. Please edit the Q and add them. We can't think for you." CreationDate="2016-04-06T07:35:15.033" UserId="471" />
  <row Id="11113" PostId="11062" Score="1" Text="How far ahead do you want to forecast? Because `prevday2` doesn't exist if you are trying to forecast less than two days. You want to forecast a week ahead - you can't use any of the `prev` variables. That will simplify things a bit. Also its impossible to see what's going on with the power usage variable in your plot - since its strictly positive, why not log-transform it? Once you've sorted all that out then look at model variable selection. Or just throw everything into a Random Forest model..." CreationDate="2016-04-06T07:45:58.893" UserId="471" />
  <row Id="11115" PostId="11062" Score="0" Text="I have explained my problem in more detail." CreationDate="2016-04-06T08:11:55.833" UserId="13291" />
  <row Id="11116" PostId="11063" Score="0" Text="Add a small chunk of your dataset in the question, which can help the  answerer have an idea about your dataset." CreationDate="2016-04-06T08:21:21.567" UserId="11097" />
  <row Id="11117" PostId="11062" Score="0" Text="What does the power consumption curve look like? How good an estimate of the next half-hour is the current half-hour? What about how good an estimate of the next half hour is the trend based on the last two half hours? Where's your justification for 48 (independent?) models? if you really only want to do one-step ahead forecasting then Kalman filter and job done." CreationDate="2016-04-06T10:03:18.990" UserId="471" />
  <row Id="11118" PostId="11060" Score="0" Text="How sparse are your features? Are they 1% filled or even less?" CreationDate="2016-04-06T12:35:57.277" UserId="11121" />
  <row Id="11119" PostId="11060" Score="2" Text="Also you should note that if your features are sparse then they should only help classify a small part of your dataset, which means overall the accuracy shouldn't change significantly. &#xA;This is kind of a guess, as I don't know what are the characteristics of your dataset." CreationDate="2016-04-06T12:40:31.017" UserId="11121" />
  <row Id="11122" PostId="11036" Score="1" Text="It doesn't look anything like anything presented as a &quot;time series&quot; to me. &quot;Time series&quot; are invariably data observed at regular (monthly, daily) time intervals, and unless you pad this data set out with zeroes for every non-donation day (at which point you have a large number of binary time series, one for each donor), you don't have a time series." CreationDate="2016-04-06T15:49:39.657" UserId="471" />
  <row Id="11123" PostId="11064" Score="0" Text="thanks for your answer!" CreationDate="2016-04-07T01:56:34.443" UserId="17644" />
  <row Id="11124" PostId="11064" Score="0" Text="I thought now that the neural network is  imitating the human's brain, so may be the neuron has something to do with human's understanding." CreationDate="2016-04-07T02:02:33.270" UserId="17644" />
  <row Id="11125" PostId="10558" Score="0" Text="Speech to text and text to intent (NLP) are done on Amazon's servers; not in the cloud. I was also [wondering the same thing](http://datascience.stackexchange.com/questions/11043/how-do-intent-recognisers-work)." CreationDate="2016-04-07T07:08:18.127" UserId="17618" />
  <row Id="11126" PostId="11062" Score="0" Text="Thanks, I am working on it. Will get back to you asap" CreationDate="2016-04-07T08:17:34.897" UserId="13291" />
  <row Id="11127" PostId="5479" Score="0" Text="Thank you very much ,but the tool you provided is non-overlapping network, how about the dynamic overlapping network?" CreationDate="2016-04-07T03:19:18.493" UserId="17676" />
  <row Id="11128" PostId="9232" Score="0" Text="Check out this link: http://blog.minitab.com/blog/adventures-in-statistics/what-is-the-f-test-of-overall-significance-in-regression-analysis (Sorry i don't hve enough reputations to add this as a comment)" CreationDate="2015-12-08T10:38:48.000" UserId="14427" />
  <row Id="11129" PostId="11060" Score="0" Text="@JoãoAlmeida They are not that sparse. They are around 5% filled. The problem is when I look at the difference in the predictions from two models, where the predictions differ, model with sparse features tend to perform better, that's why I expected it to see the boost in AUC as well when I combined them with dense features. I am getting a boost, but seems very low." CreationDate="2016-04-07T10:46:32.407" UserId="17619" />
  <row Id="11130" PostId="11060" Score="0" Text="hum... I don't have any idea for you then" CreationDate="2016-04-07T10:51:28.770" UserId="11121" />
  <row Id="11132" PostId="11082" Score="0" Text="Thanks for the answer! I'd try your suggestion once I'm available. However, I believe that TF-IDF (Term Frequency - Inverse Document Frequency) is here to help with the problems that you addressed on the first two points. The fundamental intuition of using TF-IDF is that we regard a word that is very common in all documents as an undesirable discriminator. By weighing them down and encourage the term with lower occurrence we would be able to come up a better result." CreationDate="2016-04-07T11:33:13.430" UserId="17670" />
  <row Id="11133" PostId="11082" Score="0" Text="For the point of choosing k=3, TBH i have no idea of what number should i choose for k. I heard that silhouette coefficient is one of the measures that helps to determine this value so I performed clustering with k = (3...10), and the coefficient doesn't really improve a lot when k grows." CreationDate="2016-04-07T11:35:32.973" UserId="17670" />
  <row Id="11137" PostId="11074" Score="0" Text="What are the different bounce codes? What are the english descriptions for them? You might have to &quot;weigh&quot; certain bounce codes more heavily than others." CreationDate="2016-04-07T15:12:42.257" UserId="17692" />
  <row Id="11139" PostId="11033" Score="0" Text="I don't think the distribution of the event is the key here. The event is binary (donation or no donation), which in and of itself point towards  binary logistic regression model." CreationDate="2016-04-07T17:51:55.747" UserId="17397" />
  <row Id="11140" PostId="11086" Score="0" Text="I didn't understand from your answer if you want to say feature selection should or should not be performed before using a boosted learning model.&#xA;&#xA;A somewhat related question is whether one should use as large a number of new features (for example take many different powers of continuous features) as allowed by computational constraints." CreationDate="2016-04-07T18:18:36.917" UserId="17571" />
  <row Id="11141" PostId="11086" Score="0" Text="It depends on the boosting algorithm you are using. If you are using simple boosting algorithm and you have a very large number of features it is a good idea to use feature selection , but if you are using boosting algorithm that includes feature selection ( ex viola jones object detection adaboost algorithm) no need to perform the features selection , it is already included" CreationDate="2016-04-07T18:24:30.980" UserId="17696" />
  <row Id="11142" PostId="11064" Score="0" Text="@Zieng Yes..it actually does somehow. Imagine ur gf gives u a chocolate always when she comes to ur home. Now u went to some place and u saw that chocolate. You will remember her. That's because The neurones that connected the chocolate to ur gf has been activated. It means a neuron activates when it gets enough reason to. The collection of these activation will result in some meaningful results. The reason to activate was the image of chocolate. The result is ur gf's image. When u come to the maths, the sigmoid results 1 if there was high probability of the input in the data. In the chocolate" CreationDate="2016-04-07T19:08:25.113" UserId="11141" />
  <row Id="11143" PostId="11073" Score="0" Text="No, 96 is the number of neurons that are looking 55x55 region in the depth of the input image and generate a feature map. You can say that you have 96 filters applied across the depth of the image and learning some feature maps." CreationDate="2016-04-07T21:32:21.807" UserId="16964" />
  <row Id="11144" PostId="11076" Score="0" Text="Try word 2 vector for word representation, then use k means clustering on those words. You will get more meaningful results. :)" CreationDate="2016-04-07T21:36:25.587" UserId="16964" />
  <row Id="11145" PostId="11077" Score="0" Text="Do these documents relevant to the same domain? Or each document explaining different things, that are not related wih other documents." CreationDate="2016-04-07T21:38:55.497" UserId="16964" />
  <row Id="11146" PostId="11077" Score="0" Text="@Nomi it's a large set of patient notes: same domain, but different patients." CreationDate="2016-04-07T21:44:15.543" UserId="843" />
  <row Id="11147" PostId="11096" Score="2" Text="Look up [record linkage](https://en.wikipedia.org/wiki/Record_linkage) to learn about how you can deal with partially overlapping records. You can handle spelling mistakes by [similarity searching](https://en.wikipedia.org/wiki/Similarity_search) the [n-gram](https://en.wikipedia.org/wiki/N-gram) bitstrings." CreationDate="2016-04-08T01:54:48.973" UserId="381" />
  <row Id="11148" PostId="11064" Score="0" Text="@AmanuelNegash thx! wonderful explanation~" CreationDate="2016-04-08T04:39:04.943" UserId="17644" />
  <row Id="11149" PostId="11077" Score="0" Text="If domain is same then, you can train word2vec over all the sentences in the patient files. Either you mix all files sentences or or train model file by file in single run." CreationDate="2016-04-08T05:49:50.867" UserId="16964" />
  <row Id="11151" PostId="11096" Score="2" Text="I've voted to close as &quot;too broad&quot; since there are literally hundreds of possible approaches and you've not even told us you've tried one. Fuzzy matching, keyword matching, clustering, machine learning..." CreationDate="2016-04-08T06:52:58.583" UserId="471" />
  <row Id="11153" PostId="11089" Score="2" Text="The paper doesn't explicitly compare between different regularisation approaches, except by demonstrating dropout as improving on state-of-the-art results at the time (previous results most likely did use some other forms of regularisation, but they are not listed). It also does mention maxnorm weight constraints as an effective additional regulariser to augment dropout." CreationDate="2016-04-08T08:31:58.583" UserId="836" />
  <row Id="11155" PostId="11092" Score="0" Text="Could you reword your answer into proper English?" CreationDate="2016-04-08T12:04:18.593" UserId="15527" />
  <row Id="11156" PostId="11096" Score="0" Text="@Spacedman I haven't tried anything yet to fix it since I don't know what are the best options." CreationDate="2016-04-08T12:29:38.763" UserId="17702" />
  <row Id="11157" PostId="11102" Score="0" Text="OK... If found my problem. I was also using the Paint Widget. Seems to introduce erroneous in the flow. I worked around it." CreationDate="2016-04-08T19:32:09.133" UserId="17712" />
  <row Id="11158" PostId="11107" Score="1" Text="How is this different from finding the points of high density? The &quot;paths&quot; are the manifolds where the points are concentrated, right?" CreationDate="2016-04-08T20:07:03.967" UserId="381" />
  <row Id="11159" PostId="10585" Score="0" Text="In machine learning you are not, generally, hanging any statistical theory on the estimation of your function (like you do with classical ols from stats 101). There is no distribution to be independent and identical to, and therefore you can't violate the assumption." CreationDate="2016-04-08T20:35:45.500" UserId="16538" />
  <row Id="11160" PostId="11074" Score="0" Text="Ppl still use cheques? =l" CreationDate="2016-04-08T21:46:51.723" UserId="17724" />
  <row Id="11161" PostId="8458" Score="0" Text="I just tried, neither decision tree or random-forest in scikit-learn work without one-hot encoding, as least for the scikit-learn 0.17.1. They both try to convert all values to float, as documented [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier.fit)" CreationDate="2016-04-08T22:43:45.797" UserId="17727" />
  <row Id="11162" PostId="11063" Score="0" Text="@Dawny33 hopefully the edited question enlightens you now." CreationDate="2016-04-09T11:23:55.687" UserId="807" />
  <row Id="11163" PostId="10585" Score="0" Text="@Ryan yeah, but in the machine learning jargon there's &quot;information&quot;, and you are violating the assumption that all input records supply the same amount of information to the learning machine. There's an equivalence between information and independence..." CreationDate="2016-04-09T14:20:09.757" UserId="471" />
  <row Id="11164" PostId="5479" Score="0" Text="@SamanthaChen As it is said, [here](http://mlg.ucd.ie/snam/) is for dynamic networks." CreationDate="2016-04-09T14:40:39.460" UserId="8240" />
  <row Id="11165" PostId="11102" Score="0" Text="you should answer the question yourself, also putting the workaround into the answer!" CreationDate="2016-04-09T15:27:25.027" UserId="16284" />
  <row Id="11166" PostId="11088" Score="0" Text="For the same reason that lengthy, colorful Shakespearean style dialogues are not used by medical staff during operations or by pilots and flight control over the radio. It's entirely possible but incredibly impractical and inefficiently long-winded for the situation at hand." CreationDate="2016-04-09T21:51:59.040" UserId="2723" />
  <row Id="11171" PostId="11121" Score="0" Text="A very nice non-technichal book is _Data Science for Business_, by Foster Provost and Tom Fawcett. I think it's an excellent start." CreationDate="2016-04-10T13:49:02.217" UserId="17461" />
  <row Id="11172" PostId="11121" Score="0" Text="What as a programmer?" CreationDate="2016-04-10T18:52:53.840" UserId="10879" />
  <row Id="11173" PostId="11121" Score="0" Text="As anyone interested in getting into data science." CreationDate="2016-04-10T19:31:18.773" UserId="17461" />
  <row Id="11174" PostId="10993" Score="0" Text="Thank you very much! I tried your method. :)" CreationDate="2016-04-10T23:24:12.557" UserId="17423" />
  <row Id="11175" PostId="6870" Score="0" Text="That means you want the entropy of a split in the node, not of the image." CreationDate="2016-04-11T02:08:35.160" UserId="335" />
  <row Id="11176" PostId="11094" Score="0" Text="thanks @marcus-d" CreationDate="2016-04-11T02:28:51.323" UserId="16302" />
  <row Id="11177" PostId="11122" Score="3" Text="So you have some kind of free text? **Any chance of an example or two?** Do you know what the set of diseases you are looking for is? Does the actual gene sequence (GCCATATA etc) enter into this?" CreationDate="2016-04-11T06:54:57.023" UserId="471" />
  <row Id="11178" PostId="11129" Score="0" Text="Do you just have the historic 1s and 0s? Or is there other information available?" CreationDate="2016-04-11T07:26:21.400" UserId="14904" />
  <row Id="11179" PostId="11139" Score="0" Text="Is this a Google product?  The UI looks very google-like" CreationDate="2016-04-11T11:42:48.197" UserId="11097" />
  <row Id="11180" PostId="11134" Score="1" Text="Does using TfIdf vectorizer seem suitable? Maybe in combination with word2vec to leave only top x words for each document?" CreationDate="2016-04-11T11:43:51.170" UserId="15361" />
  <row Id="11181" PostId="11139" Score="1" Text="I don't know. I am not associated with that site in any way. They have their owners and hoster information listed on the front page though." CreationDate="2016-04-11T11:49:39.393" UserId="15361" />
  <row Id="11182" PostId="11134" Score="0" Text="Well, I don't know what Tfldf is. I'm going to check it to see if it works. Thank you" CreationDate="2016-04-11T12:15:30.257" UserId="17784" />
  <row Id="11183" PostId="11134" Score="0" Text="http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting here for example" CreationDate="2016-04-11T12:34:21.760" UserId="15361" />
  <row Id="11184" PostId="11134" Score="0" Text="I just check it and I think it doesn't help me. Basically, that helper class creates a matrix for a set of documents. Each row vector (binary or wordcount) in the matrix corresponds to a document, but for CNN, we need a matrix for each document." CreationDate="2016-04-11T12:51:06.400" UserId="17784" />
  <row Id="11187" PostId="11137" Score="0" Text="I tool ML course from Cousera, due to lack of expertise in Maths and Stats, it did no go well." CreationDate="2016-04-11T17:12:22.373" UserId="10879" />
  <row Id="11188" PostId="11143" Score="0" Text="Sounds a good idea." CreationDate="2016-04-11T17:19:12.973" UserId="10879" />
  <row Id="11189" PostId="11134" Score="0" Text="The point was to leave only x non-trivial words per document ranked by their TfIdf. Then use your original encoding to build document matrices. Not sure if this two step approach idea came across." CreationDate="2016-04-11T17:38:42.377" UserId="15361" />
  <row Id="11190" PostId="11129" Score="0" Text="I have the historic 1s and 0s yes." CreationDate="2016-04-11T18:09:05.280" UserId="17780" />
  <row Id="11191" PostId="11129" Score="0" Text="I also have their associated dates." CreationDate="2016-04-11T19:35:39.133" UserId="17780" />
  <row Id="11194" PostId="11147" Score="0" Text="Thanks @ryan-zotti that is the type of direction I was looking for.  And I corrected the typo (read: brain fart)." CreationDate="2016-04-12T02:35:02.453" UserId="17795" />
  <row Id="11195" PostId="11129" Score="0" Text="@Jan van der Vegt Any insight?" CreationDate="2016-04-12T02:53:15.210" UserId="17780" />
  <row Id="11201" PostId="11159" Score="0" Text="Have you done any searching for sports outcome prediction methods rather than algorithms? The thing I'm most familiar with is the Dixon-Coles model for soccer matches, where the model predicts goals scored. It fits an attack and defence parameter for each time by maximum likelihood based on a set of games, so it is essentially a regression model which you should be able to understand. Look that up, it might give you some ideas." CreationDate="2016-04-12T07:14:31.690" UserId="471" />
  <row Id="11202" PostId="11159" Score="0" Text="You might also explain your data a bit more. Do you just have team A, team B and outcome (A wins|B wins), or do you have who batted first, how many runs or wickets they won by in how many overs, whether rain stopped play and the Duckworth-Lewis method applied etc etc?" CreationDate="2016-04-12T07:18:15.440" UserId="471" />
  <row Id="11203" PostId="11129" Score="0" Text="Are these things more/less likely to happen on weekend days? Or when the weather is hot? Are they getting more frequent or rarer? Do they happen in clusters or independent? These are all questions you need to ask of the data and of experts who know about the data before you can think about formulating a predictive model." CreationDate="2016-04-12T07:21:19.783" UserId="471" />
  <row Id="11204" PostId="11129" Score="0" Text="One approach which might help you search the literature for methods would be to consider the binary outcomes as single samples from a latent (ie unobserved), continuously-valued on (0,1), time series process, such as an autoregressive process. What you've then got is a doubly-stochastic process. Ooh look they figured this out in 1982: http://www.jstor.org/stable/2287312?seq=1#page_scan_tab_contents" CreationDate="2016-04-12T07:29:17.973" UserId="471" />
  <row Id="11205" PostId="11156" Score="0" Text="I have already tried out the ensemble techniques as well as voting classifiers. Still no luck." CreationDate="2016-04-12T08:15:14.567" UserId="17619" />
  <row Id="11206" PostId="11137" Score="0" Text="I had no Math or Stats background when I took one from Andrew Ng, but I did not find it really depressing to cover unknown terms using Wikipedia. Coursera won't give you fundamental understanding which is mostly up to you to the limits you want to dig into abstract concepts." CreationDate="2016-04-12T09:03:12.057" UserId="2573" />
  <row Id="11207" PostId="11156" Score="0" Text="So do you see a lot of overlap then between the predictions from the two datasets? May be there indeed is no new information? I.e. the data tells the same story." CreationDate="2016-04-12T09:20:34.037" UserId="15361" />
  <row Id="11208" PostId="11156" Score="0" Text="yes, I have done exactly that. Though the predictions are not entirely different, the number of samples where predictions differ are quite high (around 15-20%) of the data. For these samples model with sparse features performs better than that of model with dense features. My point is if sparse features perform better, why don't they come as important features in any of the models which I have tried so far." CreationDate="2016-04-12T09:31:22.100" UserId="17619" />
  <row Id="11209" PostId="11159" Score="0" Text="Yes, agree with @spacedman, could you provide a list of data that you have. It might assist us in giving different advice with regard to the analysis technique used." CreationDate="2016-04-12T11:25:08.407" UserId="16284" />
  <row Id="11210" PostId="11156" Score="0" Text="What predictor algorithm do you use?" CreationDate="2016-04-12T12:21:52.437" UserId="15361" />
  <row Id="11211" PostId="11166" Score="1" Text="Have a look at this link on DS.SE ... http://datascience.stackexchange.com/questions/9303/seeking-career-guidance/9304, but in this group, things that are primarily opinion based are considered off topic." CreationDate="2016-04-12T12:38:16.040" UserId="16284" />
  <row Id="11213" PostId="11168" Score="2" Text="Nice article Marcus. Although I think a good part of the different specializations you've put there usually are on only a couple of persons (from the hiring point of view). But that would be a powerful team indeed." CreationDate="2016-04-12T14:05:39.270" UserId="17806" />
  <row Id="11214" PostId="11174" Score="0" Text="Thanks for the post. Could you point me to some examples?" CreationDate="2016-04-12T14:24:27.923" UserId="17815" />
  <row Id="11216" PostId="11174" Score="0" Text="Sure. Are you looking for MLlib examples specifically, or any type of machine learning library? What's your preferred language and what is the file size of your data?" CreationDate="2016-04-12T14:29:48.030" UserId="12515" />
  <row Id="11217" PostId="11174" Score="0" Text="Yes, I'm only looking for MLib examples and I use Scala. I want to do this on a historical data set that would be say a hundred of MB in size to start with." CreationDate="2016-04-12T14:31:23.587" UserId="17815" />
  <row Id="11218" PostId="11174" Score="0" Text="Unfortunately I haven't personally used MLlib in Scala, only python, so maybe another poster can show you some of their own code (I would just end up pulling something from Google). Also I think you're trying to say MLlib instead of Mlib. There is supposed to be an extra L in there so that when fully expanded it shows: Machine Learning (ML) library (lib): MLlib" CreationDate="2016-04-12T14:38:59.687" UserId="12515" />
  <row Id="11219" PostId="11174" Score="0" Text="I'm referring to the Spark-MLib library. Sorry for not being explicit!" CreationDate="2016-04-12T14:40:04.387" UserId="17815" />
  <row Id="11220" PostId="11174" Score="0" Text="Also I believe Spark's ML package is now considered the next generation of MLlib. I've heard that &quot;ML&quot; enables model pipelines, so that you can easily swap out different models without changing your code too much. See some basic examples in the official Spark docs here: http://spark.apache.org/docs/latest/ml-guide.html#pipeline-components" CreationDate="2016-04-12T14:40:37.243" UserId="12515" />
  <row Id="11221" PostId="11159" Score="1" Text="A google (scholar) search for &quot;Cricket prediction ODI&quot; finds a few good looking papers (including predicting outcomes in-game). Howzat?" CreationDate="2016-04-12T14:52:42.327" UserId="471" />
  <row Id="11222" PostId="11166" Score="1" Text="Data Science is a huge domain itself, and the techs. are very different.  For example, you require different set of techs. for NLP, Quant. Finance, etc.  So, this qn cannot be answered and is **broad** too.  And, in it's current form, it is of a **very low quality** with absolutely no/very less information." CreationDate="2016-04-12T16:10:05.327" UserId="11097" />
  <row Id="11224" PostId="11177" Score="0" Text="Thank you for your answer.&#xA;I have read also http://scikit-learn.org/stable/modules/model_persistence.html&#xA;However, I am still curious if it is possible to compile my script and to be able to execute it on systems that don't have scikit package." CreationDate="2016-04-12T16:39:24.453" UserId="15693" />
  <row Id="11225" PostId="11177" Score="0" Text="@Acapello Yes you can although sometimes it takes some work (not just for scikit-learn). Check for instance this link: http://stackoverflow.com/questions/13505678/please-help-how-to-use-py2exe-when-it-include-sklearn or this https://sourceforge.net/p/scikit-learn/mailman/message/29974522/" CreationDate="2016-04-12T16:43:38.560" UserId="17806" />
  <row Id="11226" PostId="11156" Score="0" Text="I have tried out quite a few algorithms and settled on Gradient Boosted Model, also I do use Random Forests quite a lot for my problem." CreationDate="2016-04-12T17:27:33.913" UserId="17619" />
  <row Id="11227" PostId="11156" Score="0" Text="Sagar, it is in your interest to give us as much information as possible. Do you mind giving a snippet of code so it is clear how do you feed the data to the classifiers and which exact SciKit classifiers are those? I have a similar set up that you have - dense + sparse datasets and I use the SGDClassifier for both + kind of majority voting. So I am interested to help as your case is kind of similar but so far it is a rather tedious task to get information out of you :-) My current hypothesis is that the sparse dataset is somehow neglected by your classifier when you bring the two together." CreationDate="2016-04-12T19:01:38.673" UserId="15361" />
  <row Id="11229" PostId="11162" Score="0" Text="Thank you very much, very informative. Just what I needed." CreationDate="2016-04-12T12:24:35.793" UserId="17450" />
  <row Id="11230" PostId="11121" Score="0" Text="You guys rocks. For somebody who is com?g afresh in afresh into data science, I am doing home study with Python until I am reading your answers today. Then should I stop this exercise for major programming course like Oracle 11g. Looking out there for your help on this." CreationDate="2016-04-12T20:23:05.840" UserId="17826" />
  <row Id="11231" PostId="11096" Score="0" Text="I think this is a legitimate question; the problem is which approach (out of many existing ones) would be best to handle this problem? Given the number of potential matches, it may be better to try clustering than any type of fuzzy/approximate matching. However, I'd like to hear other's opinions." CreationDate="2016-04-11T20:35:45.637" UserId="17800" />
  <row Id="11232" PostId="11186" Score="0" Text="I don't see why k-means wouldn't work. More importantly, what's the distance measure?" CreationDate="2016-04-13T04:13:33.847" UserId="9123" />
  <row Id="11234" PostId="11157" Score="0" Text="Can you please share the relevant documentation? Didn't exactly get you what you meant?" CreationDate="2016-04-13T06:04:51.263" UserId="17619" />
  <row Id="11235" PostId="11156" Score="0" Text="sorry about that. Will share the snippet of the code in few hours." CreationDate="2016-04-13T06:05:20.577" UserId="17619" />
  <row Id="11236" PostId="11188" Score="0" Text="Is the problem that the join is taking a long time? Do you mind explaining your features? In general I advise doing the heavy lifting in the database." CreationDate="2016-04-13T06:10:04.957" UserId="381" />
  <row Id="11237" PostId="11166" Score="2" Text="`Heard that if you are python developer then easily you can become data analyst or data scientist`  &lt;--  NO" CreationDate="2016-04-13T06:18:24.333" UserId="11097" />
  <row Id="11238" PostId="9990" Score="0" Text="@r-doe. I have a few questions and some of these may or may not apply to your case depending on what the data looks like. 1. Are you imputing any values? 2. Is there a pattern to the state of charge % like a least count increment. 3. Is the data tagged with any other categorical variables? 4. Is the graph of state of charge (x) vs non cumulated consumption (y) a flat line? (Should be) 4. Any sample data?" CreationDate="2016-04-12T20:44:32.710" UserId="10345" />
  <row Id="11239" PostId="11166" Score="0" Text="@Dawny33 If it's no , then can you say how python can be helpful for become data analyst . because have seen many pycon videos , they are saying python is good way for becoming data analyst ," CreationDate="2016-04-13T06:27:00.263" UserId="17814" />
  <row Id="11240" PostId="11189" Score="0" Text="Welcome to the site! Are you asking for a review of your approach, or is there any other question in this?" CreationDate="2016-04-13T07:04:08.050" UserId="11097" />
  <row Id="11241" PostId="11188" Score="0" Text="Mine is a customer database, having billing details of customer in one table, usage details in another one, customer personal details in a third table, interactions with customer care in a fourth table etc. The final feature file is more or less an aggregated file. It is painful!" CreationDate="2016-04-13T07:18:03.543" UserId="6514" />
  <row Id="11242" PostId="11189" Score="0" Text="i want to check my approach for the same and if I am wrong then what should be the correct approach" CreationDate="2016-04-13T07:32:37.680" UserId="17841" />
  <row Id="11247" PostId="11188" Score="0" Text="Can you give an example data of all those databases, and the amount of rows you are joining?" CreationDate="2016-04-13T10:08:56.980" UserId="11121" />
  <row Id="11252" PostId="11188" Score="0" Text="It's around 4 tables, with 1 lakh + records, and an average of 10 rows per table..I don't think it is still huge." CreationDate="2016-04-13T11:15:01.453" UserId="6514" />
  <row Id="11254" PostId="11167" Score="0" Text="It sounds like you are not trying to predict anything but rather characterize the performance.  True?" CreationDate="2016-04-13T14:28:02.153" UserId="1077" />
  <row Id="11255" PostId="11186" Score="0" Text="What do you mean by similar images than? Same shape? Check: https://pdfs.semanticscholar.org/d600/634734fee223b1964eea92c4b058108f1396.pdf" CreationDate="2016-04-13T15:02:50.720" UserId="17806" />
  <row Id="11256" PostId="11186" Score="0" Text="@StudentT I tried k-means but it has a lot of inaccuracy, different cars were groupped in same clusters." CreationDate="2016-04-13T15:05:31.913" UserId="16712" />
  <row Id="11257" PostId="11186" Score="0" Text="@armatita Yes, shape, and color differences too. Because the camera doesn't see a car the same color at daylight and night." CreationDate="2016-04-13T15:07:42.933" UserId="16712" />
  <row Id="11258" PostId="11157" Score="0" Text="You can read an article about staking &quot; issues in stacking techniques, 1999&quot; read about stackingC . It is very important to know that I am talking about the whole vector (e.g. 1x36 in case of Hog) as a one feature, but not the dimensions within it. You need to track which feature used with which base learner. Be careful about the overfitting problem" CreationDate="2016-04-13T16:15:45.703" UserId="17696" />
  <row Id="11259" PostId="11157" Score="0" Text="If you give more details about the database , number of classes, number of samples , code , what things you have tried , what things you noticed, do you have data imbalance problem, noisy samples ,... Etc . All these details are important and can help in selecting the best method. Give me more details if this ok and I may help in a better way" CreationDate="2016-04-13T16:19:08.763" UserId="17696" />
  <row Id="11263" PostId="11167" Score="0" Text="Yes, I just want to use the dense data and come up with a ranking order for all the power generating units." CreationDate="2016-04-13T17:39:50.317" UserId="17815" />
  <row Id="11264" PostId="11199" Score="0" Text="What would be the output, and how sould you put it into use? There is no use in just doing something without a plan." CreationDate="2016-04-13T21:09:40.470" UserId="924" />
  <row Id="11266" PostId="11199" Score="0" Text="@marcL, You mentioned data &quot;aggregated&quot;, which implies some type of summarization -- or roll-up -- of the data. However, based on the details you've provided, it appears that you're just *stacking* three data sets that came from different sources. Can you clarify?" CreationDate="2016-04-13T21:31:49.057" UserId="17397" />
  <row Id="11267" PostId="11202" Score="1" Text="Have your tried CHAID?" CreationDate="2016-04-14T00:15:24.497" UserId="17397" />
  <row Id="11269" PostId="11204" Score="0" Text="You say &quot;I tried using weka to run number of machine learning algorithm, but I couldn't configure them right&quot; - could you please give more details about a specific thing you have tried and what went wrong? Also, some details about the nature of the data you have would help. Does it represent events, does it contain complex features such as text (identifying users as document authors is a possible task) etc?" CreationDate="2016-04-14T08:36:04.373" UserId="836" />
  <row Id="11271" PostId="11204" Score="0" Text="@NeilSlater `task i was given was to find attributes that will allow me to identify users.`  Is this a feature selection task? I was about the edit it with the appropriate tag, but wasn't really sure!" CreationDate="2016-04-14T08:37:29.547" UserId="11097" />
  <row Id="11272" PostId="11204" Score="0" Text="@Dawny33: That's how I'm reading it at the moment, but really there is too little information to be certain, it might just be a turn of phrase and the goal might simply be to use future data in the same format to identify a user." CreationDate="2016-04-14T08:42:26.220" UserId="836" />
  <row Id="11273" PostId="11204" Score="0" Text="@NeilSlater I edited the post, The data are TCP sessions with data on destination, source, ttl, start time, endtime, destination IP addresses, and more. and the task is to find attributes so that in given a difference TCP flows in the same format form the same group of users, I will be able to labeled them with the appropriate user Id." CreationDate="2016-04-14T09:08:01.953" UserId="17877" />
  <row Id="11274" PostId="11186" Score="0" Text="If k-means didn't work immediately you may need to look at the structure of your CNN. This will decide what/how many features are used to cluster your data." CreationDate="2016-04-14T10:48:46.510" UserId="5144" />
  <row Id="11277" PostId="11208" Score="0" Text="[This question](http://stackoverflow.com/q/36505537/4993513) is closed for a reason." CreationDate="2016-04-14T11:53:24.350" UserId="11097" />
  <row Id="11278" PostId="11208" Score="0" Text="@Dawny33: I would love to know what the reason is. That is something I really hate about SE: Downvotes and closes without good reason. **This is not about a recommendation.**" CreationDate="2016-04-14T12:23:44.657" UserId="14909" />
  <row Id="11279" PostId="11208" Score="0" Text="I understand that it is frustrating. But, this qn **is a recommendation**, as it does not have necessary details. It isn't clear why would you want another package similar to `caret` when there's caret?" CreationDate="2016-04-14T12:35:42.777" UserId="11097" />
  <row Id="11280" PostId="11210" Score="0" Text="Yeah, I think this was the one." CreationDate="2016-04-14T12:38:47.110" UserId="14909" />
  <row Id="11281" PostId="11208" Score="0" Text="@Dawny33: I edited my qn accordingly ;-). Giving an answer and being helpful wasn't actually that hard as it seems..." CreationDate="2016-04-14T12:42:44.787" UserId="14909" />
  <row Id="11282" PostId="11210" Score="0" Text="I do not dare to ask... but do you have a recommendation :-D in comparing those two? I haven't found anything useful on the net so far. Which one is more comprehensive and intuitive in the long run (not necessarily &quot;how fast can I learn it&quot;) in practical data science tasks? (This site is about practical data science, as far as I know.)" CreationDate="2016-04-14T12:46:04.787" UserId="14909" />
  <row Id="11283" PostId="11210" Score="1" Text="@Make42 Caught you asking for recommendations again :P. Btw, I would vote for `caret` just due to the number of contributors and the number of users using the package. I haven't used it personally, however, MLR seems to be boasting more functionalities than `caret`." CreationDate="2016-04-14T12:51:45.633" UserId="11097" />
  <row Id="11286" PostId="5442" Score="0" Text="You can find some insights in Nate Silver's popular science book &quot;The Signal and the Noise: Why So Many Predictions Fail--but Some Don't&quot;. However, if I remember correctly, his predictions were not perfect for the Scottish referendum." CreationDate="2016-04-14T13:43:45.253" UserId="6550" />
  <row Id="11288" PostId="11138" Score="1" Text="I like the idea of using markdown, however, I guess it's not the best approach/solution. It's good for a starter point.&#xA;I also use Slack, and I highly recommend it for teams coordination and meetings." CreationDate="2016-04-14T14:53:19.893" UserId="15762" />
  <row Id="11289" PostId="11138" Score="0" Text="@Jorge Yeah, I agree that it's not the best approach :)  However, I haven't found a good alternative worth spending on!" CreationDate="2016-04-14T14:58:12.870" UserId="11097" />
  <row Id="11290" PostId="11215" Score="0" Text="Welcome to the site. What do you mean by `furthest pages`?" CreationDate="2016-04-14T15:33:51.743" UserId="11097" />
  <row Id="11291" PostId="11215" Score="0" Text="Sorry. It's the degree of separation, the most links between the two pages. Thank you." CreationDate="2016-04-14T15:37:27.773" UserId="17888" />
  <row Id="11292" PostId="11214" Score="0" Text="http://stats.stackexchange.com/questions/123318/why-are-there-only-n-1-principal-components-for-n-data-points-if-the-number" CreationDate="2016-04-14T15:39:10.223" UserId="676" />
  <row Id="11293" PostId="11215" Score="0" Text="For reference, this is the [diameter](https://en.wikipedia.org/wiki/Distance_(graph_theory)) of the graph." CreationDate="2016-04-14T16:36:44.970" UserId="381" />
  <row Id="11294" PostId="11215" Score="2" Text="In the context of what technology or representation? you've defined what it means but what are you asking about how to compute it?" CreationDate="2016-04-14T16:46:24.737" UserId="21" />
  <row Id="11295" PostId="11214" Score="0" Text="Thank you stmax, I'll take a look." CreationDate="2016-04-14T17:02:35.820" UserId="17886" />
  <row Id="11296" PostId="11210" Score="0" Text="I also vote for caret package." CreationDate="2016-04-14T17:38:46.413" UserId="17116" />
  <row Id="11297" PostId="11199" Score="0" Text="@Vishal I updated my question, hope its more clear now" CreationDate="2016-04-14T21:46:55.430" UserId="13915" />
  <row Id="11298" PostId="9739" Score="0" Text="but when i was using the distance tool to find most similar words to a given word, the version with stopwords removed gave me sensible words than the version without. can you guess what's this mean?" CreationDate="2016-04-15T01:34:22.610" UserId="15303" />
  <row Id="11299" PostId="11219" Score="0" Text="Cool! That seems to be a good starting point. I will explore more on it! Thanks for the suggestion." CreationDate="2016-04-15T05:05:38.070" UserId="17815" />
  <row Id="11300" PostId="9739" Score="0" Text="Probably, you are using too narrow context: if your model looks into, say, two words back and forward, you will be having up to 2 stopwords in context and that could give worse results. If you will broaden context (which will make model bigger and training time longer), with-stopwords model will give you better results, I assume." CreationDate="2016-04-15T08:33:00.683" UserId="2573" />
  <row Id="11301" PostId="9739" Score="0" Text="thanks for the input, makes more sense now. Also since word2vec processes the input sentence by sentence, what would happen if i mix up the sentences in the input document? that should totally change output vectors right? Also again, given it's processing sent by sent, how does word2vec differ from doc2vec? thanks again." CreationDate="2016-04-15T09:31:13.723" UserId="15303" />
  <row Id="11302" PostId="9739" Score="0" Text="If you mix sentences, you basically will corrupt context information for last/first words in the sentence (since they will catch previous words as nearest, which is way not always true). Change wouldn't be total, but important, I assume. Considering w2v and d2v difference, I like the tutorial here: http://rare-technologies.com/doc2vec-tutorial/ . And basically, you are learning either an abstract vector representation of a word or a label assigned to sentence/document, so the core difference is the modelled object." CreationDate="2016-04-15T09:40:44.430" UserId="2573" />
  <row Id="11303" PostId="11224" Score="0" Text="Thank you Sean Owen for your answer." CreationDate="2016-04-15T10:59:57.377" UserId="17886" />
  <row Id="11304" PostId="11230" Score="0" Text="Thanks a lot for your response. Will check on them." CreationDate="2016-04-15T12:51:15.653" UserId="5043" />
  <row Id="11305" PostId="9739" Score="0" Text="thanks i'll have a look there. And reg the first point, even though word2vec processes the input sentence by sentence, the first word in sent 2 will look at 3 words from the end of sent 1 if the window size is 3, isn't it? I thought it considers each sentence as individual entities but im wrong i guess." CreationDate="2016-04-15T13:49:53.890" UserId="15303" />
  <row Id="11310" PostId="11232" Score="0" Text="Is this homework? Here are some relevant papers: _Location-Based Activity Recognition using Relational Markov Networks_, _The Hidden Permutation Model and Location-Based Activity Recognition_, _Inferring High-Level Behavior from Low-Level Sensors_." CreationDate="2016-04-15T15:48:18.687" UserId="381" />
  <row Id="11314" PostId="11237" Score="1" Text="Can you give us an idea of the size of the total data, in bytes? 10 million integers and 10 million book texts have very different storage requirements." CreationDate="2016-04-15T16:33:52.943" UserId="12876" />
  <row Id="11315" PostId="11237" Score="0" Text="You also sayunder 5k or 10k ... is this USD? AUD ... different value ..." CreationDate="2016-04-15T16:41:32.190" UserId="16284" />
  <row Id="11316" PostId="11225" Score="0" Text="what type of data are we looking at ?  knowing this will affect greatly the answers you will get.  Is it DNA analysis ? symtomatic measurements and observations ? Lab test results ?" CreationDate="2016-04-15T17:04:27.380" UserId="17048" />
  <row Id="11317" PostId="11220" Score="0" Text="how different are the sizes of your training and testing datasets? After building your model, are you using cross validation?" CreationDate="2016-04-15T17:05:07.503" UserId="16704" />
  <row Id="11318" PostId="11159" Score="0" Text="Unfortunately this looks more like homework stuff" CreationDate="2016-04-15T17:06:10.490" UserId="16704" />
  <row Id="11322" PostId="11245" Score="0" Text="Thanks, appreciate the ideas. I did try Orange already, and you're right, that didn't work on my Win PC either. Here's what seems to be working: I tried RapidMiner Basic (it's open source and free), and that worked." CreationDate="2016-04-16T00:08:59.607" UserId="16928" />
  <row Id="11323" PostId="11242" Score="0" Text="Thanks for the idea. I don't want to use trialware, but that may work for others out there. I tried RapidMiner Basic (it's open source and free), and that worked." CreationDate="2016-04-16T00:10:22.580" UserId="16928" />
  <row Id="11324" PostId="11247" Score="0" Text="Could you post your ROC curve and confusion matrix for clarity?" CreationDate="2016-04-16T07:39:36.943" UserId="16591" />
  <row Id="11325" PostId="11196" Score="0" Text="k-means only works with Bregman divergences, *not* with arbitrary metrics. Because of the *mean* not optimizing arbitrary metrics." CreationDate="2016-04-16T12:57:56.080" UserId="924" />
  <row Id="11326" PostId="11251" Score="0" Text="Sorry, your answer is wrong." CreationDate="2016-04-16T14:37:01.757" UserId="9123" />
  <row Id="11327" PostId="11254" Score="0" Text="Question, in the text where you say &quot;...polynomial is about 0.5&quot;, did you mean to use `0.05` instead of `0.5` ? Also, later in the paragraph too." CreationDate="2016-04-16T17:33:43.870" UserId="15027" />
  <row Id="11328" PostId="11196" Score="0" Text="What it means when you say - mean not optimizing? Add a source please." CreationDate="2016-04-16T18:12:53.907" UserId="3570" />
  <row Id="11329" PostId="11196" Score="0" Text="Search for &quot;why k-means only works with Euclidean distance&quot;." CreationDate="2016-04-16T19:00:18.757" UserId="924" />
  <row Id="11330" PostId="11256" Score="1" Text="One way of putting the *why* - if you are 99% certain of a prediction which turns out later to be true, this is 10 times stronger than being 90% certain of an event (think of what you'd be willing to bet), and this is captured by a log-likelihood objective whilst sum of squares difference would prefer you gain from 50% to 60% on some other more marginal example." CreationDate="2016-04-16T19:07:22.347" UserId="836" />
  <row Id="11331" PostId="11254" Score="0" Text="@steveb yes I mistyped" CreationDate="2016-04-16T23:00:00.423" UserId="9123" />
  <row Id="11332" PostId="11237" Score="0" Text="US$. The dataset is 35GB of voter registration records (public records)." CreationDate="2016-04-17T00:01:12.593" UserId="17913" />
  <row Id="11333" PostId="11235" Score="0" Text="This is so informative. I need answers like this one!" CreationDate="2016-04-17T00:27:09.327" UserId="17908" />
  <row Id="11335" PostId="11256" Score="0" Text="@Emre in your notation, the formula you give would be correct even if I used a function different from the logistic function?" CreationDate="2016-04-17T06:37:44.807" UserId="13736" />
  <row Id="11336" PostId="11256" Score="0" Text="@NeilSlater would you mind expanding a little more your comment?" CreationDate="2016-04-17T06:39:12.900" UserId="13736" />
  <row Id="11337" PostId="11256" Score="0" Text="Yes, @user, up until the definition of $P(x_i|w)$." CreationDate="2016-04-17T09:10:14.557" UserId="381" />
  <row Id="11338" PostId="10934" Score="2" Text="Thank you. Your intuition was right:) I compete in inclass Kaggle  competition and this solution perform worse because of increased bias." CreationDate="2016-04-17T12:14:46.547" UserId="15693" />
  <row Id="11339" PostId="11183" Score="0" Text="Adding time of the day in this way looks insightful.  Can you point to other examples where it was used like this? Thanks!" CreationDate="2016-04-17T16:48:16.197" UserId="15361" />
  <row Id="11340" PostId="11183" Score="0" Text="This representation is [standard](http://stats.stackexchange.com/questions/126230/optimal-construction-of-day-feature-in-neural-networks) [practice](https://www.reddit.com/r/MachineLearning/comments/1utxnk/how_do_you_represent_timeofday_in_artificial/)." CreationDate="2016-04-17T18:06:51.347" UserId="381" />
  <row Id="11341" PostId="9990" Score="0" Text="Power consumption is measured in kW, nor kWh neither kW/h. Are you sure you get your physics right?" CreationDate="2016-04-18T04:28:04.597" UserId="15361" />
  <row Id="11342" PostId="11265" Score="0" Text="Hi, I meant, setting the derivative to zero." CreationDate="2016-04-18T06:46:36.303" UserId="17939" />
  <row Id="11343" PostId="11265" Score="0" Text="@user3435407 ??? Setting the derivative to zero means your model is not moving." CreationDate="2016-04-18T06:47:37.623" UserId="9123" />
  <row Id="11344" PostId="11261" Score="0" Text="What do you mean with &quot;Why is it not good if you simply count it where is it 0?&quot;?" CreationDate="2016-04-18T07:16:14.927" UserId="676" />
  <row Id="11345" PostId="11265" Score="1" Text="I think what user3435407 means is setting the derivative of the loss function to zero and solving for the coefficients. That actually works for linear regression and gives the closed form solution β=(XTX)^−1 XTy. It only works for linear regression though - it does not work for logistic regression and most other generalized linear models." CreationDate="2016-04-18T07:26:34.247" UserId="676" />
  <row Id="11346" PostId="11265" Score="0" Text="@user3435407 Is this what you mean? What stmax said?" CreationDate="2016-04-18T07:27:56.340" UserId="9123" />
  <row Id="11347" PostId="11265" Score="0" Text="@stmax Thanks. If that's what he really means, I'll change my answer." CreationDate="2016-04-18T07:28:13.623" UserId="9123" />
  <row Id="11348" PostId="9990" Score="1" Text="@Diego Battery Capacity is most of the times given in kWh. Because of that I integrated the consumption over time to get the time component in there." CreationDate="2016-04-18T07:35:21.657" UserId="15690" />
  <row Id="11349" PostId="9990" Score="0" Text="@Drj: 1.) I do interpolate the two times series &quot;state of charge&quot; and &quot;power consumption&quot; in order to make them equidistant in the time domain (for direct comparison) but other than that, no. 2.) I am not sure what you mean here. (could you please explain yourself a bit more?) 3.) the data is tagged with a charging variable stating whether the cable is plugged to charge the battery or not.. however I only consider complete data samples where the cable is unplugged, so it might not be relevant." CreationDate="2016-04-18T07:42:40.413" UserId="15690" />
  <row Id="11350" PostId="9990" Score="0" Text="@Drj: 4.) Link is in the main post." CreationDate="2016-04-18T08:35:36.560" UserId="15690" />
  <row Id="11351" PostId="11266" Score="1" Text="Is there a better way of moving this question to datascience than duplicating it? For those reading, here it is on [stackoverflow](http://stackoverflow.com/questions/36618259/how-to-use-pythons-fastfm-library-factorization-machines-for-recommendation-t/36644243#36644243) ;-)" CreationDate="2016-04-18T09:56:07.877" UserId="9314" />
  <row Id="11352" PostId="11268" Score="0" Text="Wow, this has actually brought down AUC :( Not sure, what it means, need to check the feature importance and all. But my philosophy is, out of around 2.3k sparse features, I used 1k features which were explaining 0.97 variance ratio, this loss of information may have brought down AUC." CreationDate="2016-04-18T10:17:20.383" UserId="17619" />
  <row Id="11353" PostId="11271" Score="1" Text="What are your criteria for choosing one way or the other? what specific problem are you facing? otherwise this is just inviting general opinions." CreationDate="2016-04-18T13:47:50.350" UserId="21" />
  <row Id="11354" PostId="11271" Score="0" Text="Well i need a good integration with Pentaho, and a good performance in Read/Write and operational (like calculating % of variants i find in my genomes), and retrieval time (if i select a variant it gives me all the patient with that variant or the associated disease), i'm asking you in fact to see if i'm reasoning well... thank you for your answer" CreationDate="2016-04-18T14:07:03.233" UserId="17957" />
  <row Id="11355" PostId="11268" Score="0" Text="Interesting. Thanks for sharing. We have very similar dataset to yours (1k-2k sparse features). Just out of curiosity, how many principal componenets you have generated? If that number is too low, this may explain why AUC went down." CreationDate="2016-04-18T15:22:16.813" UserId="5380" />
  <row Id="11356" PostId="11258" Score="0" Text="I have a dataframe where whole reviews are in a particular column and polarity of the reviews in another column which says whether the review is positive or negative. I am feeding this as a input to training model using naive_bayes algorithm. In the test, I am just feeding the reviews as input and the model is predicting me polarity, whether the review is positive or negative" CreationDate="2016-04-18T15:54:05.017" UserId="5043" />
  <row Id="11357" PostId="11258" Score="0" Text="Can LDA be used using sklearn? I am a beginner in python and it would be great if I get some reference to start with." CreationDate="2016-04-18T15:55:27.087" UserId="5043" />
  <row Id="11358" PostId="11273" Score="1" Text="No, it is not correct." CreationDate="2016-04-18T17:14:24.043" UserId="381" />
  <row Id="11359" PostId="11268" Score="0" Text="As I said already, I have generated 1k principal components which were explaining 0.97 variance." CreationDate="2016-04-18T17:55:48.800" UserId="17619" />
  <row Id="11360" PostId="11273" Score="0" Text="Can you elaborate more why? And since its incorrect, why there is a paper published with neo4j considered as graph processing platform?" CreationDate="2016-04-18T18:06:20.077" UserId="17596" />
  <row Id="11361" PostId="11273" Score="1" Text="Because even the [neo4j staff](http://neo4j.com/blog/graph-processing-v-graph-databases-jim-webber/) do not make this claim." CreationDate="2016-04-18T18:09:21.403" UserId="381" />
  <row Id="11363" PostId="11265" Score="0" Text="yes stmax, thats what i meant, sorry Student, for expressing myself clumsy" CreationDate="2016-04-18T19:51:04.300" UserId="17939" />
  <row Id="11364" PostId="11273" Score="0" Text="Ok. Thanks for the answer." CreationDate="2016-04-18T20:25:28.093" UserId="17596" />
  <row Id="11366" PostId="11265" Score="0" Text="Setting the derivative to zero and solving is called the Newton-Raphson method, and it is one of the most popular methods in maximum likelihood modeling." CreationDate="2016-04-18T22:47:43.497" UserId="9086" />
  <row Id="11367" PostId="11286" Score="0" Text="If possible, do add a link to the file: _Image Classification and Filter Visualization(00-classification.ipynb)_" CreationDate="2016-04-19T07:16:51.143" UserId="11097" />
  <row Id="11368" PostId="11286" Score="0" Text="The link is http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb also ResNet-101 is https://github.com/KaimingHe/deep-residual-networksbecause because of reputation I couldn't post more than 2 links. Thank u" CreationDate="2016-04-19T07:20:14.953" UserId="17987" />
  <row Id="11369" PostId="11286" Score="0" Text="Thanks! I added it for you, and welcome to the site :)" CreationDate="2016-04-19T07:21:32.667" UserId="11097" />
  <row Id="11370" PostId="11218" Score="1" Text="Is the first line correct or should that also add up to 10? And is my understanding correct that for example in line 3 you don't know where the singleton B came from?" CreationDate="2016-04-19T07:42:24.450" UserId="14904" />
  <row Id="11371" PostId="11274" Score="0" Text="As a follow-up question, does the following equation from scipy.stats.entropy calculate the symmetrical KL-divergence, usable as a metric (with pk and qk representing the two vectors)? S = sum(pk * log(pk / qk), axis=0)" CreationDate="2016-04-19T08:08:09.563" UserId="17988" />
  <row Id="11372" PostId="11218" Score="1" Text="I'm not clear on your data, so its hard to suggest solution. I understand you have 5 &quot;snapshots in time&quot;. So do you have, say 20 items that you are observing and from the first line, 4 are in state A, 4 are in state B, 3 are in state C? Then for period 2, only 1 is in state A, 2 are in state B and 7 in state C? If this is true, do you have more granular data? do you know the order that states change from and to, is the state transition matrix well established." CreationDate="2016-04-19T11:16:24.633" UserId="16284" />
  <row Id="11375" PostId="11288" Score="0" Text="_understanding of all kinds of data visualizations_ makes this question very broad, as there are 100s of types of viz. So, pl consider narrowing your down, or breaking this question into multiple seperate questions." CreationDate="2016-04-19T12:03:19.507" UserId="11097" />
  <row Id="11376" PostId="11274" Score="0" Text="@piutu scipy.stats.entropy calculates the kullback leibler divergence for p and q, which is not symmetric. You can use scipy.stats.entropy to calculate the jensen-shannon divergence, which is symmetric and whose square root satisfies the triangle inequality (i.e. it's a metric): jsd(p, q) = 0.5*entropy(p, 0.5*(p+q)) + 0.5*entropy(q, 0.5*(p+q))" CreationDate="2016-04-19T14:08:18.610" UserId="676" />
  <row Id="11377" PostId="11167" Score="0" Text="Do you want an exact count of number of activations etc. or approximate counts? If approximate then there are interesting algorithms if exact then it's just a group by operation for which you can look up the algo in spark" CreationDate="2016-04-19T14:13:01.303" UserId="13686" />
  <row Id="11378" PostId="11174" Score="0" Text="@Ryan Zotti : your answer seems very generic. Can you modify it to answer the specific question ?" CreationDate="2016-04-19T14:15:15.440" UserId="13686" />
  <row Id="11379" PostId="11174" Score="0" Text="@wabbit. It's not generic. He asked which algorithms to use, and I said there are many but with a few exceptions. It's simplistic and inaccurate to suggest that the OP should only try one algorithm. He should try many algorithms and pick the one that performs best on the holdout set of data" CreationDate="2016-04-19T15:09:06.317" UserId="12515" />
  <row Id="11380" PostId="10069" Score="1" Text="This is pretty far off the mark. The &quot;natural&quot; interpretation is something imposed on the model, it's not part of the model itself. You can reverse dependencies (adding additional edges as necessary to preserve the set of dependencies represented by the network) and it's still a Bayesian network. Whether it makes sense isn't answerable by examining just the network itself. Incidentally Judea Pearl, one of the big movers behind Bayesian networks in the 80's and 90's, has been working more recently on formal models for causality, which do express causal relationships in the model." CreationDate="2016-04-19T16:29:46.737" UserId="18001" />
  <row Id="11381" PostId="11296" Score="0" Text="Thanks. And so a ReLU activation function between the 52-node hidden layer and the 26-node output layer would be optimal? I don't use convolution. I don't know what CNN's are or how they work, but I'll have to look into that. For which layers, then, should I use convolution? Between all layers excluding between the two last (52-&gt;26)?" CreationDate="2016-04-19T16:52:41.433" UserId="18000" />
  <row Id="11382" PostId="11296" Score="0" Text="@tsorn: A fully-connected layer with ReLU activation should be fine. For a CNN, you want lowest layers to be convolutional (typically Convolve2D with ReLU activation followed optionally by MaxPool and Dropout), and then at least one hidden layer between the last convolving layer and your output layer. The 52/26 idea of yours is kind of 2 stacked output layers, so you'd still want something fully-connected before them." CreationDate="2016-04-19T16:58:12.473" UserId="836" />
  <row Id="11383" PostId="11258" Score="0" Text="Yes LDA is in sklearn it is a relatively new addition [link](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)" CreationDate="2016-04-19T17:45:40.127" UserId="16538" />
  <row Id="11384" PostId="11288" Score="0" Text="@Dawny33 Thanks, appreciate the pointers. I've edited my question, hope it suffices... otherwise feel free to give me more pointers." CreationDate="2016-04-19T18:24:31.247" UserId="16928" />
  <row Id="11385" PostId="11218" Score="0" Text="@JanvanderVegt Yes, I will edit to make it add up to 10. Also, I DO know what transitions where, so I know the flow from A-&gt;B, B-&gt;C etc" CreationDate="2016-04-19T18:43:25.137" UserId="5044" />
  <row Id="11386" PostId="11218" Score="0" Text="@MarcusD I have edited the question to answer what you asked. I can track each data point, through the states for each point in time. There is no state transition matrix as such, as the probability from going from state A to B, is not fixed apriori. Let me know if that answers your questions. If not, direct me to where you would need more clarification, and I would be glad to do that." CreationDate="2016-04-19T18:51:55.747" UserId="5044" />
  <row Id="11387" PostId="10934" Score="0" Text="Thanks for experimenting. I will remember and cite this as an example if this topic ever comes up again :-)" CreationDate="2016-04-19T19:36:03.807" UserId="847" />
  <row Id="11388" PostId="11296" Score="0" Text="One more thing; do you think bias nodes are necessary in such a network?" CreationDate="2016-04-19T20:23:11.273" UserId="18000" />
  <row Id="11389" PostId="11296" Score="0" Text="Yes, I expect bias (and training of bias weights) to be critical." CreationDate="2016-04-19T20:31:27.917" UserId="836" />
  <row Id="11390" PostId="11218" Score="1" Text="This post I made in stack overflow some time ago may be of interest to you: http://stackoverflow.com/questions/32633507/r-need-help-on-multi-state-markov-and-block-bootstrap-please/32694008#32694008" CreationDate="2016-04-19T21:14:09.357" UserId="12290" />
  <row Id="11391" PostId="11218" Score="1" Text="What sort of analysis do you want to do in the end? I have an idea, but it could be completely off track, depending on what analysis you are doing." CreationDate="2016-04-19T21:15:45.773" UserId="16284" />
  <row Id="11392" PostId="11295" Score="0" Text="Which dataset do you use?" CreationDate="2016-04-19T21:18:27.103" UserId="8820" />
  <row Id="11393" PostId="11218" Score="0" Text="I have the state transitions under 4 different treatments, and I am interested to see if the state transitions in different periods look different by treatments. (From just eyeballing the data, I think there is a difference. ) I am thinking of setting up the comparison exercise by first having a nice graph of chart of the period wise transitions by treatments. I have not thought of any statistical tests for significance yet, but I am guessing that comes later. Feel free to share what you have in mind." CreationDate="2016-04-19T21:28:27.187" UserId="5044" />
  <row Id="11394" PostId="11295" Score="0" Text="It's an 8-bit grayscale version of the English-only chars74k data-set (http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/). I don't know if it's hard compared to other data sets, or what success rate to expect, but I sure can't make out some of the characters using my own eyes." CreationDate="2016-04-19T21:32:03.343" UserId="18000" />
  <row Id="11395" PostId="11295" Score="0" Text="How are you measuring success rate? A quick review of published papers would put 75% accuracy at better than state-of-the-art, which seems unlikely (I might be misunderstanding some context though). The official measure is to use 15 exemplars from each class for training and the other 15 for testing. Are you using test/train split in that way to assess your model?" CreationDate="2016-04-19T21:48:49.170" UserId="836" />
  <row Id="11396" PostId="11295" Score="0" Text="I don't use the official way to measure. I train on 90 % of the data and test on the remaining 10 %. Are they only training with 15*26 examples? That sounds low to me. If they are only training with ~400 examples and I'm using ~67k, that might explain the difference." CreationDate="2016-04-19T22:26:09.677" UserId="18000" />
  <row Id="11397" PostId="11298" Score="0" Text="That can't be right. For G1, the first and last are dependent in the absence of any known values. For G2, the first and last are not dependent in the absence of any known values. Did you mean to write `G2 = o &lt;- o -&gt; o` instead? Anyway I'm not seeing a claim about those particular graphs on the web page which you've referenced; perhaps you can be more specific." CreationDate="2016-04-19T22:37:04.333" UserId="18001" />
  <row Id="11398" PostId="11308" Score="1" Text="It's hard to say without seeing the data. If you have a lot, try using a multilayer neural network." CreationDate="2016-04-20T03:43:25.100" UserId="381" />
  <row Id="11399" PostId="11308" Score="0" Text="It's not a collosal amount - around ~10k training rows. And sadly, I can't share the data, but I could answer general questions about it." CreationDate="2016-04-20T03:44:29.913" UserId="18012" />
  <row Id="11400" PostId="11302" Score="0" Text="This answers questions (2) and (3). Do you have an idea about question (1) and (4), too? (Yes, references would be nice)" CreationDate="2016-04-20T03:45:35.977" UserId="8820" />
  <row Id="11401" PostId="11308" Score="2" Text="What kind of data is it then?" CreationDate="2016-04-20T03:46:14.913" UserId="381" />
  <row Id="11402" PostId="11308" Score="0" Text="Around ~2300 features around 100 of which are real-valued, the rest are arbitrarily large ints. One subset is around ~100 features [mostly the real-valued ones], the other is around ~2000 [mostly the integer ones]. Multiple scaling approaches based around robust statistics have been tried with no noticeable effect over just using the raw data. The int-valued features are fairly sparse [5-10% populated], but there's no noticeable difference in sparseness between the two &quot;clusters&quot;." CreationDate="2016-04-20T03:50:43.947" UserId="18012" />
  <row Id="11404" PostId="11298" Score="0" Text="@RobertDodier Thank you. I fixed it." CreationDate="2016-04-20T09:30:30.493" UserId="8820" />
  <row Id="11411" PostId="11302" Score="0" Text="(1) Consider the contrapositive: if the reversed-arrow graph has a directed cycle, then following the arrows around the cycle backwards must be a directed cycle in the original graph. (4) Bayesian networks are probabilistic models and as such don't represent causality. It's possible that some arrows actually refer to causal relations, but this is lost in a probabilistic model. Maybe `a` causes `b`, but `a -&gt; b` and `a &lt;- b` are equally valid probabilistic models." CreationDate="2016-04-20T16:02:38.063" UserId="18001" />
  <row Id="11412" PostId="11302" Score="0" Text="Some introductory references. Koller &amp; Friedman: &quot;Probabilistic Graphical Models&quot;. Cowell, Dawid, Lauritzen, and Spiegelhalter: &quot;Probabilistic Networks and Expert Systems&quot;. Castillo, Gutierrez, and Hadi: &quot;Expert Systems and Probabilistic Network Models&quot;." CreationDate="2016-04-20T16:11:26.030" UserId="18001" />
  <row Id="11413" PostId="11322" Score="0" Text="Great question. Do you have a corpus of text tagged with these skills, like how questions here are, or the option of crowd sourcing one?" CreationDate="2016-04-20T17:26:22.233" UserId="381" />
  <row Id="11415" PostId="11312" Score="0" Text="I am not looking for a quantitative or qualitative direction per se. I would know what to do with the data, and what regressions to run. I am looking for a good way to show graphically how the state transitions differ across treatments. &#xA;For example, Tguzella's comment on my post is the closest I have received to what I am looking for." CreationDate="2016-04-20T18:18:45.117" UserId="5044" />
  <row Id="11416" PostId="11312" Score="0" Text="I'd certainly look at my suggestion of neo4j/d3js then, as it will show graphically how your various states differ." CreationDate="2016-04-20T18:26:56.613" UserId="16284" />
  <row Id="11417" PostId="11329" Score="0" Text="It might be worth explaining your use case, because there could be something already inherent to existing table you are not understanding that could be clarified. Probably quite critically, the agent doesn't get to choose S', just A, so it generally makes sense to calculate expected return q(S,A) - if you are in a scenario where the agent gets to somehow know probabilities of landing on S', you can address that by modifying S (to include the factors that allow the agent to know). If somehow the agent gets to control which S' to land on, then that should be part of A." CreationDate="2016-04-20T18:42:47.047" UserId="836" />
  <row Id="11418" PostId="11309" Score="0" Text="What's the ultimate goal?" CreationDate="2016-04-20T19:03:47.117" UserId="381" />
  <row Id="11419" PostId="11315" Score="0" Text="Any chance of seeing one or two of these traffic intensity maps?" CreationDate="2016-04-20T19:06:44.170" UserId="471" />
  <row Id="11420" PostId="11329" Score="0" Text="I'm talking about model-based learning, just updated the title and description to clarify." CreationDate="2016-04-20T19:08:20.080" UserId="9814" />
  <row Id="11421" PostId="10069" Score="0" Text="You say, &quot;whether it makes sense isn't answerable by examining just the network itself.&quot; I never said it was. I said &quot;when you interpret the nodes a certain way, conditioning flows a certain way...&quot; This probably reflects my bias; you can call the stuff I work on a bayes net, but this question would never come about for me. For example, if two nodes represent the same variable at different times, there would be no question about which direction the conditioning flows. I accept the possibility, however, that there are situations where people might use these Baye's nets in a less rigid way." CreationDate="2016-04-21T00:15:44.697" UserId="15942" />
  <row Id="11422" PostId="11258" Score="0" Text="thank you. Will check it out." CreationDate="2016-04-21T03:45:29.347" UserId="5043" />
  <row Id="11423" PostId="11301" Score="0" Text="The basic model seems pretty reasonable to me. When you say, &quot;what would be the best way to determine the order for this model?&quot; Do you mean the order of ARIMA terms, or the lag structure? My gut is telling me that you probably don't have a stationary series, and maybe other issues with your error term too." CreationDate="2016-04-21T05:36:23.077" UserId="16538" />
  <row Id="11424" PostId="11308" Score="0" Text="Did you try stacking / Blending techniques ?" CreationDate="2016-04-21T05:37:51.193" UserId="17696" />
  <row Id="11425" PostId="11308" Score="0" Text="The splitting of the data set for stacking is the issue here. The two parts of the training set for stacking are currently selected using information about which of the two models performs better in that region of the data. If a way of estimating those subsets can be found so that the split can be done on data where the target is unknown, I can then use a stacked model." CreationDate="2016-04-21T06:30:23.013" UserId="18012" />
  <row Id="11427" PostId="11333" Score="1" Text="I would be *very* careful about training your model. You might not want the model to have optimum predictive performance over the classification classes. Why not? Well, how bad is it if you misclassify an &quot;URGENT&quot; case as &quot;IGNORE&quot;. People may die. Get it the other way round and all that happens is the company uses a few extra resources. Don't just throw your data into a system without thinking about it first." CreationDate="2016-04-21T07:06:42.433" UserId="471" />
  <row Id="11429" PostId="11329" Score="0" Text="I'm still not sure of your use case. You want the model to use S,A,S' tuples, but to what end? Are you wanting to predict distribution of S' from S,A using the model (and presumably it is non-deterministic and inaccessible whilst the learner is exploiting what it has learned)?" CreationDate="2016-04-21T08:27:20.530" UserId="836" />
  <row Id="11430" PostId="11329" Score="0" Text="If you are needing to predict S' from the model, then just perhaps you are looking to combine Reinforcement Learning with Planning systems? So this might help: http://lpis.csd.auth.gr/publications/rlplan.pdf" CreationDate="2016-04-21T08:32:16.717" UserId="836" />
  <row Id="11431" PostId="11322" Score="0" Text="No, I don't have a text associated with the skills, these are the skills entered by users in their profiles. Though with the input skill comes a motivation for learning it with the length of at least 140 symbols. Not sure what do you mean by crowd sourcing it." CreationDate="2016-04-21T08:35:29.750" UserId="18028" />
  <row Id="11432" PostId="10039" Score="0" Text="1: The data shouldn't be random, otherwise you can't predict. There should be some clustering. Each column element should be somewhat accurately described by several 'features' i.e.: `1 ≤ features ≤ m` values. 2: You can transpose your matrix to satisfy this condition (I don't see what would go wrong if `n=m`)." CreationDate="2016-04-21T09:15:56.920" UserId="16199" />
  <row Id="11434" PostId="11338" Score="0" Text="If you split the data *randomly* the two groups will always be similar because you know that they come from the same distribution." CreationDate="2016-04-21T12:46:57.687" UserId="676" />
  <row Id="11435" PostId="11321" Score="0" Text="Even though I replaced all items in the array by 0.00001, it still returns infinity after the computation... However, I feed the function an unnormalized version so it can transform it into a normalized one. Might this be the problem?" CreationDate="2016-04-21T13:13:32.227" UserId="17988" />
  <row Id="11436" PostId="11314" Score="0" Text="@mods, can i move/duplicate this question at stats.stackexchange.com? i believe i can get some help there as well!" CreationDate="2016-04-21T14:13:30.540" UserId="18019" />
  <row Id="11437" PostId="11339" Score="0" Text="I understand this concept. So I have a random 70% of my data in Train. I cant run grid search on the entire 70% because its too big. Ideally I would like to run grid search on 5%, will I get different tuning results on the 5% versus running grid search on the entire 70%?" CreationDate="2016-04-21T15:17:15.613" UserId="18035" />
  <row Id="11438" PostId="11339" Score="0" Text="Yes, the optimal tuning parameters could change somewhat, but that shouldn't matter. What matters is that you have a strong model. 5% of 9M is a sufficient record size to build a strong model." CreationDate="2016-04-21T15:26:28.787" UserId="12515" />
  <row Id="11439" PostId="11322" Score="0" Text="I mean let users enter text, such as a bio, that can be associated with the tags like how questions here are associated with tags. Now what do _you_ mean by &quot;with the input skill comes a motivation for learning it with the length of at least 140 symbols&quot;?" CreationDate="2016-04-21T15:38:23.843" UserId="381" />
  <row Id="11440" PostId="11321" Score="0" Text="This should be by default. Kl works on distributions. You need to convert to probability, replace any zero value with a very small value" CreationDate="2016-04-21T15:49:20.027" UserId="17696" />
  <row Id="11441" PostId="11341" Score="0" Text="Thx. Not fully understanding the concept yet, I have performed the test you suggested. Splitting into matrix A &amp; B, the &quot;result&quot; was higher than 70 (in general &gt;100). If I try A against A (or B x B) it is 0. How should I interpret the result A x B (&gt; 100)? Another point, is this approach considering the whole vector as the element of comparison?" CreationDate="2016-04-21T16:51:37.990" UserId="17933" />
  <row Id="11442" PostId="11341" Score="0" Text="The linked page states that the T-squared statistic is distributed as a [noncentral F-distribution](https://en.wikipedia.org/wiki/Noncentral_F-distribution). So for a chosen confidence level, you would find the corresponding point on the noncentral F distribution CDF, which translates to the corresponding T-squared value. The value of T2=100 corresponds to a particular significance level. You probably want the opposite: pick your confidence level and compute the corresponding T2 value to use as your threshold." CreationDate="2016-04-21T18:18:15.520" UserId="964" />
  <row Id="11443" PostId="11341" Score="0" Text="Regarding the &quot;element of comparison&quot;, the matrices `A` and `B` are each considered &quot;as a whole&quot; in the sense that their mean, covariance, &amp; sample sizes are used in the computation. But the original data matrix (before splitting into `A` and `B`) is not considered as a whole (implicitly or explicitly). In other words, if you perform two random splits of the data, you should not expect identical results for the T-squared statistic." CreationDate="2016-04-21T18:26:20.973" UserId="964" />
  <row Id="11444" PostId="11347" Score="0" Text="Please post sample data (CSV is fine) so we don't have to create our own to demonstrate how." CreationDate="2016-04-21T19:22:23.430" UserId="381" />
  <row Id="11445" PostId="11341" Score="0" Text="Thx. I am working with R, Hotelling package (hotel.test(a,b)), and after your comments I have decided to run 1000 simulations. The results are quite variable for T2 (mean 80, almost bell-shape, range 40-160) and pValue (mean 0.5, almost uniformly distributed between 0-1). Curiously, for AxA or BxB, pValue=1, T2=0. Any insights from those results?" CreationDate="2016-04-21T19:35:55.847" UserId="17933" />
  <row Id="11446" PostId="11341" Score="0" Text="Don't know what you mean by `pValue`. Assuming `A` &amp; `B` are same size, 128 observations is not a lot for 61 variables. So you may be suffering the [Curse of Dimensionality](https://en.wikipedia.org/wiki/Curse_of_dimensionality). For comparison, you might try re-running your analysis using a subset of attributes (say ~10) to see how T2 varies. Maybe do the same using the top few [principal components](https://en.wikipedia.org/wiki/Principal_component_analysis). And for extra credit, plot the histogram of Mahalanobis distances of all points in group `B` from the mean/covariance of group `A`." CreationDate="2016-04-21T20:41:22.507" UserId="964" />
  <row Id="11447" PostId="11341" Score="0" Text="From your initial link: &quot;_In order to calculate a p-value, multiply the t2 statistic by the above constant and use the F-distribution_&quot;. [P-value](http://www.wikiwand.com/en/P-value) is a reference to disregard the null hypothesis. If I am not mistaken, T2, PCA and Mahalonobis are linear approaches and due to that, they should not be conclusive. Wouldn't be better something like cosine similarity?" CreationDate="2016-04-21T22:52:27.273" UserId="17933" />
  <row Id="11449" PostId="11350" Score="0" Text="This looks very promising!&#xA;Could you kindly tell me what software+command you used to generate this, and help me in how to read the plot. For example, what does the orange/ green band mean?" CreationDate="2016-04-22T00:16:38.083" UserId="5044" />
  <row Id="11450" PostId="11350" Score="0" Text="I have added more detail to my answer." CreationDate="2016-04-22T01:45:12.537" UserId="18077" />
  <row Id="11451" PostId="11347" Score="0" Text="Done, added some sample data." CreationDate="2016-04-22T04:05:53.030" UserId="18058" />
  <row Id="11452" PostId="11349" Score="0" Text="Thanks for that. However, sometimes receipt is repeated (when the date is different too). Therefore, we are aggregating all the different occurrences of receipt even on different dates, when we really want to know the number of receipt by transaction - however there is not a unique transaction ID variable. I don't think receipt repeats on the same day - can we use date as a way to group? `df.groupby('reciept')['date'].count()` gives the same result as `df.groupby('reciept')['prod_name'].count()`" CreationDate="2016-04-22T04:27:26.683" UserId="18058" />
  <row Id="11453" PostId="5128" Score="0" Text="could you please explain why is Cosine index better for identify plagiarism and not good for identifying mirror sites?" CreationDate="2016-04-22T07:09:23.743" UserId="14925" />
  <row Id="11454" PostId="9819" Score="0" Text="did you find an answer for this? im having exactly the question" CreationDate="2016-04-22T08:07:23.337" UserId="15303" />
  <row Id="11455" PostId="11358" Score="0" Text="Welcome to the site, Adam!  That's a very nice, detailed answer for a new user :)" CreationDate="2016-04-22T08:25:56.533" UserId="11097" />
  <row Id="11456" PostId="11361" Score="0" Text="(+1) A nice work-around.  However, there needs to be a function which allows concatenation of multiple dataframes. Would be quite handy!" CreationDate="2016-04-22T08:39:44.307" UserId="11097" />
  <row Id="11457" PostId="11361" Score="0" Text="I don't disagree with that" CreationDate="2016-04-22T08:40:31.363" UserId="14904" />
  <row Id="11458" PostId="11322" Score="0" Text="On the platform users are looking for a teacher for a skill they would like to learn. They enter the skill they want to learn and the motivation, why they want to learn this skill.&#xA;Then no, it is not an option, at least for now." CreationDate="2016-04-22T10:30:18.597" UserId="18028" />
  <row Id="11459" PostId="11341" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/38729/discussion-between-bogatron-and-rgrnormand)." CreationDate="2016-04-22T12:30:40.267" UserId="964" />
  <row Id="11460" PostId="11362" Score="2" Text="Any reason you want to use ML instead of looking for image metadata in the DOM? Unless a site is deliberately obfuscating how it presents images, this should be much cleaner and more reliable. PhantomJS allows you to interact with DOM elements, even those created dynamically." CreationDate="2016-04-22T12:46:00.900" UserId="836" />
  <row Id="11461" PostId="6800" Score="1" Text="I'm the dlib author.  Feel free to post that image wherever you want :).  Also, &gt;20k samples means you have 20k vectors or whatever.  How many variables are in each sample is a separate issue." CreationDate="2016-04-22T13:24:58.063" UserId="12300" />
  <row Id="11462" PostId="11362" Score="0" Text="I knew I shouldn't have made that example! a website's snapshot is just an example and I know crawling and scraping a website is a better way to do it... I just wanted to know the ways ML could help to solve a problem like this even if it meant more calculation price and time... I'm just looking for an insight on how to use ML on a case like this... Thanks..." CreationDate="2016-04-22T13:29:45.707" UserId="14504" />
  <row Id="11463" PostId="11309" Score="0" Text="@Emre To model the presence of people at a location which can be further used to deduce interesting information. This will be particularly helpful in places like retail shops or shoping malls" CreationDate="2016-04-22T13:33:16.353" UserId="18014" />
  <row Id="11464" PostId="11362" Score="0" Text="Would finding posters/paintings or other flat objects in a photograph be similar enough for you? There are a lot of important details to do with how the target differs from the rest of the image which means there is not necessarily going to be a simple answer. The general concept you can search on is called &quot;image segmentation&quot;. If you have a specific use case other than the example it may be worth explaining it to get more targeted answer" CreationDate="2016-04-22T14:21:17.697" UserId="836" />
  <row Id="11465" PostId="11368" Score="0" Text="Welcome to DS.SE. Would you be able to expand on your answer with regard to a few statements 1) &quot;very application specific&quot; - what applications? 2) &quot;second approach&quot; I dont know much about this technique, but could you supply some external references to help on the usual approach, then supplying information on what is different about yours? 3) &quot;the rest is obvious&quot; ... if you could expand slightly it would make this a great answer that will educate and inform ..." CreationDate="2016-04-22T14:57:44.287" UserId="16284" />
  <row Id="11466" PostId="11351" Score="0" Text="Basically I have two matrices where dimensionality is reduced via tSNE to two 100x2 matrices, and visualized via scatter plot. What I want to measure is how much the second distribution of points differs from the first distribution. So I assume the steps to be taken are: - normalize the values via `pk = 1.0*pk / np.sum(pk, axis=0)` for both matrices&#xA;- flatten the matrices to dimensionality 200x1&#xA;- feed them to `scipy.stats.entropy` Is this correct or will the results not be meaningful?" CreationDate="2016-04-22T08:15:25.733" UserId="17988" />
  <row Id="11467" PostId="11366" Score="0" Text="Link-only answers are discouraged. Better to include your explanation of the key points that answer this question." CreationDate="2016-04-22T15:41:18.070" UserId="21" />
  <row Id="11468" PostId="6479" Score="0" Text="Sorry to say SE says this is &quot;too old to migrate&quot; but yes it should be closed here and asked instead on the OpenData SE" CreationDate="2016-04-22T15:42:47.187" UserId="21" />
  <row Id="11469" PostId="11322" Score="0" Text="Then you have the search log, and ratings of classes/teachers (which?), right?" CreationDate="2016-04-22T15:52:15.520" UserId="381" />
  <row Id="11470" PostId="11371" Score="0" Text="Hi there, welcome to DS/SE. your question is very general and doesnt meet the requirements for the site, which is specific question/answers on data science problems. However, have a look at this http://datascience.stackexchange.com/questions/2609/good-books-for-hadoop-spark-and-spark-streaming/2616#2616" CreationDate="2016-04-22T16:01:24.497" UserId="16284" />
  <row Id="11471" PostId="11371" Score="0" Text="Thank you, @MarcusD I'll keep in mind to be more specific about my questions." CreationDate="2016-04-22T17:03:46.630" UserId="17612" />
  <row Id="11472" PostId="11351" Score="0" Text="@Piutu, the steps you describe will produce a KL-divergence metric. You don't need to normalize if you want to use entropy() built-in normalization. You still should know what your matrix represents. I quickly read about tSNE implementation from SKlearn and I believe each row of your 100x2 matrix is a sample (as it is on a design matrix), so you should be calculating the KL-divergence between each row from your 2 matrices (you will have a 100x100 resulting matrix). Please confirm you actually have 100 samples in each matrix. If that is the case, then you should not flatten the matrix." CreationDate="2016-04-22T19:41:52.500" UserId="16001" />
  <row Id="11473" PostId="11374" Score="0" Text="Look up _imbalanced multi-class classification_." CreationDate="2016-04-22T20:00:56.543" UserId="381" />
  <row Id="11474" PostId="11349" Score="0" Text="Add date as a parameter inside the groupby call. Edited my answer above to group by multiple vars." CreationDate="2016-04-22T21:51:56.090" UserId="16538" />
  <row Id="11475" PostId="11361" Score="0" Text="@JanvanderVegt Thanks, it works and the idea of adding labels to filter out the training and testing dataset, I did it already.  Thank you very much for your help." CreationDate="2016-04-23T03:27:34.873" UserId="17116" />
  <row Id="11476" PostId="10853" Score="0" Text="you can add the link as `[link](http://example.com)`, what is your **base point to decide the main responses are** ?, do you want classification of the sentences into positive and negative sentence?" CreationDate="2016-04-23T03:34:31.067" UserId="17116" />
  <row Id="11477" PostId="11362" Score="0" Text="Thanks... actually I don't really have a specific case... I was just looking for an approach when dealing with a problem like this... What I'm saying is that my case is a General Purpose one : how can someone classify parts of an Image as an individual image itself just by having lots of images with highlighted areas ? and I want it to be an ML approach (Neural Networks perhaps) and not just raw Computer Vision algorithms. Thanks again..." CreationDate="2016-04-23T05:38:49.370" UserId="14504" />
  <row Id="11478" PostId="11375" Score="1" Text="Try using $L_p$ regularization instead of doing that to fight overfitting." CreationDate="2016-04-23T06:38:03.867" UserId="381" />
  <row Id="11479" PostId="11362" Score="0" Text="I don't do this professionally, but AFAIK, the usual approach *is* to try and find a robust simple CV algorithm if it looks possible. You can train a neural network to do image segmentation, and it might be the best choice for some data, but it could be a lot of effort to go to (and take a lot of training examples plus compute resource), so it isn't necessarily the first or best choice." CreationDate="2016-04-23T07:40:28.390" UserId="836" />
  <row Id="11480" PostId="11362" Score="0" Text="Thank you for your answers... They helped..." CreationDate="2016-04-23T09:21:01.520" UserId="14504" />
  <row Id="11481" PostId="11322" Score="0" Text="We do have the search log, but no ratings yet. We have reviews for teachers, but the skill, for which they're reviewed, is not specified." CreationDate="2016-04-23T10:05:30.780" UserId="18028" />
  <row Id="11482" PostId="11374" Score="0" Text="So basically your data set is biased towards 1 and 3 right?" CreationDate="2016-04-23T14:26:56.117" UserId="17116" />
  <row Id="11483" PostId="11308" Score="0" Text="Did you try to determine most informative features for each of two models and then feeding only them to the models on both train and test? Maybe I've missed this part in your text." CreationDate="2016-04-23T15:25:01.543" UserId="15361" />
  <row Id="11484" PostId="11380" Score="0" Text="You can extract the most frequent n-grams though!" CreationDate="2016-04-23T16:45:20.300" UserId="11097" />
  <row Id="11485" PostId="11374" Score="0" Text="yes. It is predicting only one value for policy 4 and little above for 2. And we have high values for 1 and 3. Could you please suggest some option other?. In the mean time I am checking on imbalanced multi-class classification." CreationDate="2016-04-23T18:35:32.823" UserId="5043" />
  <row Id="11486" PostId="11378" Score="0" Text="Thanks for your reply! Currently, the test errors of my SVD in most cases are not much better than those of linear regression or average-based deterministic baseline. My guess is that the training data is still sparse although the training data contain frequent users and items. In other words, I concern users do not share enough common items. So I wonder how you calculated sparsity, the fraction of known cells in the user-item matrix? What's the sparsity threshold you often use?" CreationDate="2016-04-23T18:40:21.130" UserId="10464" />
  <row Id="11487" PostId="11385" Score="1" Text="No, it is not. [Read about encoding categorical variables](http://www.willmcginnis.com/2015/11/29/beyond-one-hot-an-exploration-of-categorical-variables/)." CreationDate="2016-04-23T22:07:43.020" UserId="381" />
  <row Id="11488" PostId="11387" Score="0" Text="Usually you want a specific measure to use a specific measure based on the confusion matrix. The measure to choose should be according to your goals. For details see http://datascience.stackexchange.com/questions/806/advantages-of-auc-vs-standard-accuracy/8796#8796" CreationDate="2016-04-24T06:28:12.893" UserId="13727" />
  <row Id="11489" PostId="11392" Score="0" Text="that's interesting observation, thanks!" CreationDate="2016-04-24T08:39:57.777" UserId="14070" />
  <row Id="11490" PostId="5710" Score="0" Text="Also, suspect if it has something to do with the loss function http://www.csie.ntu.edu.tw/~cjlin/papers/spark-liblinear/spark-liblinear.pdf&#xA;(Sec - IIIA) @SparkUser - is it possible to run LogisticRegressionWithLBFGS without the defaults and then compare the coefficients with the default R - glm." CreationDate="2016-04-23T17:47:52.937" UserId="18124" />
  <row Id="11492" PostId="10885" Score="0" Text="@tedghosh: Is there a mistake your sentence (that blueSerpent quoted)? It doesn't make sense, since you describe the *training* set twice in two different incompatible ways. Is the second one actually a *test* set?" CreationDate="2016-04-24T15:32:46.300" UserId="836" />
  <row Id="11493" PostId="10885" Score="0" Text="@NeilSlater oops sorry just realized my mistake now and edited the question. You are right, the second one is the test set (500 tweets)." CreationDate="2016-04-24T16:34:02.063" UserId="17313" />
  <row Id="11494" PostId="11395" Score="0" Text="For prediction (future scoring), are you using the *same* population but just at a different time?" CreationDate="2016-04-24T17:32:08.253" UserId="17397" />
  <row Id="11495" PostId="11399" Score="1" Text="The order of the data does make a difference. Determining the optimal order is actually a research problem called [curriculum learning](http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf). If you have any questions after reading the paper I'll try to answer." CreationDate="2016-04-24T19:53:36.907" UserId="381" />
  <row Id="11496" PostId="11380" Score="0" Text="Perfect. Thanks! @Dawny33" CreationDate="2016-04-24T20:09:03.273" UserId="18115" />
  <row Id="11497" PostId="11399" Score="0" Text="Specifically, I'm training a character classifier. As there is no easy way to automatically determine the difficulty for each picture (that I know of), and therefore no way of sorting the pictures internally in each class by difficulty, does it then make sense to sort the classes themselves by difficulty? One could imagine that classifying `O`, `o` and `I` is easier than `E` and `F`." CreationDate="2016-04-24T23:21:40.107" UserId="18000" />
  <row Id="11498" PostId="11399" Score="0" Text="That sounds sensible to me." CreationDate="2016-04-25T00:02:50.423" UserId="381" />
  <row Id="11499" PostId="11399" Score="0" Text="After thinking about it there may be ways to roughly assess the difficulty of a picture. Signal-to-Noise ratio comes to mind, but I don't know much about such analysis and it would depend on the data set. I'm working with the [chars74k image data set](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/)" CreationDate="2016-04-25T01:35:42.390" UserId="18000" />
  <row Id="11500" PostId="11212" Score="0" Text="Maybe they just failed to get really good clusters? Clustering is very difficult." CreationDate="2016-04-25T05:45:01.677" UserId="924" />
  <row Id="11501" PostId="11403" Score="1" Text="It's kind of hard to create a 33-dimensional plot, that's why you normally only use 2 or 3 principal components for visualization. Why do you need 33 components and what's your goal - visualization, classification,...?" CreationDate="2016-04-25T07:05:52.947" UserId="676" />
  <row Id="11502" PostId="11351" Score="0" Text="(I can't comment yet due to missing reputation)&#xA;Yes, indeed each row represents a sample. Does it mean that I now need to feed each row of the two matrices to `scipy.stats.entropy` and sum over all 100 samples?" CreationDate="2016-04-25T07:56:12.577" UserId="17988" />
  <row Id="11503" PostId="11403" Score="1" Text="Hi stamm, because I'm a noob. Now I understand why he used only 2 PC. Thank you for your answer :)" CreationDate="2016-04-25T09:31:31.700" UserId="17886" />
  <row Id="11504" PostId="10329" Score="0" Text="I guess you could try that tutorial (or something else), and see how long time it takes on your computer: https://www.tensorflow.org/versions/r0.8/tutorials/mnist/pros/index.html. Anyway training deep nets with interesting datasets should be possible on regular computers, NN are not only suitable for big distributed clusters." CreationDate="2016-04-25T11:07:32.767" UserId="17208" />
  <row Id="11505" PostId="11403" Score="0" Text="That was easy :p" CreationDate="2016-04-25T11:51:57.777" UserId="676" />
  <row Id="11506" PostId="11408" Score="0" Text="The goal is a bit vague. What will be the best outcome for the choice of colour - best visibility, best artistic appearance? Do you have any data, such as lists of paired colours that you want your algorithm to emulate? Are you working with RGB graphics? If you want standard results of contrast or complementary colours in any standard colour space, then there are well-known non-ML solutions, such  as use of the colour wheel e.g. see http://www.sessions.edu/color-calculator" CreationDate="2016-04-25T11:56:43.143" UserId="836" />
  <row Id="11507" PostId="11408" Score="0" Text="Thanks Neil. I just took a look at the link. Thing I wanted exactly is similar. For a given background colour , I want the model to suggest contrast colour for the font. The colour wheel does similar with the first harmony. May I know how this could be implemented ?" CreationDate="2016-04-25T12:02:25.950" UserId="18158" />
  <row Id="11508" PostId="11408" Score="0" Text="Sorry, it's not really a Data Science topic. If this is for workflow for yourself, then there are lots of tools that can help: http://leaverou.github.io/contrast-ratio/#orange-on-blue or http://snook.ca/technical/colour_contrast/colour.html#fg=33FF33,bg=333333 - if you want to create a similar tool, you will need to research how these things work. Unfortunately, for being on-topic here, they do not use any Machine Learning or Data Science." CreationDate="2016-04-25T12:13:06.267" UserId="836" />
  <row Id="11509" PostId="11351" Score="0" Text="tSNE can produce very different results for two sets of data, even if the data is similar. I don't think two runs of tSNE should be used to compare the similarity of the samples." CreationDate="2016-04-25T12:22:41.037" UserId="676" />
  <row Id="11510" PostId="11395" Score="0" Text="No, I will always use new instances." CreationDate="2016-04-25T12:43:24.083" UserId="17484" />
  <row Id="11511" PostId="11376" Score="1" Text="oh! this is an interesting thought process that never occurred to me. I shall give it a thought." CreationDate="2016-04-25T13:59:12.850" UserId="18019" />
  <row Id="11512" PostId="11382" Score="0" Text="could you please elaborate?" CreationDate="2016-04-25T13:59:33.743" UserId="18019" />
  <row Id="11513" PostId="11413" Score="0" Text="Is your data on a daily basis or a monthly basis? Do you have 24 numbers from 2 years of monthly data and you want to compute a *one month* rolling average? Its just the data. But if you have 2*365 **daily** numbers then its a rolling mean computed every day for a two-month window. But what exactly is two months from 31st of January? Is that your basic problem?" CreationDate="2016-04-25T15:16:45.610" UserId="471" />
  <row Id="11514" PostId="11406" Score="0" Text="Thanks for the guidance.I am looking into the topics mentioned by you. But the technique which I used to undersample (reducing the data randomly) is a right way of doing?" CreationDate="2016-04-25T15:23:19.933" UserId="5043" />
  <row Id="11515" PostId="11406" Score="0" Text="You can use it if your database is very large. But if your database is small you will lose some of the information. Read the Rus-Boosting , in this method they use random under sampling as part of the boosting algorithm to avoid loosing information. They under sample the sub set that will be used for training the next base learner but not the whole database" CreationDate="2016-04-25T15:41:20.990" UserId="17696" />
  <row Id="11516" PostId="11409" Score="0" Text="(class_weight=&quot;balanced&quot;) is not giving sufficient improvements when I tried to use it" CreationDate="2016-04-25T15:49:12.997" UserId="5043" />
  <row Id="11517" PostId="11406" Score="0" Text="My dataset has nearly 80k records which I am using it as training set. I am implementing this in python. I was looking for some packages in sklearn or something else in python. I couldnt find them. Is this something for which I should right some logic in place to have them implementedt?" CreationDate="2016-04-25T15:56:50.803" UserId="5043" />
  <row Id="11518" PostId="11406" Score="0" Text="I do not think there is any implementation for these methods. The data imbalance problem is still under research. If you have a good implementation for Adaboost.M1 or M2 . You can easily modify it to become Rus Boost" CreationDate="2016-04-25T16:01:30.650" UserId="17696" />
  <row Id="11519" PostId="11406" Score="0" Text="I think the database you have is quite large and if you want you can use viola - jones  classifier. For this one you may find available implementation" CreationDate="2016-04-25T16:02:43.157" UserId="17696" />
  <row Id="11520" PostId="11406" Score="0" Text="thanks for the guidance. I will check on how to implement it." CreationDate="2016-04-25T16:15:36.240" UserId="5043" />
  <row Id="11521" PostId="11413" Score="0" Text="My data is monthly. Yes.. I have 24 months of data. Generally we hear 3 week rolling average where we take average for 3 weeks and this will be continued for the new data too.Similarly for 2 months rolling average we take every 2 consecutive months and get their average. If I just say 'monthly rolling average' instead of 2-month or 5- month rolling average, what does that mean?" CreationDate="2016-04-25T17:03:02.593" UserId="13417" />
  <row Id="11522" PostId="11410" Score="0" Text="As far as of today, I figured out that a lower learning rate drastically reduces the variability on the output KPIs. I.e. noise is reduced by learning slower which I interpret as less deviations in the Stochastic Gradient / ADAM." CreationDate="2016-04-25T17:04:59.597" UserId="18120" />
  <row Id="11523" PostId="11414" Score="0" Text="Yes.. thank you.. Even I was thinking the same.  If I had daily data for 24 months.. then in that case , each months average would be my monthly rolling average right since I am just considering monthly." CreationDate="2016-04-25T17:13:04.997" UserId="13417" />
  <row Id="11524" PostId="11413" Score="0" Text="If you say it, you should know what it means. I suspect it means &quot;one month rolling average&quot;, which by construction from the definition of &quot;N-month moving average&quot; for monthly data is... just the data." CreationDate="2016-04-25T17:13:44.333" UserId="471" />
  <row Id="11525" PostId="11413" Score="0" Text="Yes.. it is one month rolling average." CreationDate="2016-04-25T17:16:05.190" UserId="13417" />
  <row Id="11526" PostId="11414" Score="0" Text="If you had daily data then a monthly rolling average *of the daily data* would be an average of about the next 30 to 31 days computed every day. You'd get 365 of them per year, and it would be smoothing the data." CreationDate="2016-04-25T17:22:33.023" UserId="471" />
  <row Id="11527" PostId="11414" Score="0" Text="Got it.. thank you." CreationDate="2016-04-25T17:44:34.503" UserId="13417" />
  <row Id="11528" PostId="10901" Score="0" Text="Till, if you're still paying attention...the link you sent to the stemming discussion makes me think it's not yet functional within sklearn.  I can certainly clean my train files with nltk, but is there a stemmer within sklearn I'm missing?" CreationDate="2016-04-25T17:52:43.323" UserId="17309" />
  <row Id="11529" PostId="10901" Score="0" Text="@tadamhicks For the stemming part you can use the nltk package. Here is an [example](http://stackoverflow.com/questions/26126442/combining-text-stemming-and-removal-of-punctuation-in-nltk-and-scikit-learn). You can also check this [discussion](https://github.com/scikit-learn/scikit-learn/issues/1156)" CreationDate="2016-04-25T18:41:45.533" UserId="17324" />
  <row Id="11530" PostId="11416" Score="0" Text="Could you please give some sample data, IM not clear on what your data looks like." CreationDate="2016-04-25T18:48:54.910" UserId="16284" />
  <row Id="11531" PostId="11416" Score="0" Text="I added the table here but it just came in one line.. I dont know how to get that in a table.. :(" CreationDate="2016-04-25T18:54:33.927" UserId="13417" />
  <row Id="11532" PostId="11406" Score="0" Text="Srinath, you can try this code , it is also a well known approach for data imbalance http://lamda.nju.edu.cn/code_EasyEnsemble.ashx" CreationDate="2016-04-25T19:25:36.563" UserId="17696" />
  <row Id="11533" PostId="11382" Score="0" Text="Does confidence interval for the mean of a distribution sample sound familiar to you?" CreationDate="2016-04-25T23:37:59.273" UserId="15361" />
  <row Id="11534" PostId="11418" Score="0" Text="That was a good explanation. thank you so much. It solved my problem. And one more ques: if I just need top three marks from this output, what would the code be?" CreationDate="2016-04-26T01:36:19.797" UserId="13417" />
  <row Id="11535" PostId="11351" Score="0" Text="@Piutu, you will make a call to scipy.stats.entropy for each pair of rows that you have. The regular way to compare would be compare between 2 samples (two rows, one from each matrix). I am not sure if there is a way to summarize a set of divergences, though. Maybe you could average each dimension and then apply the KL-Divergence. Do you need one single value, is that it? What use do you intend to use from this similarity measure?" CreationDate="2016-04-26T01:37:36.783" UserId="16001" />
  <row Id="11537" PostId="11418" Score="0" Text="I tried to put a function here:                                                                                                               groups=unique(a$group) &#xA;check=function(x)&#xA;{&#xA;  subdata=a[a$group==x,]&#xA;  data=subdata[order(subdata$marks,decreasing=TRUE)[1:3],]&#xA;  data&#xA;}&#xA;this was my ouput-&#xA;check(groups)       &#xA;  group marks upd&#xA;6     c     7  up&#xA;2     b     3  up&#xA;1     a     2  up   . Here, it just gave top 1 record of each group. But I wanted output to display all the top 3 records of marks of each group." CreationDate="2016-04-26T02:08:10.250" UserId="13417" />
  <row Id="11538" PostId="11416" Score="0" Text="Is it okay if I can suggest an alternate solution (a simpler and a nice looking one)?" CreationDate="2016-04-26T02:33:09.837" UserId="11097" />
  <row Id="11539" PostId="11416" Score="0" Text="@Dawny: Sure..." CreationDate="2016-04-26T03:15:07.063" UserId="13417" />
  <row Id="11540" PostId="11416" Score="0" Text="Never mind. Luke has included it in the answer. Use `dplyr` when manipulating data. It is much simpler and looks more elegant :)" CreationDate="2016-04-26T04:57:56.103" UserId="11097" />
  <row Id="11541" PostId="11382" Score="0" Text="I wasn't very familiar, but gave it a reading just now. I guess I get where you are coming from. This seems to be a simple approach I can use." CreationDate="2016-04-26T05:09:52.980" UserId="18019" />
  <row Id="11542" PostId="11354" Score="0" Text="Thanks for your reply." CreationDate="2016-04-26T06:42:26.537" UserId="5044" />
  <row Id="11544" PostId="11418" Score="0" Text="E.g. `a &lt;- a[order(a$group, -a$marks), ];do.call(rbind, lapply(split(a, a$group), head, 3))` or `a %&gt;% arrange(group, desc(marks)) %&gt;% group_by(group) %&gt;% slice(1:3)`" CreationDate="2016-04-26T08:32:57.960" UserId="15202" />
  <row Id="11546" PostId="11427" Score="0" Text="Which one of sklearn's linear regression classes are you using exactly? There are several in sklearn.linear_model, like LinearRegression, Ridge, Lasso,..." CreationDate="2016-04-26T10:28:52.227" UserId="676" />
  <row Id="11547" PostId="11427" Score="0" Text="Standart LinearRegression." CreationDate="2016-04-26T10:33:39.857" UserId="18184" />
  <row Id="11548" PostId="11424" Score="0" Text="_because you are referring to the order of $b$, not of $a$_  &lt;-- Wrong. She wants the order of _a_" CreationDate="2016-04-26T11:08:14.297" UserId="11097" />
  <row Id="11549" PostId="11429" Score="0" Text="What are oneR and jRip? Haven't heard them before. Can you explain a bit and add some details to your question?" CreationDate="2016-04-26T11:42:10.777" UserId="11097" />
  <row Id="11552" PostId="11429" Score="0" Text="These are functions in R. OneR makes a single classification rule. While jRip will make as many rules as necessary" CreationDate="2016-04-26T11:47:05.977" UserId="17959" />
  <row Id="11554" PostId="11429" Score="0" Text="Let me know if my edit helps?" CreationDate="2016-04-26T11:51:44.813" UserId="11097" />
  <row Id="11558" PostId="11429" Score="0" Text="Cool. No problem" CreationDate="2016-04-26T12:01:06.057" UserId="17959" />
  <row Id="11559" PostId="11408" Score="0" Text="This pet project is pretty relevant: https://harthur.github.io/brain/" CreationDate="2016-04-26T13:10:54.170" UserId="15527" />
  <row Id="11560" PostId="11367" Score="0" Text="Do you know how the model will be used? some examples:  Will they call the top 20% of people that are most likely to buy? Will they send letters or emails to all people the model predicts will buy?" CreationDate="2016-04-26T13:11:01.290" UserId="14913" />
  <row Id="11561" PostId="11424" Score="0" Text="b has only a subset of the rows of a, so it is shorter. Therefore the sort order needs to be as long as b, not as long as a. that is what I meant. I updated the answer to clarify. By the way, sorting is a heavy process, so t is better to sort the shorter vector rather than the longer one, if the datasets is large." CreationDate="2016-04-26T16:50:29.517" UserId="18179" />
  <row Id="11562" PostId="11439" Score="0" Text="Can you explain the data you have? The features and the labels. What kind of predictions you are interested in ?" CreationDate="2016-04-26T16:53:53.370" UserId="17696" />
  <row Id="11563" PostId="11439" Score="0" Text="I try to predict a debit, so a number of cars who pass by a certain street, every hour. I have 2 years of this value, and I try to predict the following year.&#xA;The features that I have:&#xA;- Calendar features: year, month, day, hour, day of the week, holiday, working day, school holiday.&#xA;- Weather features: daily mean temperature, precipitation, wind.&#xA;Basically I want to predict the patterns of the traffic for the next years. So I should see typical daily/weekly pattern, as well as specific patterns for holidays (which days can vary in the calendar), maybe detect a pattern with temperature..." CreationDate="2016-04-26T17:10:40.710" UserId="18195" />
  <row Id="11564" PostId="11439" Score="0" Text="@user2076688 Are you trying to predict a year forward what the traffic will be like? Or are you trying to predict a week in the future lets say? Predicting a full year into the future seems very hard and somewhat impossible due to the chaotic nature of your parameters (e.g. weather). I believe you should be able to predict a week or couple weeks forward in the future rather accurately." CreationDate="2016-04-26T17:18:07.640" UserId="14779" />
  <row Id="11565" PostId="11439" Score="0" Text="@ArmenAghajanyan: I am trying to predict a year in the future. And I don't mind that I have some imprecision due to te chaotic nature of the weather: for example, I cannot say which day it is going to snow in advance, but I know that in the summer, the weather will be nice and people will stay out late, while in the winter the cold will make them come back home fast. Similarly to the fact that I cannot predict the exact temperature on the next July 15th, but I know July will be warm compared to January." CreationDate="2016-04-26T17:50:22.863" UserId="18195" />
  <row Id="11566" PostId="11427" Score="0" Text="Could you be running out of memory?" CreationDate="2016-04-26T19:31:20.147" UserId="381" />
  <row Id="11567" PostId="11367" Score="0" Text="They just want to know how their best customers have been acquired, ie. thru web, tradeshows, free trials; so they can target them later. I need to get this data from the tree/model. I've set bool values for each kind of acquisition, as they were all in one column(hence, used get_dummies(). That's why there are so many columns. If there is a way to remove the ones with less correlation, I would do so, but I don't know how!" CreationDate="2016-04-26T19:48:41.333" UserId="18098" />
  <row Id="11569" PostId="11416" Score="0" Text="isn't this a question about R and thus should it be on SO instead? http://datascience.stackexchange.com/help/on-topic -&gt; &quot;If your question is not specifically on-topic for Data Science Stack Exchange, it may be on topic for another Stack Exchange site. &quot;" CreationDate="2016-04-26T22:38:56.893" UserId="6478" />
  <row Id="11570" PostId="11430" Score="0" Text="In the future, please post the solution as an answer." CreationDate="2016-04-26T23:00:41.163" UserId="434" />
  <row Id="11572" PostId="11444" Score="0" Text="For the reference you can check the decision tree reference [here](http://datascience.stackexchange.com/questions/9424/spark-mllib-multiclass-logistic-regression-how-to-get-the-probabilities-of-all)" CreationDate="2016-04-27T02:43:13.730" UserId="17116" />
  <row Id="11573" PostId="11446" Score="0" Text="Can you describe in more or less specific terms the nature of your time series, and of your anomalies?" CreationDate="2016-04-27T07:48:58.183" UserId="12527" />
  <row Id="11575" PostId="11447" Score="0" Text="Manohar Swamynathan and  Laurent Duval. Thanks for your response. The analysis is unsupervised in nature and considering a single variable (inlet pressure). The objective is to develop a model which can detect if there is any significant deviation in the parameter value in real time." CreationDate="2016-04-27T09:17:39.497" UserId="18207" />
  <row Id="11576" PostId="11447" Score="0" Text="@ Sunil for your use case, the uni-variate methods discussed in the paper [anomaly detection in machine generated data](http://www.diva-portal.se/smash/get/diva2:621699/FULLTEXT01.pdf) has been implemented in python and the same can be found in [here](https://github.com/aeriksson/ad-eval)" CreationDate="2016-04-27T10:36:26.993" UserId="8465" />
  <row Id="11578" PostId="11447" Score="0" Text="@ Manohar, thanks for the document. Ya, I am going through the material. Can you let me know if there is any document where the method is implemented in R ?" CreationDate="2016-04-27T11:12:54.830" UserId="18207" />
  <row Id="11580" PostId="11450" Score="0" Text="Welcome to the site.  I think the question can be broken down into multiple questions. Points 1&amp;2 together, 3, 4, 5 together and the last one separately. :)" CreationDate="2016-04-27T12:50:24.427" UserId="11097" />
  <row Id="11581" PostId="8694" Score="0" Text="` Random Forest, Regularized Random Forest, Guided Random Forest` which implementations do you think about ? Python, WEKA or R ?" CreationDate="2016-04-27T13:57:44.210" UserId="8237" />
  <row Id="11582" PostId="11447" Score="0" Text="I'm not sure of implementation of this paper content in R, however you can look at [AnomalyDetection](https://github.com/twitter/AnomalyDetection) R package and also there is a new package by Rob J Hyndman [Anomalous time series package](https://github.com/robjhyndman/anomalous)." CreationDate="2016-04-27T14:44:27.087" UserId="8465" />
  <row Id="11584" PostId="11420" Score="1" Text="Just like to add that I've done the same, that is, used Theano on a laptop to verify that a given architecture is able to decrease the log loss consistently. I would then rent time on a GPU provider in the cloud (e.g AWS), and let it run for tens of hours, till the elbow on the validation set turns up." CreationDate="2016-04-27T15:58:16.863" UserId="12786" />
  <row Id="11587" PostId="11393" Score="0" Text="could you share some info on what sort of analysis you want to carry out in the end? That might give a better idea of what to suggest." CreationDate="2016-04-27T16:54:00.603" UserId="16284" />
  <row Id="11588" PostId="10377" Score="0" Text="That's a general comment since I'm not sure what you have done but I do agree that looks odd.It could happpen for some data partitions. Repeat it changing the seed several times and check those results. In case it remains I would review the entire code. Good luck!" CreationDate="2016-02-27T06:34:40.823" UserId="16560" />
  <row Id="11589" PostId="10377" Score="0" Text="Thanks Rafael. I changed the seed point but it didn't change a lot." CreationDate="2016-02-29T04:28:07.360" UserId="12867" />
  <row Id="11590" PostId="11393" Score="0" Text="Ditto @MarcusD comment... Why are you doing this? Are you trying to make it more human readable for management, are you feeding this into a learning algorithm, or are you trying to produce insightful intelligence using statistical inference?  Our answers will depend upon the purpose." CreationDate="2016-04-27T17:39:07.570" UserId="9420" />
  <row Id="11591" PostId="11462" Score="2" Text="That's exactly what I'm looking for, thank you!" CreationDate="2016-04-27T19:51:07.497" UserId="18239" />
  <row Id="11594" PostId="11459" Score="2" Text="I'm not sure I understand the issue here. The standard technique would be to use a histogram to approximate a distribution. Could you provide more details?" CreationDate="2016-04-27T22:20:39.857" UserId="4710" />
  <row Id="11595" PostId="11446" Score="0" Text="Semi supervised SVMs are quite efficient on IT security matters. You need a few supervised learning to get a good anomaly detection. See that for an example:&#xA;http://rslab.disi.unitn.it/papers/R86-TGRS-Jordi.pdf" CreationDate="2016-04-27T22:22:03.297" UserId="17208" />
  <row Id="11596" PostId="11459" Score="2" Text="The issue of statistical significance arises when you're comparing two groups or when you're assessing whether a given value is likely to be different from some reference value. It's hard to tell from your question what you're trying to find out. Are you just interested in the mean response for each question or are you interested in checking whether there are statistically significant differences in the mean response when comparing different questions (e.g., is buyer's characteristic A more important than characteristic B)?" CreationDate="2016-04-27T22:53:30.937" UserId="376" />
  <row Id="11597" PostId="11459" Score="1" Text="Also, what is your universe of sellers? Is it just the sellers at this particular bazaar or do you hope to generalize to sellers at other bazaars? Do the 180 sellers you surveyed represent nearly all the sellers at the bazaar or a small sample of them?" CreationDate="2016-04-27T22:54:55.273" UserId="376" />
  <row Id="11598" PostId="11457" Score="2" Text="This is the [community detection](https://en.wikipedia.org/wiki/Community_structure) problem. Here's a solid survey: [Community detection in graphs](http://www.uvm.edu/~pdodds/teaching/courses/2009-08UVM-300/docs/others/everything/fortunato2010a.pdf) [PDF]." CreationDate="2016-04-27T23:00:20.823" UserId="381" />
  <row Id="11599" PostId="11464" Score="0" Text="That's an interesting idea, ideally I'd prefer it to be done all on a computer since emotion and a scale can vary from person to person and it'd take time to have mini focus groups, but it certainly is a valid option and something interesting to think about, thanks!" CreationDate="2016-04-27T23:14:00.087" UserId="18239" />
  <row Id="11600" PostId="11228" Score="0" Text="Not what I was looking for but this gave me a direction to go on to follow a similar problem" CreationDate="2016-04-27T23:21:02.440" UserId="16676" />
  <row Id="11601" PostId="11464" Score="0" Text="One possible solution is to use amazon's mechanical turk - a task crowd-sourcing platform, where you can hire people to perform certain tasks at a certain pay rate (e.g., read and rate 100 facebook posts at .10 cents a post). Its possible to get data quite quickly with this approach, but there are caveats concerning reliability, bias, etc., though some research suggests its as reliable as other techniques (http://pps.sagepub.com/content/6/1/3.short). We've used it extensively for building ML NLP systems." CreationDate="2016-04-27T23:37:54.010" UserId="6478" />
  <row Id="11602" PostId="11471" Score="0" Text="Reference: [Coursera Regression Models class](https://www.coursera.org/learn/regression-models/), I have recently completed it." CreationDate="2016-04-28T04:06:53.917" UserId="17116" />
  <row Id="11604" PostId="11393" Score="0" Text="I'm going to use this variable into a classification algorithm (probably decision tree) where the target variable is a binary 0/1 that identifies whether or not a customer contacted the call center in a given month. I want to discriminate between those who did not renew the contract and those who are still on their first contract and never had the chance to do so. This information is important because I believe that those who are not renewing might indeed be unsatisfied customers who therefore are more likely to contact the call center." CreationDate="2016-04-28T09:06:41.423" UserId="17709" />
  <row Id="11605" PostId="11477" Score="1" Text="In the last line you forgot &quot;degrees of freedom&quot;. It should be `chi2, pvalue = scipy.stats.chisquare(obs.ravel(), exp.ravel(), ddof=sum(obs.shape)-2)`" CreationDate="2016-04-28T11:19:11.703" UserId="6550" />
  <row Id="11606" PostId="11450" Score="0" Text="Indeed, let's just keep the categorical topic. (title and question have been rephrased)" CreationDate="2016-04-28T11:37:52.920" UserId="18221" />
  <row Id="11607" PostId="11478" Score="1" Text="Any chance you can post a small chunk of the code you are using and a snapshot of your data?  In my experience with trees and class imbalance, most of your terminal nodes should be &quot;Not Won&quot;.  Have you tried any other modeling techniques to see if they always predict &quot;Won&quot;?  It may give some insight into whats going on in your code/data." CreationDate="2016-04-28T12:39:00.083" UserId="14913" />
  <row Id="11609" PostId="11399" Score="0" Text="Simple random sorting would be the standard and should be sufficient for most tasks. At least try this before moving on to more complex methods." CreationDate="2016-04-28T13:02:23.963" UserId="5144" />
  <row Id="11610" PostId="11480" Score="0" Text="thanks for the link to the paper, it'll have a look at." CreationDate="2016-04-28T13:19:23.543" UserId="14070" />
  <row Id="11611" PostId="11478" Score="1" Text="hmm, thanks for updating.  I'm not super familiar with pandas, so I'm hoping someone else might be able to find something, but the cross validation score of 0.95 ~(38000-1700)/38000 accuracy would support that the classifier is calling almost everything as &quot;Not Won&quot;, which would mean your visualization is off. Does the order of `class_names=[&quot;Won&quot;,&quot;Lost&quot;]` in your `export_graphviz` statement make a difference?  I would think that the first position would correspond to 0, which should be &quot;Lost&quot;, so it should be `class_names=[&quot;Lost&quot;,&quot;Won&quot;]`?" CreationDate="2016-04-28T13:21:26.910" UserId="14913" />
  <row Id="11612" PostId="11478" Score="0" Text="Well if thats the case, shucks!!! I understood the CV thing from this site [Decision trees in python](http://chrisstrelioff.ws/sandbox/2015/06/25/decision_trees_in_python_again_cross_validation.html) Im really not sure if the class thing is wrong, hope that someone who's good at pandas can help. Thanks a lot anyways!!!" CreationDate="2016-04-28T13:32:42.230" UserId="18098" />
  <row Id="11614" PostId="11478" Score="1" Text="Consider looking into the class weights argument in the DecisionTreeClassifier.  The issue is that the model will be most accurate by always predicting &quot;Not Won&quot; since that is the vast majority.  By changing the class weights to weight &quot;Won&quot; observations more, you tell the model that misclassifying a &quot;Won&quot; observation is more important than just focusing on &quot;Not Won&quot;.  The weights you use will affect the model, so try some different ones and see when you start getting a better split of final predictions." CreationDate="2016-04-28T13:46:37.310" UserId="14913" />
  <row Id="11616" PostId="11489" Score="0" Text="The letter n-grams." CreationDate="2016-04-28T17:26:45.307" UserId="381" />
  <row Id="11617" PostId="11489" Score="0" Text="Look it up in a dictionary? What are you trying to do here, this isn't clear. Give examples." CreationDate="2016-04-28T17:35:56.413" UserId="471" />
  <row Id="11619" PostId="11477" Score="0" Text="Thanks for the great response! Is it common for a p-value to be returned as 0.0? I assume this means a p-value so small that it exceeds the amount of significant figures that python can store?" CreationDate="2016-04-28T17:52:58.147" UserId="15141" />
  <row Id="11620" PostId="11489" Score="0" Text="Sorry, maybe I didn't explain the problem completely.&#xA;There is description of task that I found [there](https://github.com/hola/challenge_word_classifier)&#xA;&#xA;*You have to write a program that can answer whether a given word is English. This would be easy — you'd just need to look the word up in the dictionary — but there is an important restriction: your program must not be larger than 64 KiB.*&#xA;&#xA;So, I thought it would be possible to use logistic regression for solving the problem. I don't have a lot of experience with data-mining but the task is interesting for me." CreationDate="2016-04-28T18:19:17.803" UserId="17971" />
  <row Id="11621" PostId="11490" Score="0" Text="I had a hunch this was the way to go, thank you for confirming!" CreationDate="2016-04-28T19:46:16.800" UserId="18271" />
  <row Id="11622" PostId="11494" Score="0" Text="Thank you for such detailed answer" CreationDate="2016-04-29T07:44:13.963" UserId="17971" />
  <row Id="11624" PostId="11494" Score="0" Text="fyi - no need to sample from words.txt, just use the entire list (i've been in big data land for a cpl years so the first words out of my mouth are always  &quot;take a random sample&quot; lol)." CreationDate="2016-04-29T10:49:30.637" UserId="6478" />
  <row Id="11625" PostId="11494" Score="0" Text="I think the winner of this competition is likely to add some further insight than ngrams, although it may well include them as part of the strategy. I think that using solely ngrams will lead to many false positives, as the test description implies that &quot;believable&quot; but non-English words are in the the test set." CreationDate="2016-04-29T11:46:35.310" UserId="836" />
  <row Id="11628" PostId="11492" Score="0" Text="That's for sure a clear and good answer !&#xA;DBSCAN algo looks indeed interesting.&#xA;Thanks a lot." CreationDate="2016-04-29T12:28:41.253" UserId="18221" />
  <row Id="11629" PostId="11503" Score="0" Text="Kaggle recently had a [Rossmann](http://blog.kaggle.com/?s=Rossmann) store challenge, which was at the store level (not product) but some of the interviews might be good to browse." CreationDate="2016-04-29T13:26:14.333" UserId="14913" />
  <row Id="11630" PostId="11409" Score="0" Text="@Srinath what do you understand by improvement? What metric are you using? If both your training and your validation is imbalance, you cannot use accuracy scores. What `class_weight` does is to build a cost matrix for you where for each class $k$, $C_k=2\frac{N_k}{N}$. You should either pass `sample_weight=[C_k for k in y]` to `accuracy_score` or use something like `f1_score`." CreationDate="2016-04-29T13:37:01.057" UserId="16853" />
  <row Id="11631" PostId="11503" Score="0" Text="Waaaay too broad a question. GLM. RandomForest. Neural Network. Time Series analysis. Kalman FIltering dynamic regression... This is more like an essay question than something that can be answered here." CreationDate="2016-04-29T14:22:30.400" UserId="471" />
  <row Id="11632" PostId="11493" Score="0" Text="Sounds great. Thank you for your advice. I use a learning rate of 0.01 with expo. decay 0.96 every 1k steps. I tried also 0.1 and 0.001 as starrting values but could not get better. This brings me to your second point: I use all values, no batches. I.e. a total of 400k values. If I get you right, you say to introduce batch sizes (e.g. 100) to avoid averaging." CreationDate="2016-04-29T14:25:13.387" UserId="18120" />
  <row Id="11633" PostId="11503" Score="0" Text="These are answers to the second question, I would like to know which functions you would use in each case!" CreationDate="2016-04-29T14:25:32.517" UserId="18295" />
  <row Id="11634" PostId="11493" Score="0" Text="You suggest 50x50x50. Some others say descending, some discuss increasing number of neurons. I am little puzzled because at the moment I feel like being in the monkey business and not following a high-tech procedure, i.e. fully deductive :-)" CreationDate="2016-04-29T14:27:03.353" UserId="18120" />
  <row Id="11635" PostId="11503" Score="0" Text="There's probably several packages for each of those approaches. Do some research by reading the R Task Views on such things. e.g.:  https://cran.r-project.org/web/views/TimeSeries.html" CreationDate="2016-04-29T14:32:29.247" UserId="471" />
  <row Id="11636" PostId="11503" Score="0" Text="I already did this and didn't find it useful as far as a number of the techniques you mentioned are concerned and in particular neural networks." CreationDate="2016-04-29T14:38:19.643" UserId="18295" />
  <row Id="11638" PostId="11488" Score="0" Text="default = 1 just for a matter of performance ? ok, that make sense... thanks" CreationDate="2016-04-29T15:23:19.690" UserId="18221" />
  <row Id="11639" PostId="11507" Score="0" Text="Thanks so much. That was awesome!" CreationDate="2016-04-29T15:46:27.080" UserId="18248" />
  <row Id="11640" PostId="11494" Score="0" Text="A generalization of your _consonants &amp; vowels_ strategy is [skip-grams](https://en.wikipedia.org/wiki/N-gram#Skip-gram)." CreationDate="2016-04-29T17:10:42.513" UserId="381" />
  <row Id="11642" PostId="11501" Score="0" Text="Thanks for the response :) I'll probably accept this answer in a day or two it's super helpful! A few comments/questions: In Looking up DTW I came across this [paper](http://wearables.cc.gatech.edu/paper_of_week/DTW_myths.pdf) which said DTW can't be used to compare sequences of different lengths? (see example 3 in the paper). I'm actually using python at the moment. I know I didn't ask but are you familiar with libraries which include the Frechet distance or PDC in python? I'm having trouble finding both (makes me think about jumping into r). Thanks!" CreationDate="2016-04-29T19:19:37.737" UserId="18232" />
  <row Id="11645" PostId="11516" Score="0" Text="You may want to add some more detail to your question: what features you are using, what you are predicting. Sometimes a minimal reproducible example is useful (see http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) and will help others give you more instructive feedback and useful answers." CreationDate="2016-04-30T00:31:58.663" UserId="6478" />
  <row Id="11647" PostId="11513" Score="0" Text="You are expected to identify the probabilty of the each customer (in the&#xA;&#xA;loyalty program) making a purchase in next 12 months?" CreationDate="2016-04-30T04:41:05.410" UserId="18139" />
  <row Id="11648" PostId="11519" Score="0" Text="welcome to DS.SE. Could you include a sample of your data please? That might give a broader range of answers." CreationDate="2016-04-30T09:08:34.810" UserId="16284" />
  <row Id="11649" PostId="11521" Score="0" Text="but the question said the data is **not** normal?" CreationDate="2016-04-30T09:11:04.607" UserId="16284" />
  <row Id="11650" PostId="11521" Score="1" Text="Yeah  I notticed that now , sorry ,  I have not  read the  didn't" CreationDate="2016-04-30T09:39:28.320" UserId="18321" />
  <row Id="11651" PostId="11503" Score="0" Text="&quot;Which approach should I use?&quot; There are entire **books** written on multivariate forecasting, and in them you may find the answer to your &quot;which approach?&quot; question. Without data, or context, all we can do (and I've done it in comments) is spew all the techniques we know and say &quot;now you go figure it out&quot;." CreationDate="2016-04-30T13:05:14.800" UserId="471" />
  <row Id="11652" PostId="11451" Score="0" Text="Thanks for your caught, you were right. I updated the code. However, the results are better but I don't have a clear explanations for that. Here is the updated graph. [![enter image description here](http://i.stack.imgur.com/nPT1U.png)](http://i.stack.imgur.com/nPT1U.png) Thank you again for your insights and feedback." CreationDate="2016-04-30T03:07:50.653" UserId="12867" />
  <row Id="11654" PostId="11520" Score="1" Text="Isnt it just a web API? You just use the usual GET requests? Does this help:  http://stackoverflow.com/questions/29267760/error-connecting-to-azure-blob-storage-api-from-r" CreationDate="2016-04-30T16:01:21.000" UserId="471" />
  <row Id="11655" PostId="11520" Score="0" Text="@Spacedman tx it seems to be the solution.. thanks.. Could you post an answer to be able to accept it?" CreationDate="2016-04-30T19:40:16.380" UserId="18320" />
  <row Id="11656" PostId="11519" Score="0" Text="The question is not clear. Can you edit your question and make it more clear" CreationDate="2016-04-30T21:09:49.910" UserId="17696" />
  <row Id="11657" PostId="11523" Score="0" Text="Thanks Daia, I am aware of h2o and indeed it seems very powerful" CreationDate="2016-05-01T08:41:39.990" UserId="18295" />
  <row Id="11658" PostId="10982" Score="0" Text="I think you have a typo. Either use `melt(d, id = &quot;breaks&quot;)` (quote `breaks`) or `melt(d, id = 1)`." CreationDate="2016-05-01T13:28:13.843" UserId="8479" />
  <row Id="11659" PostId="10982" Score="0" Text="If you are getting an error running this code, it could be a package conflict. Try restarting R." CreationDate="2016-05-01T13:19:44.547" UserId="3510" />
  <row Id="11660" PostId="11451" Score="0" Text="if you continue on the x axis &quot;Index&quot; for another 20 variables, do you start seeing the error increase?  The fact that the validation error flattens out like that when you presumably add too many variables to the model is as suspicious as the lower validation set MSE." CreationDate="2016-05-01T17:52:15.870" UserId="14913" />
  <row Id="11661" PostId="11537" Score="0" Text="What are the continuous base learners in Gradient boosting? If the answer is decision trees, could you please explain how they are continuous?" CreationDate="2016-05-02T04:33:10.503" UserId="8820" />
  <row Id="11662" PostId="11477" Score="0" Text="@OneChillDude have a look at the cumulative density function of the chi2 distribution - you can calculate the p-value of the chi2 statistic using 1-scipy.stats.chi2.cdf(x, df=2). For a chi2 statistic of x=5.991 this results in a p-value of 0.05. For x=50 it's already 1.39e-11. Your chi2 statistic is 48376.. which is really far away and thus results in a p-value of 0.0 due to loss of precision." CreationDate="2016-05-02T07:15:23.447" UserId="676" />
  <row Id="11663" PostId="11501" Score="0" Text="Glad it helped. Unfortunately I'm not familiar with python libraries but I found [this](https://pythonhosted.org/MDAnalysis/documentation_pages/analysis/psa.html) and [this](https://github.com/Becksteinlab/PSAnalysisTutorial) for Frechet. Couldn't find anything on PDC. I guess you could look into translating it to python ([here](http://scidok.sulb.uni-saarland.de/volltexte/2012/4545/pdf/Andreas_M_Brandmaier_Dissertation.pdf) is the author A.Brandmaier's thesis on this topic). Alternatively, you could also look into calling R from python libraries." CreationDate="2016-05-02T07:43:59.560" UserId="12341" />
  <row Id="11664" PostId="11447" Score="0" Text="Thanks Manohar..:)" CreationDate="2016-05-02T11:42:14.000" UserId="18207" />
  <row Id="11665" PostId="11537" Score="0" Text="I have updated my answer. The key is to use continuous tree-like predictors." CreationDate="2016-05-02T12:36:03.230" UserId="18077" />
  <row Id="11666" PostId="11499" Score="0" Text="Thanks HonzaB!!! I will now try to downsample the 'sale failed'. Is a 50-50 split OK? The biggest problem I have is even if I use another kind of prediction model, **I don't know how to implement data visualisation for clustering, rand. forests etc.** Any idea?" CreationDate="2016-05-02T12:37:36.647" UserId="18098" />
  <row Id="11667" PostId="11370" Score="0" Text="The only answer that helped me to start. Thanks!!" CreationDate="2016-05-02T12:55:23.940" UserId="18098" />
  <row Id="11668" PostId="11513" Score="0" Text="Your comment makes the question clearer. I've edited my answer accordingly (see the bottom)" CreationDate="2016-05-02T13:16:50.553" UserId="12515" />
  <row Id="11669" PostId="11499" Score="0" Text="Just try different settings for the undersampling. Take all samples from the undersampled class and add 2, 3, 4 times much of second class and see what works best. BUT! You must test on your original dataset and not on undersampled one. You cant really visualize black-box models with a lot of dimensions. Aim for confusion matrix or ROC curve." CreationDate="2016-05-02T13:43:41.923" UserId="17290" />
  <row Id="11670" PostId="11499" Score="0" Text="I'm sorry, but could you please explain what it means to test on the org. dataset. For eg. I have 9/10 'lost's and 1/10 'won's. Hence I randomly sample an equal 1/10 from 'lost's and add it to the 'won's and train on this set. I don't understand what it means to try different settings of undersampling. Sorry for asking an obvious question.(Unobvious for me :-(  )                          PS doing the latter,**I got a CV mean of 0.3, with std of 0.03**. Is that good? I switched the lost won thing too. Thanks" CreationDate="2016-05-02T13:53:20.860" UserId="18098" />
  <row Id="11671" PostId="11499" Score="0" Text="Dont worry, I have been there :). So train set: all your 'won' samples plus arbitraty number of 'lost' samples (this is what I mean by different settings of undersampling - how much 'lost' you will take to train set). And then test set - lets say 20% from original data with original proportion. I can't judge the metrics. Try a lot of models and parameters, different approaches. Maybe thats all you can get. Maybe you can get more." CreationDate="2016-05-02T16:01:33.817" UserId="17290" />
  <row Id="11673" PostId="1151" Score="0" Text="I think those two maybe help you: [Introduction to Data Science](https://www.coursera.org/course/datasci?from_restricted_preview=1&amp;course_id=346&amp;r=https%3A%2F%2Fclass.coursera.org%2Fdatasci-001) [Dataquest.io](https://dataquest.io/)" CreationDate="2015-02-26T12:10:17.283" UserId="8437" />
  <row Id="11674" PostId="11533" Score="0" Text="Could you please explain in a few words what feature construction is? Is it simply hand-crafting of features e.g. for classification tasks?" CreationDate="2016-05-02T20:47:33.587" UserId="8820" />
  <row Id="11675" PostId="11551" Score="0" Text="I will try your method and let you know.&#xA;The link that you have provided is really helpful!&#xA;&#xA;Alternatively, can I use multinomial logit regression?&#xA;It seems to support categorical as well as quantitative variables" CreationDate="2016-05-03T05:29:53.907" UserId="18369" />
  <row Id="11676" PostId="11533" Score="0" Text="Having an input dataset, the _feature construction_ is the process of creating new features by combining the original one. This is done in order to improve the quality of the model that will be build. An example of feature construction algorithm, is FRINGE (Pagallo &amp; Haussler, 1990), where from the original dataset, a decision tree is built.Then, splitting variables (the variables at the nodes of the tree) are combined in a specific way in order to created new ones. This showed a good improvment in the prediction quality of models." CreationDate="2016-05-03T06:17:10.850" UserId="18341" />
  <row Id="11677" PostId="11533" Score="0" Text="Does this have any advantages compared to neural networks (which build also features themselves)?" CreationDate="2016-05-03T06:29:37.277" UserId="8820" />
  <row Id="11678" PostId="11533" Score="1" Text="To be true I don't know. I think it could be advantageous to have a feature construction step when working with others models." CreationDate="2016-05-03T06:55:34.590" UserId="18341" />
  <row Id="11679" PostId="11548" Score="0" Text="Thanks for the answer Martin. I am doing this in a scientific setting - what did you mean by &quot;this might be more difficult&quot;?." CreationDate="2016-05-03T07:22:46.397" UserId="14411" />
  <row Id="11681" PostId="11548" Score="0" Text="@Nimrodshn The problem is that you want to compare what you do with other people. This is best if you work on a dataset on which other people are working, too. If there is no established dataset, you want to be at least consistent with youself and get models which you can compare with other models you had. (Assuming you do research in the direction &quot;How can I solve a classification problem in domain X&quot;). If you want a more substantial answer, you should probably post your research question / aim." CreationDate="2016-05-03T08:34:53.157" UserId="8820" />
  <row Id="11682" PostId="11533" Score="0" Text="It could also be advantegeous for neural networks, depending on what the feature construction algorithms do. Recent papers (e.g. &quot;Deep residual networks&quot;) suggest that neural networks seem in practice not to be as good at finding features as some people suggested. Thanks for pointing it out to me :-)" CreationDate="2016-05-03T08:37:31.123" UserId="8820" />
  <row Id="11683" PostId="11536" Score="0" Text="More people would probably look at the problem if you could reduce it to fewer lines of code. What seems strange are the really high training/testing errors you're getting - like 200k MSE for a baby's weight is really far off. Maybe there's something wrong with the gradient calculation - try to remove the calculation of the gradient and use a minimization function that doesn't require the gradient. And how much data have you got (samples + features)?" CreationDate="2016-05-03T08:48:59.623" UserId="676" />
  <row Id="11684" PostId="11501" Score="0" Text="Update : You might find this [link](https://www.dennogumi.org/2011/05/multiscale-bootstrap-clustering-with-python-and-r/) useful." CreationDate="2016-05-03T09:57:06.387" UserId="12341" />
  <row Id="11688" PostId="11536" Score="0" Text="I don't really know octave, but the docs suggest [fminunc](https://www.gnu.org/software/octave/doc/v4.0.1/Minimizers.html#XREFfminunc) - set GradObj to &quot;off&quot; and make your linearRegCostFunction *not* return grad. See if that helps..." CreationDate="2016-05-03T13:01:33.330" UserId="676" />
  <row Id="11689" PostId="11536" Score="0" Text="@stmax thank you for your reply. I have posted all my code because maybe anyone wants to reproduce the execution. What minimization function do you suggest? My dataset have 189 samples and 8 features originally (Without considering polynomial terms). Now I tried increase the number of iterations to 100000000 instead of 400... My script it's still running after 24 hs! but for degree 9 fmincg did 4115163 Iteration with a cost = 1.674962e+005. I don't understand what is going on." CreationDate="2016-05-03T13:06:47.370" UserId="18353" />
  <row Id="11690" PostId="11561" Score="0" Text="Thanks for your effort. I understood you answer. I have updated my question with the UPDATE section. Can you please read it again?" CreationDate="2016-05-03T13:18:59.293" UserId="13291" />
  <row Id="11691" PostId="11561" Score="0" Text="@Haroon I updated my answer, sorting your plots and highlight the true &quot;1&quot;s would represent the AUC score better." CreationDate="2016-05-03T13:42:33.817" UserId="14913" />
  <row Id="11692" PostId="11564" Score="0" Text="The easiest way (and first thing to try) is to set class_weight=&quot;balanced&quot;. See if that improves your score..." CreationDate="2016-05-03T14:04:26.787" UserId="676" />
  <row Id="11693" PostId="11071" Score="0" Text="Welcome to SE. Your answer isnt clear if you are answering the original question, or saying you have the same problem. Normally if you don't have a full answer, then putting ideas in as comments can be very helpful." CreationDate="2016-05-03T14:07:14.457" UserId="16284" />
  <row Id="11695" PostId="11564" Score="0" Text="Thanks, but I tried that and the O/P wasn't any better. Is 'auto' also an option for class_weight? I tried using a dict like {'Lost':0.5,'Won':1}, but that just threw an error" CreationDate="2016-05-03T14:12:52.443" UserId="18098" />
  <row Id="11697" PostId="11477" Score="0" Text="@stmax neat, thanks for the info. Does such a low p-value have any negative connotation for chi squared results, or is it just simply acceptable?" CreationDate="2016-05-03T14:50:08.157" UserId="15141" />
  <row Id="11698" PostId="11477" Score="0" Text="@OneChillDude I'd say it's acceptable... in your experiment 270k people did nothing in group A, while only 140k people did nothing in group B. That's such a huge difference that one can be almost 100% certain that it didn't happen by chance.. so a p-value of 0.0 makes total sense here." CreationDate="2016-05-03T15:03:49.227" UserId="676" />
  <row Id="11699" PostId="11561" Score="0" Text="Thanks. I want to understand it further. Does this mean that  ROC curve do not consider False positive Rate (FPR)? If I consider 0.75 as  threshold then `badaI` has highest number of false positives. Similarly, at other thresholds `badaI` have highest FPR as compared to other two approaches. How should I show this?" CreationDate="2016-05-03T15:38:40.593" UserId="13291" />
  <row Id="11700" PostId="11561" Score="1" Text="The ROC curve considers every threshold between the smallest value and the largest value.   If you have 31 probabilities, then you can think of 31 confusion matrices, one for each cutpoint.  Those confusion matrices each have their own TPR and FPR.  If you plotted TPR and FPR for each of those tables as you increased the threshold, you will get the ROC curve.  ROC's always start at (0,0) because at the extreme case, it classifies everything as negative (threshold is the max(p)), so there are no TP or FP.  Similar intuition for min(p) where the threshold calls everything positive, end at (1,1)" CreationDate="2016-05-03T16:31:47.720" UserId="14913" />
  <row Id="11701" PostId="11569" Score="0" Text="Interesting question. Have you tried weighting the loss by the size of the tree below it?" CreationDate="2016-05-03T17:25:51.143" UserId="381" />
  <row Id="11702" PostId="11551" Score="0" Text="You have alternatives there like  SparseM, a linear kernel svm from e1071 (equivalent to logistic regression) or MatrixModel (See ?MatrixModels:::lm.fit.sparse)" CreationDate="2016-05-03T17:29:56.567" UserId="13023" />
  <row Id="11705" PostId="11572" Score="0" Text="Yes, I see, it is just  a way to compact everything into a single expression." CreationDate="2016-05-03T19:37:19.293" UserId="13736" />
  <row Id="11706" PostId="11574" Score="0" Text="This is essentially a link-only answer, which is discouraged. Elaborate with some detail of how this addresses the problem." CreationDate="2016-05-03T20:02:38.710" UserId="21" />
  <row Id="11707" PostId="11576" Score="0" Text="I need dataset that contains only those groups having upd as 'down' and 'middle'.   and also i need a  function because I have to send each group into the function and check if its upd contains 'down' and 'middle' and then  should give an output as dataframe having only those groups.  Your code would also give groups having just 'down' or 'middle'." CreationDate="2016-05-03T20:07:14.737" UserId="13417" />
  <row Id="11709" PostId="11567" Score="0" Text="Yes, the variables acts like its name suggests it. Actually, I built a model only taking the advertised customers into account. However, the model did not work well on a hold out set. The &quot;score&quot; on the training data was roughly 1, whereas the score on the hold out data was merely 0.9 indicating strong overfitting. Adding the customers without advertisement put the test score on 0.99. The scikit learn random forest classifier has a built in method to account for class imbalances. Nevertheless, I'll conduct further tests tomorrow. Nevertheless, thanks for the reply" CreationDate="2016-05-03T20:13:43.113" UserId="8816" />
  <row Id="11710" PostId="11566" Score="0" Text="Actually, I did this approach, but the results weren't as good as i expected. The ultimate goal is to build a chain of models: the first model provides the purchase probability under the condition that the target obtained adverts or not. The second model predicts the net value a customer purchases for, if he does purchase (under the condition of adverts or not). &quot;Multiplication&quot; should yield an expected net profit of the customer under the condition of adverts or not. With this KPI the amount spent on (print) marketing campaigns could be optimized. Thanks for the reply" CreationDate="2016-05-03T20:19:16.127" UserId="8816" />
  <row Id="11711" PostId="11553" Score="0" Text="The answer to this question depends on what model you are using for your sentiment analysis algorithm. So what model are you using? Naive Bayes, LogReg, Recurrent Net?" CreationDate="2016-05-03T20:20:44.763" UserId="14779" />
  <row Id="11712" PostId="11347" Score="0" Text="Problem solved? Please tick an answer if so." CreationDate="2016-05-03T23:29:16.637" UserId="381" />
  <row Id="11713" PostId="11580" Score="1" Text="look at package {tidyr} function spread. https://rpubs.com/bradleyboehmke/data_wrangling" CreationDate="2016-05-04T02:52:16.123" UserId="6478" />
  <row Id="11714" PostId="11440" Score="1" Text="Have you consider [Pandas](http://pandas.pydata.org/) library? please show the sample real data so that I can process it." CreationDate="2016-05-04T03:51:57.590" UserId="12866" />
  <row Id="11715" PostId="11559" Score="0" Text="Thanks alot for the answer. I am reading about SAX, but isn't this time series? Is it possible for me to apply time series on this dataset, because it has just 12 values." CreationDate="2016-05-04T04:55:21.647" UserId="6514" />
  <row Id="11716" PostId="11440" Score="0" Text="@AlexanderYau Pls check the description with real data part. I did try Pandas but my data does not have headers and putting the column values and reading them in array is a bit troublesome. But my main question here how can use two values i.e. x, y both together and show how they are distributed as pair? and how can show relationship between them ?" CreationDate="2016-05-04T05:13:06.597" UserId="3486" />
  <row Id="11717" PostId="11553" Score="0" Text="I am using Naive Bayes" CreationDate="2016-05-04T05:30:15.083" UserId="15905" />
  <row Id="11718" PostId="11574" Score="0" Text="This does not provide an answer to the question. Once you have sufficient [reputation](http://datascience.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://datascience.stackexchange.com/help/privileges/comment); instead, [provide answers that don't require clarification from the asker](http://meta.stackexchange.com/questions/214173/why-do-i-need-50-reputation-to-comment-what-can-i-do-instead). - [From Review](/review/low-quality-posts/7143)" CreationDate="2016-05-04T05:45:27.833" UserId="11097" />
  <row Id="11719" PostId="11576" Score="0" Text="This does not provide an answer to the question. Once you have sufficient [reputation](http://datascience.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://datascience.stackexchange.com/help/privileges/comment); instead, [provide answers that don't require clarification from the asker](http://meta.stackexchange.com/questions/214173/why-do-i-need-50-reputation-to-comment-what-can-i-do-instead). - [From Review](/review/low-quality-posts/7146)" CreationDate="2016-05-04T05:45:35.773" UserId="11097" />
  <row Id="11720" PostId="11581" Score="0" Text="What else do you know; who played which song, perhaps?" CreationDate="2016-05-04T06:02:00.040" UserId="381" />
  <row Id="11721" PostId="11581" Score="0" Text="@Emre, yes, and I got the data who played the song. I have update the question." CreationDate="2016-05-04T07:33:54.377" UserId="12866" />
  <row Id="11722" PostId="11569" Score="0" Text="No, I haven't tried that. Could you please tell me more about how to do it?" CreationDate="2016-05-04T07:57:55.987" UserId="18409" />
  <row Id="11723" PostId="11573" Score="0" Text="HonzaB you are a legend!!! Thanks for your help, it worked. Now to grid search some possible combinations..." CreationDate="2016-05-04T08:01:50.990" UserId="18098" />
  <row Id="11724" PostId="11561" Score="0" Text="@HaroonRashid: When using AUROC, you will not get insight by choosing a single threshold for comparison. You have to ignore the value output entirely, and consider the predicted ranking that it produces in your test set. For intuition, consider finding the optimal threshold for each model separately, and see what the TPR/FPR is for that. In this case, `badaI` is strictly better than the other models, when you want to minimise FPR for any given TPR." CreationDate="2016-05-04T08:55:23.020" UserId="836" />
  <row Id="11725" PostId="11574" Score="1" Text="As far as I understood, Amir asked for an implementation of 3-order FMs. For some reason, it was not implemented anywhere, that's why we implemented it and I provided the link in case he still needs it. What would make the answer better? I can for example tell about the implementation details, but I'm not sure if that is of interest." CreationDate="2016-05-04T09:43:37.757" UserId="18414" />
  <row Id="11726" PostId="11559" Score="0" Text="Anything that is a homogenous variable and has a distinct time dimension, like your monthly bandwidth upload, download, and billed amount, is a time series (three series, likely correlated, in your case).&#xA;Since you only have 12 values in each series, I don't think a length-wise reduction is necessary, so I guess I would skip the windowing step and just do MDL or equal-frequency discretization. You can try both kinds in [Discretize widget](http://docs.orange.biolab.si/3/visual-programming/widgets/data/discretize.html) in [Orange Data Mining](http://orange.biolab.si/) suite." CreationDate="2016-05-04T11:51:12.447" UserId="15527" />
  <row Id="11727" PostId="11559" Score="0" Text="I updated the answer a bit." CreationDate="2016-05-04T12:08:48.280" UserId="15527" />
  <row Id="11728" PostId="11540" Score="0" Text="What's the data set size in number of instances? There might also not be any interesting sequence patterns to discover." CreationDate="2016-05-04T12:09:28.707" UserId="15527" />
  <row Id="11732" PostId="8458" Score="0" Text="your statements regarding that sklearn can accept categorical input variables appears to be incorrect" CreationDate="2016-05-04T15:54:49.340" UserId="3554" />
  <row Id="11733" PostId="11590" Score="0" Text="Thank you, then it is not possible to do this using a logistic sigmoid, since it restricts the value to (0,1)" CreationDate="2016-05-04T15:59:22.207" UserId="13736" />
  <row Id="11734" PostId="11581" Score="0" Text="So your data is one row for every time a song is played, with the exact time (to the second?), the song identifier, and the user identifier? How many different songs, how many different users, how many different songs per user, etc etc etc. Do a bit more basic exploration and tell us the summaries before asking." CreationDate="2016-05-04T16:11:26.047" UserId="471" />
  <row Id="11735" PostId="11378" Score="0" Text="Actually, the fraction of known cells in the user-item matrix to all of it will be density. Sparsity is the opposite (100% - density) :-). What is curious in Your case, that You should expect that MF method should perform better when sparsity is raising. For threshold - see de article in the answer. I think the good starting point for everyone starting with CF algos." CreationDate="2016-05-04T16:26:09.840" UserId="9314" />
  <row Id="11736" PostId="11590" Score="0" Text="No it should still be possible to learn this with a logistic sigmoid, it should just learn the thresholds/weights differently" CreationDate="2016-05-04T16:34:33.310" UserId="14904" />
  <row Id="11737" PostId="11589" Score="1" Text="You can predict XOR using that structure. In fact you don't even need biases (see [here](http://stats.stackexchange.com/questions/12197/can-a-2-2-1-feedforward-neural-network-with-sigmoid-activation-functions-represe))." CreationDate="2016-05-04T16:48:12.913" UserId="18282" />
  <row Id="11738" PostId="11576" Score="0" Text="How can I migrate this question to Stack Overflow?" CreationDate="2016-05-04T17:18:55.813" UserId="13417" />
  <row Id="11739" PostId="11589" Score="0" Text="Do I need to initialise my weights in any special way to get convergence? I a trying a simple neural net with weights between (-1,1) initialised randomly, but I cannot get it to converge (even using biases)" CreationDate="2016-05-04T17:27:30.367" UserId="13736" />
  <row Id="11740" PostId="11347" Score="0" Text="Thanks everyone, yes this problem was solved." CreationDate="2016-05-04T17:45:50.927" UserId="18058" />
  <row Id="11741" PostId="11415" Score="0" Text="Worked great, thanks!" CreationDate="2016-05-04T17:47:32.147" UserId="18058" />
  <row Id="11742" PostId="11586" Score="0" Text="I've not heard of randomizing the order each epoch before. Do you know of any comparisons between a static but random order and randomizing each epoch?" CreationDate="2016-05-04T18:12:34.680" UserId="18000" />
  <row Id="11743" PostId="11574" Score="0" Text="What is missing in the answer is whether the implementation is &quot;good&quot; - it is currently unclear (i.e. no examples, no unit tests etc.). It would be interesting to get some more details / experimental results." CreationDate="2016-05-04T18:14:14.857" UserId="3592" />
  <row Id="11745" PostId="11589" Score="0" Text="Actually, using the logistic sigmoid it does converge sometimes, but not all the times, it depends on the initial choice of random weights." CreationDate="2016-05-04T21:18:33.933" UserId="13736" />
  <row Id="11746" PostId="11596" Score="0" Text="Yes, that is what I am observing, with some seed values it does converge, others, it won't. Also, if I use hyperbolic tangent instead of sigmoid it works rather well all the times, with sigmoid it does depend on the seed, as you observed. What is the reason for it to be so tricky?" CreationDate="2016-05-04T21:21:08.240" UserId="13736" />
  <row Id="11747" PostId="11540" Score="0" Text="Looks more like 37 columns?" CreationDate="2016-05-04T21:23:20.027" UserId="15527" />
  <row Id="11748" PostId="11596" Score="0" Text="I'm not entirely sure what the mathematical reason is, this is just from my experience writing test suite around learning xor. In my case, adding momentum helped, but I think pretty much any adjustment away from the simplest network and/or optimiser helps." CreationDate="2016-05-04T21:32:17.983" UserId="836" />
  <row Id="11749" PostId="11597" Score="0" Text="look at function `glm` for logistic regression" CreationDate="2016-05-04T22:05:13.823" UserId="6478" />
  <row Id="11750" PostId="11589" Score="0" Text="Your range seems quite large, try (-0.1, 0.1). Otherwise you risk that input signal to a neuron might be large from the start in which case learning for that neuron is slow. You might also want to decrease learning rate and increase number of iterations." CreationDate="2016-05-04T22:43:52.297" UserId="18282" />
  <row Id="11751" PostId="11597" Score="0" Text="I assume you suggest using `glm(School~Time, data = data, family = binomial())`. I can't seem to understand how will it give me the probabilities, edited the question to add more details." CreationDate="2016-05-04T23:23:07.893" UserId="18248" />
  <row Id="11752" PostId="11597" Score="0" Text="I think binomial should be a string not a function - &quot;binomial&quot;" CreationDate="2016-05-04T23:26:41.603" UserId="6478" />
  <row Id="11753" PostId="11597" Score="0" Text="Gives the same results. Can you please explain how you see using `glm` results as probabilities?" CreationDate="2016-05-04T23:27:52.780" UserId="18248" />
  <row Id="11754" PostId="11597" Score="0" Text="I think you want multinomial logistic regression -- see the examples here: http://www.ats.ucla.edu/stat/r/dae/mlogit.htm" CreationDate="2016-05-05T03:29:30.720" UserId="6478" />
  <row Id="11755" PostId="11540" Score="0" Text="Yes, 3*12 columns + customer_id column=37 columns and 1 lakh rows." CreationDate="2016-05-05T04:01:24.420" UserId="6514" />
  <row Id="11756" PostId="11589" Score="0" Text="On the contrary, larger values make it converge faster. I have tried smaller learning rate and many iterations. I think Neil Slater's answer below sums up the issues, though I am still not sure why." CreationDate="2016-05-05T05:22:23.023" UserId="13736" />
  <row Id="11757" PostId="11598" Score="1" Text="Welcome to DataScience.SE! Is this a homework question?" CreationDate="2016-05-05T06:07:25.950" UserId="381" />
  <row Id="11758" PostId="11586" Score="0" Text="@tsorn: Randomising on each epoch is common practice and some libraries will do that by default. Randomising order just once at the start is also common. I did a quick search but did not find any formal comparisons of the approaches." CreationDate="2016-05-05T09:53:15.387" UserId="836" />
  <row Id="11759" PostId="11586" Score="0" Text="I haven't seen any formal comparisons unfortunately. It'd be an interesting subject for an experiment." CreationDate="2016-05-05T10:14:25.650" UserId="18282" />
  <row Id="11760" PostId="11581" Score="0" Text="@Spacedman, I have updated the question and added detail information of the data." CreationDate="2016-05-05T11:12:00.483" UserId="12866" />
  <row Id="11761" PostId="11574" Score="0" Text="OK, sparse data support and examples/experiments are on the way, stay tuned!" CreationDate="2016-05-05T11:38:23.013" UserId="18414" />
  <row Id="11762" PostId="11593" Score="0" Text="thank you, I have updated the question and provided more information of the data." CreationDate="2016-05-05T12:29:11.960" UserId="12866" />
  <row Id="11763" PostId="11451" Score="0" Text="@ TBSRounder, I do hear you and thanks for the comment. However, the contribution of some of the variables are so low but the coefficients' sign that found were making sense and they were significant also. Do you think I need to remove those; however, they were meaningful and significant? Thank you." CreationDate="2016-05-05T17:35:34.607" UserId="12867" />
  <row Id="11766" PostId="11609" Score="0" Text="this is a programming or coding related question, and suits Stackoverflow than Data Science. I think this should be moved to Stackoverflow!" CreationDate="2016-05-06T02:48:31.040" UserId="8465" />
  <row Id="11767" PostId="11603" Score="1" Text="+1 BH85, is it also possible to give some links/reference for ensemble techniques that you have listed? either Python or R implementation is fine." CreationDate="2016-05-06T02:59:49.053" UserId="8465" />
  <row Id="11768" PostId="11612" Score="0" Text="Awesome.  Thanks very much!" CreationDate="2016-05-06T03:43:50.320" UserId="16244" />
  <row Id="11770" PostId="11576" Score="0" Text="This response answers the question.  You may have to add a column specifier for results.  e.g. subset(a,upd==&quot;down&quot;, c(&quot;group&quot;,&quot;marks&quot;,&quot;upd&quot;)).  The subset function is intended for interactive use.  For a script, better to use the format:  new_dataframe &lt;- a[a$upd  %in% (&quot;middle&quot;,&quot;down&quot;),c(&quot;group&quot;,&quot;marks&quot;,&quot;upd&quot;)]" CreationDate="2016-05-06T11:06:51.613" UserId="434" />
  <row Id="11771" PostId="11615" Score="0" Text="Yes giving the ML strings without any significations will not work for  sure, but I think that I have to devide the request itself and the URL into single parts then I think that the ML will be able to work with the data." CreationDate="2016-05-06T11:43:01.363" UserId="18458" />
  <row Id="11772" PostId="11615" Score="0" Text="Not unless you turn it into categorical values, which you would need a LOT of training examples for to learn anything significant. Representing it in other ways that are much easier to learn from will work significantly better. There has been a lot of work done on ML with log files of webserver, take a look at those examples" CreationDate="2016-05-06T11:45:12.273" UserId="14904" />
  <row Id="11773" PostId="11451" Score="0" Text="No Problem! It depends on your goals.  If you're looking for best prediction with some storytelling, I would leave them in as long as your validation error gets better.  If you're planning on inference, I would take a close look at the variables and decide what to leave in/out based on the context and multiple testing constraints.  Are these variables in your pool for subset selection already trimmed down from a bunch of variables?  Are you saying when you have 14 variables in your model that all of them are significant?" CreationDate="2016-05-06T12:23:10.797" UserId="14913" />
  <row Id="11774" PostId="11451" Score="0" Text="Thank you TBSRounder. In fact there are 13 variables which 2 of them are categorical ones at 3 levels (in total 11+2+2=15 variables that is shown in the Figure). However, the significant level is different for various variables but all variables are significant at the level of 10%. Thanks again for your insights and feedback." CreationDate="2016-05-06T16:32:28.777" UserId="12867" />
  <row Id="11775" PostId="11451" Score="0" Text="If you're looking to find what variables are significant at the 0.05 level, you  need a p-value to be &lt; 0.003 (alpha=0.05/p=15, Bonferroni correction).  Even with that, stepwise testing is generally disliked because of uncontrollable multiple testing comparisons.  However, for predictive modeling, criteria like AIC is common.  AIC balances model fit with model complexity.  At some point, (prob ~5 or 12 variables in your updated plot), the added value of adding another variable does not warrant the complexity.  So do  subsets and pick the one with the lowest AIC is my recommendation :)" CreationDate="2016-05-06T17:11:55.003" UserId="14913" />
  <row Id="11776" PostId="11451" Score="0" Text="Also, check out LASSO (library(glmnet)) for a variable selection technique that is preferable over step wise." CreationDate="2016-05-06T17:17:14.630" UserId="14913" />
  <row Id="11777" PostId="11621" Score="2" Text="You can get good intuition for differences of RNN model from http://karpathy.github.io/assets/rnn/diags.jpeg - a much copied graphic. CNNs are along with MLPs and other non-recursive models as implementing the one-to-one model case only." CreationDate="2016-05-06T17:44:35.520" UserId="836" />
  <row Id="11778" PostId="10720" Score="0" Text="what libraries you need to use ?" CreationDate="2016-05-06T19:20:28.463" UserId="8752" />
  <row Id="11779" PostId="11622" Score="0" Text="Do you have a specific problem you are trying to solve? What specifically are you finding hard to understand about OOP? Just asking for a generic code example - without even a problem to solve - is not really what this site is about." CreationDate="2016-05-06T20:35:19.710" UserId="836" />
  <row Id="11780" PostId="11623" Score="0" Text="yes, but you still can do it with only a function. I don't understand what is the gain over a simple function." CreationDate="2016-05-06T20:44:14.490" UserId="14691" />
  <row Id="11781" PostId="11623" Score="0" Text="Well, first of all, you need to know that OOP is basically a &quot;practical&quot; way of programming. That means, OOP treats everything as an object, just like the real world around you. Just imagine that you need to create a strategy game; in that case, you can't just write long, repetitive, procedural scripts to make your way out of it, but you'll need to define specific classes.&#xA;Apart from that, OOP offers Data Abstraction, Encapsulation, Inheritance, Polymorphism, and you might need to look for these terms to understand the true power of OOP." CreationDate="2016-05-06T20:52:59.750" UserId="18150" />
  <row Id="11782" PostId="11623" Score="0" Text="yes, I know about inheritance, etc. But I'm asking what is the use for data science. Because until now I solved everything with functions." CreationDate="2016-05-06T21:08:49.740" UserId="14691" />
  <row Id="11783" PostId="11623" Score="0" Text="In your specific case, you won't necessarily need user-defined object, it's true that you can solve almost everything with some good written scripts.&#xA;But when i comes to python's modules, libraries, and all that, you'll need to work with object, member functions, and all that. Otherwise, you're good, even without creating object.&#xA;But organizing your code in an OOP format makes it more readable, understandable, and usable for generic purposes. That's another slight advantage." CreationDate="2016-05-06T21:21:49.273" UserId="18150" />
  <row Id="11784" PostId="11615" Score="0" Text="Hi @Jan , Can you please give me reference to some of the previous work  on web server log files, because that really helps me to follow the technique they use." CreationDate="2016-05-06T23:00:10.060" UserId="18458" />
  <row Id="11785" PostId="11598" Score="0" Text="Thanks!. Yes. I am taking an online class but the support is very minimal. Its ok if you can give a few tips that will help me solve this question" CreationDate="2016-05-07T02:15:43.663" UserId="18452" />
  <row Id="11786" PostId="11520" Score="0" Text="It is a solution by generating the Authorization header is really heavy.. If someone eared about a R package to do it for it should be great.." CreationDate="2016-05-07T06:34:30.740" UserId="18320" />
  <row Id="11787" PostId="11629" Score="0" Text="could dummy factors help out? http://stats.stackexchange.com/questions/52915/how-to-deal-with-an-svm-with-categorical-attributes" CreationDate="2016-05-07T15:49:34.283" UserId="6478" />
  <row Id="11788" PostId="11603" Score="1" Text="most of these techniques are published in IEEE or science direct, so you need an account to access them. but still you can find some papers: http://www.cs.put.poznan.pl/jstefanowski/pub/CORES2013.pdf https://www3.nd.edu/~nchawla/papers/ECML03.pdf" CreationDate="2016-05-08T07:10:18.197" UserId="17696" />
  <row Id="11789" PostId="11635" Score="0" Text="Interesting point. Do you have any experience with MinHash algorithm? How could it be compared in terms of reliability to tf-idf using multi-words features, as you suggest?" CreationDate="2016-05-08T10:18:41.467" UserId="18407" />
  <row Id="11790" PostId="11639" Score="0" Text="thanks for your answer" CreationDate="2016-05-08T10:26:37.093" UserId="18516" />
  <row Id="11791" PostId="11633" Score="1" Text="Some algorithms that _can_ work with categoricals, are decision tree (by extension, random forest), naive bayes (only for classification), knn." CreationDate="2016-05-08T11:15:07.603" UserId="15527" />
  <row Id="11792" PostId="11641" Score="0" Text="Thanks for your answer." CreationDate="2016-05-08T11:15:43.477" UserId="18516" />
  <row Id="11793" PostId="11641" Score="0" Text="@ZAHSAN If you like my answer, you should upvote it ([how to upvote](http://meta.stackexchange.com/a/173400/158075))" CreationDate="2016-05-08T11:32:16.037" UserId="8820" />
  <row Id="11794" PostId="11603" Score="0" Text="@ BH85, thanks for the link!" CreationDate="2016-05-08T14:36:49.273" UserId="8465" />
  <row Id="11795" PostId="11637" Score="0" Text="Thanks. Why would the number of cores matter if we have already looked at the Tflops? Good point regarding the ability to do low precision arithmetic, e.g. Theano 0.8 supports float 16." CreationDate="2016-05-08T18:22:00.157" UserId="843" />
  <row Id="11796" PostId="11638" Score="0" Text="Yes, definitely if one doesn't have enough RAM, the GPU won't be very useful." CreationDate="2016-05-08T18:26:31.593" UserId="843" />
  <row Id="11798" PostId="11645" Score="0" Text="As I said, I don't want to actually classify, I just want those features that define a &quot;likes&quot;, &quot;dislikes&quot; for user more strongly than other features.&#xA;&#xA;Haven't looked into Decision Trees though...I'll have a look." CreationDate="2016-05-08T19:55:42.773" UserId="18529" />
  <row Id="11799" PostId="11645" Score="0" Text="So you want **feature selection**? Decision trees do also select features; and you can also analyze the features by their naive bayes scores..." CreationDate="2016-05-08T19:57:04.383" UserId="924" />
  <row Id="11800" PostId="11645" Score="0" Text="Yes, basically..I will look more into naive bayes scoring..." CreationDate="2016-05-08T20:08:25.940" UserId="18529" />
  <row Id="11801" PostId="11642" Score="0" Text="You can look up for how to select features based on information gain, calculating the entropies. That's exactly what you need. Decision tree classifiers are also based on these calculations." CreationDate="2016-05-08T22:04:07.930" UserId="18150" />
  <row Id="11804" PostId="9990" Score="0" Text="It's been a while since I did my B.Eng (elec), but I have a recollection that discharge curves were different for different types of batteries Lead-Acid / NiCd and now NiMH. What type of battery are you using, or have I missed that? I'm pretty sure that the discharge curves of the last two are quite different to Lead-Acid and solid chemical battery." CreationDate="2016-05-09T07:53:04.653" UserId="16284" />
  <row Id="11805" PostId="11652" Score="0" Text="Hi Welcome to DS.SE. Could you let us know which country you are in, that will help in knowing what companies you are interested in and a better answer" CreationDate="2016-05-09T08:09:29.763" UserId="16284" />
  <row Id="11806" PostId="11622" Score="1" Text="you can have a look at the most popular Bioconductor Packages for Bioinformatics. One of them is called [SummarizedExperiment](http://bioconductor.org/packages/release/bioc/html/SummarizedExperiment.html). Programmers  work with objects which are called, well, SummarizedExperiment, and these are actually mini-databases comprising 3 tables patientmetadata  - treatmentconditionsmetadata - runresult (This is extremely simplified) . Here working with objects and methods instead of matrices linked by &quot;raw&quot; functions has some benefits (although working with R's S4 object system has pros+cons)" CreationDate="2016-05-09T08:16:32.283" UserId="14588" />
  <row Id="11807" PostId="9990" Score="0" Text="@MarcusD according to the manufacturer, the battery is an ithium ion;&#xA;Battery weight: 260kg;&#xA;Capacity (kWh): 22kwh;&#xA;Number of elements: 192 cells;&#xA;Nominal voltage: 240 to 400;&#xA;From 0 to 100%: 8h;&#xA;From 0 to 50%: 3h30;&#xA;From 50 to 100%: 4h30;&#xA;From 20 to 80%: 4h30;&#xA;Maximum Current drawn from 220V main: 3KW;&#xA;Slow Charge (A): 16A" CreationDate="2016-05-09T08:49:24.333" UserId="15690" />
  <row Id="11808" PostId="9990" Score="0" Text="@MarcusD The discharging curve seems pretty linear to me (in the area of   40% to 90%). However, I do expect some non-linear progression beyond that, but I do not know how to take them into account since I have only very few data beyond 40% or above 90%. The discharging from 90% to 40% will take several hours of driving an electrical car." CreationDate="2016-05-09T08:54:36.907" UserId="15690" />
  <row Id="11809" PostId="11461" Score="0" Text="How exactly is matching relevant here?" CreationDate="2016-05-09T11:11:05.967" UserId="6550" />
  <row Id="11810" PostId="11652" Score="0" Text="I think you would have better luck posting this question on opendata - http://opendata.stackexchange.com/" CreationDate="2016-05-09T12:19:42.360" UserId="6478" />
  <row Id="11811" PostId="11645" Score="0" Text="Try  this link https://cran.r-project.org/web/packages/Boruta/Boruta.pdf" CreationDate="2016-05-09T12:27:59.957" UserId="18139" />
  <row Id="11812" PostId="11655" Score="0" Text="Where did you learn abt the algorithm? The paper?" CreationDate="2016-05-09T14:36:11.060" UserId="11097" />
  <row Id="11813" PostId="11655" Score="0" Text="No. Someone else suggested it for my problem. I tried SMOTE but it didn't seem to work. I'll be really grateful if someone could help. As I said it's generating 9 matrices of my attributes and 9 corresponding arrays of output. Thanks" CreationDate="2016-05-09T15:50:27.763" UserId="18098" />
  <row Id="11814" PostId="11661" Score="0" Text="You could give a link to &quot;@RISK&quot; and outline what it does, what you've looked for already, which packages you have already excluded from doing the thing you want to do, or outline the thing you want to do." CreationDate="2016-05-09T17:23:45.047" UserId="471" />
  <row Id="11815" PostId="11661" Score="0" Text="I've updated my question, thank you for your post. Also, I'm open to any software, I've yet to exclude anything. I would just like to be able to perform these tasks more cost effectively." CreationDate="2016-05-09T17:45:53.457" UserId="18557" />
  <row Id="11816" PostId="11663" Score="0" Text="How often do they occur?  Are they caused by random missing data or by some specific procedure?" CreationDate="2016-05-09T18:34:23.163" UserId="14913" />
  <row Id="11817" PostId="11663" Score="0" Text="Mostly by random missing data, and they occur quite often." CreationDate="2016-05-09T18:39:46.537" UserId="17497" />
  <row Id="11818" PostId="11663" Score="0" Text="However, some are dependent on each other. Say one of the fields in the original data set is empty, then all features that depend on that field will be null." CreationDate="2016-05-09T18:47:08.917" UserId="17497" />
  <row Id="11819" PostId="11664" Score="0" Text="Thank you, I think this is what I was looking for. It will absolutely get me in the right direction." CreationDate="2016-05-09T19:24:48.230" UserId="18557" />
  <row Id="11820" PostId="11663" Score="0" Text="Can't you just replace the nulls? Or take rows without them?" CreationDate="2016-05-09T19:57:26.483" UserId="17290" />
  <row Id="11821" PostId="11672" Score="0" Text="How can you &quot;predict which product to sell to the customer&quot;? You can only *try* and sell it. Do you mean that? Do you want to figure out which product to promote in the hope that the customer will buy it? Or have they already decided to buy something? Or what?" CreationDate="2016-05-10T07:20:52.100" UserId="471" />
  <row Id="11822" PostId="11672" Score="0" Text="Yes, recommendation of a product. The product that is most suited to the customer based on 200 predictors which would maximize my revenue.&#xA;I have historical data containing their past purchases based on which I should recommend them new products." CreationDate="2016-05-10T07:25:15.833" UserId="18369" />
  <row Id="11824" PostId="11670" Score="0" Text="&quot;Then let's look at subspace clustering. And you will quickly see that FIM is like a binary version of this continuous problem.&quot; - I don't see that. Please elaborate on it." CreationDate="2016-05-10T08:23:21.950" UserId="8820" />
  <row Id="11825" PostId="9837" Score="0" Text="This was true before 2006 - https://www.thestar.com/news/world/2015/04/17/how-a-toronto-professors-research-revolutionized-artificial-intelligence.html. As I understand, things are done today differently, but the performance is becoming better and better ever since." CreationDate="2016-05-10T09:37:11.643" UserId="16874" />
  <row Id="11826" PostId="11676" Score="0" Text="I can break down the whole problem into two parts, one is product prediction and the other is revenue maximization. I can use logistic regression to predict the probability of recommending a given product but for that wouldn't it require the product variable to be dummy coded? (since products are nominal in nature)" CreationDate="2016-05-10T11:55:28.433" UserId="18369" />
  <row Id="11827" PostId="11676" Score="0" Text="Yes you would need to dummy code them, and then split them up per product column, where some of the entries will be 1 and most 0, then train your regressor on this column as target" CreationDate="2016-05-10T12:30:24.287" UserId="14904" />
  <row Id="11828" PostId="11676" Score="0" Text="Dummy coding will result in creation of 2000+ new columns over my existing ones. How do I go about it in R or Python? I have a RAM limitation of 8GB" CreationDate="2016-05-10T12:34:24.473" UserId="18369" />
  <row Id="11829" PostId="11676" Score="0" Text="You don't need to create them at the same time, just iterate over your set for product 1, make a 1/0 column, train your model and export it. This will be very computationally expensive but that will be the case regardless with this amount of data" CreationDate="2016-05-10T12:53:03.367" UserId="14904" />
  <row Id="11830" PostId="11676" Score="0" Text="You could use sklearn's stochastic gradient descent classifier (sklearn.linear_model.SGDClassifier) with the log loss function. That way you don't have to load everything into memory, instead just feed batches to it. Or have a look at vowpal wabbit, it can train logistic regression models from terabytes of data.. on a single machine." CreationDate="2016-05-10T13:31:49.313" UserId="676" />
  <row Id="11831" PostId="11604" Score="0" Text="You can start by looking for NA's in your data, via. &quot;is.na(add.APV$scope)&quot; etc." CreationDate="2016-05-10T19:57:42.097" UserId="12241" />
  <row Id="11833" PostId="11685" Score="0" Text="How many categories do you have? And number of rows and features?" CreationDate="2016-05-11T08:25:44.943" UserId="14904" />
  <row Id="11834" PostId="11675" Score="0" Text="I get dividing the training data into k folds and repeating the training of the neural network k times.  I've done this, okay, it produces k values for loss, accuracy etc for each iteration/epoch.   What I don't get is how do you know which epoch is the &quot;best&quot; one?  In real life, the results are such that in many cases we can't tell whether one epoch has a higher or lower loss/accuracy using a suitable statistical test such as Wilcoxon signed-rank or Mann Whitney U." CreationDate="2016-05-11T08:52:06.920" UserId="26" />
  <row Id="11835" PostId="11575" Score="0" Text="library(dplyr); new.a &lt;- a %&gt;% filter(upd %in% c('down','middle'))" CreationDate="2016-05-11T12:31:13.410" UserId="8113" />
  <row Id="11836" PostId="11675" Score="0" Text="I agree with you ! It is not significant to make decisions based on one test. But, always there is no guarantee that the model you generate is the best for real life." CreationDate="2016-05-11T14:31:41.703" UserId="17696" />
  <row Id="11837" PostId="11105" Score="0" Text="Now that you are a student, get a computer science or statistics course or audit one. After getting a sense of any of those, you can decide. You can try MOOCs as well. you can take a course for free and start learning as soon as possible. You can decide after going into data science." CreationDate="2016-05-11T15:07:05.320" UserId="3151" />
  <row Id="11838" PostId="11604" Score="0" Text="Thanks Alex, as far as I know there isn't any missing value; however, I checked your advice also and got &quot;logical(0)&quot;." CreationDate="2016-05-11T21:11:21.943" UserId="12867" />
  <row Id="11840" PostId="11689" Score="0" Text="The model I am working on is the Pacejka Tyre Mode and some of the parameters have a physical significance as the parameters can be used as indicators of tyre force and moment characteristics and used in the tyre design and development process. &#xA;&#xA;The NNs fitted my data more precisely then the model. Thus I thought I can somehow force my model in the NNs and find the parameters. &#xA;&#xA;I have a few different tyres and on the one I did a few test (e.g. different loads) and thus have 5 different data sets. I can do more tests and gather more data, would it be then possible to use the NNs?" CreationDate="2016-05-12T07:51:51.867" UserId="18609" />
  <row Id="11841" PostId="11689" Score="0" Text="@Lior: I'm not really sure, but I don't think so. Would the 5 different data sets correspond to 5 different solutions for your tyre model parameters, or is the model already designed to cover the variation between the different loads/tyres, and together point to a better single solution? How are you measuring fit for the NNs? You need to take active measures to prevent over-fitting with the NNs (regularisation, using cross-validation) that you probably don't need to take with the semi-empirical model." CreationDate="2016-05-12T08:33:34.677" UserId="836" />
  <row Id="11843" PostId="11700" Score="0" Text="Well, BoW size is 5. Which number do you consider an appropriate window size for averaging to work?" CreationDate="2016-05-12T09:16:53.860" UserId="18619" />
  <row Id="11844" PostId="11700" Score="0" Text="window size of 5 means, in total it considers 10 words and a common english sentence could be written in 10 words. So that sounds fine for me." CreationDate="2016-05-12T09:27:08.440" UserId="16024" />
  <row Id="11845" PostId="11689" Score="0" Text="The model covers the variation between the different loads. I am using MSE  as a measure of how much my predictions are far away from the real data." CreationDate="2016-05-12T11:24:19.140" UserId="18609" />
  <row Id="11846" PostId="11123" Score="0" Text="Thanks a lot for your interesting answer." CreationDate="2016-05-12T18:48:46.403" UserId="17752" />
  <row Id="11847" PostId="11709" Score="0" Text="Thanks a lot for your answer." CreationDate="2016-05-12T18:48:48.217" UserId="17752" />
  <row Id="11848" PostId="11712" Score="0" Text="Welcome to DS.SE. Could you give a bit more detail on what you would like to achieve with your end model or analysis? That will help give a better answer to your question." CreationDate="2016-05-12T18:51:30.480" UserId="16284" />
  <row Id="11851" PostId="11709" Score="0" Text="You are welcome :)" CreationDate="2016-05-12T19:04:11.950" UserId="18626" />
  <row Id="11852" PostId="11716" Score="0" Text="The tag is probably chosen really bad, but I have no idea what would be a better tag." CreationDate="2016-05-12T19:22:05.713" UserId="8820" />
  <row Id="11853" PostId="11721" Score="0" Text="There's no global crime datasets, I've found some from countries like [France](https://www.data.gouv.fr/fr/datasets/chiffres-departementaux-mensuels-relatifs-aux-crimes-et-delits-enregistres-par-les-services-de-police-et-de-gendarmerie-depuis-janvier-1996/), or cities like [Salt Lake City](https://github.com/kylesykes/stl-crime-data).&#xA;&#xA;Keep in mind that even with a perfect data set you'll be correlating age/race/etc. with rate of crime *convictions*, which can be the result of social/institutional bias that makes it likely different than the number of crimes *commited*." CreationDate="2016-05-12T23:51:10.480" UserId="18657" />
  <row Id="11854" PostId="11722" Score="0" Text="Looks like you have installed cloudera QickStart VM. Is this not added in the office documentation of VM so that we can know before install of what version of spark, scala, hadoop are in VM." CreationDate="2016-05-13T02:58:16.883" UserId="17116" />
  <row Id="11855" PostId="11703" Score="0" Text="Oh right, there is no point back-propagating through the non-maximum neurons - that was a crucial insight. &#xA;&#xA;So if I now understand this correctly, back-propagating through the max-pooling layer simply selects the max. neuron from the previous layer (on which the max-pooling was done) and continues back-propagation only through that." CreationDate="2016-05-13T05:35:39.633" UserId="18632" />
  <row Id="11856" PostId="11721" Score="0" Text="What you want isn't *crime data*, but *convictions*. I doubt this is available, even if the personal data is anonymised. I know people working on this sort of problem - they work very closely with the responsible government department and have very strong restrictions on data access." CreationDate="2016-05-13T07:25:03.150" UserId="471" />
  <row Id="11857" PostId="11712" Score="1" Text="I don't know anything about grammar checking but you could check typos relatively easily using something like NLTK, use this to count the fraction of typos made and use this as a feature in your dataset" CreationDate="2016-05-13T08:41:53.703" UserId="14904" />
  <row Id="11859" PostId="11732" Score="0" Text="Tried it getting following error                                                                              &#xA; unused arguments (y = factor(traing$TargetBuy), x = model.matrix(TargetBuy ~ ., data = traing), trControl = train_control, method = &quot;nb&quot;)" CreationDate="2016-05-13T11:10:54.297" UserId="18139" />
  <row Id="11860" PostId="11732" Score="0" Text="try specifying the package you are using explicitly - `caret::train`. Also notice that it works with build-in dataset: model &lt;- train(y = factor(mtcars$vs), x = model.matrix(vs ~ ., data = mtcars), trControl = train_control, method = &quot;nb&quot;)" CreationDate="2016-05-13T11:14:15.600" UserId="17646" />
  <row Id="11862" PostId="11732" Score="0" Text="Please see how I edited the answer. I have changed `train` to `caret::train` in the query.&#xA;For more reference see `?'::'` in R" CreationDate="2016-05-13T11:18:33.743" UserId="17646" />
  <row Id="11863" PostId="11732" Score="0" Text="Working but getting new error       Error: nrow(x) == n is not TRUE" CreationDate="2016-05-13T11:22:47.777" UserId="18139" />
  <row Id="11864" PostId="11732" Score="0" Text="It seems like you had some missing values in your dataset. I have modified my answer." CreationDate="2016-05-13T11:33:16.677" UserId="17646" />
  <row Id="11865" PostId="11712" Score="0" Text="Nice! Thanks for the suggestion!" CreationDate="2016-05-13T12:29:42.277" UserId="13768" />
  <row Id="11866" PostId="11705" Score="0" Text="Can you please explain the logic behind the &quot;np.random.seed(1337)&quot; used in the code? Why 1337?" CreationDate="2016-05-13T13:01:30.447" UserId="15412" />
  <row Id="11867" PostId="11705" Score="0" Text="Nothing special about 1337 for the purposes of the script except repeatability. It is good practice to seed your RNG so that you you can repeat your successful work exactly on another occasion. The number is a bit of an inside joke for hackers - http://www.urbandictionary.com/define.php?term=1337" CreationDate="2016-05-13T13:04:06.527" UserId="836" />
  <row Id="11868" PostId="11735" Score="0" Text="Do you just want a NN to function over inputs of 1:2 or all integers or integers 0:9? Are there any constraints on number of nodes or hidden layers?" CreationDate="2016-05-13T13:06:02.023" UserId="6478" />
  <row Id="11869" PostId="11738" Score="0" Text="can you add links to the papers (or at least the abstracts)?" CreationDate="2016-05-13T13:07:06.323" UserId="6478" />
  <row Id="11870" PostId="11733" Score="0" Text="{'User-Agent' : &quot;Magic Browser&quot;} explained [here](http://stackoverflow.com/questions/3336549/pythons-urllib2-why-do-i-get-error-403-when-i-urlopen-a-wikipedia-page)" CreationDate="2016-05-13T13:21:52.827" UserId="18676" />
  <row Id="11871" PostId="11738" Score="0" Text="Sure! I've added them." CreationDate="2016-05-13T13:30:30.833" UserId="18680" />
  <row Id="11872" PostId="11740" Score="0" Text="To confirm - you are wondering why use a test set (20%) of the data to test binary classification errors rather than just a set of a few thousand samples?" CreationDate="2016-05-13T13:51:31.140" UserId="14913" />
  <row Id="11873" PostId="11741" Score="1" Text="It looks like the categorical variables that you turn into sparse indicator (0/1) columns are having a bigger impact than the continuous variables.  I would try scaling your data by each columns' SD prior to PCA" CreationDate="2016-05-13T13:56:44.377" UserId="14913" />
  <row Id="11874" PostId="11744" Score="1" Text="I think this is what OP is looking for. Usually on this site we prefer longer answers, 1 or 2 paragraphs - so perhaps a short summary of *why* reinforcement learning is a good match to the OP's problem would improve the answer." CreationDate="2016-05-13T14:18:01.887" UserId="836" />
  <row Id="11875" PostId="11740" Score="0" Text="yes............" CreationDate="2016-05-13T14:28:42.370" UserId="18682" />
  <row Id="11879" PostId="11759" Score="0" Text="Sorry if this description is murky - its surprisingly hard to describe this - does anybody know if there is a design pattern name for this type of ensemble?" CreationDate="2016-05-13T23:38:49.560" UserId="6478" />
  <row Id="11880" PostId="1165" Score="0" Text="can you suggest me any topic for master's thesis in predictive analytics in the context of big data." CreationDate="2016-05-14T03:23:26.700" UserId="18704" />
  <row Id="11881" PostId="11751" Score="0" Text="plotly is now open source and can be self-hosted." CreationDate="2016-05-14T15:47:51.227" UserId="471" />
  <row Id="11882" PostId="11741" Score="0" Text="What exactly is that a scatter plot of? What are the points? Why are the X-coordinates nearly all integer+0.5? What covariates have you got? And why do you suspect something is wrong with the data? Maybe the data just doesn't fit the model which means **the model is too wrong**." CreationDate="2016-05-14T15:59:17.007" UserId="471" />
  <row Id="11884" PostId="11725" Score="0" Text="Thanks! This pointed me in the right direction. I'm currently exploring the arules library in R." CreationDate="2016-05-14T17:38:11.897" UserId="18650" />
  <row Id="11885" PostId="11772" Score="0" Text="Thanks, your answer was very thorough and helpful. I'll look into RL algorithms and check out that book suggestion." CreationDate="2016-05-14T18:14:18.837" UserId="18660" />
  <row Id="11887" PostId="11774" Score="0" Text="if you use python than take a look at http://scikit-learn.org/stable/modules/feature_extraction.html" CreationDate="2016-05-14T21:29:35.187" UserId="8752" />
  <row Id="11888" PostId="11759" Score="0" Text="I have a similar problem I'm working on right now. It's not unlike taking the groups from the first level of a regression tree, then taking the data that fall into those groups, and running a separate model depending on which group it falls into. I'm definitely lacking the vocabulary here. I just want to take the first two turns of Guess Who, not play the whole game ..." CreationDate="2016-05-15T00:53:50.780" UserId="18719" />
  <row Id="11890" PostId="11778" Score="0" Text="Wow. Thanks for the clear answer. Very much appreciated :)" CreationDate="2016-05-15T19:44:09.507" UserId="18721" />
  <row Id="11892" PostId="11778" Score="0" Text="Welcome, don't forget to accept the answer if you understand how to do this." CreationDate="2016-05-16T02:17:41.853" UserId="17116" />
  <row Id="11893" PostId="11785" Score="0" Text="Thanks for the reply, but I am not sure if I understand ur answer. Can u make it more clear?" CreationDate="2016-05-16T11:03:39.923" UserId="14736" />
  <row Id="11894" PostId="11779" Score="0" Text="Closing because many simple definitions are readily available on the internet. Without understanding why those are insufficient it's almost impossible to answer" CreationDate="2016-05-16T13:40:43.473" UserId="21" />
  <row Id="11895" PostId="11791" Score="0" Text="$X$ must be $N x K$ to make this make sense. Is that what you mean, or is that the problem?Are you looking for the ALS algorithm?" CreationDate="2016-05-16T15:43:10.533" UserId="21" />
  <row Id="11896" PostId="11791" Score="0" Text="Yes, I meant $NxK$ for $X$. I've corrected the question." CreationDate="2016-05-16T16:03:36.477" UserId="18761" />
  <row Id="11897" PostId="11783" Score="0" Text="Yes, I am trying to do a Sensitivity Analysis. I want to do this analysis to identify which parameters have generally more impact on the output. I have updated my question." CreationDate="2016-05-16T17:53:16.183" UserId="18696" />
  <row Id="11898" PostId="11782" Score="0" Text="Yes, I change all parameters at the same time since some have combined effects. I use a Monte Carlo approach where I randomly change the value of each parameter. I have updated my original question with an example. I hope it will help clarify the issue!" CreationDate="2016-05-16T17:57:02.390" UserId="18696" />
  <row Id="11899" PostId="11783" Score="0" Text="Following your recommendation I am exploring the possibility of using SALib to perform a Sobol Sensitivity Analysis." CreationDate="2016-05-16T19:27:49.117" UserId="18696" />
  <row Id="11900" PostId="11783" Score="0" Text="nice, if only you are looking for a sensitivity measure across the whole input space(Global). But in most applications you may want to divide your input space (Localized). Also as you have many parameters, it is a good idea to consider interaction of these parameters.For example maybe three of your parameters in interaction together make the most effect, while each of them individually have negligible effects." CreationDate="2016-05-16T23:32:35.137" UserId="5200" />
  <row Id="11901" PostId="11797" Score="0" Text="Possible duplicate of [How to binary encode multi-valued categorical variable from Pandas dataframe?](http://datascience.stackexchange.com/questions/8253/how-to-binary-encode-multi-valued-categorical-variable-from-pandas-dataframe)" CreationDate="2016-05-17T06:11:09.663" UserId="381" />
  <row Id="11902" PostId="11798" Score="1" Text="What activation functions are you using? Are you normalizing the inputs? Have you tried more than 10 neurons? I get ok-results with 2x100 ReLU units on normalized data, Adadelta weight optimization and lots of epochs.. not very pleasing though." CreationDate="2016-05-17T08:41:12.180" UserId="676" />
  <row Id="11903" PostId="11805" Score="0" Text="I don't think such a generic method would suffice: A document with 50 views should be prominent if only 10 employees (including me) have access to it, but not prominent if 100000 employees have access to it." CreationDate="2016-05-17T13:50:09.820" UserId="18790" />
  <row Id="11904" PostId="11805" Score="0" Text="I described not the method, but general idea. Collaborative filtering is more complicated and the link I have provided is a good entry point, while you can search for different implementations and approaches and find most suitable for your particular data specifics." CreationDate="2016-05-17T14:06:09.303" UserId="2573" />
  <row Id="11905" PostId="11798" Score="0" Text="I'm using sigmoid. I should probably try rectifier." CreationDate="2016-05-17T14:32:12.733" UserId="18778" />
  <row Id="11906" PostId="11779" Score="0" Text="I change my problem.....kindly make it" CreationDate="2016-05-17T15:18:40.230" UserId="18139" />
  <row Id="11907" PostId="11783" Score="0" Text="By divining my input space, do you mean to create group of parameters? For instance if I have 12 parameters, make two (or more) groups where I keep half constant and vary the others." CreationDate="2016-05-17T16:50:09.870" UserId="18696" />
  <row Id="11909" PostId="11811" Score="0" Text="RNN sounds interesting! But I have to learn it from scratch. What do you mean by insight into the internal state of the system, as I am aware from where the data is coming." CreationDate="2016-05-17T17:18:53.213" UserId="10108" />
  <row Id="11910" PostId="11770" Score="0" Text="Can you specify your use case for this question? That might lead to a more useful answer" CreationDate="2016-05-17T17:57:21.650" UserId="13686" />
  <row Id="11912" PostId="11811" Score="0" Text="@GKS: Yes there is a lot to learn if you are just starting to look at different types of model. For insight I mean you might have some understanding of the size/complexity of factors that generate the series, and maybe even be able to partially model them just from domain knowledge. Then the learning phase can be fitting parameters of that model to observed sequences and/or deriving most-probable internal state of the model from a specific sequence." CreationDate="2016-05-17T18:21:30.077" UserId="836" />
  <row Id="11913" PostId="11549" Score="0" Text="Welcome to DataScience.SE! If your question has been resolved, please tick the answer." CreationDate="2016-05-17T21:00:28.190" UserId="381" />
  <row Id="11915" PostId="11798" Score="0" Text="Maybe if you give us some code we might help you. Have you implemented this net on your own or you used some package? What kind of topology (exact not only number of layers) have you used? Did you normalize your input, etc. We need more information to answer your question." CreationDate="2016-05-17T22:28:33.403" UserId="18802" />
  <row Id="11916" PostId="11788" Score="0" Text="There isn't anything wrong with datasets that dont split perfectly down the middle per se. What modeling technique are you using? If the technique relies on &quot;balanced&quot; data, you may be using the wrong technique." CreationDate="2016-05-18T01:00:35.307" UserId="18812" />
  <row Id="11917" PostId="11805" Score="0" Text="Did I describe my data specifics clearly enough in my question? If not, please feel free to ask for any information that is needed before a specific approach can be recommended. Thanks a lot :-)" CreationDate="2016-05-18T01:55:10.833" UserId="18790" />
  <row Id="11920" PostId="11798" Score="0" Text="I didn't use any package or library. I wrote everything myself." CreationDate="2016-05-18T03:56:16.583" UserId="18778" />
  <row Id="11921" PostId="11798" Score="2" Text="A couple of thoughts: In polynomial space, the sine function you drew is 21st order. I'm not convinced that the 3rd order ANN will display much curvature beyond what is shown (is it a straight line or is there some curve there) since it is a 3rd order poly trying to approximate a 21st order function.  Could you maybe try sticking an x^2 function in there and test that before switching to a sine function with that high a wave number.  If x^2 works, maybe try a sine function with the wave number set to include only 3 extrema and see how that works." CreationDate="2016-05-18T04:42:28.613" UserId="9420" />
  <row Id="11922" PostId="11798" Score="0" Text="I put the code into a notebook, it uses keras so it's quite short. Have a look if you want to try to improve the results: https://gist.github.com/anonymous/78808afc357b1af942361fa9a8f4e1c4" CreationDate="2016-05-18T07:12:10.713" UserId="676" />
  <row Id="11924" PostId="11753" Score="0" Text="What version of Mahout are you using? I think that Spark job execution supported was provided since Mahout 0.10." CreationDate="2016-05-18T09:47:36.760" UserId="75" />
  <row Id="11925" PostId="9433" Score="0" Text="Check out http://stackoverflow.com/questions/24869304/scala-how-can-i-generate-numbers-according-to-an-expected-distribution/24869852#24869852" CreationDate="2016-05-18T09:59:39.183" UserId="75" />
  <row Id="11926" PostId="11822" Score="0" Text="Thanks for the fantastic response, I will look into BeautifulSoup!" CreationDate="2016-05-18T10:34:21.170" UserId="12202" />
  <row Id="11928" PostId="11817" Score="0" Text="Yes, I have install the NLTK for all users. Thank you very much for your information." CreationDate="2016-05-18T11:54:05.987" UserId="17116" />
  <row Id="11929" PostId="11821" Score="0" Text="I'm a bit confused (but that's normal since I'm just starting to work with this kind of tools...) because according to the [doc](https://cran.r-project.org/web/packages/dbscan/dbscan.pdf) a eps parameter still need to be set for OPTICS algorithm. So we don't really get rid of it, do we ?" CreationDate="2016-05-18T12:00:00.953" UserId="18221" />
  <row Id="11930" PostId="11821" Score="0" Text="The eps parameter of OPTICS is different. It is an upper bound, not a fixed value. It mainly has influence on how fast the algorithm runs." CreationDate="2016-05-18T12:33:01.673" UserId="8820" />
  <row Id="11931" PostId="11812" Score="0" Text="Could you explain what do you mean by &quot;if future time series data is in accord with previous time series data &quot;." CreationDate="2016-05-18T12:36:42.647" UserId="75" />
  <row Id="11932" PostId="11821" Score="0" Text="Indeed but, at the end, if we want to extract clusters we have to make a visual analysis of the reachability plot and set xi parameter manually Is there a way to set it in a cleaner way ? (I'm going off topic...)" CreationDate="2016-05-18T13:28:09.407" UserId="18221" />
  <row Id="11933" PostId="11783" Score="0" Text="No. assume you have 2 parameters ( x1 and x2 ) which can vary in (0, 1) interval (No loss of generality). then you scan have 4 sub-spaces as follow: sub1: 0&lt;x1&lt;0.5 and 0&lt;x2&lt;0.5 -- sub2: 0&lt;x1&lt;0.5 and 0.5&lt;x2&lt;1 -- sub3: 0.5&lt;x1&lt;1 and 0&lt;x2&lt;0.5, sub4: 0.5&lt;x1&lt;1 and 0.5&lt;x2&lt;1" CreationDate="2016-05-18T13:40:19.517" UserId="5200" />
  <row Id="11934" PostId="11819" Score="0" Text="My weights are initialized to a random float between -1 and 1." CreationDate="2016-05-18T14:27:04.297" UserId="18778" />
  <row Id="11935" PostId="11819" Score="0" Text="@user255919: That *might* be OK with for the first hidden layer, but maybe a bit high for the second layer. The second hidden layer may also be a problem for GA search, due to dependencies between the layers meaning good values in first layer won't generally combine well with good values in second layer from a different genome. That means cross-over between genomes will not be as effective." CreationDate="2016-05-18T15:51:20.853" UserId="836" />
  <row Id="11936" PostId="11798" Score="0" Text="@AN6U5: An NN with non-linear transfer function such as sigmoid is not restricted to approximate only a certain order of polynomial, according to number of layers (I'm guessing you got &quot;3rd order poly&quot; from the number of layers?) There are limits for simple networks though - more based on number of neurons - and the rest of the advice is sound." CreationDate="2016-05-18T15:59:01.993" UserId="836" />
  <row Id="11937" PostId="11798" Score="0" Text="@NeilSlater, I agree that it is not strictly polynomial, but there is some restriction to the non-linearity. I'm not enough of an ANN expert to comment on how nonlinear a two hidden layer network can get, but testing it on a function with 20 extrema while trying to figure out what is going on is not a good problem solving strategy.  I was mostly just trying to tease out the logic, but you are correct.  Thanks for the clarification!" CreationDate="2016-05-18T16:05:28.580" UserId="9420" />
  <row Id="11938" PostId="11805" Score="0" Text="What I find confusing is the absence of clear idea why a document with 10000 views is not worth showing as recommendation, and one with 50 view is OK. What about 100? Or 51? If you have a definite percentage of audience which makes view count irrelevant you can just exclude such cases from training set and still stick with collaborative approaches. If not, you might have a classification or clusterization problem of some kind, which is a way broader topic." CreationDate="2016-05-18T16:09:49.133" UserId="2573" />
  <row Id="11940" PostId="11783" Score="0" Text="I see, I am going to try to create sub-spaces then. Thank you. &#xA;&#xA;I tried the Sobol method but it will require to much computational time so I might switch to the Morris method instead." CreationDate="2016-05-18T17:24:36.217" UserId="18696" />
  <row Id="11941" PostId="11798" Score="0" Text="Where is your back propagation of the error? Where is the bias term?" CreationDate="2016-05-18T20:47:20.150" UserId="11141" />
  <row Id="11942" PostId="11831" Score="1" Text="Link-only answers are discouraged; summarize what about these resources answers the question" CreationDate="2016-05-18T22:22:59.140" UserId="21" />
  <row Id="11943" PostId="10707" Score="0" Text="Martin could you clarify why restricting your domain is necessary in training the network?" CreationDate="2016-05-18T23:04:58.700" UserId="18841" />
  <row Id="11944" PostId="10707" Score="0" Text="One the one hand you have to have a look at the range of values of the activation function of the last layer. Now you might simply take a linear output, but then the last weights might get enormous. Even small fluctuations at previous layers might so have a huge impact on the last one and this on the output" CreationDate="2016-05-18T23:10:48.347" UserId="8820" />
  <row Id="11945" PostId="11753" Score="0" Text="The output of  'rpm -qa | grep mahout' is 'mahout-0.9+cdh5.7.0+29-1.cdh5.7.0.p0.79.el6.noarch'. So I guess Cloudera CDH5.7 comes with Mahout 0.9. Is that right?" CreationDate="2016-05-18T23:48:27.530" UserId="18588" />
  <row Id="11946" PostId="11834" Score="0" Text="I've upvoted you because you answered the question as I had originally written it and I agree with your points. However, I'm looking for an approach that could be applied to motion/video data in general. I used the sign language example simply because it was a lot easier to explain than my real problem" CreationDate="2016-05-19T00:24:30.403" UserId="12515" />
  <row Id="11947" PostId="11805" Score="0" Text="Where does the 10000 figure come from? If you meant 100000, then I was not clear enough: &quot;have access to it&quot; does not mean &quot;have viewed it&quot;, it means &quot;have the permission to access it if they wish&quot;. In other words, the first document has been viewed an average of 10 times by each person who has the permission to view it, but the second document has been viewed only an average of 0.0005 times by each person who has the permission to view it." CreationDate="2016-05-19T02:30:02.980" UserId="18790" />
  <row Id="11948" PostId="11805" Score="0" Text="&quot;*If you have a definite percentage*&quot;: There is no such percentage, it must be gradual. &quot;*exclude such cases from training set*&quot;: Such cases are the norm, because most of the documents can only be viewed by certain teams, not by the whole company, so I can not exclude them, that's the whole point of the question, and that's precisely why my question is a different class of problem." CreationDate="2016-05-19T02:30:10.043" UserId="18790" />
  <row Id="11949" PostId="2594" Score="0" Text="The link is still broken" CreationDate="2016-05-19T08:32:03.573" UserId="18852" />
  <row Id="11950" PostId="11841" Score="0" Text="I know that with valid you get a smaller output. This is the kind of convolution I am used to, where you get a smaller output since the filter moves across the original image. You are saying that with &quot;same&quot; you go you of the original image to get the same size. What are the input for those points then, do we assume they are zero?" CreationDate="2016-05-19T08:47:10.913" UserId="13736" />
  <row Id="11951" PostId="11841" Score="1" Text="Yes, see above - &quot;the area outside of the input is normally padded with zeros&quot;. Some libraries allow you to specify which value should be used for padding. Keras uses zeros." CreationDate="2016-05-19T10:10:38.253" UserId="676" />
  <row Id="11952" PostId="453" Score="0" Text="Have you looked at *middle-out* compression algorithms? http://www.piedpiper.com/" CreationDate="2016-05-19T10:32:02.457" UserId="6478" />
  <row Id="11953" PostId="11842" Score="3" Text="I don't think SVMs do this. It is one of the criticisms of the approach listed in the wikipedia article that SVMs are too &quot;black box&quot;" CreationDate="2016-05-19T12:48:34.857" UserId="836" />
  <row Id="11954" PostId="11842" Score="2" Text="I have good experience with random forests for feature selection. Evaluate features with random forests, then feed the best ones to an SVM (or just stick to RF if model size doesn't matter :)." CreationDate="2016-05-19T14:02:10.147" UserId="676" />
  <row Id="11955" PostId="11843" Score="0" Text="Just get one with 4GB - there's a price difference of 10EUR." CreationDate="2016-05-19T14:43:42.910" UserId="676" />
  <row Id="11956" PostId="5620" Score="0" Text="# example of melt function &#xA;library(reshape)&#xA;mdata &lt;- melt(mydata, id=c(&quot;id&quot;,&quot;time&quot;))" CreationDate="2016-05-19T15:21:42.580" UserId="9324" />
  <row Id="11957" PostId="6664" Score="0" Text="+1 for using `.N`.  This is probably the most efficient." CreationDate="2016-05-19T15:32:11.817" UserId="18865" />
  <row Id="11958" PostId="11812" Score="0" Text="Suppose I have a set of 30 time series data. Next time I get a new time series data, I have to check if it is from the same class of the 30 sequences I already have, but I don't have to include the new one in the set." CreationDate="2016-05-19T15:38:28.367" UserId="14153" />
  <row Id="11959" PostId="11812" Score="0" Text="Do you also have an _opposite class_, i.e. one that isn't that of the first 30 sequences?" CreationDate="2016-05-19T16:53:34.160" UserId="15527" />
  <row Id="11964" PostId="11842" Score="0" Text="@stmax Thank you. Could you show how to evaluate features with random forests?" CreationDate="2016-05-20T06:13:43.257" UserId="11062" />
  <row Id="11966" PostId="11844" Score="0" Text="Thanks @Dawny33 for the edits, I will try to inculcate such practices in the future. :)" CreationDate="2016-05-20T06:26:08.787" UserId="14789" />
  <row Id="11968" PostId="11842" Score="0" Text="Here's an example from the sklearn-docs: http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html This one uses the ExtraTreesClassifier, but you can just change that to RandomForestClassifier.. both should give about the same results." CreationDate="2016-05-20T07:46:09.967" UserId="676" />
  <row Id="11969" PostId="11847" Score="0" Text="can you add a toy example to illustrate what your are describing?" CreationDate="2016-05-20T11:26:59.680" UserId="6478" />
  <row Id="11970" PostId="11849" Score="0" Text="Works for me, using xgboost v 0.4-2 with R 3.2.3.  You can just save bst as a .RData object for a workaround." CreationDate="2016-05-20T12:19:09.383" UserId="14913" />
  <row Id="11971" PostId="11847" Score="0" Text="@BrandonLoudermilk done! It's getting to look like a blog post, mind." CreationDate="2016-05-20T13:07:35.817" UserId="5290" />
  <row Id="11972" PostId="11848" Score="1" Text="The data are categorical. If they were ordinal or some kind of ranking, then rounding them would make sense. Predicting on categorical variables by treating them as continuous can give you bad results if e.g. there is some trouble separating 1 from 5 meaning the network predicts 3 instead (which in a categorical system could be completely different and inappropriate prediction)" CreationDate="2016-05-20T14:01:14.463" UserId="836" />
  <row Id="11973" PostId="11812" Score="0" Text="No, I don't. The only data I have previously is the 30 sequences that are from the positive class." CreationDate="2016-05-20T17:14:50.470" UserId="14153" />
  <row Id="11975" PostId="11811" Score="0" Text="I have updated my question, anything simpler you can think of?" CreationDate="2016-05-20T18:55:50.973" UserId="10108" />
  <row Id="11976" PostId="11848" Score="0" Text="@eulerleibniz: The question has been updated. I hope this makes it clearer." CreationDate="2016-05-20T18:57:52.733" UserId="10108" />
  <row Id="11977" PostId="11846" Score="0" Text="What feedback do you get from your system (a game, I guess?) that tells you whether your prediction is good or bad in the end? You can only assess your strategy if there is feedback." CreationDate="2016-05-20T19:22:14.470" UserId="836" />
  <row Id="11978" PostId="11856" Score="0" Text="But don't you end up getting different weights if you run it 30x30 times? That is where I am getting confused. If you run backpropagation 30x30 times, how do you end up with just one filter?" CreationDate="2016-05-20T20:31:27.530" UserId="13736" />
  <row Id="11979" PostId="11856" Score="0" Text="No, you sum up the gradients and apply the summed version to the weights of the single filter." CreationDate="2016-05-20T20:33:01.233" UserId="836" />
  <row Id="11980" PostId="11846" Score="0" Text="I should get a score between 0 and 100, and compare with subjectives values established by me" CreationDate="2016-05-20T21:20:48.967" UserId="14478" />
  <row Id="11981" PostId="11799" Score="0" Text="@Drj If this answers your question, please tick it off, otherwise indicate what's wrong. This helps keep our site useful." CreationDate="2016-05-20T21:48:54.630" UserId="381" />
  <row Id="11984" PostId="11857" Score="0" Text="I am asking a different question. My question is about the bias. From the formula, it looks that for each feature map there are different weights, but the same bias is used across all the feature maps." CreationDate="2016-05-21T05:38:02.190" UserId="13736" />
  <row Id="11985" PostId="11856" Score="0" Text="I see, so you basically calculate the gradient for all the weights, but then when you do the update you use some sort of average, like a momentum, to update the weights?" CreationDate="2016-05-21T07:13:44.143" UserId="13736" />
  <row Id="11987" PostId="11856" Score="0" Text="@user: Not &quot;some sort of average&quot;. Literally just use the sum (I guess as we have a learning rate multiplier applied during weight update, it is mathematically the same thing as an average though, if that helps you visualise the problem - but you don't need any extra divisions etc). And you don't need to calculate the gradients for &quot;all the weights&quot; separately then combine at the end. Instead you just have one set of weight deltas which you build up by adding together all the combinations from each position." CreationDate="2016-05-21T07:58:37.637" UserId="836" />
  <row Id="11988" PostId="11856" Score="0" Text="Although having said that, the *important* thing that makes a CNN work is that there is just one set of weights shared across all positions that the kernel is evaluated. How you rationalise and construct that is not important to the network generalising - however it may be important to optimisation." CreationDate="2016-05-21T08:03:05.960" UserId="836" />
  <row Id="11989" PostId="11856" Score="0" Text="Thank you. For the bias, there is just one bias for all the feature maps, correct? So in that case we add up the gradients across the feature maps as well, is that right?" CreationDate="2016-05-21T08:52:42.480" UserId="13736" />
  <row Id="11990" PostId="11856" Score="0" Text="I'm not entirely sure about what you are referring to as the feature map. I would call each deeper 2D image a &quot;feature map&quot; and say that your example has one feature map (of 30x30) showing existence of the detected feature in all possible locations. In that case there is one bias (and one bias weight to train) per feature map, not one shared bias for all feature maps." CreationDate="2016-05-21T09:27:12.630" UserId="836" />
  <row Id="11991" PostId="11838" Score="0" Text="Question: if you or an &quot;expert&quot; were shown an instance would you be able to reliably classify it into the positive or negative class?" CreationDate="2016-05-21T12:04:08.713" UserId="6478" />
  <row Id="11992" PostId="11848" Score="0" Text="OK. then you don't need to have all next 5 seconds as output. it is better to use Multi-Step ahead prediction. you should also test your data to see how much correlation, Auto-correlation, you find in your data. maybe you need more or less lags in your data set for predicting." CreationDate="2016-05-21T12:32:06.217" UserId="5200" />
  <row Id="11994" PostId="11825" Score="0" Text="what industry are you in? I imagine if standards exist, then they likely differ by subdomain, e.g., credit card fraud vs. insurance fraud" CreationDate="2016-05-21T14:21:22.340" UserId="6478" />
  <row Id="11995" PostId="11856" Score="0" Text="Thank you @Neil-Slater Can you also look at http://datascience.stackexchange.com/questions/11853/question-about-bias-in-convolutional-networks" CreationDate="2016-05-21T15:19:23.927" UserId="13736" />
  <row Id="11996" PostId="11864" Score="0" Text="@Brandon, thanks for the edits. Will try to use correct technical terms in future." CreationDate="2016-05-21T15:25:00.327" UserId="17187" />
  <row Id="11999" PostId="11861" Score="0" Text="Thanks for such an elaborated reply. I have gone through  Janssens' dissertation earlier and again revisited the mentioned chapter by you. However as per my understanding, he didn't propose any new method other than proposed by Tax in DD Toolbox, to select optimal parameter for one class classifiers. I may have overlooked if it is there. Please point out the page number if you find." CreationDate="2016-05-22T06:55:51.993" UserId="18818" />
  <row Id="12001" PostId="11870" Score="0" Text="Thanks Neil. Clear as always." CreationDate="2016-05-22T12:31:14.550" UserId="13736" />
  <row Id="12002" PostId="11863" Score="0" Text="- I find the solution to find the varimax rotation document laoding and got rotation matrix  but there after the same rotation matrix is applied to to term loading (US*T). What are your suggestions, that the order of multiplication should not make any difference.  (US*T) = (T*US).  (termloading and document loading contains topic are columns)&#xA;&#xA;- while performing the varimax rotation on terms matrix, there are terms loading for one topic but have NULL values for other topics, but after varimax rotation those cells contain certain values. Is that is right?" CreationDate="2016-05-22T14:40:11.840" UserId="18896" />
  <row Id="12006" PostId="11828" Score="0" Text="The paper appears to be very encouraging. I'll get in touch later on as the paper is very comprehensive." CreationDate="2016-05-23T07:07:50.940" UserId="18744" />
  <row Id="12008" PostId="11884" Score="1" Text="Please post this as a comment, or ask a new question" CreationDate="2016-05-23T09:29:13.987" UserId="378" />
  <row Id="12009" PostId="11875" Score="0" Text="When I use goodnes of fit test on my possion regression model I get a low p-value so this indicates that my model fit the observed data? I entered the code above." CreationDate="2016-05-23T10:33:55.510" UserId="18787" />
  <row Id="12010" PostId="11881" Score="3" Text="TBH it was my frustration at that exact text -which uses the word 'interpretable' many times in relation to different models, and at one stages says '...data mining application require interpretable models. It is not enough to simply produce predictions' (section 10.7), without my being able to find material on how to identify an interpretable model - which prompted the question. Though I was and am loathe to appear critical of such a highly regarded text. Similarly TIbshirani's paper introducing the LASSO lists 'interpretable' as one of its virtues without saying what 'interpretable' is." CreationDate="2016-05-23T10:40:36.780" UserId="18843" />
  <row Id="12011" PostId="11882" Score="0" Text="It seems that your problem can be solved using natural language processing itself and need not require sophisticated machine learning techniques. Could you shed some more light on the dataset you are using." CreationDate="2016-05-23T11:14:59.203" UserId="75" />
  <row Id="12012" PostId="11876" Score="0" Text="Could you please point me to the section/page where it says that implementation is &quot;available in R&quot;? I do not want to read through the 90 pages." CreationDate="2016-05-23T11:22:18.163" UserId="75" />
  <row Id="12013" PostId="11866" Score="0" Text="I am not sure if you have gone through this already: http://nerds.airbnb.com/aerosolve/" CreationDate="2016-05-23T11:27:20.393" UserId="75" />
  <row Id="12015" PostId="11875" Score="1" Text="No - a low model p-value just means that you can reject the null hypothesis that random chance fit the data better than your model. It doesn't mean that your model necessarily fit the data well." CreationDate="2016-05-23T11:49:20.690" UserId="18843" />
  <row Id="12016" PostId="11875" Score="0" Text="What will be the best way to compare two poission regression models as I describe above ?" CreationDate="2016-05-23T12:07:59.063" UserId="18787" />
  <row Id="12017" PostId="11875" Score="1" Text="Try anova(fit1, fit2). It should provide a test between fit1 and fit2. But this is just off memory. I'll provide a reference later. You can also try AIC and BIC functions." CreationDate="2016-05-23T15:13:06.420" UserId="16220" />
  <row Id="12018" PostId="11891" Score="0" Text="Sounds like Bayesian data analysis can help here quantifying the uncertainties. It will naturally take into account the number of observations, e.g. 500 observations make you more certain about some outcome rather than 1. Can you detail bit more the task and what questions you are trying to answer?" CreationDate="2016-05-23T17:34:45.253" UserId="7848" />
  <row Id="12019" PostId="11891" Score="0" Text="Tried to provide a more detailed description of the problem" CreationDate="2016-05-23T18:15:29.080" UserId="16855" />
  <row Id="12020" PostId="11876" Score="0" Text="section: &#xA;4.3 Computational Study of the Minimum Norm Algorithm, in the&#xA;&#xA;first paragraph" CreationDate="2016-05-23T20:14:48.063" UserId="13100" />
  <row Id="12021" PostId="11890" Score="1" Text="Welcome to DataScience.SE! [Here's an example](http://datascience.stackexchange.com/questions/10471/linear-regression-with-non-symmetric-cost-function/10474) of a skewed cost function for _regression_ in tensorflow." CreationDate="2016-05-23T21:51:04.010" UserId="381" />
  <row Id="12022" PostId="11882" Score="0" Text="@Shagun what you said is right. Honestly I'm not familiar how to do NLP and its applications." CreationDate="2016-05-24T01:13:35.693" UserId="18931" />
  <row Id="12023" PostId="10227" Score="0" Text="`If we specify a single label to multiple sentences in a paragraph, it means that all the sentences in the paragraph are required to convey the meaning.` I am not sure I am understanding this correctly. From the algorithms POV, is it all the sentences with same tag are needed for the semantic definition or all the sentences with the same tag describe same thing? In first case, no single sentence is self-sufficient by itself, in the second case, a single sentence is self-sufficient." CreationDate="2016-05-24T01:59:13.317" UserId="634" />
  <row Id="12024" PostId="10230" Score="0" Text="Please correct me if I am wrong. In your last paragraph, you are suggesting that each document should be tagged like we tag questions in this site. And after training, we will get vector representation of each tag. When a new document arrive, we can just use similarity metric for suggesting tags for the new document." CreationDate="2016-05-24T02:05:53.800" UserId="634" />
  <row Id="12025" PostId="11882" Score="0" Text="Welcome to Datascience.SE! This kind of [information extraction](https://en.wikipedia.org/wiki/Information_extraction) task is well solved with [Conditional Random Fields](https://en.wikipedia.org/wiki/Conditional_random_field), for which [standalone implementations](http://www.chokkan.org/software/crfsuite/) and [python libraries](https://pystruct.github.io) exist." CreationDate="2016-05-24T02:22:20.693" UserId="381" />
  <row Id="12026" PostId="11876" Score="0" Text="I don't think they said the implementation is &quot;available in R&quot;. All they said is that &quot;MNA was performed with statistical software R&quot; and it does not look like they shared their implementation. A cursory search on GitHub did not get me any implementation.  You might have to implement it yourself. And if you do, please keep it open :)" CreationDate="2016-05-24T07:01:40.527" UserId="75" />
  <row Id="12028" PostId="10227" Score="0" Text="@user: It is the second case, from the algorithm POV, a tag is defined as an entity which when used in a single sentence, it captures the meaning of all the words. In the same way when used in multiple sentences it also captures the words in all other sentences." CreationDate="2016-05-24T10:38:03.607" UserId="16024" />
  <row Id="12029" PostId="11812" Score="0" Text="I think it'll be hard / impossible to detect a useful pattern if you only have 30 samples and ~100 features. You could try to come up with some smart manual feature engineering to reduce the 100 features down to 2 or 3 meaningful features and then try to use a one-class SVM, local outlier factor or gaussian mixture model.." CreationDate="2016-05-24T10:44:25.030" UserId="676" />
  <row Id="12030" PostId="11902" Score="0" Text="Yeah, that's my opinion too, which is why I found that software odd." CreationDate="2016-05-24T10:58:58.910" UserId="917" />
  <row Id="12031" PostId="11897" Score="0" Text="Not sure if this is what you are looking for http://stats.stackexchange.com/questions/19048/what-is-the-difference-between-test-set-and-validation-set" CreationDate="2016-05-24T11:05:01.917" UserId="18970" />
  <row Id="12032" PostId="11902" Score="0" Text="It's probably aimed more at analyst that are not trained in ML/statistics but want to build models.  Makes it sound more powerful and easier for them to use." CreationDate="2016-05-24T11:14:41.300" UserId="14913" />
  <row Id="12034" PostId="10892" Score="1" Text="Shouldn't P(pos|sample) + P(neg|sample) be 1?" CreationDate="2016-05-24T19:30:26.300" UserId="676" />
  <row Id="12035" PostId="11903" Score="0" Text="**Thanks for the answer.** My first approach to the problem involved the DTW, another measure of difference. The problem is that I don't have a _negative_ class to compare the distances and decide. I'll try to use a neural network in sequence." CreationDate="2016-05-24T20:03:18.547" UserId="14153" />
  <row Id="12036" PostId="11901" Score="0" Text="Thanks for the complete answer. My data is a bit similar (all of them have the same number of peaks, around the same number of medium values), but some of them have some disparity in time. I'll go for exploratory analysis and may post some of my findings here." CreationDate="2016-05-24T20:10:17.117" UserId="14153" />
  <row Id="12037" PostId="11899" Score="0" Text="Thanks for the clear answer. I was a bit worried about the performance of the SVM for time series data, but as I commented in @MightyCurious answer, I'll go for exploratory analysis and try the SVM too." CreationDate="2016-05-24T20:12:22.627" UserId="14153" />
  <row Id="12038" PostId="11909" Score="0" Text="Welcome to DataScience.SE! I hope you find it useful. Do you have a social graph relating the new user to existing users? Are all the traits binary? Do you have any other metadata about the users?" CreationDate="2016-05-24T21:30:04.147" UserId="381" />
  <row Id="12039" PostId="11909" Score="0" Text="Thanks! all the traits are unfortunately binary and there is no relation listed between the existing users or new ones. The source as I understand was from online surveys where people participated from across the world. Their answers determined the assigned traits and this is the only data I have." CreationDate="2016-05-24T21:41:40.127" UserId="18992" />
  <row Id="12040" PostId="11893" Score="0" Text="I was looking for a solution and I found in the book [Bayesian Methods for Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) an example &quot;Inferring Behavior from Text-Message Data&quot;. Maybe what I need to find the **switchpoint** in the time series. Like in this [question in stackoverflow](http://stackoverflow.com/questions/35922022/pymc3-select-data-within-model-for-switchpoint-analysis). What do you people think? Is there another method?" CreationDate="2016-05-24T21:42:05.487" UserId="18612" />
  <row Id="12041" PostId="10230" Score="0" Text="@user - yes, it is possible and sometimes beneficial to do that, though it's not necessary." CreationDate="2016-05-24T21:52:47.663" UserId="8278" />
  <row Id="12042" PostId="11898" Score="0" Text="Which base features scales (nominal, ordinal, interval, ratio) does the software claim to be able to take? For which talks does it claim to produce good results (classification, regression, RL)? Does it claim to work for sequence to sequence (then I would be sure it would simply not work)?" CreationDate="2016-05-24T22:30:22.653" UserId="8820" />
  <row Id="12043" PostId="11900" Score="0" Text="Are you looking for sequence tagging where the labeling is context specific? Take a look at &quot;Triangular-chain Conditional Random Fields&quot;, https://github.com/minwoo/TriCRF" CreationDate="2016-05-24T23:38:07.327" UserId="7848" />
  <row Id="12044" PostId="11900" Score="0" Text="You might also find the paper by McDonald, &quot;Fine to Coarse Sentiment Analysis&quot; (2007) interesting. https://github.com/JohnLangford/vowpal_wabbit/issues/995" CreationDate="2016-05-24T23:39:08.700" UserId="7848" />
  <row Id="12045" PostId="11900" Score="0" Text="Tree CRF, extension of CRFSuite, supports tree structured CRFs https://github.com/WladimirSidorenko/CRFSuite. You provide the context (global label) and sequence labeling for that context as features." CreationDate="2016-05-24T23:40:44.577" UserId="7848" />
  <row Id="12046" PostId="10340" Score="0" Text="Will be easier to help you if you post some code, maybe some toy-model" CreationDate="2016-05-24T23:57:38.197" UserId="9178" />
  <row Id="12047" PostId="11887" Score="0" Text="Now that I've had more of a chance to consider the linked paper by Ribeiro et al., I'd like to say that Section 2 'The Case for Explanation' contains something of a useful definition of 'explainability', and does a decent job of outlining its importance, and as such, deserves to be widely read within the Data Science community." CreationDate="2016-05-25T00:25:06.657" UserId="18843" />
  <row Id="12048" PostId="11900" Score="0" Text="Thanks @xeon, that seems to be what im looking for. Let me try it out." CreationDate="2016-05-25T05:13:04.260" UserId="18665" />
  <row Id="12049" PostId="10892" Score="0" Text="Yes it should, you could classify it as neutral for example when neither of them is over 0.65" CreationDate="2016-05-25T07:19:57.180" UserId="14904" />
  <row Id="12050" PostId="11912" Score="1" Text="Do neural networks have principles? The principle for black box methods is to try everything and see what works" CreationDate="2016-05-25T07:38:46.967" UserId="17959" />
  <row Id="12051" PostId="11914" Score="0" Text="Is there a fixed amount of Action types?" CreationDate="2016-05-25T08:36:28.240" UserId="14904" />
  <row Id="12052" PostId="11914" Score="0" Text="Yes, there are about 10 different Actions" CreationDate="2016-05-25T08:57:11.367" UserId="19003" />
  <row Id="12053" PostId="11914" Score="1" Text="Then one-hot encoding your actions is likely the best way to go, one column for every action type and it's a 1 if it is that action and a 0 if it's not that action" CreationDate="2016-05-25T08:59:35.627" UserId="14904" />
  <row Id="12054" PostId="11914" Score="0" Text="Thank you for the input, I will try it out! One more question: will this also work if there are more different Actions (&gt;50)?" CreationDate="2016-05-25T09:15:05.080" UserId="19003" />
  <row Id="12055" PostId="11914" Score="2" Text="That mostly depends on the size of your dataset, since you will have much more features. This is not a problem if you have enough data" CreationDate="2016-05-25T09:16:15.507" UserId="14904" />
  <row Id="12056" PostId="11914" Score="0" Text="Okay I understand!&#xA;Thank you" CreationDate="2016-05-25T09:16:44.743" UserId="19003" />
  <row Id="12057" PostId="11916" Score="0" Text="This makes so much sense. I will try to see what happens when I do it for &quot;good&quot; users." CreationDate="2016-05-25T09:56:43.303" UserId="18787" />
  <row Id="12058" PostId="11907" Score="2" Text="If I could up vote twice, I'd up vote again for the phrase 'it takes a great deal of manual effort to do something automatically' which I now intend to steal." CreationDate="2016-05-25T11:40:10.263" UserId="18843" />
  <row Id="12059" PostId="11902" Score="0" Text="SPSS Modeler has a an automatic model tuning option which I've long suspected is used primarily in sales demos - this sounds like a similar principle." CreationDate="2016-05-25T11:47:45.633" UserId="18843" />
  <row Id="12060" PostId="11898" Score="0" Text="It says it connects to sensor, so I assume it would be signals. It's for classification results, I don't think that they claimed that it worked for sequence to sequence." CreationDate="2016-05-25T12:56:51.153" UserId="917" />
  <row Id="12061" PostId="11898" Score="0" Text="Ok, I guess you are talking about software which is meant for very specific sensors (e.g. image / sound). Otherwise, this is too general to work currently. And your question is far to broad - I don't want to guess what your actually asking. You have to either give us more specific details, or you will only get this kind of answer." CreationDate="2016-05-25T13:12:13.420" UserId="8820" />
  <row Id="12062" PostId="11907" Score="0" Text="@RobertdeGraaf Thanks!" CreationDate="2016-05-25T13:27:57.907" UserId="13413" />
  <row Id="12063" PostId="11921" Score="0" Text="don't think so, even if it was implemented, it would probably lack support for using. I would suggest Tensorflow or Skflow for python, caffe for C++ or caffe on spark for Apache Spark." CreationDate="2016-05-25T13:34:56.077" UserId="18969" />
  <row Id="12064" PostId="11921" Score="0" Text="@GameOfThrows Thanks, I think you're right. I use Tensorflow and Caffe but I'd just *like* to use R." CreationDate="2016-05-25T13:38:54.737" UserId="2723" />
  <row Id="12065" PostId="11920" Score="0" Text="Hi Alex, Welcome to DS.SE. This is a Q&amp;A site that has the richest answers floating to the top via voting. Someone has voted you down, perhaps since your answer is quite short and generally explaining the solution (e.g.) isnt explaining the details of hyperparameters, a term which wasnt used by the original poster." CreationDate="2016-05-25T14:00:50.737" UserId="16284" />
  <row Id="12066" PostId="8458" Score="0" Text="In theory, decision tree and forests work fine with categorical values. But in sklearn, this is not so. It sucks to one-hot encode when you have dozens of categories." CreationDate="2016-05-25T14:33:41.117" UserId="16853" />
  <row Id="12067" PostId="11812" Score="0" Text="What _kind_ of time series are you analyzing (address randomness, independence, and stationarity-- if you don't know, tell us more about your data). That very much determines how you'd look for an answer to your question-- if the future is &quot;in accord&quot; with the past." CreationDate="2016-05-25T16:42:46.657" UserId="1077" />
  <row Id="12068" PostId="11903" Score="0" Text="What if the data are completely uncorrelated to begin with, as in a random, independent, stationary process?  What if the data are correlated, say temperatures from the same thermometer, but the future set is still uncorrelated to the past set (enough time has elapsed)?" CreationDate="2016-05-25T16:53:02.507" UserId="1077" />
  <row Id="12070" PostId="11719" Score="0" Text="Worth adding: The vanishing gradient problem tends to be about progressive changes over the *depth* of a network, and not directly about the properties of neuron transfer functions." CreationDate="2016-05-25T18:55:40.893" UserId="836" />
  <row Id="12071" PostId="11887" Score="0" Text="Although the premise of my question was not accepted on CV, @SeanEaster helped me with this useful link: https://www.jstage.jst.go.jp/article/bhmk1974/26/1/26_1_29/_article" CreationDate="2016-05-26T00:07:06.757" UserId="18843" />
  <row Id="12072" PostId="11844" Score="0" Text="l have an  experimental proof with my data set, but not sure that whether these results holds good for all." CreationDate="2016-05-26T04:16:36.623" UserId="14789" />
  <row Id="12073" PostId="11893" Score="0" Text="Welcome to Datascience.SE! It's not so much a [change detection](https://en.wikipedia.org/wiki/Change_detection) problem as an [anomaly detection](https://en.wikipedia.org/wiki/Anomaly_detection) problem. [Here](https://vimeo.com/89644371) is a presentation." CreationDate="2016-05-26T07:05:53.970" UserId="381" />
  <row Id="12074" PostId="11903" Score="0" Text="@Pete from what I understood from the question : the OP is looking for &quot;future time series data is in accord with previous time series&quot; - so I assumed the two data sets are correlated and the future process is somewhat dependent on the past process - maybe with hidden states - but I guess that the OP is only looking for a 2 state model, either it changed or it did not change and this threshold/distribution can only be trained/realized by what the OP decide to train on." CreationDate="2016-05-26T08:09:36.813" UserId="18969" />
  <row Id="12075" PostId="11926" Score="0" Text="In my case 'promotion' is a value for the cost of the promotion, fe a television advertisement. So promotion with value 88.31 has fe been in television more times than a promotion with value 12.09." CreationDate="2016-05-26T11:33:40.313" UserId="18787" />
  <row Id="12076" PostId="11919" Score="1" Text="My guess would be The Central Limit Theorem but I have no justification." CreationDate="2016-05-25T17:07:10.713" UserId="19021" />
  <row Id="12078" PostId="11931" Score="0" Text="X_test is the numpy array. Just updated the df_test in the original question, still got the same error ..." CreationDate="2016-05-26T14:56:07.243" UserId="17310" />
  <row Id="12079" PostId="11926" Score="0" Text="Thanks for the clarification. It makes sense now. The concept of plateau applies in your case. Read about it [here](http://www.communicus.com/2015/03/24/expanding-campaign-impact-by-adding-a-second-tv-execution-to-the-pool/)  *Disclaimer*: I tried searching for articles explaining the concept and ended up searching the knowledge base at Communicus, an advertisement research company I worked with before." CreationDate="2016-05-26T15:46:52.650" UserId="18893" />
  <row Id="12080" PostId="11935" Score="0" Text="Could you post a sample CSV?" CreationDate="2016-05-26T17:21:16.180" UserId="381" />
  <row Id="12081" PostId="11891" Score="0" Text="I am not sure I understood the problem. But the model you have chosen, decision trees, is scale invariant. It does not matter if the feature is within [0, NObs] or [0, 1]." CreationDate="2016-05-26T17:51:58.760" UserId="16853" />
  <row Id="12082" PostId="11940" Score="0" Text="Can you provide some more information? What type of SVM are you using—as in, what kernel function, and how did you optimize the parameters? Can you give us some more information on the pre-processing methods you used? Also, I noticed you said there were mostly two classes...how are you handling the other classes?" CreationDate="2016-05-26T19:07:56.530" UserId="13413" />
  <row Id="12083" PostId="11940" Score="0" Text="_&quot;two categories are really close&quot;_ - can you name them (or similar ones)?" CreationDate="2016-05-26T21:19:57.340" UserId="15202" />
  <row Id="12084" PostId="11938" Score="0" Text="I understand that in Computer Vision one wants to be invariant for places in the image, but I don't get the age example." CreationDate="2016-05-26T22:10:30.720" UserId="8820" />
  <row Id="12085" PostId="11938" Score="0" Text="I took that to mean age and weight are dependent, but I'm not sure; it's not my presentation! Or maybe they meant they literally use the wrong column and we want to detect that." CreationDate="2016-05-26T22:19:56.287" UserId="381" />
  <row Id="12087" PostId="11938" Score="0" Text="@sdream I only made a comment; Emre gave the answer. (But you should probably still accept it). The point with CNNs is that not only one feature changes when an object is somewhere else, but a complete pattern is at a different input." CreationDate="2016-05-27T07:29:41.937" UserId="8820" />
  <row Id="12088" PostId="11938" Score="0" Text="@Emre, what I am getting from your answer is that, no matter from which direction a specific property is input, the feature which this specific property causes should be invariant to input dimension of this property. Thanks! :). Still waiting for some more specific answers, otherwise will mark your reply as answer." CreationDate="2016-05-27T07:41:05.637" UserId="19040" />
  <row Id="12089" PostId="11938" Score="0" Text="@MartinThoma, edited my comment. yes I will accept his answer :)" CreationDate="2016-05-27T07:41:46.517" UserId="19040" />
  <row Id="12090" PostId="11946" Score="1" Text="+1, spot on. The fitted response to `Exp` is parabola-shaped (because you have a squared term, but no higher powers), and the parabola opens downwards (because the estimated leading coefficient $\beta_2$ is negative). Depending on where you are on the $x$ = `Exp` axis, you see an increasing response with a diminishing positive slope, or a decreasing response with a negative slope that increases in absolute value. I recommend plotting the estimated equation over the range of `Exp` you have actually observed." CreationDate="2016-05-27T09:04:07.340" UserId="2853" />
  <row Id="12097" PostId="11950" Score="0" Text="Welcome to DataScience.SE! I would recommend extending the existing Spark classes." CreationDate="2016-05-27T16:05:27.130" UserId="381" />
  <row Id="12098" PostId="11948" Score="0" Text="Welcome to DataScience.SE! You want a [hypothesis test](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing) for a change in the mean. Do you only have two data points?" CreationDate="2016-05-27T17:42:59.230" UserId="381" />
  <row Id="12099" PostId="11941" Score="0" Text="What correlation statistic does this method compute, Pearson? &#xA;&#xA;If there is ranked data, categorical, binary, or any data that is not from a normal distribution, then a Pearson correlation will not provide you with accurate results. Also, note his sample size for each variable. I would recommend a nonparametric procedure, else, you risk getting erroneous results." CreationDate="2016-05-27T18:51:15.133" UserId="16220" />
  <row Id="12100" PostId="11934" Score="0" Text="Can you describe the variables? Is the variable of study/interest continuous, ranked, categorical? Are the other other 26 variables continuous, ranked, categorical? Is there a sampling design to go along with how this data was procured? See my comment for the answer below." CreationDate="2016-05-27T19:01:13.747" UserId="16220" />
  <row Id="12101" PostId="11972" Score="0" Text="This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post. - [From Review](/review/low-quality-posts/7375)" CreationDate="2016-05-28T03:04:14.377" UserId="11097" />
  <row Id="12102" PostId="11948" Score="0" Text="These x and y can be considered as columns containing several data points. And I am interested in finding out the percentage variance/ the acceptable change in value from one column to other. So that from it a variable can be created based on that if y is less than this acceptable value then 'drop' if it is more then 'increase' otherwise the value is given as 'neutral'." CreationDate="2016-05-28T07:17:51.837" UserId="9359" />
  <row Id="12103" PostId="11970" Score="3" Text="I'm not that well versed with such things, so I can't really write up a good answer, but you can have a look at `ipywidgets` (examples at https://github.com/ipython/ipywidgets/blob/master/docs/source/examples/Index.ipynb) or `bokeh` (http://bokeh.pydata.org/en/latest/)." CreationDate="2016-05-28T07:38:12.603" UserId="19099" />
  <row Id="12104" PostId="9777" Score="0" Text="What kind of &quot;further processing&quot; are you planning to do? That should determine what representation fits best. For classification and regression, there are methods that can deal with categorical variables inherently, for example decision trees." CreationDate="2016-05-28T09:58:02.040" UserId="12280" />
  <row Id="12105" PostId="4967" Score="0" Text="Did you consider using a non-linear regression method? Or do you need the coefficients for interpretation?" CreationDate="2016-05-28T10:01:19.290" UserId="12280" />
  <row Id="12106" PostId="11976" Score="1" Text="Just to note, this is called multicollinearity." CreationDate="2016-05-28T20:36:07.313" UserId="19094" />
  <row Id="12107" PostId="11977" Score="0" Text="Look up seasonality correction and change detection in time series. There are plenty of books on this matter." CreationDate="2016-05-28T21:22:00.657" UserId="924" />
  <row Id="12108" PostId="11978" Score="0" Text="I hadn't thought of this, it is a good point, and the food example highlights it. This depends very much on what the set, that the OP is trying to predict, represents. In my answer I am assuming something like &quot;set of favourite items&quot;, which are independent enough that you won't get the recipe problem. But if the sets are recipe-like, then your answer would be closer to the mark." CreationDate="2016-05-29T08:42:05.247" UserId="836" />
  <row Id="12110" PostId="11977" Score="0" Text="I'd say all those are anomalous except 10:30-12:00 and 21:00-22:00?" CreationDate="2016-05-29T15:51:40.213" UserId="15527" />
  <row Id="12114" PostId="11938" Score="0" Text="The age example is supposed to highlight a dataset that does not have dimension-hopping. Age and weight do not &quot;hop&quot; or swap values randomly between examples - they are not interchangeable and the example is showing how odd that would be (and how difficult it would make simple tasks such as linear regression). Pixel values in images (and similar data in many signal processing tasks) do interchange or move easily due to the nature of the problem." CreationDate="2016-05-29T18:01:37.870" UserId="836" />
  <row Id="12116" PostId="11981" Score="0" Text="How does this differ from GDBSCAN? Why do you think the intervals have changed?" CreationDate="2016-05-29T20:35:39.310" UserId="924" />
  <row Id="12117" PostId="11987" Score="1" Text="`temp &lt;- list.files(path, pattern = &quot;*.json&quot;, full.names = TRUE); movies &lt;- purrr::map_df(temp, function(x) { purrr::map(jsonlite::fromJSON(x), function(y) ifelse(is.null(y), NA, y)) })`" CreationDate="2016-05-30T01:49:26.550" UserId="710" />
  <row Id="12118" PostId="11987" Score="0" Text="This works exactly as I would I have liked. Thank you. Do you want to put it out as an answer? I will accept it." CreationDate="2016-05-30T02:21:02.703" UserId="10345" />
  <row Id="12119" PostId="11990" Score="0" Text="Welcome to the site, Helene :)" CreationDate="2016-05-30T06:02:28.470" UserId="11097" />
  <row Id="12120" PostId="11882" Score="0" Text="I update some information... can someone help, with example. python would be great" CreationDate="2016-05-30T09:12:25.617" UserId="18931" />
  <row Id="12121" PostId="11972" Score="0" Text="If he take a look at the encog github, he will find some simple examples using neural networks in C# that he can copy and paste. Sorry if this does not help. https://github.com/encog" CreationDate="2016-05-30T12:14:08.787" UserId="19095" />
  <row Id="12122" PostId="11990" Score="0" Text="have you looked at chi square? or Fisher's exact test?" CreationDate="2016-05-30T13:38:09.213" UserId="6478" />
  <row Id="12123" PostId="9163" Score="1" Text="`when the data has a non-linear shape, then a linear model cannot capture the non-linear features` This is a common misconception. First of all, a simple linear regression can model even harmonic series http://stats.stackexchange.com/questions/60500/how-to-find-a-good-fit-for-semi-sinusoidal-model-in-r. Secondly, feature interaction can be introduced and, of course, there are generalized linear model where a non-linear function on the linear terms is introduced (for instance, the logistic regression)." CreationDate="2016-05-30T14:12:16.670" UserId="16853" />
  <row Id="12124" PostId="9162" Score="0" Text="The only rule of thumb I have read is that regressions handle noise better than random forests, which sounds true because decision trees are discrete models, but I never saw this quantitatively tested." CreationDate="2016-05-30T14:14:18.373" UserId="16853" />
  <row Id="12125" PostId="11986" Score="0" Text="These types of questions are generally viewed as off-topic for StackExchange" CreationDate="2016-05-30T14:17:18.303" UserId="21" />
  <row Id="12126" PostId="11978" Score="0" Text="Only the OP can say what they're thinking about ! I agree that your answer applies if the individual objects were to be ranked for each individual" CreationDate="2016-05-30T14:37:23.183" UserId="13686" />
  <row Id="12128" PostId="11981" Score="0" Text="I am afraid that since i modified the algorithm with the mentioned restriction, there is no way to reach the optimal value of the metric which is the one that matches the no-restriction clustering. Could you cliarify your comment about GDBSCAN?" CreationDate="2016-05-30T19:44:42.037" UserId="16250" />
  <row Id="12129" PostId="11981" Score="0" Text="It sounds as if you reinvented a simple instantiation of the GDBSCAN pattern. Also I don't think Silhouette or C-index work well with noise and non-spherical clusters." CreationDate="2016-05-30T19:47:06.050" UserId="924" />
  <row Id="12130" PostId="12001" Score="1" Text="look into fuzzy string matching, partial string matching" CreationDate="2016-05-30T20:14:33.510" UserId="6478" />
  <row Id="12131" PostId="11980" Score="0" Text="Could you provide sample data? It wiĺl make the question more clear." CreationDate="2016-05-30T20:27:06.817" UserId="10620" />
  <row Id="12132" PostId="12000" Score="1" Text="Be aware, however: http://www.stata.com/support/faqs/statistics/stepwise-regression-problems/" CreationDate="2016-05-30T20:52:47.810" UserId="11124" />
  <row Id="12133" PostId="11769" Score="0" Text="Interesting question - I have often wondered the same. I have models in production and I normally rely on the obtained scores from cross-validation or the test set. Though I have noticed that this often fails (with wrong/silly predictions with high confidence) when completely new / unexpected data comes in that hasn't been seen before.. I guess the training set is still too small / &quot;incomplete&quot; - but I have no idea how big would be big enough." CreationDate="2016-05-31T07:36:04.797" UserId="676" />
  <row Id="12134" PostId="11985" Score="0" Text="yeah, you're right. 10 observations are unsufficient... but it's a matter of time. I couldn't do more than 10 ones." CreationDate="2016-05-31T07:52:11.297" UserId="19051" />
  <row Id="12135" PostId="11980" Score="0" Text="can you show us what your sankey diagram looks like?" CreationDate="2016-05-31T11:15:20.637" UserId="6478" />
  <row Id="12138" PostId="11812" Score="0" Text="@Pete The data is collected from an equipment and the configuration shall not change. The variability of the data can be described as a stochastic proccess." CreationDate="2016-05-31T15:58:25.387" UserId="14153" />
  <row Id="12139" PostId="12006" Score="0" Text="Thanks for the comment, I think your advice about communicating with less technical people is really helpful and definitely something I need to work on." CreationDate="2016-05-31T16:41:25.823" UserId="19161" />
  <row Id="12140" PostId="12006" Score="0" Text="I added some info about the objective too" CreationDate="2016-05-31T16:55:36.437" UserId="14904" />
  <row Id="12141" PostId="8421" Score="1" Text="Look up &quot;implicit matrix factorization&quot; and &quot;implicit collaborative filtering&quot;." CreationDate="2016-05-31T17:09:47.677" UserId="381" />
  <row Id="12142" PostId="11974" Score="0" Text="You are dealing with [compositional data](https://en.wikipedia.org/wiki/Compositional_data), which can be statistically modeled by a [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution)." CreationDate="2016-05-31T17:16:46.560" UserId="381" />
  <row Id="12143" PostId="11980" Score="0" Text="If I understand correctly, I think you want a [stacked area chart](https://visage.co/data-visualization-101-area-charts/)." CreationDate="2016-05-31T19:03:55.507" UserId="381" />
  <row Id="12144" PostId="11171" Score="1" Text="Look into [bootstrap aggregation](https://en.wikipedia.org/wiki/Bootstrap_aggregating), or &quot;bagging&quot;; cf. e.g. [Confidence Intervals for Random Forests](http://jmlr.org/papers/volume15/wager14a/wager14a.pdf) or [Prediction intervals for Random Forests](http://blog.datadive.net/prediction-intervals-for-random-forests/)" CreationDate="2016-05-31T19:05:25.513" UserId="381" />
  <row Id="12145" PostId="12006" Score="0" Text="Very helpful, I will keep this in mind moving forward. (Guess I can't up-vote until I have a higher reputation)" CreationDate="2016-05-31T19:18:20.183" UserId="19161" />
  <row Id="12146" PostId="12007" Score="0" Text="Thanks. I am going to start with building reference map as you suggested from static data analysis. May be later if required I will introduce fuzzy string matching on the fly based on @Brandon Loudermilk suggestion" CreationDate="2016-05-31T19:34:51.813" UserId="19147" />
  <row Id="12147" PostId="12003" Score="2" Text="tip #1: never pet a burning dog" CreationDate="2016-05-31T19:54:03.930" UserId="6478" />
  <row Id="12148" PostId="12001" Score="0" Text="Look up [record linkage](https://en.wikipedia.org/wiki/Record_linkage) and [canonicalization](https://en.wikipedia.org/wiki/Canonicalization)." CreationDate="2016-05-31T22:26:15.680" UserId="381" />
  <row Id="12149" PostId="11941" Score="0" Text="I didn't notice the sample size. Regarding the method Pearson is the default but you have the option to change it to Kendall or Spearman. But generally speaking it is a good method to perform one to one comparison although with this sample size the data doesn't tell you anything useful and trustable!" CreationDate="2016-05-31T23:01:20.360" UserId="12535" />
  <row Id="12150" PostId="12010" Score="0" Text="What kind of a graphic is it, a chart? What do you mean by &quot;regenerate&quot;; run a series of commands that duplicates the EPS file? I want to make sure you don't want to merely load it." CreationDate="2016-05-31T23:44:59.720" UserId="381" />
  <row Id="12151" PostId="12003" Score="0" Text="If you do not mind, please share the industry you are in. The maths and the concepts remain the same, however the structure of the data varies and also how one may approach it. The below advises are very apt and if practiced are going to be a big help. I hope by knowing the industry I may be able to share something that you can directly relate." CreationDate="2016-06-01T00:35:45.683" UserId="10345" />
  <row Id="12152" PostId="12009" Score="0" Text="Cool! I'll have a look." CreationDate="2016-06-01T01:45:10.277" UserId="19094" />
  <row Id="12153" PostId="11980" Score="0" Text="@Emre: The stacked area chart captures only half of what I want to visualize. In addition to the change in sales of the buckets, I also want to portray the contribution of each previous bucket in the current one.&#xA;For example, if 20 stores moved from 0-5 tr bucket in Q1 to 5-10 tr bucket in Q2, I want to capture the amount of sales coming from 0-5 tr bucket into 5-10 tr bucket in Q1 and Q2 resp." CreationDate="2016-06-01T06:03:15.977" UserId="15255" />
  <row Id="12154" PostId="11980" Score="0" Text="@BrandonLoudermilk: Added the diagram in the post" CreationDate="2016-06-01T06:03:24.757" UserId="15255" />
  <row Id="12155" PostId="12003" Score="0" Text="I hope whoever interviewed you for this job is now reading this and thinking &quot;why didn't we ask those questions at interview?&quot;." CreationDate="2016-06-01T07:13:45.627" UserId="471" />
  <row Id="12156" PostId="11990" Score="0" Text="@BrandonLoudermilk the chi square test only applies for frequency / counted data, not to measured means. The t-test would be the correct choice to compare the 4cm difference. I'm not sure how to incorporate the initial 2cm difference though.." CreationDate="2016-06-01T08:03:15.737" UserId="676" />
  <row Id="12158" PostId="12014" Score="0" Text="How much RAM memory does your computer have?" CreationDate="2016-06-01T08:42:24.480" UserId="14904" />
  <row Id="12159" PostId="12014" Score="0" Text="16 GB. But I am more worried about the time it takes than the memory consumption, so that's why I'm asking whether faster solutions exist." CreationDate="2016-06-01T09:07:07.177" UserId="19185" />
  <row Id="12160" PostId="12014" Score="0" Text="Well, you are loading and then sorting 27 GB of text of numbers, I feel (not sure if this is true) that you are going to walk into problems with regards to memory regardless. However even if that's not the case, the ecdf has to sort all those numbers. Can you tell us something about the distribution of these numbers?" CreationDate="2016-06-01T09:12:27.947" UserId="14904" />
  <row Id="12161" PostId="12014" Score="0" Text="Not much, they are in the [0,50] interval, mostly condensed below 35. I am able to plot them individually, but together it takes too much time. I am wondering whether I can preprocess each file separately, and then plot them together. Maybe computing the x-y values and plot them instead of computing the ecdf directly on all the values." CreationDate="2016-06-01T09:16:27.697" UserId="19185" />
  <row Id="12162" PostId="12010" Score="0" Text="@Emre It's a chart. Yes, I can re-create the graphic chart using the same commands that originally created it. But I was wondering what my options were if I only had the .eps file and needed to see what the chart looked like." CreationDate="2016-06-01T09:35:23.220" UserId="16928" />
  <row Id="12163" PostId="12013" Score="1" Text="Furthermore, if the EPS format is a problem, any Linux system comes with an `eps2pdf` command to convert it to PDF." CreationDate="2016-06-01T09:49:13.107" UserId="16853" />
  <row Id="12164" PostId="12017" Score="0" Text="What do you mean by `diagonal`?  The one with the column names?" CreationDate="2016-06-01T15:23:30.990" UserId="11097" />
  <row Id="12165" PostId="12003" Score="0" Text="Drj, I will be working in part with data from the manufacturing process and in part with customer feedback data. It seems like a broad spectrum.  I'm coming from academia where the data was produced by my own experiments and I had very clear goals." CreationDate="2016-06-01T15:47:58.597" UserId="19161" />
  <row Id="12166" PostId="12011" Score="0" Text="Thanks, that's great feedback.  I especially like 'defining the objective is too important (and possibly too difficult!) to be left to the client (alone)'.  I will definitely look into CRISP-DM." CreationDate="2016-06-01T15:53:15.223" UserId="19161" />
  <row Id="12167" PostId="12017" Score="0" Text="@Dawny33 exactly" CreationDate="2016-06-01T16:36:28.147" UserId="19191" />
  <row Id="12168" PostId="12017" Score="0" Text="This doesn't make sense. A boxplot only looks at one variable, so how can you have a boxplot of anything *except* the &quot;diagonal&quot;?" CreationDate="2016-06-01T16:52:49.057" UserId="471" />
  <row Id="12169" PostId="12017" Score="0" Text="My data are the result of a comparison. I can explain their semantics but I don't think it's relevant. Some research article used this. Here is an example (page 15): www.tik.ee.ethz.ch/sop/publicationListFiles/zdt2000a.pdf" CreationDate="2016-06-01T17:07:20.550" UserId="19191" />
  <row Id="12170" PostId="12021" Score="0" Text="Thank you for this helpful answer. The name attribute describes a person's first name. So I think, it makes more sense to convert it to character. For taking efficiency into account, maybe it is interesting, that the ratio between the unified vector's length and the original vector's length is approximately 0.5." CreationDate="2016-06-01T17:24:57.497" UserId="19113" />
  <row Id="12171" PostId="12017" Score="0" Text="So what do the rows and columns of your data look like? Give us a sample, and tell us which columns are the factors you want paired off and which constitute factors to remain in the boxplots in each individual &quot;plotlet&quot;?" CreationDate="2016-06-01T17:40:10.060" UserId="471" />
  <row Id="12172" PostId="11980" Score="0" Text="It seems straightforward to extend your illustration temporally. To aid legibility I would visually de-emphasize all buckets except the selected one; the image above is a mess.  I don't know of any off-the-shelf solutions, but you could probably code it up in D3: https://bost.ocks.org/mike/sankey/" CreationDate="2016-06-01T18:03:57.317" UserId="381" />
  <row Id="12173" PostId="12018" Score="0" Text="Character is a string form while factor is a nominal label." CreationDate="2016-06-01T18:21:14.837" UserId="19196" />
  <row Id="12175" PostId="12017" Score="0" Text="@Spacedman please see my edit" CreationDate="2016-06-01T19:38:53.460" UserId="19191" />
  <row Id="12176" PostId="12020" Score="1" Text="What we need is the derivative of the error w.r.t. the weights. In other words, _how the error changes when we change the weights_. But due to the chain rule (illustrated in your image), we end needing the term you mention. To know how the error changes when we change weights, we first need to know how the error changes when the output changes. Then how the output changes when the weights change. It's a chain." CreationDate="2016-06-02T03:43:46.840" UserId="9814" />
  <row Id="12177" PostId="12021" Score="0" Text="I would also add that if/when the character value itself (string) is of interest  to you, for instance if you want to perform substring or regular expressions, you should consider text to avoir multiply calls to `as.character()`." CreationDate="2016-06-02T06:31:32.130" UserId="18722" />
  <row Id="12179" PostId="12014" Score="0" Text="This sequence of rbinds may be [not efficient](http://stackoverflow.com/questions/15673550/why-is-rbindlist-better-than-rbind). I think if you manually write a procedure for counting numbers in prespecified intervals, it would take the same time as to read those files without loading anything in memory." CreationDate="2016-06-02T08:21:29.463" UserId="6550" />
  <row Id="12180" PostId="628" Score="1" Text="You seem to assume that the training error is a decent estimate of the bias. The bias (for the simple case of the MSE loss function) is defined as the expected error you make *on the new data*, when you average your prediction *over all the different training sets*. What makes J_train (not averaged out across training sets, and not using new data) a decent estimate of the bias?" CreationDate="2016-06-02T08:45:07.263" UserId="13901" />
  <row Id="12181" PostId="12030" Score="0" Text="Let me check if I understood correctly. That E (error) term is not for one output of the network but sum of errors on all three outputs of the network. Long story short, we have only one E for the network, not E1, E2 and E3 for each output. Is that true?" CreationDate="2016-06-02T10:58:07.737" UserId="18619" />
  <row Id="12182" PostId="12030" Score="0" Text="Yes there is one E to minimise, which is calculated from the *outputs* of the network compared to the training example labels. However, the usual way to calculate E for multiple outputs in the last layer is simply E1+E2+E3. I am not clear whether your comment refers to 1,2,3 as labels for multiple output neurons in the final layer, or for the layers? Note that if your last layer is e.g. softmax, then in general the partial derivatives are not fully independent (they are if you use the usual multiclass logloss that goes with it - which is one reason to use it)" CreationDate="2016-06-02T11:23:38.257" UserId="836" />
  <row Id="12183" PostId="12032" Score="0" Text="Your data is not at all Poisson distributed so that this is not giving a good fit makes sense, I do not know a better approach however" CreationDate="2016-06-02T11:28:38.440" UserId="14904" />
  <row Id="12184" PostId="12032" Score="0" Text="It seems like possibly the design of the analysis is out of whack in some sense. For a start, have you plotted the new users against either the days or against the cumulative number of promotions? I also note that apparently you expect a non-linear relationship (or at least a relationship that implies a square term - you want to find the 'sweet spot') but you don't appear to have attempted to fit a model that will account for that effect." CreationDate="2016-06-02T11:39:06.137" UserId="18843" />
  <row Id="12185" PostId="6590" Score="1" Text="You didn't mention your context - note that if it's in a business context, every feature that remains in the model is going to require maintenance at some level - there is a literal money cost involved if you have more variables, in the sense that it requires, for example, more data collection effort,  DBA time and programming time. This consideration obviously doesn't apply if it's for a Kaggle comp or similar, and if using an extra 200 of the available features in the dataset gives you a 0.01% boost in performance you're in the money." CreationDate="2016-06-02T11:53:02.083" UserId="18843" />
  <row Id="12186" PostId="12032" Score="0" Text="Yes I have plotted new users against promotion here: http://datascience.stackexchange.com/questions/11915/chose-the-right-regression-analysis/11926?noredirect=1#comment12079_11926 . So by square you mean I should make a regression model where I square the independent variable ?" CreationDate="2016-06-02T12:04:20.700" UserId="18787" />
  <row Id="12187" PostId="11977" Score="0" Text="You have included the mean and median as reference values, but not the standard deviation.  Yet standard deviation is a key component of outlier detection.  Are you able to obtain the standard deviation or not?" CreationDate="2016-06-02T23:43:11.797" UserId="9420" />
  <row Id="12188" PostId="12032" Score="0" Text="Having a quick look at the other question I note that your plots suggest 'new users' has a relationship with promotion, but 'new good users' does not . Yes, squaring the promotion term is broadly what I meant, as a quadratic has a maximum, which you say is what you are trying to find, but again while the 'new users' vs 'promotion' plot looks like a maximum could plausibly found in those data, the plot of 'new good users' doesn't look all that likely to yield such a relationship. I say 'broadly what I meant' because it may really be non-linear, as said previously in the other question by XR SC." CreationDate="2016-06-03T01:36:07.563" UserId="18843" />
  <row Id="12190" PostId="937" Score="0" Text="Using hypothesis tests is a terrible method of feature selection. You'll have to do a lot of them and, of course, you'll get a lot of false positives and negatives." CreationDate="2016-06-03T08:51:59.837" UserId="16853" />
  <row Id="12191" PostId="12041" Score="0" Text="I used matplotlib first then tried seaborn but both were the same. In reality my dataset is a lot bigger and has 981878, but that shouldn't change much right?" CreationDate="2016-06-03T09:24:29.917" UserId="19241" />
  <row Id="12192" PostId="12041" Score="0" Text="Oh, so your plot above is not of those five specified values? See my comment above." CreationDate="2016-06-03T09:54:18.033" UserId="15527" />
  <row Id="12193" PostId="12039" Score="0" Text="Could it be that you have a small group of outliers way at 1e8 while most observations fall in 1e7 range? Histogram uses linearly-spaced bins, so it could result in the image above. Can you `plt.hist()` a small subsample of data?" CreationDate="2016-06-03T09:54:22.220" UserId="15527" />
  <row Id="12194" PostId="12039" Score="0" Text="@K3---rnc Yes, that's definitely it. I can't believe I didn't think about that! The max is 9.825000e+07 where as the means is 2.949922e+05. Thanks!" CreationDate="2016-06-03T10:11:47.667" UserId="19241" />
  <row Id="12195" PostId="5162" Score="0" Text="Try SGD classifier from sklearn with class_weight = “balanced”" CreationDate="2016-06-03T12:18:51.823" UserId="15361" />
  <row Id="12196" PostId="12042" Score="0" Text="Welcome to DS.SE. Your question seems to be focused at the TPU custom chip, rather than a data science question as such. Im wondering if this would be better asked in a hardware/cloud SE? (A qn for the mods?)" CreationDate="2016-06-03T14:29:36.267" UserId="16284" />
  <row Id="12197" PostId="12032" Score="1" Text="Your plot is **wrong**. plotting `point(model$fitted)` will plot the points with 1:N on the X-axis, yet the fitted values are those for the corresponding values of promotion *in the data*. Try plotting `points(data$promotion, m$fitted)`. it should look a bit better, but there's still no obvious linear trend in your data..." CreationDate="2016-06-04T12:18:38.517" UserId="471" />
  <row Id="12198" PostId="12032" Score="0" Text="Also, your model summary doesn't come from the glm call you gave, because the text isn't right. If you can make a reproducible example we can probably help you." CreationDate="2016-06-04T12:20:25.297" UserId="471" />
  <row Id="12199" PostId="11627" Score="0" Text="See if this helps http://stats.stackexchange.com/questions/116856/hierarchical-or-two-step-cluster-analysis-for-binary-data" CreationDate="2016-06-04T15:41:01.680" UserId="19276" />
  <row Id="12200" PostId="11978" Score="0" Text="Sorry for the late reply! I updated my original post with the answer to your questions. I think a lot of the 'recipyness' comes from universal rules about not taking too much of the same stat in all your items, would this be something a neural network would be able to learn? Even so, there is likely a part of the 'recipyness' that comes from certain combinations not working as well and I'm not sure how small or substantial that is." CreationDate="2016-06-04T19:31:27.953" UserId="19067" />
  <row Id="12201" PostId="12063" Score="0" Text="I don't think this is a clustering problem, but finding common subsequences? Word2vec clearly is nonsense to use here." CreationDate="2016-06-04T21:32:38.577" UserId="924" />
  <row Id="12202" PostId="8421" Score="0" Text="[A Gentle Introduction to Recommender Systems with Implicit Feedback](https://jessesw.com/Rec-System/)" CreationDate="2016-06-05T01:36:00.677" UserId="381" />
  <row Id="13202" PostId="11951" Score="0" Text="I think the update to the question makes my answer less usable, although you should maybe try it. If choice of items is flexible enough and driven mostly by the character stats or other data you have, then it may still work. The alternative might be to treat the item selection as a sequence and use RNN, but unless you have label data that actually shows 1st, 2nd 3rd items chosen, then this could be applying *too much* structure to the problem. It does make your problem interesting, I'm not aware of any classifiers that deal with this kind of structure." CreationDate="2016-06-05T06:26:46.433" UserId="836" />
  <row Id="13203" PostId="11951" Score="0" Text="Out of interest, is there a natural way to sequence item decisions, e.g. is there a in-game cost to each item so they could be ranked, or do they take &quot;slots&quot; with big decisions such as weapon type or armour type needing to be covered first?" CreationDate="2016-06-05T06:42:55.780" UserId="836" />
  <row Id="13204" PostId="12070" Score="0" Text="Thanks! From date i have day-of-week, month-of-year. From time i guess i might need hour and minute dummies too. I am sure about the date part. I might have to test for hour and minute. Possibly this will be hour only. But can you imagine 6 dummies from day-of-week, 11 dummies from month-of-year, 23 dummies from hour-of-day. I am just curious if there is any other simple way. Can you elaborate on **encode these two variables using sines and cosines with their respective periodicities (7 and 24). Also create a column for the UNIX/epoch time.** not sure what this means." CreationDate="2016-06-05T07:11:22.187" UserId="13100" />
  <row Id="13206" PostId="12063" Score="0" Text="@Anony-Mousse, can you please elaborate more here. I don't understand why word2vec is a bad idea. Word2vec aims at doing something similar to what I am doing. About common subsequences, I don't think there will ever be an exactly same subsequence two times (except for very small lengths (&lt;3~4) of subsequences)" CreationDate="2016-06-05T09:31:24.513" UserId="19278" />
  <row Id="13207" PostId="12070" Score="1" Text="UNIX/epoch time is the number of seconds since 1st january of 1970, this allows for a linear time variable. The hour of the day for example is a cyclic occurence, after 23 you go back to 0 even though they are very close together, using it as a single variable loses that relationship. By mapping it on a circle with the sine and the cosine function you can maintain that relationship. This creates two instead of one variable but requires less dimensions than using a dummy variable which would require 24 variables." CreationDate="2016-06-05T09:46:58.737" UserId="14904" />
  <row Id="13208" PostId="11951" Score="0" Text="I would have to collect some extra data, but it would be possible to determine in which order the items were bought. As for your second comment, I'm unsure what you're asking. Item's have an ingame cost in gold and are bought over time, you usually build the most important items first for this reason." CreationDate="2016-06-05T10:35:16.190" UserId="19067" />
  <row Id="13209" PostId="12074" Score="1" Text="I am using python, I like your idea, its not that i haven't thought about it. People have some more characters( 'RT' and the handle of an account etc). I could preprocess the data while removing RT and the account handles. I think i will try that now and get back to you." CreationDate="2016-06-05T11:52:21.503" UserId="20288" />
  <row Id="13210" PostId="12080" Score="0" Text="Thanks a lot for your suggestion." CreationDate="2016-06-05T16:22:58.477" UserId="5043" />
  <row Id="13211" PostId="11951" Score="0" Text="I was just wondering if there was some other way to determine a sequence of choices based purely on item stats. The stricter that items can be sequenced in a way that matches how decisions are made in the game, the better match this could be to a sequence-predicting network like an RNN. RNNs are complex though, so maybe first try the simplest per-item classifier and see if it is good enough for your needs." CreationDate="2016-06-05T16:39:48.500" UserId="836" />
  <row Id="13212" PostId="12082" Score="0" Text="You should be able to directly read the csv file using `read.csv`. I am not sure I follow you completely. Are you saying you don't have the original file anymore just the spreadsheet? Or are you trying to read from the spreadsheet which for some reason you must do?" CreationDate="2016-06-05T18:33:28.200" UserId="10345" />
  <row Id="13213" PostId="12072" Score="0" Text="Thanks. Two small questions : what is RNN? do you know some open-source project that could do such crawling, that it would be worth trying before diving into complex algorithms?" CreationDate="2016-06-05T19:26:55.257" UserId="19261" />
  <row Id="13214" PostId="12084" Score="1" Text="I know of the old version of this course which is still using the old paradigm. Maybe the next one is new?" CreationDate="2016-06-05T20:09:01.670" UserId="12241" />
  <row Id="13215" PostId="12072" Score="0" Text="Recurrent Neural Network. But it probably takes one experienced fulltime PhD for a couple of months to make it work - if at all possible. I'd recommend the two above mentioned libraries to help detecting interesting entities. Apart from that, PDF or DOC reading libraries, of course (havent tried any). And in general, definitely using Python with all it's data structures and maybe regex. No sure what would be worth trying beyond that. I doubt anyone has a general solution for your problem yet, but it can be possible to some extend." CreationDate="2016-06-05T21:53:30.997" UserId="723" />
  <row Id="13216" PostId="12076" Score="0" Text="how about missing value in matrix problem? how to apply this?" CreationDate="2016-06-06T03:43:55.550" UserId="20293" />
  <row Id="13217" PostId="12085" Score="0" Text="I don't know about this error, however Spark has significant overhead and I doubt 3GB is enough to function properly" CreationDate="2016-06-06T07:26:42.057" UserId="14904" />
  <row Id="13218" PostId="12082" Score="0" Text="How are you going to get data from a file if it was already lost? how would reading in R help that?" CreationDate="2016-06-06T08:11:30.460" UserId="21" />
  <row Id="13219" PostId="12019" Score="0" Text="I am also very new to the field and so after designing this predictive model I am very interested to understand and gain insights into how professionals approach this fundamental concept. I couldn't find any clear guides online and so thought it would be interesting to start a discussion here. I very much agree with you that my dataset is extremely small. I am intending to begin collecting much more to validate the quality of the model and optimise further." CreationDate="2016-06-06T11:03:17.017" UserId="18713" />
  <row Id="13220" PostId="12032" Score="0" Text="I have now updated my question." CreationDate="2016-06-06T12:08:25.077" UserId="18787" />
  <row Id="13221" PostId="12055" Score="0" Text="What do you mean exactly by unstructured? Neural networks for instance have many parameters and when you fit your data, they find the best match. Could it be that you are looking for &quot;non-parametric&quot; models? All parametric models have a shape (parameters)." CreationDate="2016-06-06T13:50:01.250" UserId="17937" />
  <row Id="13222" PostId="12042" Score="0" Text="Hi, @MarcusD. You have a good point. When I chose to post in DS.SE, it was because here are the users intended to use this new technology. But both communities seem good places to post this question." CreationDate="2016-06-06T13:54:27.837" UserId="19172" />
  <row Id="13223" PostId="12044" Score="0" Text="Thanks @vigos. I'm trying to reach friends at Google to find out more." CreationDate="2016-06-06T13:55:05.830" UserId="19172" />
  <row Id="13224" PostId="12042" Score="0" Text="It is a fair point. the DS community cuts across a number of communities (including hardware, systems, data sources, analytics, visualisation) which is why it exists ..." CreationDate="2016-06-06T14:50:39.177" UserId="16284" />
  <row Id="13225" PostId="12092" Score="1" Text="I'd ask them for a white paper. I can't imagine anybody would be willing to pay for their product without understanding how it works. The people behind TDA at Ayasdi have lots of papers. I have not encountered any research on scheme theory in machine learning." CreationDate="2016-06-06T17:38:15.887" UserId="381" />
  <row Id="13226" PostId="12097" Score="0" Text="Before you test with data: Have you tested the weight gradients by checking against measured gradients? Worth doing if you have implemented backprop algorithm yourself." CreationDate="2016-06-06T18:25:48.447" UserId="836" />
  <row Id="13227" PostId="12082" Score="0" Text="The csv contain a lot of rows (I don't know which is the limit). The xslx does not. Go back to your csv and use that to read. I had your same issue some days ago and I solved in that wat." CreationDate="2016-06-06T18:39:22.080" UserId="10024" />
  <row Id="13228" PostId="12097" Score="0" Text="I have implemented a BPTT algorithm. But can you be more precise about the procedure? Where can i find the measured gradients?" CreationDate="2016-06-06T21:24:11.630" UserId="20321" />
  <row Id="13229" PostId="12096" Score="0" Text="Thank you Eric,for the suggestion.!" CreationDate="2016-06-07T05:27:28.540" UserId="20302" />
  <row Id="13230" PostId="12083" Score="1" Text="The command above cleared my confusion. Yes,you understood my question correctly- I did have the original file,but I had to stick to reading from  the spreadsheet for some reason." CreationDate="2016-06-07T05:31:28.403" UserId="20302" />
  <row Id="13231" PostId="12095" Score="0" Text="Duplicate of http://stackoverflow.com/q/37661101/562769" CreationDate="2016-06-07T07:29:02.767" UserId="8820" />
  <row Id="13232" PostId="12097" Score="0" Text="You measure an approximate gradient for a parameter by calculating the loss for two slightly different values of the param (e.g. change a single weight by +1.0e-3) over the data set whilst keeping all other params the same. I.e. feed forward the network as normal on a small sample data set with two very slightly different weights. Then divide the difference between the loss by the difference you used for the weight param. This value should be close to the value you calculated using BPTT. Do for all weights in a simple network, one by one. This is a kind of unit test for your BPTT algorithm." CreationDate="2016-06-07T07:42:28.160" UserId="836" />
  <row Id="13233" PostId="12101" Score="0" Text="Some questions: 1) Are your documents and queries all the same length, so you have a vector of field &quot;score&quot; the same length for many examples? 2) What data do you have in &quot;past, known results&quot; - do you have a clear relevant/not-relevant label for multiple documents and queries? If the answer is yes to both questions, then this looks a lot like a logistic regression problem (although that would involve changing your averaging routine, it isn't necessary of you don't want). There are other possibilities depending on your answers though." CreationDate="2016-06-07T07:51:55.283" UserId="836" />
  <row Id="13234" PostId="11951" Score="0" Text="I don't think so, it's not like you always build defence before offence or something like that. I'll definitely give your suggestions a try!" CreationDate="2016-06-07T11:38:15.367" UserId="19067" />
  <row Id="13235" PostId="11978" Score="0" Text="@HrishikeshGanu thanks for your suggestion, I will start by trying out Neil Slater's suggestion, just because that would seem like a more convenient option IF it works for my dataset, but if it doesn't then I like your idea and I agree that chance occurences are indeed not interesting and it may even be favorable in this case to filter them out (because angry players might intentionally build something bad just because they want to destroy the game for other players)" CreationDate="2016-06-07T11:41:01.217" UserId="19067" />
  <row Id="13236" PostId="12083" Score="0" Text="@shawn38 Ohh Ok. For R, the source does not really matter once the data is in. I deal with a lot of spreadsheets, but i prefer converting them to csv before reading in R to avoid formatting (especially dates and negative numbers) and other issues that spreadsheets bring. I guess what is good for human users is not necessarily good for R.  I am glad you figured it out." CreationDate="2016-06-07T13:04:25.643" UserId="10345" />
  <row Id="13238" PostId="12101" Score="0" Text="I'm not entirely sure what you're asking in question 1... My &quot;documents&quot; are a collection of fields, with each having one of several data types (free text, dates, categorical, etc. The documents can easily be represented as a row in a spreadsheet/CSV. The queries against these fields depend on the data type (search words/phrases against text fields, date/location &quot;proximity&quot;, categorical set union, etc). Each of these queries results in a score, from 0-1, which indicate the strength of the match for that field." CreationDate="2016-06-07T23:20:52.867" UserId="20338" />
  <row Id="13239" PostId="12101" Score="0" Text="For your second question, yes, I have a collection of past documents and an &quot;outcome&quot; for each. The outcome can be one of several states, but I could reduce this down to just two (i.e. &quot;relevant&quot; and &quot;not relevant&quot;). The outcome is for the whole document, not the individual queries. They are a new way to predict results for a document, so that's why I'm searching for a way to train the appropriate weights that will result in the known outcomes." CreationDate="2016-06-07T23:24:07.797" UserId="20338" />
  <row Id="13240" PostId="12101" Score="0" Text="And again, a binary or categorical outcome would be useful, but my ideal is for the outcome to be a relevance &quot;score&quot;, e.g. a value from 0-1 indicating _how_ relevant the document is." CreationDate="2016-06-08T02:13:02.983" UserId="20338" />
  <row Id="13241" PostId="12110" Score="3" Text="Just to make sure I understood it: &quot;Deconvolution&quot; is pretty much the same as convolution, but you add some padding? (Around the image / when s &gt; 1 also around each pixel)?" CreationDate="2016-06-08T05:00:37.477" UserId="8820" />
  <row Id="13242" PostId="12101" Score="0" Text="For question 1, the important part of my question is whether the number of fields is fixed and same for all documents and queries (I understand each field can have a different data type and is converted to numeric type by your scoring system that combines document with query). Your comment implies that is the case, but you have not directly said so." CreationDate="2016-06-08T06:41:01.823" UserId="836" />
  <row Id="13243" PostId="12101" Score="0" Text="Hi Neil, yes, the number of fields is always the same." CreationDate="2016-06-08T07:34:46.743" UserId="20338" />
  <row Id="13244" PostId="10177" Score="0" Text="How did you solve it?" CreationDate="2016-06-08T10:20:18.770" UserId="20310" />
  <row Id="13246" PostId="12106" Score="1" Text="&quot;analyses at work would run much faster in Java (compared to R or Python)&quot;: in fact, most of Python data science libraries are based on numpy which is in fact devellopped in &quot;pseudo-c&quot; (cython), which is almost as fast as C code. I'm personnally using cython a lot, and this is more or less as simple as python is.&#xA;Just for my 2 cents, I don't have the intention to make a troll..." CreationDate="2016-06-08T12:48:42.707" UserId="3024" />
  <row Id="13247" PostId="12116" Score="0" Text="Thanks, Sean. Very helpful. I hope it's not *too* off-topic, I was aware that was a risk, but in my defense there are a lot of language comparison questions on this SE and I meant for it to be more of an objective comparison to help me understand the tradeoffs. &#xA;&#xA;Regarding Spark I *could* ask for it at any time, I thought about doing so to take advantage of &quot;Sparkling Water&quot;, but there's so much data that would need to be piped into it that it would be a big project and I can't promise my team it would be worth it when we already have Hortonworks and SQL Server and are considering Storm." CreationDate="2016-06-08T12:53:13.847" UserId="2723" />
  <row Id="13248" PostId="12125" Score="0" Text="You're looping over 10 images here, and could loop over 1. Is your question really how to do that? or why they're not classified correctly? because classifying 1 image is something you've already done." CreationDate="2016-06-08T13:50:16.973" UserId="21" />
  <row Id="13249" PostId="12125" Score="1" Text="yes @sean, I have looped over 1 image as well and the result is the same: misclassification, when one would expect something closer to correct classification, given the model accuracy is ~81%. Ten images here is to prove this isn't a fluke. My question is not about how to loop N images, it is about the correct procedure to input a single image against a classifier, if others have experience with this, and why the unexpected results. Thanks for pitching in!" CreationDate="2016-06-08T14:00:09.850" UserId="20374" />
  <row Id="13250" PostId="12125" Score="0" Text="I think the unanswered posts are not relevant here. You have a bug/fault in your logic either in the predictor or the training code export or in how they match up. At the top level your attempted approach looks sound. The other posts presumably also have a bug or fault, and the reason they are unanswered is that the problem is not explainable other than debugging the implementation in detail - answers to those other posts would not help you, and it is likely to be hard work to pick through the code, have a conversation with OP, find the fault. All to fix one likely-to-be unique problem." CreationDate="2016-06-08T15:25:24.017" UserId="836" />
  <row Id="13251" PostId="12125" Score="0" Text="I have in the past found and fixed bugs in NN implementations posted here or SO, just to get the practice. So I do have some appreciation for how much effort it takes, and how little the end Q&amp;A pair is appreciated by anyone other than the OP (and even that is not guaranteed)" CreationDate="2016-06-08T15:28:36.407" UserId="836" />
  <row Id="13252" PostId="12125" Score="0" Text="thanks@NeilSlater, I fully understand what you mean -- it's just interesting that this specific case use is apparently straightforward but not discussed much -- OTH, there's a plethora of tutorials in which a classic dataset (MNIST, CIFAR) is trained and validated against huge test sets  -- I'll keep researching" CreationDate="2016-06-08T15:49:06.617" UserId="20374" />
  <row Id="13253" PostId="12125" Score="0" Text="@pepe: If you are using a validation set, you can test that you are correctly saving and loading the network across two scripts by loading the data as you are here and continuing the training process. If the training and validation losses shoot up to similar to start of training then the problem is likely with model persistence code." CreationDate="2016-06-08T15:54:28.100" UserId="836" />
  <row Id="13254" PostId="12125" Score="0" Text="I just did that: the losses start where the prior training ended and progressively get smaller in successive sessions with saving and loading. So for example I did a run of 200 epochs and reached 0.7 loss and saved the weights/architecture. Then I loaded both and continued 20 epochs from there: it starts at 0.7 and goes down to ~0.6. I did this successively  and it seems convincing the model save/restore is working." CreationDate="2016-06-08T15:59:22.347" UserId="20374" />
  <row Id="13255" PostId="12125" Score="0" Text="OK fix the title maybe? it says this is about &quot;how to test a single input&quot;" CreationDate="2016-06-08T16:29:37.520" UserId="21" />
  <row Id="13256" PostId="12125" Score="0" Text="@SeanOwen sure, I changed the title and removed initial commentary as it does not seem to help. Let me know if you'd like additional modifications." CreationDate="2016-06-08T17:17:59.030" UserId="20374" />
  <row Id="13257" PostId="12117" Score="0" Text="thanks a lot for your answer. But, I am already loading only mini-batches of training samples in the memory, which is why it is not giving me memory issues there. The memory restriction comes in when I try to build vocabulary by iterating over entire documents. While training, the model has to load the vocab datastructure (a table in LUA in my case) all at once. How to handle this problem? Is there a way to partly load vocab in memory while training?" CreationDate="2016-06-08T18:03:18.543" UserId="20358" />
  <row Id="13258" PostId="12125" Score="0" Text="Looking at the Keras sample code, it uses data augmentation by default. Did you switch on zca whitening or center or normalisation? That would change the pixel feature values . . ." CreationDate="2016-06-08T20:47:22.903" UserId="836" />
  <row Id="13259" PostId="8458" Score="0" Text="If you're using other languages (R/SAS) then your answer is correct. However, the question is specific to **sciki-learn**, which **does not** accept categorical/string variables." CreationDate="2016-06-08T21:11:12.313" UserId="17397" />
  <row Id="13260" PostId="12126" Score="0" Text="Are you reading Huang's ELM papers?" CreationDate="2016-06-08T22:28:58.560" UserId="9814" />
  <row Id="13261" PostId="12099" Score="1" Text="The answer given is right, but 'wide format to long format' might be even more specific, also, I recommend reshape2 instead of reshape, reshape is, as I understand it, underdocumented" CreationDate="2016-06-08T22:42:39.797" UserId="20351" />
  <row Id="13262" PostId="12099" Score="0" Text="@Shape Do you know of any more synonyms of &quot;reshape&quot;? It shocks me that when I look for 'reshape' and Weka or Rapidminer I get nothing on google." CreationDate="2016-06-09T06:08:00.690" UserId="16928" />
  <row Id="13263" PostId="12117" Score="0" Text="So, if the vocabulary datastructure is too huge for ex. 2GB, your question is how to use only 1GB of the vocabulary for training. The purpose of building your vocabulary with entire documents is to have a huge vocabulary size. If you don't want to have such a huge vocabulary, better build the vocabulary with fewer documents. Or, during the building of vocabulary set the `min_count` parameter to be high so that only the highly frequent words will be added to the vocabulary." CreationDate="2016-06-09T06:39:20.677" UserId="16024" />
  <row Id="13264" PostId="12085" Score="0" Text="regarding the execstack warning this might help http://linux.die.net/man/8/execstack" CreationDate="2016-06-09T08:47:24.740" UserId="108" />
  <row Id="13265" PostId="12099" Score="0" Text="It's &quot;normalization&quot; in database theory." CreationDate="2016-06-09T08:55:59.057" UserId="15361" />
  <row Id="13266" PostId="12126" Score="0" Text="No, but work based on it (though I should really read that one as well)" CreationDate="2016-06-09T12:16:59.473" UserId="19067" />
  <row Id="13267" PostId="12133" Score="0" Text="Thanks so much! That really got me puzzled!" CreationDate="2016-06-09T12:17:51.103" UserId="19067" />
  <row Id="13268" PostId="6029" Score="0" Text="@Anony-Mousse: Interesting. Do you know any sources?" CreationDate="2016-06-09T13:01:28.033" UserId="20397" />
  <row Id="13270" PostId="12130" Score="0" Text="I updated the answer a little. Thank you." CreationDate="2016-06-09T14:05:06.030" UserId="108" />
  <row Id="13271" PostId="8028" Score="0" Text="We've actually proposed a StackExchange site to specifically help with CS-related education questions like this one. Come support us here: http://area51.stackexchange.com/proposals/92460/computer-science-educators?referrer=9Z3MnermjDx7JWcMHelYkQ2" CreationDate="2016-06-09T14:47:55.283" UserId="20401" />
  <row Id="13272" PostId="12139" Score="0" Text="Latitude and longitude are position data not acceleration data.  Are you suggesting that position vs. time be used to derive acceleration, or do you additionally have acceleration data from the phone's accelerometer?  Could you post some samples of your data?  This will take this question from highly abstract to a more concrete example that we can help you with.  Thanks!" CreationDate="2016-06-09T15:55:07.020" UserId="9420" />
  <row Id="13273" PostId="6029" Score="0" Text="e.g. http://tiborsimko.org/postgresql-mongodb-json-select-speed.html and http://www.enterprisedb.com/postgres-plus-edb-blog/marc-linster/postgres-outperforms-mongodb-and-ushers-new-developer-reality from the other answer. A key reason is: Postgres has good indexes, while indexes in MongoDB are not worth it. Furthermore, Postgres got BSON support and other additions for handling JSON, that did improve performance considerably. That is why it got a lot faster than in the first versions." CreationDate="2016-06-09T17:12:47.157" UserId="924" />
  <row Id="13274" PostId="12106" Score="0" Text="Python because of numpy if you are using anything that will overbook your RAM you need sparsity. JVM based libraries cannot do sparse matrices worth anything. Also, numpy is faster than every matrix library except ND4J (at least in the major calculations like dot product, norm; etc.) ND4J offers a nice comparison chart for their dense matrix speed improvements http://nd4j.org/benchmarking.html. If all you need is dense, consider Scala and the ND4S version of Spark." CreationDate="2016-06-09T18:48:38.537" UserId="8823" />
  <row Id="13275" PostId="12139" Score="0" Text="@AN6U5 Sorry, I might not be able to make it clear enough what I want to achieve then. Longitudinal and lateral just means that I am able to gather acceleration data separately in forward/backward direction (longitudinal) as well as in left/right direction (lateral). I also added a picture for better understanding and I will try to add a data sample by tomorrow too." CreationDate="2016-06-09T19:22:52.853" UserId="15690" />
  <row Id="13276" PostId="12106" Score="0" Text="@MichaelHooreman Fair enough, but like I was saying I just wanted to compare these 2 languages as there are multiple reasons I want to use one or the other. I already use Python, R, and Julia in different use cases. For instance I want to take advantage of being able to use Java or Scala in DS to build familiarity with the Java ecosystem for non-DS applications like hybrid mobile development, etc, etc." CreationDate="2016-06-09T19:28:02.333" UserId="2723" />
  <row Id="13277" PostId="12072" Score="0" Text="I can validate what @Gerenuk said. I know people who are paid big bucks to handle unstructured text. It is not an easy problem. I would use a natural language toolkit with a lot of hard-coded heuristics thrown in." CreationDate="2016-06-09T22:57:01.960" UserId="16853" />
  <row Id="13278" PostId="12140" Score="0" Text="I had though about the weather as well. One idea I had was to subset the data to group, for example 2 groups where one group 'low' has promotion-value less than 50 and one group 'high' has promotion-value higher than 50. Then I would test if there is a significant difference from the two groups. And there actually is (I used Wilcox). Now one could simply make 10 new groups the same way and see if there are groups where there is no significant difference. This means we have found the sweet spot.  Do you think this approach is useful? Thanks." CreationDate="2016-06-10T07:18:03.527" UserId="18787" />
  <row Id="13279" PostId="12145" Score="0" Text="Thanks @BH85, the features are not correlated in time. I would like to hear you opinion on how to select the negative examples." CreationDate="2016-06-10T07:21:05.820" UserId="13891" />
  <row Id="13280" PostId="12144" Score="0" Text="Thank you for your help! I sort of realigned the orientation of smartphone and vehicle in a calibration process. The general idea here is to be able to identify different maneuvers, and from there, classifying them differently. The combination of all the maneuver classification could then lead to a driving style score or classification. However, for now I just have the separated accelerations. I like your way, but I have two questions: Can I do this outlier detection online? And do I have to change anything in order for it to work on the two components separately?" CreationDate="2016-06-10T07:43:57.160" UserId="15690" />
  <row Id="13281" PostId="12144" Score="0" Text="Does this [Image](http://i.stack.imgur.com/IscJn.png) show sort of what you have in mind?" CreationDate="2016-06-10T08:00:43.757" UserId="15690" />
  <row Id="13282" PostId="12125" Score="0" Text="Go over your whole test set again. You might just have had bad luck with 10 images. - another idea, without looking at your code - is that you forgot preprocessing steps" CreationDate="2016-06-10T10:23:00.980" UserId="8820" />
  <row Id="13283" PostId="12147" Score="0" Text="Given that you could get 99.74% accuracy with a model that just predicted &quot;no claim&quot; for your case, why do you think the blog post is applicable? I suggest look at http://datascience.stackexchange.com/questions/1107/quick-guide-into-training-highly-imbalanced-data-sets and  http://datascience.stackexchange.com/questions/4944/how-to-learn-a-classifier-from-a-dataset-with-high-imbalance/4948#4948 to see if they help" CreationDate="2016-06-10T11:58:18.377" UserId="836" />
  <row Id="13284" PostId="12147" Score="0" Text="@NeilSlater the purpose of my assignment is, in fact, to predict the _claims_. I was looking at boosting methods as a possible solution since (from what I understand) it would overcome the high bias problem.&#xA;&#xA;Perhaps my understanding is incorrect (is it?). In any case, I will go through the recommended links." CreationDate="2016-06-10T12:15:55.530" UserId="13450" />
  <row Id="13285" PostId="12147" Score="0" Text="The accuracy metric, which is the one you picked up on in the blog, is the total frequency of true positives plus true negatives. I was merely pointing out that claims of 83% accuracy in someone else's test data mean very little to you. What metric are you actually getting a poor result on and want to improve? Some models are better at dealing with imbalanced training sets without making changes. I don't think you will resolve your problem purely by selecting a different model. However, it is worth a try in a black-box kind of way - just train different models, use a cv set, and pick best." CreationDate="2016-06-10T12:36:26.173" UserId="836" />
  <row Id="13286" PostId="12147" Score="0" Text="The current requirement is to improve the true positive values, ie., the accurately predicted claims. You're right in saying that results on someone else's test data may mean very little in my case. It's only that I particularly dislike the black-box approach and wanted to know what makes the aforementioned class of model &quot;click&quot; and if it was really worth working with. Rookie question, I suppose. Thanks anyway." CreationDate="2016-06-10T12:49:22.590" UserId="13450" />
  <row Id="13287" PostId="12147" Score="1" Text="I'm not sure I could answer that, even with more knowledge of your situation. However, maybe someone else could give insight if they knew a fair bit more about your data and the data from the blog. In a hand-wavy sense (i.e. not good enough for an answer): Neural networks improve on logistic regression where there are strong non-linear relationships between features and target. Tree boosting models, like xgboost, also deal OK with non-linear relationships, and xgboost seems to beat RF routinely in recent Kaggle competitions." CreationDate="2016-06-10T12:58:58.573" UserId="836" />
  <row Id="13288" PostId="12147" Score="1" Text="Most models though will need some pre-processing of training data, or changes to objective to help with such a strong skew. Note you can improve true positive rate to 100% by predicting every customer as &quot;claim&quot; - I guess you are not really *just* using true positive rate as your goal. What *exactly* is the metric you are using, and what value do you currently get that is disappointing? This is key, because without a pre-agreed useful metric, you will be hard pushed to decide whether a different model is an improvement." CreationDate="2016-06-10T13:01:08.367" UserId="836" />
  <row Id="13291" PostId="5996" Score="0" Text="might want to checkout snowflake. It can handle both structured and semi-structured data together. www.snowflake.net" CreationDate="2015-09-24T02:41:53.323" UserDisplayName="user12961" />
  <row Id="13292" PostId="12144" Score="0" Text="No, don't worry about the orientation.  Worrying about the orientation will just make your learner perform poorly and only work for a well-calibrated phone that is perfectly mounted within the car.  Turning the acceleration vector into a scalar acceleration magnitude and scalar speed removes the need for calibration.  If you replace the axes on your image with `speed` and `acceleration magnitude` then this is basically what I am describing." CreationDate="2016-06-10T15:19:21.930" UserId="9420" />
  <row Id="13294" PostId="12127" Score="0" Text="I'm not sure I understand your question. Please review it and describe your problem as input data -&gt; target. Also why did you tag it with apache-spark ?" CreationDate="2016-06-10T16:49:56.800" UserId="5177" />
  <row Id="13295" PostId="12127" Score="0" Text="I want to learn machine learning. How do I predict future alarm from past alarm?" CreationDate="2016-06-10T16:56:20.863" UserId="3230" />
  <row Id="13296" PostId="10087" Score="0" Text="@Dawny33  Wouldn't you want to perform transformations, scaling etc before splitting your dataset into training and testing?" CreationDate="2016-06-10T20:12:56.300" UserId="2647" />
  <row Id="13297" PostId="12154" Score="0" Text="It looks like some vague attempt at constructing (upper) confidence intervals." CreationDate="2016-06-10T21:55:28.153" UserId="12241" />
  <row Id="13298" PostId="12148" Score="0" Text="The rank of a feature does not really translate between different classifiers. To test if the model is nonlinear, see here for example: http://stats.stackexchange.com/questions/35893/how-do-i-test-a-nonlinear-association" CreationDate="2016-06-10T21:58:57.447" UserId="12241" />
  <row Id="13299" PostId="10087" Score="0" Text="@Minu Sometimes before and sometimes after :)" CreationDate="2016-06-11T04:43:29.240" UserId="11097" />
  <row Id="13300" PostId="12148" Score="1" Text="Feature importances are only a suggestion based on &quot;heuristics&quot;. They can be unreliable at times. I'd usually trust random forest more than Lasso." CreationDate="2016-06-11T06:51:55.157" UserId="723" />
  <row Id="13301" PostId="11204" Score="0" Text="@user5276077 So it seems like you are doing feature selection, namely to find out which features are most indicative of identifying the userid. You could try the standard feature selection techniques, such as PCA. My guess is that IP will contain most of the variance since users usually use the same computer / location. Perhaps not." CreationDate="2016-06-11T07:28:56.550" UserId="20445" />
  <row Id="13302" PostId="12144" Score="0" Text="How would you do this as an online analysis task, which classifies while driving from normal to aggressive and even from aggressiv to normal back again?" CreationDate="2016-06-11T07:40:49.953" UserId="15690" />
  <row Id="13304" PostId="12147" Score="0" Text="I see what you mean. I need to re-think and re-plan my approach, thank you for the insight!" CreationDate="2016-06-11T12:16:55.633" UserId="13450" />
  <row Id="13305" PostId="10087" Score="0" Text="@Dawny33 Could you give me a scenario where you would want to perform transformation and scaling after splitting? I'm assuming when you say after splitting, you apply those transformations only to the training data set." CreationDate="2016-06-11T12:56:38.270" UserId="2647" />
  <row Id="13306" PostId="10087" Score="0" Text="@Minu  `I'm assuming when you say after splitting, you apply those transformations only to the training data set` &lt;-- **Yes**" CreationDate="2016-06-11T12:59:15.613" UserId="11097" />
  <row Id="13307" PostId="10087" Score="0" Text="Any reason why you'd perform variable transformations and scaling only to the training data? How would you then adjust the test data to match? Just curious." CreationDate="2016-06-11T13:10:01.343" UserId="2647" />
  <row Id="13308" PostId="11775" Score="0" Text="Just to understand the use case, given the image of a QR code, taken at an angle, you want to map it to some other image of the same QR code, taken at a better angle right? I mean the focus is on localization or QR codes? If localization, then you can use any existing datasets. For example, [ImageNet challenge also has a localization challenge](http://www.image-net.org/challenges/LSVRC/2014/) from where you can get your dataset and benchmarks as well." CreationDate="2016-06-11T14:13:08.357" UserId="75" />
  <row Id="13309" PostId="9767" Score="0" Text="Wouldn't how exactly you model your data depend on the implementation you are using for clustering?" CreationDate="2016-06-11T14:17:56.300" UserId="75" />
  <row Id="13310" PostId="12154" Score="0" Text="It would help if you could share the source of the code." CreationDate="2016-06-11T14:27:42.643" UserId="75" />
  <row Id="13311" PostId="11605" Score="0" Text="Are you using the term *precision* in its normal sense? It usually means true positives as ratio of all known positives. It's not clear what subset you are counting &quot;all positives&quot; out of in order for that to make sense (it seems a little circular to me). Do you actually mean some other ratio has the 0.9 value? Also, how are you determining this 0.9 value - do you have actual non-heuristic ground truth for some data? If so, why not use that as your test data? Testing against heuristics treats your heuristic as ground truth and it is highly unlikely you will then develop a better model than it." CreationDate="2016-06-11T15:19:54.880" UserId="836" />
  <row Id="13312" PostId="12156" Score="0" Text="Do you have pre-defined categories, or are trying to determine them from the data?" CreationDate="2016-06-11T15:27:51.203" UserId="836" />
  <row Id="13313" PostId="12161" Score="0" Text="Shagun, thank you. However, first, I could not find how to get probabilities of estimations, second, looks to me all time series theory assumes random walk, however I want to check hypothesis if variables are at least partly moved by free will of players of the game under some unknown to me constraints and goals, third, IMHO as I do not assume randomness, I cannot detrend etc. I will edit my question." CreationDate="2016-06-11T15:28:22.187" UserId="20448" />
  <row Id="13314" PostId="12156" Score="1" Text="I don't have predefined categories as u said trying to determine them from the data" CreationDate="2016-06-11T15:30:20.760" UserId="15613" />
  <row Id="13315" PostId="12156" Score="1" Text="Another question: Can a thread be considered to be in more than one category at the same time - e.g. Crash and Database - or are you looking to strictly classify into most-appropriate issue for each thread (I think this will be harder). Note I don't know enough to answer your question, just I believe that this information will be useful to someone who can." CreationDate="2016-06-11T15:34:14.137" UserId="836" />
  <row Id="13316" PostId="12149" Score="0" Text="&gt;&gt; A poll of Hillary v Trump D-5 of election is more accurate compared to Hillary v Trump D-200. &lt;&lt;&#xA;But is it twice as accurate? I'd say it's at least 10 times as accurate. :D" CreationDate="2016-06-11T16:27:06.613" UserId="15527" />
  <row Id="13317" PostId="12161" Score="0" Text="I would say you try with different models like linear regression and see which one works for you. Your problem is more concerned with finding the correct model and not the correct tool to implement it. So basically you start with some estimations and see if the estimations and assumptions hold and then you refine." CreationDate="2016-06-11T16:27:41.863" UserId="75" />
  <row Id="13318" PostId="12149" Score="0" Text="Which is why I'm looking method to find the correct decay rate. At least I hope the question #2 will be answered." CreationDate="2016-06-11T16:29:07.137" UserId="20420" />
  <row Id="13319" PostId="12156" Score="0" Text="Yes that's an option too, each thread can contain multiple topics thanks for commenting" CreationDate="2016-06-11T17:03:35.327" UserId="15613" />
  <row Id="13320" PostId="12149" Score="0" Text="Well, assigning more weight to recent time series measurements compared to old is, if the decay is indeed exponential in nature, called [exponential smoothing](https://en.wikipedia.org/wiki/Exponential_smoothing)." CreationDate="2016-06-11T17:20:47.560" UserId="15527" />
  <row Id="13321" PostId="12154" Score="0" Text="@Shagun I added the code. Actually, the data comes from csvfile. So I used numpy to get `std` and `mean`, and I added `x` when I saw a matplotlib example. I would like to know how to interpret `x` value." CreationDate="2016-06-11T17:59:16.513" UserId="17636" />
  <row Id="13322" PostId="12119" Score="0" Text="Thank you for pointing towards a matured thread. The problem is the same one." CreationDate="2016-06-11T20:02:08.773" UserId="9359" />
  <row Id="13323" PostId="12169" Score="0" Text="Looks like a homework problem? Look at https://en.wikipedia.org/wiki/Binary_search_algorithm - it's not really a data science  question" CreationDate="2016-06-12T07:20:39.590" UserId="836" />
  <row Id="13324" PostId="12164" Score="1" Text="I guess you could filter out words based on a certain length or based repeating n-grams." CreationDate="2016-06-11T22:39:31.010" UserId="16971" />
  <row Id="13325" PostId="12170" Score="0" Text="Thanks for your input. I was thinking about brute forcing it indeed, because of my low number of products/stores, but I was hoping, maybe even expecting, there was a smarter way to find an optimal solution." CreationDate="2016-06-12T08:55:35.593" UserId="12302" />
  <row Id="13326" PostId="12070" Score="0" Text="@JanvanderVegt would you have some intuitive description or worked-out example for this kind of transformation. I understood from your answer and comment that such a method exists, but i need a bit more detail as in an example of how to do this. Thanks in anticipation!!" CreationDate="2016-06-12T17:22:48.417" UserId="13100" />
  <row Id="13327" PostId="11876" Score="0" Text="Ah i see. Thanks!" CreationDate="2016-06-12T17:23:45.047" UserId="13100" />
  <row Id="13330" PostId="12171" Score="0" Text="Can you define grammar in this context?" CreationDate="2016-06-13T11:58:08.173" UserId="14904" />
  <row Id="13331" PostId="12171" Score="0" Text="Sure. I meant a format grammar https://en.wikipedia.org/wiki/Formal_grammar and more specifically a context-free grammar https://en.wikipedia.org/wiki/Context-free_grammar ; For example, with this http://www.nltk.org/_modules/nltk/parse/generate.html you can generate sentences from a grammar, BUT they are somewhat drawn in a graph-traversal order, I would like to generate randomly (with uniform distribution over all possible derivations, if possible)" CreationDate="2016-06-13T12:03:13.090" UserId="7966" />
  <row Id="13332" PostId="12140" Score="0" Text="The data is noisy, but the lower bound to the data shows some trend. I would split the data into quartiles per day and look at the trends per quartile : whats  the trend in minimum expected pay off per promotional spend, median, maximum. Then try to identify explanatory differences between these groupings." CreationDate="2016-06-13T12:17:22.900" UserId="13955" />
  <row Id="13335" PostId="12176" Score="0" Text="the `mice` (multiple imputation by chained equations) package is very popular for missing data in R.  Its not exactly inexpensive, but it may be a good balance between `missForest` and `impute`" CreationDate="2016-06-13T13:15:37.523" UserId="14913" />
  <row Id="13336" PostId="12176" Score="0" Text="`mice` has been running for the past 4 hours, appears to be doing better than `missForest` (efficiency-wise). Are there any metrics to actually compare these imputation methods?" CreationDate="2016-06-13T13:28:13.497" UserId="13450" />
  <row Id="13337" PostId="12176" Score="0" Text="Perhaps this is where I should mention that my computational resources are _very_ limited, hence the emphasis on speed and efficiency." CreationDate="2016-06-13T13:36:22.910" UserId="13450" />
  <row Id="13338" PostId="12180" Score="0" Text="Could you attach a few sample images you've got in mind? CNNs might be overkill for the task, but then on the other hand worth studying and experimenting with if your top priority is the usage of neural networks in contrast to &quot;traditional&quot; CV algorithms." CreationDate="2016-06-13T14:13:02.260" UserId="20504" />
  <row Id="13339" PostId="12176" Score="0" Text="Probably the best way to compare them would be to make a fake data set with variables that are similar to yours, artificially remove data, run both methods, and see which was closer to the true missing values (rmse/accuracy).  Probably not worth it in your limited resources scenario though." CreationDate="2016-06-13T14:54:18.723" UserId="14913" />
  <row Id="13340" PostId="12176" Score="1" Text="You can try avg-filling all columns except one, then predict on that column with the data you do have, and use that model to predict the values that are missing.  So you would do this 4 times, with each iteration being predicted on a different column that contains missing values.  Not perfect or super-efficient, but might be easy for a simple algorithm like KNN" CreationDate="2016-06-13T14:57:25.767" UserId="14913" />
  <row Id="13341" PostId="12180" Score="0" Text="@NikoNyrh could provide sample images, but I want to use the technique for various classes of textures. The goal is to extract shader parameters from images. The brick example contains mostly intuitive parameters, but other shaders will probably use parameters that can't be easily obtained by a well engineered algorithm. I'm currently testing different approaches and wanted to give neural networks a try as I can generate &quot;infinite&quot; training data." CreationDate="2016-06-13T17:05:38.547" UserId="20487" />
  <row Id="13342" PostId="12158" Score="0" Text="Thanks, this gives me direction." CreationDate="2016-06-13T17:35:09.790" UserId="20437" />
  <row Id="13343" PostId="12178" Score="0" Text="1. So as per your answer, following anything shouldn't affect the result?" CreationDate="2016-06-13T17:38:31.370" UserId="20472" />
  <row Id="13344" PostId="12055" Score="0" Text="@PeyM87 By unstructured I mean from an unstructured database. that is data that doesn't always have the same number of fields or data format. See [here](https://en.wikipedia.org/wiki/Unstructured_data)" CreationDate="2016-06-13T17:49:55.370" UserId="8517" />
  <row Id="13345" PostId="12171" Score="0" Text="Uniform distribution is a problem if the grammar defines a potentially infinite output. Often generators will limit repeated segments for this reason." CreationDate="2016-06-13T18:54:28.937" UserId="836" />
  <row Id="13346" PostId="12186" Score="0" Text="if you give us a sample dataset, or a fake dataset that is sufficiently like your real dataset, or a picture of a real or fake dataset, you might get a better response. You don't event say if its 2d or 3d -- or 4d..." CreationDate="2016-06-13T20:48:57.663" UserId="471" />
  <row Id="13347" PostId="12186" Score="0" Text="I didn't think it would have to be so specific. Updated it anyways" CreationDate="2016-06-13T20:58:19.193" UserId="20509" />
  <row Id="13348" PostId="12186" Score="0" Text="Ooh that's a lot more interesting than I thought. You've got a whole cloud of points that belong to a large number of different lines and some noisy points that don't, and ideally you want to find all the lines, even the little ones like the 3 or 4 in the bottom right..." CreationDate="2016-06-13T21:12:09.190" UserId="471" />
  <row Id="13349" PostId="12187" Score="1" Text="That's a whole lot of questions. Can you narrow down to one clear one? it may mean opening a new question." CreationDate="2016-06-13T21:34:57.453" UserId="21" />
  <row Id="13350" PostId="12186" Score="0" Text="I'm glad the problem is interesting, now I hope someone can help me with it :)" CreationDate="2016-06-13T21:43:45.367" UserId="20509" />
  <row Id="13351" PostId="12186" Score="0" Text="ah, but its not continuous x,y,T point coordinates but a bunch of binary (0/1) rasters? And if two tracks cross you might get a pixel that belongs to more than one track..." CreationDate="2016-06-13T22:04:59.090" UserId="471" />
  <row Id="13352" PostId="12187" Score="0" Text="Sure. I am revising it soon,Sir." CreationDate="2016-06-14T03:57:06.787" UserId="20513" />
  <row Id="13353" PostId="12186" Score="0" Text="Yes, the points are in a discrete raster, as we get them from a camera. They are available either as frames of a 0/1 raster, or as a list of coordinates of all pixels and frame numbers where the pixels are 1 (this is after denoising and binarizing the image)." CreationDate="2016-06-14T07:29:58.973" UserId="20509" />
  <row Id="13354" PostId="12171" Score="0" Text="Indeed. Let's say that my grammar generates a finite number of possible derivations (including terminals which are a finite set in my case)." CreationDate="2016-06-14T08:03:51.977" UserId="7966" />
  <row Id="13355" PostId="12186" Score="0" Text="Regarding the particles crossing: That might indeed happen, but also the points here are the centroids of filmed particles, so if particles cross, their blobs will merge and we will see the centroid of that merged blob until they have passed each other." CreationDate="2016-06-14T08:09:00.747" UserId="20509" />
  <row Id="13356" PostId="12186" Score="0" Text="Do you know how many lines there should be? That would make things easier" CreationDate="2016-06-14T09:26:48.043" UserId="14904" />
  <row Id="13357" PostId="12186" Score="0" Text="No, and the count of lines / particles will change in every dataset. With sequential RANSAC, which searches line after line, I planned to stop it after the fitted lines drop below a quality threshold." CreationDate="2016-06-14T09:35:32.370" UserId="20509" />
  <row Id="13358" PostId="12188" Score="0" Text="Welcome to the site. Possible to add a small chunk of your data here?  (Would help in answering better) :)" CreationDate="2016-06-14T12:05:11.550" UserId="11097" />
  <row Id="13359" PostId="12189" Score="0" Text="So, is ConnectR a bunch of ScaleR functions (RxDataSource with extensions) that act as adapters to connect to data from SAS, SPSS, Hadoop, Teradata, other ODBCs etc?  &#xA;&#xA;And can DeployR capability be used to integrate/connect to PowerBI or Visual Studio?" CreationDate="2016-06-14T13:07:32.993" UserId="2647" />
  <row Id="13360" PostId="11656" Score="0" Text="&quot;I am now getting most results as class 1 when I test my model.&quot; Why is this surprising? What is the question exactly? You are biasing your model to favor class 1 by creating more observations of that class, so it obviously favors that class. Was this not what you wanted? Have I missed something?" CreationDate="2016-06-14T13:22:59.000" UserId="16853" />
  <row Id="13361" PostId="12127" Score="0" Text="it would be useful to add a snippet of sample data so we understand what you have to work with." CreationDate="2016-06-14T15:03:14.717" UserId="6478" />
  <row Id="13362" PostId="12144" Score="0" Text="I've added a brief statement on online learning.  BTW, online learning is **not** what you mention in your last comment above.  What you describe in your comment is just running the learning algorithm continuously.  Online learning is continuously updating the learning algorithm so that it gives potentially different answers to the classification over time." CreationDate="2016-06-14T15:08:18.903" UserId="9420" />
  <row Id="13363" PostId="12127" Score="0" Text="@BrandonLoudermilk I have added sample data." CreationDate="2016-06-14T15:50:33.257" UserId="3230" />
  <row Id="13364" PostId="12127" Score="0" Text="@kinkajou, the task seems challenging, and i'm not sure if this is a good point to start. by the way you need to tell us a little more about data. first tell us about how many nodes you have and how many alarm types you have, and what you are really intrested in. for example you may be intrested in predicting the next node which has some alarms, or you may want to just predict type of next alarm, or both. By the way predicting events is always challenging." CreationDate="2016-06-14T16:59:02.517" UserId="5200" />
  <row Id="13365" PostId="12127" Score="0" Text="@kinkajou, Do you have any additional data as to why those events occurred? You might want to include those variables into your model.&#xA;&#xA;Depending on the data you have, and the problem you want to solve,&#xA;I can think of a categorical time-series model using something like a Markov Chain." CreationDate="2016-06-14T18:47:31.047" UserId="2647" />
  <row Id="13366" PostId="12187" Score="0" Text="Tried to revise, please see." CreationDate="2016-06-14T19:53:56.243" UserId="20513" />
  <row Id="13367" PostId="12127" Score="0" Text="@eulerleibniz I have edited the question. Actually I am not able to get started." CreationDate="2016-06-14T23:46:02.857" UserId="3230" />
  <row Id="13368" PostId="12188" Score="0" Text="@Dawny33 actually this data is confidential to company and I can't copy paste here...it will be helpful if somebody can throw light on approach and algorithm for mix and unsupervised data." CreationDate="2016-06-15T03:14:09.110" UserId="20517" />
  <row Id="13371" PostId="9326" Score="0" Text="I am struggling with the T.arange(y.shape[0]) part, as I understand this is a list of indices from 0 to n-1 (n being the number of examples in the mini batch), where is this used in the code and for what?" CreationDate="2016-06-15T08:43:36.067" UserId="14904" />
  <row Id="13372" PostId="12204" Score="0" Text="Welcome to the site. Are you open to library recommendations in any programming language? If yes, then do add what is your preferred programming language :)" CreationDate="2016-06-15T10:39:53.607" UserId="11097" />
  <row Id="13373" PostId="12190" Score="0" Text="You are using some regularization method, such as limiting maximum depth or minimum samples per leaf, on your decision tree, right? Otherwise, you may have a awful number of clusters and they can vary dramatically, since the tree will grow until each leaf only has customers of one type which is subject to noise." CreationDate="2016-06-15T11:20:37.200" UserId="16853" />
  <row Id="13375" PostId="12193" Score="0" Text="«&quot;ordinary&quot; neural networks for supervised and deep neural networks for unsupervised tasks.» - I am sorry but your comment is just wrong. Deep neural networks were a set of techniques that were discovered to overcome the vanishing gradient problem which was severely limiting the depth of neural networks. Deep neural networks can be used for both unsupervised (e.g. auto-encoders) or supervised (e.g. convolutional neural networks) problems." CreationDate="2016-06-15T11:26:16.793" UserId="16853" />
  <row Id="13376" PostId="12207" Score="0" Text="Can you please provide the error and the code you are using to try to load the csv file?" CreationDate="2016-06-15T12:14:26.230" UserId="6478" />
  <row Id="13377" PostId="12198" Score="0" Text="You have any experiments about tuning RNN parameters? In particular, I still have stucked in choosing optimal parameter." CreationDate="2016-06-15T14:31:31.487" UserId="12285" />
  <row Id="13378" PostId="12190" Score="0" Text="Exactly! I added conditions on minimum leaf's size. So now I have 13 leaves which is reasonable I think." CreationDate="2016-06-15T14:45:15.527" UserId="20392" />
  <row Id="13379" PostId="12198" Score="0" Text="@TrầnĐứcNhuận: No, sorry. I am still learning about RNNs and don't have much experience yet, just what I have seen in blogs, video lectures." CreationDate="2016-06-15T15:16:46.093" UserId="836" />
  <row Id="13380" PostId="11785" Score="0" Text="@MpizosDimitris Basically what I think he is saying is that, after estimating the coefficients of the logistic function, you could potentially manipulate it to get the inverse function. I think he is right, but I am not sure it would help to build a probability density function which is what you need to then sample observations." CreationDate="2016-06-15T15:21:17.890" UserId="16853" />
  <row Id="13382" PostId="12193" Score="0" Text="I agree. That really is wrong, I removed that part and left the link." CreationDate="2016-06-15T16:58:19.883" UserId="19260" />
  <row Id="13383" PostId="12211" Score="0" Text="https://github.com/pbstark/MX14/blob/master/sprt.ipynb" CreationDate="2016-06-15T17:09:22.380" UserId="75" />
  <row Id="13384" PostId="12211" Score="0" Text="Thank you. Can I use it also for Multiple SPRT?" CreationDate="2016-06-15T17:23:19.223" UserId="18300" />
  <row Id="13386" PostId="12219" Score="0" Text="Hi. I was just about deleting it, as I've read here: http://meta.stackexchange.com/a/254090 that datascience ist the right forum for this question. Sorry for the delay." CreationDate="2016-06-15T19:10:44.763" UserId="18744" />
  <row Id="13387" PostId="12188" Score="0" Text="From what I understand, looks like you are on the right path. Since the data is unsupervised, I'd cluster it into natural groups first, and then use those cluster assignments as the target for predicting the test data.&#xA;&#xA;See here for something similar: http://datascience.stackexchange.com/questions/985/can-i-use-unsupervised-learning-followed-by-supervised-learning?rq=1" CreationDate="2016-06-15T19:13:25.053" UserId="2647" />
  <row Id="13388" PostId="12219" Score="0" Text="OK. I only noticed because I'd never heard of Replicator NNs and searched - the cross-validated question came up. I would agree that Data Science is better place for this question." CreationDate="2016-06-15T19:17:08.590" UserId="836" />
  <row Id="13389" PostId="12200" Score="0" Text="+1 The clue with association rules was good one. Is this the same with timeseries and other dataset?" CreationDate="2016-06-16T01:32:34.823" UserId="3230" />
  <row Id="13390" PostId="11965" Score="0" Text="But since you are anyways alright with false positives as long as the fraction of folks required to be removed from the spammers list is small you don't need to update the filter" CreationDate="2016-06-16T01:59:04.597" UserId="13686" />
  <row Id="13391" PostId="12223" Score="0" Text="your initial thoughts are appropriate." CreationDate="2016-06-16T04:23:56.093" UserId="16145" />
  <row Id="13392" PostId="12204" Score="0" Text="If you don't want to code it, there are commercial packages available." CreationDate="2016-06-16T04:28:31.270" UserId="16145" />
  <row Id="13393" PostId="12222" Score="1" Text="how are you measuring &quot;importance&quot;?" CreationDate="2016-06-16T04:29:43.910" UserId="16145" />
  <row Id="13394" PostId="12177" Score="0" Text="Could you elaborate a little please, correlations in my case are actually functions (of time shift), not values. So if we have for all i in I, Xi is a correlation function to some X0, where Xi(t) shows how correlated X0(w) is to Xi(w+t), then to do linear regression we need I*T dimensions, which I am not sure is possible. I am probably wrong and would love to know why. I completely agree with the last point about giving too much weight to duplicates but I don't quite get the proposed alternative method." CreationDate="2016-06-16T04:37:50.287" UserId="20475" />
  <row Id="13395" PostId="12200" Score="1" Text="You should first decide how large is your &quot;basket&quot;. You can take all the history or some recent events. Afterwards, if you'll be using an existing implementation you will need some data manipulation in order to represent the time series as a basket but this is technical." CreationDate="2016-06-16T06:01:21.057" UserId="13727" />
  <row Id="13396" PostId="12200" Score="0" Text="You might find https://en.wikipedia.org/wiki/Hidden_Markov_model useful also but I would have try the methods above first. HMM might be more complex and prone to error and they are less interpretable." CreationDate="2016-06-16T06:02:52.083" UserId="13727" />
  <row Id="13398" PostId="12207" Score="0" Text="just added that to the original post. Thanks!&#xA;With regards to loading the CSV, I'm using the GUI, havent written any codes to do that." CreationDate="2016-06-16T06:43:17.870" UserId="20565" />
  <row Id="13400" PostId="12226" Score="0" Text="thank you @ Neil Slater. i changed the title." CreationDate="2016-06-16T06:54:48.590" UserId="20585" />
  <row Id="13401" PostId="12222" Score="1" Text="Single decision trees tend to overfit the data - that might be the reason why you see a bigger gap between the most and least important variables - while in reality the identified variables might just contain noise that the single decision tree has fit to. The &quot;diluted&quot; (averaged) variable importances obtained from decision tree ensembles (like random forests) might actually be more realistic. Just guessing though.." CreationDate="2016-06-16T08:10:33.070" UserId="676" />
  <row Id="13402" PostId="12207" Score="1" Text="You said you tried downsampling from 67k to 65k. Try to be more drastic and downsample to 30k or even less. You need to do some debugging like that so that we can narrow down the bug." CreationDate="2016-06-16T08:56:43.680" UserId="16853" />
  <row Id="13404" PostId="12208" Score="0" Text="Awesome answer, thanks. (Just one remark : isn't there something odd with the last sentence of your first paragraph ?)" CreationDate="2016-06-16T12:00:07.017" UserId="18306" />
  <row Id="13405" PostId="12208" Score="0" Text="thanks for edit -- also you can look into pre-packed solutions... e.g., one class SVM e.g., https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf" CreationDate="2016-06-16T12:15:28.410" UserId="6478" />
  <row Id="13406" PostId="9956" Score="0" Text="This is what SVMs do: http://66.media.tumblr.com/0e459c9df3dc85c301ae41db5e058cb8/tumblr_inline_n9xq5hiRsC1rmpjcz.jpg. They try to find the best hyperplane that separates your labeled data. I know there are regression versions of SVM. Is that what you had in mind? I am not I understand your problem." CreationDate="2016-06-16T15:23:57.870" UserId="16853" />
  <row Id="13408" PostId="12232" Score="0" Text="Thank you Jan for your reply. However, is there any other algorithms rather than using topic modeling ones ? because I was asked by my teacher to find the alternatives.." CreationDate="2016-06-17T00:04:59.833" UserId="20590" />
  <row Id="13409" PostId="12239" Score="0" Text="Thanks a lot for such a detailed reply. I shall try oversampling the minority class since it will be quick and proceed to use SVM. Also, I did not know about F1-score. Thanks for that!" CreationDate="2016-06-17T05:13:36.350" UserId="18369" />
  <row Id="13410" PostId="12238" Score="0" Text="+1 for giving to the point answer and for identifying and mentioning packages to implement the ML algorithms :)" CreationDate="2016-06-17T05:16:45.320" UserId="18369" />
  <row Id="13412" PostId="12243" Score="1" Text="Auto-encoders and CNNs are not mutually exclusive architectures. You can have an auto-encoding CNN. E.g. https://swarbrickjones.wordpress.com/2015/04/29/convolutional-autoencoders-in-pythontheanolasagne/ - so for clarification is your question about difference between using a classifier vs auto-encoder for search? Is it important to you to compare a CNN with a non-CNN autoencoder (probably no-one would use a non-CNN autoencoder for image search nowadays)?" CreationDate="2016-06-17T05:55:06.170" UserId="836" />
  <row Id="13414" PostId="12248" Score="1" Text="Mind adding more details?  This is a bit broad (and/or) unclear atm :)" CreationDate="2016-06-17T09:54:44.183" UserId="11097" />
  <row Id="13415" PostId="12248" Score="1" Text="I think this is comparable to visual question answering (VQA), where you pose a (textual) question about an image and the model gives an answer. Sreejithc321 would like to do that but with wikipedia articles about movies instead of images. See https://github.com/abhshkdz/neural-vqa or http://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook for examples. Neural networks can be used for that - but you'll need LOTS of questions &amp; answers &amp; movie articles for training." CreationDate="2016-06-17T10:39:38.930" UserId="676" />
  <row Id="13416" PostId="12240" Score="0" Text="Cheers Ryan, that's a great start - it looks like this is going to be a much bigger job than I had anticipated." CreationDate="2016-06-17T11:42:22.343" UserId="20596" />
  <row Id="13418" PostId="12256" Score="1" Text="And another one - true positive rate." CreationDate="2016-06-17T20:13:39.870" UserId="676" />
  <row Id="13419" PostId="12254" Score="0" Text="Thank you Ryan. Therefore, you mean that is because of the input. I mean, if the inputs of PCA be all relevant to the target the result of that compression by this method will be better than the way we act blindly and employ it on whole dataset (if we presume that most of the features aren't  relevant to the target)? and also can we say that the number of features after CFS is more less than and PCA can handle it better." CreationDate="2016-06-17T21:32:45.060" UserId="12345" />
  <row Id="13420" PostId="12255" Score="0" Text="Thank you mandata. You mean the more efficient(important) and less input we give to PCA, the more accurate results we get?" CreationDate="2016-06-17T21:37:24.707" UserId="12345" />
  <row Id="13421" PostId="12254" Score="1" Text="Yes, CFS is better because it takes variable relevance into account. Therefore, in theory, PCA + CFS should always be better than PCA alone if predicting the target variable is ultimately what you care about. Regarding your last question - PCA isn't impacted by the number of features. It's not feature reduction that makes CFS better; it's the selection of variables that have a strong correlation with the target variable that matters" CreationDate="2016-06-17T22:13:05.477" UserId="12515" />
  <row Id="13422" PostId="12243" Score="0" Text="Yes, non-CNN autoencoders. What you mention problem might result in me posting a new question though, because I read negative things about CNN autoencoders, but I'm rather new to ML in general, so I wouldn't know. I found autoencoders being suggested for image search here: http://deeplearning4j.org/neuralnetworktable" CreationDate="2016-06-17T22:26:52.140" UserId="20616" />
  <row Id="13423" PostId="12244" Score="0" Text="+1 let me look into the documents. It was helpful." CreationDate="2016-06-18T01:11:39.760" UserId="3230" />
  <row Id="13424" PostId="12200" Score="0" Text="HMM and association rule is what I am going after." CreationDate="2016-06-18T01:12:20.543" UserId="3230" />
  <row Id="13425" PostId="12255" Score="0" Text="I would imagine that would depend on the data. What I was stating was that if variables are redundant (highly correlated, or even just (lowly?) correlated but with no other information, then they should be removed.  If they have other information, then I guess it depends how much influence they have on a valid answer." CreationDate="2016-06-18T01:34:16.283" UserId="16145" />
  <row Id="13429" PostId="19" Score="0" Text="It's precisely 1 GB. That's the cutoff in the rule book. There is no room for ambiguity." CreationDate="2016-06-18T02:14:37.553" UserId="2723" />
  <row Id="13430" PostId="12262" Score="0" Text="But i was just informed by others that PCA doesnt eliminate redundant features" CreationDate="2016-06-18T04:45:19.130" UserId="20635" />
  <row Id="13431" PostId="12262" Score="0" Text="@ArmonSafai The way you use PCA for feature selection is to look at the factor loading on the principal components and determine which correlated variables are measuring the same principal component then pick the top 1 or few variables to represent that latent variable, eliminating highly correlated non-distinct features" CreationDate="2016-06-18T04:52:14.227" UserId="2723" />
  <row Id="13432" PostId="12262" Score="0" Text="@ArmonSafai On a related note http://datascience.stackexchange.com/questions/12250/feature-selection-and-pca?rq=1" CreationDate="2016-06-18T04:54:40.067" UserId="2723" />
  <row Id="13433" PostId="12256" Score="0" Text="Thank you, then its sense is close to &quot;retrieve&quot;, &quot;retrieve&quot; itself was better!" CreationDate="2016-06-18T04:55:17.057" UserId="3436" />
  <row Id="13434" PostId="12260" Score="0" Text="can you please explain the &quot; if A &quot;on&quot; = B &quot;off&quot;, and A &quot;off&quot; = B &quot;on&quot;&quot;?" CreationDate="2016-06-18T05:15:19.730" UserId="20635" />
  <row Id="13435" PostId="12256" Score="0" Text="I modified my question a bit." CreationDate="2016-06-18T06:08:13.207" UserId="3436" />
  <row Id="13436" PostId="12255" Score="0" Text="Tank you mandata. I really appreciate your help." CreationDate="2016-06-18T08:09:54.803" UserId="12345" />
  <row Id="13437" PostId="12254" Score="0" Text="Tank you Ryan. I really appreciate your help." CreationDate="2016-06-18T08:10:21.963" UserId="12345" />
  <row Id="13439" PostId="12032" Score="0" Text="Your attempt at grouping the data for the wilcox test doesnt work with the data frame you gave. `data$promotion %in% 1:10` matches where promotion is one of the integer values from 1 to 10, not any value between 1 and 10. So when I try this with the `data` you say you have, I get empty data frames and wilcox.test fails with an error message. So we have no idea what you have done and we can't help you and the business with the wilcox test is so completely different to your original post that you should probably make a new question and it should probably be on the statistics stackexchange site." CreationDate="2016-06-18T08:41:06.227" UserId="471" />
  <row Id="13440" PostId="12032" Score="0" Text="I'm voting to close this question as off-topic because this is a statistics question and needs migrating to the stats site" CreationDate="2016-06-18T08:42:09.373" UserId="471" />
  <row Id="13441" PostId="5128" Score="0" Text="I feel like some parts of this answer are non-intuitive. &quot;For example, if you have two objects both with 10 attributes, out of a possible 100 attributes. Further they have all 10 attributes in common. In this case, the Jaccard index will be 1 and the cosine index will be 0.001.&quot; This would translate to something like ``cosine_similarity(10*[1]+90*[0], 10*[1]+90*[0])``. Of course, the cosine similarity would also be 1 here, as both measure ignore those elements that are zero in both vectors." CreationDate="2016-06-18T10:35:13.640" UserId="20642" />
  <row Id="13442" PostId="12267" Score="0" Text="That's for you to decide, and an exp. in .NET solely, cannot get translated to DS!" CreationDate="2016-06-18T12:25:38.107" UserId="11097" />
  <row Id="13443" PostId="12247" Score="0" Text="(1 - 0) / sqrt(10000) = 0.01 bins?? How does that work?" CreationDate="2016-06-18T13:22:53.950" UserId="15527" />
  <row Id="13444" PostId="12247" Score="0" Text="you square root the number of data points i.e (√144)= 12 initial bins" CreationDate="2016-06-18T13:36:18.517" UserId="17959" />
  <row Id="13445" PostId="12269" Score="0" Text="Sure. (10 more to go)" CreationDate="2016-06-18T09:04:20.593" UserDisplayName="Andrew Grimm" />
  <row Id="13446" PostId="12247" Score="0" Text="Yes, 10k data points, normalized to [0, 1], squared, as above. Gives 0.01 bins." CreationDate="2016-06-18T15:57:27.713" UserId="15527" />
  <row Id="13447" PostId="12070" Score="0" Text="@JanvanderVegt i got different opinion for using sine and cosine transformation for economical data. Basically i have transactional data http://stats.stackexchange.com/questions/57652/using-stl-seasonal-decomposition-with-loess-for-weekly-data/58340#58340" CreationDate="2016-06-18T16:01:39.797" UserId="13100" />
  <row Id="13448" PostId="12267" Score="0" Text="How do we know? You might not like data science. If you do like data science, learn data science. If you don't like it, don't learn it. If you don't know what it is, do some research (it is, after all, science, and that requires research)." CreationDate="2016-06-18T16:59:53.727" UserId="471" />
  <row Id="13449" PostId="12269" Score="0" Text="There are web sites with attributed quotes, if you believe their authenticity then you could just do a lookup, but it might need some fuzzy matching (google site search might help). But &quot;All web sites are bunk&quot;, as Albert Einstein said." CreationDate="2016-06-18T17:01:56.260" UserId="471" />
  <row Id="13450" PostId="12246" Score="0" Text="I'm voting to close this question as off-topic because its a stats question answered here: http://stats.stackexchange.com/questions/798/calculating-optimal-number-of-bins-in-a-histogram" CreationDate="2016-06-18T17:04:18.980" UserId="471" />
  <row Id="13451" PostId="12247" Score="0" Text="I don't remember saying to normalize the data." CreationDate="2016-06-18T21:10:48.623" UserId="17959" />
  <row Id="13452" PostId="12260" Score="0" Text="I was trying to demonstrate two variables whose values are entirely dependent on each other, perhaps a better example would be true/false. If A and B are mutually exclusive where B is always false when A is true, and A is always false when B is true, then there's no need to encode and train on both features, simply encoding one will be just as useful as there is no information gain by encoding both" CreationDate="2016-06-19T01:17:50.633" UserId="18039" />
  <row Id="13453" PostId="12274" Score="1" Text="More reference, if possible?  Something above or below that line (from the text you found this)?" CreationDate="2016-06-19T05:46:40.933" UserId="11097" />
  <row Id="13454" PostId="12277" Score="1" Text="Thanks! In your context, Doesn't it regular consider X as the variable and A as the parameter?" CreationDate="2016-06-19T07:35:36.173" UserId="20655" />
  <row Id="13455" PostId="12277" Score="1" Text="Are you sure X is the parameters? I would say the matrix A is the parameters . . ." CreationDate="2016-06-19T07:37:23.060" UserId="836" />
  <row Id="13456" PostId="12269" Score="0" Text="Your title is about machine learning, but the body of the question is about finding a service. The latter would be off topic here. The former *might* be on-topic, but does seem a little broad, especially if you are not asking about the technical details. I think the migration is in error, it should either be kept on meta.skeptics or closed if not suitable even for meta there. If you want to keep the question open here, you should focus on some details about the problem and what you have tried in a data science context." CreationDate="2016-06-19T07:44:08.407" UserId="836" />
  <row Id="13457" PostId="12269" Score="0" Text="@NeilSlater I ought to have asked for it to be migrated to software recommendations. Feel free to close it." CreationDate="2016-06-19T07:58:30.833" UserId="20659" />
  <row Id="13458" PostId="12269" Score="0" Text="This question appears to be off-topic in terms of being unrelated to data science, but it suggested a different question which I posted (and proposed a potential answer) -&gt;http://datascience.stackexchange.com/questions/12279/can-data-science-be-applied-to-establishing-the-author-of-a-text" CreationDate="2016-06-19T11:33:12.897" UserId="18843" />
  <row Id="13459" PostId="12271" Score="1" Text="This genre of question appears a lot, and I always think of the Mozart anecdote that ends with Mozart saying  'but the eight year old Mozart didn't have to ask'." CreationDate="2016-06-19T11:46:47.840" UserId="18843" />
  <row Id="13460" PostId="12186" Score="0" Text="I think the general term for this kind of problem is the &quot;Subspace clustering&quot;" CreationDate="2016-06-19T15:44:14.753" UserId="10620" />
  <row Id="13461" PostId="12277" Score="0" Text="Thanks. Late night errors :)" CreationDate="2016-06-19T16:27:16.427" UserId="381" />
  <row Id="13462" PostId="12284" Score="0" Text="Can you give more information about this? Like what kind of binary classifiers?" CreationDate="2016-06-19T16:57:32.063" UserId="12557" />
  <row Id="13463" PostId="12247" Score="0" Text="Sorry, it was already normalized. But I just used it as an example. The real data has values between 14.03 and 14.93, roughly normally distributed. So?" CreationDate="2016-06-19T17:05:35.213" UserId="15527" />
  <row Id="13464" PostId="12274" Score="0" Text="This looks like, and its hard to tell from the lack of context, a stats question, and should probably be in an expanded form on the stats stackexchange site." CreationDate="2016-06-19T17:31:18.593" UserId="471" />
  <row Id="13465" PostId="12280" Score="0" Text="How long did it take you to find the answer to the question you yourself asked?" CreationDate="2016-06-19T17:32:33.607" UserId="471" />
  <row Id="13467" PostId="12289" Score="0" Text="I hope I haven't changed the meaning with my edit - if so please revert or correct as you wish (at the least I hope I have shown you enough LaTex formats that you can keep an fix them). I have seen and used mappings of categorical variables to their frequency (and the frequency can be binarised), and this is a similar idea. I expect it *might* work, and it is something worth trying. But whether it works well will depend on your specific case. A large number of categories is going to introduce sampling noise however you then turn into a feature." CreationDate="2016-06-19T19:18:14.377" UserId="836" />
  <row Id="13468" PostId="12283" Score="0" Text="That is basically what I'm doing with RANSAC (except that I use random sampling instead of trying all combinations).&#xA;&#xA;The problem for me isn't fitting some lines, the problem is that I fit too much lines, because just with so many near points, even a skewed line will find enough inliers within any reasonable threshold. So I'm searching for a criterion to distinguish lines that fit &quot;real&quot; lines from others." CreationDate="2016-06-19T21:54:36.740" UserId="20509" />
  <row Id="13469" PostId="12271" Score="0" Text="@RobertdeGraaf very apt." CreationDate="2016-06-19T23:33:40.743" UserId="10345" />
  <row Id="13470" PostId="12291" Score="0" Text="Clear. &#xA;So, what's the alternative to have topics from twitter data-set ?" CreationDate="2016-06-19T23:40:04.720" UserId="20590" />
  <row Id="13471" PostId="12280" Score="0" Text="The question kind of came second - I wrote the answer for a question that was kind off topic and then realised it didn't answer that question at all, but I still thought it was interesting, so I wrote the question around the answer, kind of like Jeopardy." CreationDate="2016-06-19T23:50:15.793" UserId="18843" />
  <row Id="13472" PostId="11502" Score="0" Text="Awesome answer, I think it's a good practice. Thanks a lot." CreationDate="2016-06-20T01:38:27.567" UserId="16756" />
  <row Id="13474" PostId="12070" Score="0" Text="If my data only has date and no time. Should i still use a unix/epoch time variable, other than sine and cosine variable for day of week, week of the year?" CreationDate="2016-06-20T03:33:17.633" UserId="13100" />
  <row Id="13475" PostId="12248" Score="0" Text="@Dawny33 Please check now" CreationDate="2016-06-20T05:39:27.737" UserId="5091" />
  <row Id="13476" PostId="12291" Score="0" Text="I don't think Twitter data clusters. Nor will it have well defined &quot;topics&quot;. It's too diverse. Topic modeling works reasonably when you have e.g. 20news data which has much longer texts, and much better separated topics." CreationDate="2016-06-20T05:40:06.580" UserId="924" />
  <row Id="13477" PostId="12291" Score="0" Text="So my recommendation is: don't bother. Find a way to work without this. *Why* would you need to cluster Tweets? It's not as if there is anything important on Twitter." CreationDate="2016-06-20T05:41:07.523" UserId="924" />
  <row Id="13478" PostId="12248" Score="0" Text="@Sreejithc321 Looks fine now. Thanks for making it clear. [Voted to reopen] :)" CreationDate="2016-06-20T05:43:08.343" UserId="11097" />
  <row Id="13479" PostId="12070" Score="0" Text="Use number of days since $x$, for some arbitrary date $x$, instead." CreationDate="2016-06-20T05:43:56.403" UserId="381" />
  <row Id="13480" PostId="12291" Score="0" Text="I wanna discover topics from a large twitter data-set, that's why. I was thinking that clustering can be a solution.." CreationDate="2016-06-20T06:09:10.870" UserId="20590" />
  <row Id="13481" PostId="12249" Score="0" Text="For example I clastered my data-set into N cluster. And i wanna know the topic of each one.&#xA;How can I define automatically the category X from an unsupervised ML cluster." CreationDate="2016-06-20T06:30:49.003" UserId="20590" />
  <row Id="13482" PostId="12248" Score="0" Text="I have voted to re-open, but I still think it could do with some refining to make it less broad. I suggest drop the implementation question, and ask it separately once you have made an informed decision about the approach (with choice of approach made and some specific examples of data and goals)." CreationDate="2016-06-20T06:56:35.303" UserId="836" />
  <row Id="13483" PostId="12249" Score="0" Text="How about looking at the most common words or phrases from each cluster (filtering out stopwords)?" CreationDate="2016-06-20T07:54:30.777" UserId="12492" />
  <row Id="13485" PostId="12249" Score="0" Text="Looking at the most common words will be like an LDA unsupervised solution that return the topics.&#xA;Thus it will be more easy LDA to my twitter data-set without performing any clustering.&#xA;However I'm searching the LDA alternatives, using unsupervised or supervised techniques..." CreationDate="2016-06-20T09:04:26.597" UserId="20590" />
  <row Id="13486" PostId="12300" Score="0" Text="Thank you Jan. I have already normalized my data (0-1). Now, you mean if I multiply my feature values with say 0.7, is somehow giving importance to it?  Actually increasing variance is equal to giving weight to PCA?" CreationDate="2016-06-20T09:38:40.360" UserId="12345" />
  <row Id="13487" PostId="12300" Score="1" Text="First of all you should standardize it (mean = 0, variance = 1) due to the nature of PCA, it only makes sense if your data is centered around the origin. I'll explain a bit more in an edit" CreationDate="2016-06-20T09:46:11.183" UserId="14904" />
  <row Id="13489" PostId="12300" Score="0" Text="Thank you Jan. I get the point." CreationDate="2016-06-20T09:55:05.230" UserId="12345" />
  <row Id="13490" PostId="12070" Score="0" Text="@Emre Thanks a lot for your patience and help. I will mark this as closed now. I wish i could upvote your answer. Thanks anyways." CreationDate="2016-06-20T09:56:15.153" UserId="13100" />
  <row Id="13491" PostId="12301" Score="0" Text="Thank you. Yes you are right but my problem set and application is a bit different. I have labels and know the importance of this feature but I should deploy a unsupervised method." CreationDate="2016-06-20T09:59:57.787" UserId="12345" />
  <row Id="13492" PostId="10976" Score="0" Text="Multivariate analysis or logistic regression could be implemented if you are able to find enough numerical data from permits . This problem is similar to predicting bad loans . example - http://www.tgslc.org/pdf/tamu_multivariate_analysis.pdf" CreationDate="2016-06-20T11:32:23.700" UserId="20682" />
  <row Id="13493" PostId="12291" Score="0" Text="Good luck then... you are looking for something which probably does not exist." CreationDate="2016-06-20T11:33:30.820" UserId="924" />
  <row Id="13494" PostId="12078" Score="0" Text="Please accept one of the answers as correct, if it helped solve your problem." CreationDate="2016-06-20T11:33:57.180" UserId="10345" />
  <row Id="13495" PostId="12291" Score="0" Text="@amirladhar you should use a text mining technique like LDA or LSA." CreationDate="2016-06-20T11:38:25.613" UserId="16853" />
  <row Id="13496" PostId="12303" Score="0" Text="Thank you for your reply. This however does not solve my issue of not getting the expected revenue right in crossvalidation, you are trying to get me to use a different metric which is not applicable for my use case. It is interesting though (for different use cases)" CreationDate="2016-06-20T11:59:36.570" UserId="14904" />
  <row Id="13497" PostId="12230" Score="0" Text="What do you mean with &quot;topics&quot;? Are you sure that a &quot;topic&quot; from topic modelling (like LDA) matches your concept of topic? Or do you want to do some clustering or labelling task?" CreationDate="2016-06-20T12:03:13.750" UserId="10169" />
  <row Id="13498" PostId="12248" Score="0" Text="you can also use this approach [*gensim Word2Vec Model*](https://radimrehurek.com/gensim/models/word2vec.html)" CreationDate="2016-06-20T11:12:50.957" UserId="10423" />
  <row Id="13500" PostId="12284" Score="0" Text="Have added the information. Do you want more details?" CreationDate="2016-06-20T14:36:44.007" UserId="13686" />
  <row Id="13501" PostId="12230" Score="0" Text="I wanna discover topics by two methods, LDA and doing clustering or labeling tasks." CreationDate="2016-06-20T14:48:22.727" UserId="20590" />
  <row Id="13502" PostId="12303" Score="0" Text="One of your questions was `Is there a good alternative to do it in one model?`. A quantile regression combines your two models for revenue and probabilities." CreationDate="2016-06-20T15:01:45.887" UserId="16853" />
  <row Id="13504" PostId="12297" Score="0" Text="Welcome to DataScience.SE! The traditional way to deal with limited data is to introduce assumptions through Bayesian priors, or stronger regularization. What do you know about your data generation process?" CreationDate="2016-06-20T18:36:02.417" UserId="381" />
  <row Id="13506" PostId="12297" Score="0" Text="Could you tell me how small is your training set? Basically if you could provide more details on regarding your problem would be nice." CreationDate="2016-06-20T20:22:04.487" UserId="20544" />
  <row Id="13507" PostId="11126" Score="0" Text="This is not actually an answer, I am actually doing the same project, a self driving car with Q-learning (RL) (still in the beginning), and I would like to ask if your project code is available some where online, it would be very helpful for me. This is the project I am following right now: https://medium.com/@harvitronix/using-reinforcement-learning-in-python-to-teach-a-virtual-car-to-avoid-obstacles-6e782cc7d4c6#.k5obs1w12&#xA;he uses RL with NN like google deep Q learning (the Atari paper), but he doesn't use Supervised learning. His code is available here: https://github.com/harvitronix/reinfo" CreationDate="2016-06-20T15:56:33.747" UserId="20692" />
  <row Id="13508" PostId="12314" Score="0" Text="Perhaps the overflow occurs after the sum is cast into your variable's type, rather than during the calculation with internal variables? I think theano relies on numpy." CreationDate="2016-06-20T23:46:05.013" UserId="381" />
  <row Id="13509" PostId="12310" Score="1" Text="With that many features and the results that you describe, I would consider it possible that you have not yet found the best parameter regime for your nonlinear SVM.  You haven't spoken to the regularization tuning or distance tuning in the RBF.  Also, I suggest applying several different amounts of PCA, reducing your input dimensions to 75, 50, 25.  Since analogue methods can be significantly effected by noisy examples, its important to tune the model using proper cross validation. I've also seen several cases where PCA performs much better in high variance scenarios than regularization." CreationDate="2016-06-21T05:08:08.520" UserId="9420" />
  <row Id="13510" PostId="12248" Score="0" Text="how gensim Word2Vec Model ? as said I don't have any questions and answers to train. All I have is a set of documents." CreationDate="2016-06-21T05:33:49.570" UserId="5091" />
  <row Id="13511" PostId="12312" Score="0" Text="Thank you for your answer Hobbes. As far as I understand from the code, you (or the author of this document) initialise $\alpha$ and $\beta$ but not $\tau$. I guess this is because if you have calculated all the previous ones you can also compute $\tau$. &#xA;&#xA;But I would like to ask you, how could you set the type of flat prior for $\sigma_{\eta}^{2}$ (the $\sigma_{\eta}^{-2}$ ). ( For the $\beta$ and the $\gamma$ I know that you can just as prior a uniform distribution. &#xA;&#xA;Thanks !" CreationDate="2016-06-21T09:04:25.420" UserId="19212" />
  <row Id="13512" PostId="12314" Score="0" Text="Obviously, the results are going to be different because the optimization process is updating the weights using $\partial W = \eta \times C$. One doing the average, you are using $C:=C/N$. Equivalently, you could also do $\eta:=\eta/N$, where $\eta$ is the learning rate. But if you divide one but not the other, obviously weights are being updated differently." CreationDate="2016-06-21T09:32:46.567" UserId="16853" />
  <row Id="13513" PostId="12279" Score="0" Text="You can. This data mining competition has included in the past author identification contests and the results were very good: http://pan.webis.de/clef16/pan16-web/author-identification.html. You use things like topics, the adverbs they use, etc. They have published papers on the best performing models." CreationDate="2016-06-21T09:41:33.990" UserId="16853" />
  <row Id="13515" PostId="12312" Score="0" Text="This is not my work, just an interesting implementation I found. τ was initialized to 1 in this example.  As for σ2η, this is, I believe, wrapped into the rgamma function and is defined by the shape, alpha + n/2.  So I think it depends on your distribution and the α you decide to use.  I hope I'm not misinterpreting it, this is an interesting problem but beyond my normal expertise." CreationDate="2016-06-21T15:59:25.707" UserId="19161" />
  <row Id="13516" PostId="12327" Score="1" Text="Are you jittering your features or something, because the sanity of matching students with mentors based on their name, gender, **height**, and **weight** has a low likelihood of success and might be illegal. People often try to obtain more anonymity when asking questions by changing or simplifying the problem, but its much easier to answer the actual question rather than a transformation of the question i.e. if you don't have the answer then you likely don't know how to transform it correctly." CreationDate="2016-06-21T16:52:54.887" UserId="9420" />
  <row Id="13517" PostId="12329" Score="0" Text="+1 for trying to treat this as a recommended system.  I would add that you likely need to do some sort of feature engineering and feature extraction to add additional signal to your problem." CreationDate="2016-06-21T16:54:02.347" UserId="9420" />
  <row Id="13518" PostId="12314" Score="0" Text="Agreed, but why does the result changes so drastically in both cases? I mean with sum sometimes cost comes out to be nan, and with mean it comes out to be normal." CreationDate="2016-06-21T17:55:35.710" UserId="13518" />
  <row Id="13519" PostId="12330" Score="1" Text="I would start with [online HDP](https://github.com/blei-lab/online-hdp), which covers all three of your points, to get a baseline before considering other approaches. [Here](https://radimrehurek.com/gensim/models/hdpmodel.html) is another implementation." CreationDate="2016-06-21T18:16:42.680" UserId="381" />
  <row Id="13520" PostId="12332" Score="0" Text="This probably should be posted to the stats SE though." CreationDate="2016-06-21T19:09:22.687" UserId="19161" />
  <row Id="13521" PostId="10853" Score="0" Text="you can always compute n-grams for your corpus and determine their frequency so that they can be displayed in a word cloud (or more accurately a phrase cloud)." CreationDate="2016-06-21T23:07:10.970" UserId="6478" />
  <row Id="13522" PostId="12320" Score="1" Text="Could you be less vague and provide some actual context? What model are you using? Which function from which library? What are you trying to predict?" CreationDate="2016-06-21T23:31:03.723" UserId="12241" />
  <row Id="13523" PostId="12292" Score="0" Text="Just to make sure I understood you correctly: say if we have colours and materials, are you saying I should split my data into $|C|*|M|$ buckets, sort, and create a list for each colour and material separately comprised of relevant row numbers, which I then average to get my $C-&gt;N$ and $M-&gt;N$ mappings?" CreationDate="2016-06-22T04:03:33.733" UserId="20475" />
  <row Id="13526" PostId="12335" Score="0" Text="&quot;What is DTW exactly trying to match? The &quot;shape&quot; of the time series?&quot; I am glad you asked. Keogh and Mueen are giving a tutorial on this at SIGKDD 2016. Email Keogh and he will give you an advance peek at the slides. Yes, DTW is matching the shapes, with invariance to distortions in the time axis." CreationDate="2016-06-22T05:17:36.030" UserId="20734" />
  <row Id="13527" PostId="9408" Score="0" Text="Can you provide a source where they define $y′i=\frac{ki}{N}$? [Here](http://cs231n.github.io/linear-classify/) they define it as a one-hot distribution for the current class label. What is the difference?" CreationDate="2016-06-22T07:47:49.610" UserId="14053" />
  <row Id="13528" PostId="12331" Score="0" Text="Too broad - how your dataset looks like? What did you try so far?" CreationDate="2016-06-22T08:10:39.013" UserId="17290" />
  <row Id="13529" PostId="12340" Score="2" Text="Something similar (instead of dividing x to y, divide the difference to y) is called mean absolute percentage error (https://en.wikipedia.org/wiki/Mean_absolute_percentage_error) but percentages/ratios require absolute zeros. Regression does not have that requirement and the calculations are quite different actually." CreationDate="2016-06-22T08:12:20.453" UserId="8664" />
  <row Id="13530" PostId="12196" Score="0" Text="Hi Rahul, thank you very much for your reply. I've looked into the &quot;rioja&quot; package but I couldn't quite grasp, how the constraints are added to applying the function. In the manual it only states that hierarchical clustering is performed &quot;with clusters constrained by sample order&quot;. Not sure how that would work though. If you are familiar with the use of the chclust()-function I would really appreciate your help." CreationDate="2016-06-22T09:08:39.373" UserId="20373" />
  <row Id="13531" PostId="9408" Score="0" Text="In the MNIST TensorFlow tutorial they define it in terms of one-hot vectors as well." CreationDate="2016-06-22T09:32:51.020" UserId="14053" />
  <row Id="13532" PostId="12343" Score="2" Text="Try the `encoding` parameter [from the documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html)" CreationDate="2016-06-22T09:49:51.400" UserId="11097" />
  <row Id="13533" PostId="12345" Score="0" Text="In the above probability node, actual label value is `1.0` but the predicted label is 0.0 because the probability of 0.0 label is highest, its clearly visible from the probability node. anyway its wrong prediction." CreationDate="2016-06-22T11:37:30.017" UserId="17116" />
  <row Id="13534" PostId="12344" Score="0" Text="It is difficult to answer without knowing anything about your problem, your objectives. So as for labelling, if it is about human labeling, you may be interested in Amazon Mechanical Turk --https://www.mturk.com/mturk/welcome" CreationDate="2016-06-22T11:45:26.813" UserId="18722" />
  <row Id="13535" PostId="12344" Score="0" Text="I want to classify my search results obtained in a search engine as relevant or not(1/0) based on the query entered. So, to build my classifier what should be the minimum size of my data set to get good results." CreationDate="2016-06-22T11:51:36.737" UserId="20748" />
  <row Id="13536" PostId="12147" Score="0" Text="Hi, @NeilSlater I've posted a new question on the obtained characteristics. It'd be awesome if you could check it out [here](http://datascience.stackexchange.com/q/12347/13450). Thanks!" CreationDate="2016-06-22T13:36:45.127" UserId="13450" />
  <row Id="13538" PostId="12347" Score="0" Text="I think you should first split the training / test sets and then use SMOTE on both sets, otherwise you're leaking training data into the test set, which results in too optimistic scores." CreationDate="2016-06-22T14:34:57.970" UserId="676" />
  <row Id="13539" PostId="12347" Score="0" Text="Of course! I'll try that. My assumption was that since I am randomly sampling a mere 2000 entries from 155,000 things wouldn't wrong. But now that I think of it, because I'm also using SMOTE ..." CreationDate="2016-06-22T14:41:17.843" UserId="13450" />
  <row Id="13540" PostId="12351" Score="0" Text="The reason why I didn't convert my comment to an answer is because I didn't have an example which could be helpful to the OP.  So, pl add an example if you can :)" CreationDate="2016-06-22T16:19:59.630" UserId="11097" />
  <row Id="13543" PostId="12196" Score="0" Text="I have edited the answer please check. Can you post some of your data and constrained detail so that I can give more detail." CreationDate="2016-06-22T18:20:38.670" UserId="20517" />
  <row Id="13544" PostId="12354" Score="1" Text="Talk to the managers to understand their data-oriented business goals." CreationDate="2016-06-22T20:14:52.377" UserId="381" />
  <row Id="13545" PostId="12313" Score="0" Text="Without training data (pairs of questions and their answers) you would need to work with unsupervised machine learning algorithms, which extract patterns in the data themselves. There aren't any out of the box algorithms for using these for question answering, but there are various approaches you might try including deep natural language processing techniques (which often require specialized knowledge to implement) and topic modeling techniques such as latent dirichlet allocation. Neither of these will get you all of the way there, but they will provide a basic language model that you can use." CreationDate="2016-06-22T21:08:08.550" UserId="20702" />
  <row Id="13546" PostId="12348" Score="0" Text="Can you simulate data if you don't have any data?" CreationDate="2016-06-22T22:28:04.030" UserId="471" />
  <row Id="13547" PostId="12348" Score="0" Text="I don't have any data on the problem. I was given the problem statement but it's quite open. How would you simulate it?" CreationDate="2016-06-22T22:33:36.397" UserId="10155" />
  <row Id="13548" PostId="12254" Score="0" Text="Ryan, it just occurred to me to ask that, in your idea, in what rare situations the result of employing CFS before PCA will be worse than employing only PCA. I mean has a negative effect.The probable reasons." CreationDate="2016-06-23T00:05:20.823" UserId="12345" />
  <row Id="13549" PostId="12254" Score="1" Text="In most cases I think it would be better, but if I were to play &quot;devil's advocate&quot; I'd say that the approach would be problematic if you implemented CFS in such a way that you selected only the N most useful features thereby completely missing out on some features that had small but still helpful predictive value. If you think this might affect you, I recommend using a technique like ridge regression; it has the effect of being like CFS and PCA combined into a single algorithm" CreationDate="2016-06-23T00:26:01.310" UserId="12515" />
  <row Id="13551" PostId="2406" Score="0" Text="I was brought here by a blog post referencing your updated diagram. I think this is a big improvement on the original Conway version, although I can't quite get past the notion - implied by the size of the overlap - that a Statistics Prof  is someone with equal skills in statistics and communication." CreationDate="2016-06-23T02:02:14.337" UserId="18843" />
  <row Id="13552" PostId="12358" Score="0" Text="I thought I should start with Hadoop and then advance to Spark. Thanks btw." CreationDate="2016-06-23T02:44:13.167" UserId="20766" />
  <row Id="13553" PostId="12358" Score="0" Text="You can still use the Hadoop distributed file system, just don't use Map Reduce." CreationDate="2016-06-23T02:49:24.397" UserId="12515" />
  <row Id="13554" PostId="12358" Score="0" Text="I am a beginner, so as far as I know HDFS is for moving files into the Hadoop ecosystem, how can I use it to do something which is mentioned above, a normal code running across cluster ?" CreationDate="2016-06-23T02:52:06.123" UserId="20766" />
  <row Id="13555" PostId="12358" Score="0" Text="Yes, you use HDFS for storing the data and Spark for your distributed code. There is nothing to gain from learning Map Reduce. Learning it doesn't help you better understand the newer technologies, and it also looks bad on a resume because it indicates the candidate is still new to Big Data (people with several years of Big Big data experience know how obsolete it is and know that it's no longer a marketable skill)" CreationDate="2016-06-23T03:39:47.960" UserId="12515" />
  <row Id="13556" PostId="12358" Score="0" Text="Spark is better than Map Reduce in nearly every single way expect for possibly greedy memory consumption, which only becomes problematic when you have many users on the same Hadoop cluster" CreationDate="2016-06-23T03:43:15.743" UserId="12515" />
  <row Id="13557" PostId="12186" Score="0" Text="Can any assumptions regarding isotropy of the particle velocity (both orientation and magnitude) be made or do these tend to be an anisotropic distribution?  If anisotropic, is there a singular point of origin?  Is there interaction between particles? Is there a force field leading to a lack of straightness or are these statistical fluctuations due only to measurement? Is the vertical axis time?  How do things look if all times are stacked rather than added as an additional dimension.  I have some ideas, but answers to these questions will help focus the solution.  Thanks!" CreationDate="2016-06-23T04:41:37.997" UserId="9420" />
  <row Id="13558" PostId="12186" Score="0" Text="Also, have you tried separating the orthogonal spatial dimensions to produce two 2D plots of x vs t and y vs. t.  Since travel through time is constant, it seems like this would produce parallel world lines that could be tracked more easily in each spatial dimension and then combined for the master solution." CreationDate="2016-06-23T04:48:38.233" UserId="9420" />
  <row Id="13559" PostId="12360" Score="1" Text="Thank you. I split each kernel an trying now." CreationDate="2016-06-23T05:12:52.873" UserId="8752" />
  <row Id="13560" PostId="12360" Score="1" Text="Same results ... in R does provide the correct scores ..." CreationDate="2016-06-23T05:14:58.703" UserId="8752" />
  <row Id="13563" PostId="12346" Score="0" Text="Thanks a lot for your answer.Just one thing.By parameters in model it does not mean for exmple slope and intercept for regression? when you fit let's say a linear regression for example which parameters are fitted in fit method? Normalization parameters or model parameters like slope and intercept?" CreationDate="2016-06-23T07:29:07.823" UserId="15064" />
  <row Id="13564" PostId="12254" Score="0" Text="Thank you. I will test the ridge regression,too." CreationDate="2016-06-23T07:55:46.170" UserId="12345" />
  <row Id="13565" PostId="12196" Score="0" Text="Thanks, edited the question in return ;)" CreationDate="2016-06-23T08:37:17.930" UserId="20373" />
  <row Id="13566" PostId="12186" Score="0" Text="The particles are from an impact experiment. The particles have different speeds and different directions of travel (which are the things I want to detect). They don't have a single point of origin, rather an area of around 20 pixels. The particles don't really interact and travel in straight lines, the fluctuations come from tumbling and noise. Yes, the vertical axis here is time. Stacking all times up or viewing the lines in x-t and y-t does not work, because the points become too densely packed and you can't distinguish lines anymore. Thanks for your interest!" CreationDate="2016-06-23T08:42:03.607" UserId="20509" />
  <row Id="13567" PostId="12186" Score="0" Text="Addendum: the directions are within a cone with an opening of around 30°." CreationDate="2016-06-23T08:58:08.647" UserId="20509" />
  <row Id="13568" PostId="12358" Score="0" Text="Well, Thanks, I will switch to Spark, can you suggest me some index topics to learn ?" CreationDate="2016-06-23T09:10:43.960" UserId="20766" />
  <row Id="13569" PostId="12292" Score="0" Text="I added a small numeric example which should make things a little bit clearer" CreationDate="2016-06-23T09:43:12.940" UserId="14904" />
  <row Id="13570" PostId="12345" Score="0" Text="hi @krishna prasad when i give model.transform(train) it says naivebayesmodel has no attribute named transform." CreationDate="2016-06-23T11:02:20.803" UserId="9322" />
  <row Id="13571" PostId="12358" Score="0" Text="I recommend learning about Spark &quot;actions&quot; and &quot;transformations&quot;. Those are the basics" CreationDate="2016-06-23T12:10:44.033" UserId="12515" />
  <row Id="13572" PostId="12358" Score="0" Text="Okay, Thanks for the help!" CreationDate="2016-06-23T12:15:38.453" UserId="20766" />
  <row Id="13573" PostId="12346" Score="0" Text="I mean parameters internal to the transforms ($\mu$ and $\sigma$ in case of StandardScaler). Whatever transform's `get_params()` method returns. See this chapter on imputation, for example: http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values" CreationDate="2016-06-23T14:40:19.633" UserId="15527" />
  <row Id="13574" PostId="12365" Score="0" Text="Are you attempting to employ KNN as a smoothing technique?  I'm not really seeing your endgame." CreationDate="2016-06-23T16:20:00.163" UserId="9420" />
  <row Id="13576" PostId="12357" Score="0" Text="I agree that MongoDB is not a good choice for storing analytical data but it was a good choice when the product was designed previously. Analytical capability is a new requirement (just an idea so far) and I want to see if it is feasible." CreationDate="2016-06-23T17:32:07.060" UserId="20722" />
  <row Id="13577" PostId="12368" Score="0" Text="Can you clarify what &quot;gini&quot; you're referring to? Gini for classification trees is a measurement of node purity, while the KS test is used to determine if your sample follows a specific pre-defined distribution. I'm not sure how comparable these are. Is this the Gini that you're working with? Or do you mean the Gini that's somewhat similar to the AUC measurement, and is used in terms of comparing model gains and performance?" CreationDate="2016-06-23T19:20:35.607" UserId="18495" />
  <row Id="13578" PostId="1192" Score="0" Text="http://openrefine.org" CreationDate="2016-06-23T21:05:04.940" UserId="381" />
  <row Id="13579" PostId="12345" Score="0" Text="@VigneshMohan Please check it now, I have changed the model now its proper way of doing ML using pipeline. Hope this will work for you." CreationDate="2016-06-24T02:09:42.610" UserId="17116" />
  <row Id="13580" PostId="12186" Score="0" Text="&quot;Stacking all times up or viewing the lines in x-t and y-t does not work, because the points become too densely packed and you can't distinguish lines anymore&quot; Isn't that kind of the point?  Why not standardize your data, then collapse or scale down time and stretch out (scale up) space and then employ a density based clustering algorithm like DBSCAN?" CreationDate="2016-06-24T05:19:21.217" UserId="9420" />
  <row Id="13582" PostId="12381" Score="0" Text="This is not exactly what I was asking.  I was asking for each sample you feed a trained RF and ask it to predict a class or regression value, what features contribute to that specific sample being assigned that class or value .  This question was partly answered on crossvalidated where I crossposted here:  http://stats.stackexchange.com/questions/174229/feature-importance-for-random-forest-classification-of-a-sample    .  Additional information and an implementation can be found here:  http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/" CreationDate="2016-06-24T07:17:15.350" UserId="13002" />
  <row Id="13583" PostId="12346" Score="0" Text="Thx a lot for clarification!" CreationDate="2016-06-24T08:04:04.737" UserId="15064" />
  <row Id="13585" PostId="12186" Score="0" Text="@AN6U5 I don't quite understand how that should help, that way I'm losing information that distinguishes lines. And also DBSCAN is completely isotropic, so I think it won't find separate lines if they are too close. Here's an image where I ran DBSCAN over all points in the XY plane (ignoring their time coordinate): http://i.imgur.com/CVp1xUP.png" CreationDate="2016-06-24T09:26:36.903" UserId="20509" />
  <row Id="13586" PostId="12355" Score="0" Text="recall=0 shows that predictions for all classifiers are constantly negative, so accuracy is simply the proportion of negative elements in the test set. Try making the training set more balanced by using `class_weight` parameter or by subsampling." CreationDate="2016-06-24T10:40:37.590" UserId="6550" />
  <row Id="13587" PostId="12303" Score="0" Text="I'm sorry about that comment I was a little grumpy. You might be right that this is what I need, I'll read up about it a bit more, thanks." CreationDate="2016-06-24T11:35:11.377" UserId="14904" />
  <row Id="13588" PostId="12375" Score="0" Text="I'm voting to close this question as off-topic because this is a simple R problem and not data science enough." CreationDate="2016-06-24T12:13:06.460" UserId="471" />
  <row Id="13589" PostId="12375" Score="0" Text="Tough one; since R is so commonly used in data science, I could imagine some basic R questions being OK here, even if they might be somewhat better on Stack Overflow." CreationDate="2016-06-24T12:50:28.397" UserId="21" />
  <row Id="13590" PostId="12383" Score="0" Text="Please see my edit." CreationDate="2016-06-24T13:13:43.483" UserId="3151" />
  <row Id="13591" PostId="12376" Score="0" Text="I don't know how to integrate it with my code." CreationDate="2016-06-24T13:16:02.703" UserId="3151" />
  <row Id="13592" PostId="12186" Score="0" Text="Okay, your right.  Its tough to grasp the problem without being able to play with the data." CreationDate="2016-06-24T14:39:19.223" UserId="9420" />
  <row Id="13593" PostId="12397" Score="0" Text="Yeah, I am trying to vectorize the update equations for ui and vj. As you pointed out correctly, for each i I am summing over all j, which is very costly. I want to vectorize ui." CreationDate="2016-06-24T15:19:51.347" UserId="13518" />
  <row Id="13594" PostId="12394" Score="0" Text="Zip codes are polygons/contiguous objects, so they don't have a natural distance from each other. If you want to find distances between zip codes you'll need to first identify a &quot;center&quot;. This is obviously subjective to the use case. If you want to use curvature of the earth you'll need to project the data." CreationDate="2016-06-24T15:48:52.097" UserId="16220" />
  <row Id="13595" PostId="12188" Score="0" Text="You say this is an unsupervised problem, but then you say you have &quot;categorised data&quot;. This is contradictory; what's the task here really?" CreationDate="2016-06-24T16:26:01.320" UserId="21" />
  <row Id="13596" PostId="12398" Score="0" Text="I'm not sure I'd call either of those &quot;classification&quot;, but it does sound like a clustering problem. SOMs aren't really a clustering technique as much as a form of dimension reduction." CreationDate="2016-06-24T16:27:03.007" UserId="21" />
  <row Id="13597" PostId="12398" Score="0" Text="That is a good point. One possibility is to build an SOM, and then cluster with k-means once the SOM is produced." CreationDate="2016-06-24T16:44:33.857" UserId="13974" />
  <row Id="13598" PostId="12398" Score="0" Text="Additionally @SeanOwen : I can't comment on his post (not enough reputation yet), however I believe by &quot;categorised&quot; data he did not mean the original dataset,  but one produced after an initial clustering task." CreationDate="2016-06-24T16:48:17.527" UserId="13974" />
  <row Id="13599" PostId="12383" Score="0" Text="Yes, when using the `read.table()` call above I get 1217 observations of 33 variables" CreationDate="2016-06-24T16:52:12.283" UserId="14625" />
  <row Id="13600" PostId="12366" Score="0" Text="I don't think Orange has anything like that. How would you design a widget for this? If seems a reasonable enhancement to have, why not submit them a feature proposal, or even a pull request? https://github.com/biolab/orange3" CreationDate="2016-06-24T18:08:33.150" UserId="15527" />
  <row Id="13601" PostId="12394" Score="0" Text="I was given latitudes and longitudes in my dataset that I will treat as the centers of the zip codes for calculating distances." CreationDate="2016-06-24T18:56:53.310" UserId="20835" />
  <row Id="13602" PostId="12386" Score="1" Text="Did you read the Wikipedia articles on [BFGS](https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm) and the [Hessian matrix](https://en.wikipedia.org/wiki/Hessian_matrix)?" CreationDate="2016-06-24T22:22:23.460" UserId="381" />
  <row Id="13603" PostId="12387" Score="0" Text="can you please give counter examples for both cases i stated? I cant seem to understand this intuitively." CreationDate="2016-06-25T06:12:55.767" UserId="20635" />
  <row Id="13604" PostId="12400" Score="0" Text="what exactly do you mean when you say relationships may not be intuitive? Can you please give me an example?" CreationDate="2016-06-25T06:17:01.900" UserId="20635" />
  <row Id="13605" PostId="12400" Score="0" Text="My comment about intuitiveness was with respect to _a priori_ feature selection, i.e.: when not using automated or embedded techniques.  In some domains an expert may make an intuitive guess about what features are relevant (e.g.: temperature and ice cream sales), whereas in other domains such intuition is not possible (e.g.: gene expression microarrays; sports telemetry data for specific physical activities).  There is, of course, a wide variety of possible cases in between." CreationDate="2016-06-25T07:10:43.750" UserId="14625" />
  <row Id="13606" PostId="12400" Score="0" Text="But, to wrap it back around to your question, it is a good idea to reduce features in order to reduce training time and reduce the chance of overfitting and bias, but only when not using embedded feature selection techniques or after performing an automated sweep to verify significance. [Another short article](http://machinelearningmastery.com/feature-selection-to-improve-accuracy-and-decrease-training-time/)" CreationDate="2016-06-25T07:20:26.263" UserId="14625" />
  <row Id="13608" PostId="12383" Score="0" Text="The data is loaded by your script but there is a problem with one of my text variables: language . see my edit." CreationDate="2016-06-25T15:56:39.623" UserId="3151" />
  <row Id="13610" PostId="12404" Score="0" Text="but intuitively how does it make sense for it to be symmetric?" CreationDate="2016-06-25T17:51:36.583" UserId="20635" />
  <row Id="13612" PostId="12404" Score="0" Text="According to the definition mutual information is a measure of how the joint distribution of two random variables deviates from the case where they are independent from each other. For the joint distribution there's no asymmetry, so mutual information is symmetric." CreationDate="2016-06-25T18:29:09.330" UserId="16213" />
  <row Id="13613" PostId="12404" Score="0" Text="@aventurin But i was told from many sources that mutual information is something like  &quot;I(X;Y) measures the average reduction in uncertainty of&#xA;X that results from knowing Y&quot;" CreationDate="2016-06-25T19:01:18.570" UserId="20635" />
  <row Id="13615" PostId="12404" Score="0" Text="I have extended my answer. Does this help?" CreationDate="2016-06-25T21:16:47.180" UserId="16213" />
  <row Id="13616" PostId="12383" Score="0" Text="It doesn't necessarily solve the character encoding issues, but I realized that &quot;#&quot; is the default comment character; comment processing should be ignored:&#xA;`dev &lt;- read.table(&quot;iran_it_status_1394_detail_data_jadi_net.tsv&quot;,&#xA;                   header=TRUE, sep=&quot;\t&quot;,&#xA;                   na.strings=&quot;&quot;, encoding=&quot;UTF-8&quot;,&#xA;                   stringsAsFactors=FALSE, skipNul = TRUE, fill=T, quote=&quot;&quot;, comment.char=&quot;&quot;)`" CreationDate="2016-06-26T01:31:44.603" UserId="14625" />
  <row Id="13617" PostId="12404" Score="0" Text="Well back to my question: intuitively how does it make sense for it to be symmetric? given what we just said is the definition?" CreationDate="2016-06-26T02:06:56.347" UserId="20635" />
  <row Id="13620" PostId="12404" Score="0" Text="The intuition is the the mutual information of two variables is indeed, the mutual, common information. Once you have the mutual information of A &amp; B and you have B, than H(B) &lt;= H(B|A). The reduction in entropy is the results of knowing the mutual information." CreationDate="2016-06-26T06:42:31.313" UserId="13727" />
  <row Id="13621" PostId="12414" Score="0" Text="Lol.... thx for clearing out this confusion. :)" CreationDate="2016-06-26T09:44:40.970" UserId="20873" />
  <row Id="13622" PostId="12375" Score="1" Text="@SeanOwen Useful criterion: If it might be useful to non data-science R users, it should go in Stack Overflow. There's not a single byte of data science in this question. Any R user might want to read a data file like this in. Hence close, move to stack overflow." CreationDate="2016-06-26T10:43:03.510" UserId="471" />
  <row Id="13623" PostId="12375" Score="0" Text="Are there non-data-science R users? anything you'd do with R seems like part of data science to me. I think this is more on-topic for SO, but that's different from blocking it here. Some SEs overlap. People have to do stuff like this to do their data science work." CreationDate="2016-06-26T11:22:50.260" UserId="21" />
  <row Id="13624" PostId="12283" Score="1" Text="I'm not sure if it is realy the same approach. I do not distinct between the point **on a line** and **outlier**. I'm considering if **two vectors** may or may not belong to a same line. I thing this could be much more exact. Additioanly I use parameters *line width*, *minimum line length* and *minimum line points* to control the selection." CreationDate="2016-06-26T14:23:11.950" UserId="10620" />
  <row Id="13625" PostId="12418" Score="0" Text="k-means is really not well suited for 1-dimensional data" CreationDate="2016-06-26T15:29:55.760" UserId="14904" />
  <row Id="13635" PostId="12283" Score="0" Text="ok, I see. Though with 10k points and (10E+5 choose 2) = 5E+11 possible pairs, I will have to do random sampling. Also this is probably quite sensitive on deviations from a straight line, which might change the intercept. But I'll give it a try! Thinks like minimum length and minimum no. of points on line I already used in my attempts to clean up the results." CreationDate="2016-06-26T19:50:43.287" UserId="20509" />
  <row Id="13636" PostId="12186" Score="0" Text="@AN6U5 I'll ask my supervisor if I may share some sample data." CreationDate="2016-06-26T19:51:26.177" UserId="20509" />
  <row Id="13637" PostId="12432" Score="0" Text="http://ankivil.com/installing-keras-theano-and-dependencies-on-windows-10/" CreationDate="2016-06-26T20:57:12.783" UserId="381" />
  <row Id="13644" PostId="12366" Score="0" Text="I thought about making a feature proposal but I didn't know if I was not able to find them because there not present or because I'm a beginner.&#xA;&#xA;I'll make a issue on github, &#xA;Thank you :)" CreationDate="2016-06-27T08:04:08.407" UserId="14562" />
  <row Id="13645" PostId="12438" Score="0" Text="There are so many possible answers that you'd have to narrow it down. What else are your requirements?" CreationDate="2016-06-27T10:54:46.847" UserId="21" />
  <row Id="13646" PostId="12438" Score="0" Text="@SeanOwen for now I just need to do this type of operations on JSON, the main issue is efficiency because data set which I have is quite big. The operations might get a bit complex than this example." CreationDate="2016-06-27T11:01:51.160" UserId="20908" />
  <row Id="13647" PostId="12345" Score="0" Text="@VigneshMohan Did you able to replicate it?" CreationDate="2016-06-27T11:41:20.590" UserId="17116" />
  <row Id="13648" PostId="12430" Score="0" Text="I've up-voted because this would have suited my purpose earlier when I was willing to trade-off speed for accuracy, but as it turns out any standard impute function only messes up the classification later on. A regular `hmisc::impute` works just as well for me in terms of speed. Can I somehow speed-up the `mice` based imputation?" CreationDate="2016-06-27T12:08:38.540" UserId="13450" />
  <row Id="13649" PostId="12429" Score="0" Text="I believe that if I run this (or a similar function) on my dataset broken down by state that it would be a sane computational load. This is a great example and I can clearly see the support behind the idea. I'm still new to R, so I will have to learn how to write a user defined function, but this is a great start!" CreationDate="2016-06-27T14:26:13.237" UserId="20835" />
  <row Id="13650" PostId="12301" Score="0" Text="@HonzaB, PCA is an unsupervised feature selection algorithm. It reduces dimensions by finding redundancies in the data. There are supervised methods of feature selection which reduce dimensions by finding which variables relate better to the data. I think his idea is an ingenuous approach to introduce some of the advantages of supervised feature selection into PCA." CreationDate="2016-06-27T15:05:52.790" UserId="16853" />
  <row Id="13651" PostId="12383" Score="0" Text="You are right. Now C# is read correctly but there are extra \&quot; at the beginning and the end of my strings." CreationDate="2016-06-27T15:18:51.913" UserId="3151" />
  <row Id="13652" PostId="12442" Score="0" Text="Thanks! I am using support vector machine, neural network apart from traditional techniques such as linear regression and couple of other techniques for a regression problem. I work in a challenging environment where quick turn around is expected. So i was exploring how to best utilize existing resources and if anyone has found solution to this problem so far. So my question is can spark do things faster than R in the same machine if i use SparkR? Or anything else can be done to fasten things?" CreationDate="2016-06-27T15:58:26.257" UserId="13100" />
  <row Id="13653" PostId="12443" Score="0" Text="ReLU( 0 ) = 0, Sigmoid( 0 ) = 0.5. I don't think they are similar for small values, at least not without adjusting bias terms." CreationDate="2016-06-27T16:03:06.263" UserId="836" />
  <row Id="13654" PostId="12382" Score="0" Text="One thing not mentioned here why redundant features are removed is interpretability. You cannot interpret the coefficients of a linear regression if you have correlated covariates. But the prediction can be good nevertheless." CreationDate="2016-06-27T16:09:48.950" UserId="16853" />
  <row Id="13657" PostId="12438" Score="0" Text="@SeanOwen can you please suggest some of the simpler option?" CreationDate="2016-06-27T20:11:47.940" UserId="20908" />
  <row Id="13658" PostId="12438" Score="0" Text="How many lines is your data. Why not just create a cursor and read it line by line? You could also read it in batches if you want to be more efficient." CreationDate="2016-06-27T20:16:15.143" UserId="12241" />
  <row Id="13659" PostId="12438" Score="0" Text="@AlexR. Lines? around 20 millions and unique keys like `b0:47:bf:af:c1:42` are around 1 million so a huge data set and a bit more sub keys inside sub keys." CreationDate="2016-06-27T20:21:21.520" UserId="20908" />
  <row Id="13660" PostId="12423" Score="0" Text="What is the shape of your data? How are you representing a game or a single step in a game?" CreationDate="2016-06-27T21:08:51.570" UserId="12608" />
  <row Id="13661" PostId="12423" Score="0" Text="That's my question -- how I should shape my target data. Each record will be a single move." CreationDate="2016-06-27T21:15:19.537" UserId="12515" />
  <row Id="13662" PostId="12446" Score="0" Text="I just get a blank page when I click the link. Can you re-post it?" CreationDate="2016-06-27T21:46:59.047" UserId="12515" />
  <row Id="13663" PostId="12445" Score="0" Text="The ideal solution would be to have a probability distribution over the parameters and [marginalize](https://en.wikipedia.org/wiki/Marginal_distribution) the missing ones." CreationDate="2016-06-27T21:47:49.147" UserId="381" />
  <row Id="13664" PostId="12446" Score="0" Text="@RyanZotti: should be fixed now, thanks." CreationDate="2016-06-27T22:32:12.680" UserId="12241" />
  <row Id="13665" PostId="12438" Score="0" Text="I don't know what kind of infrastructure you're working with, but even on a single machine you can parallelize this on however many cores you have, splitting your file up and reading it line by line to extract the info. If you have a cluster then spark would work perfectly on this as well." CreationDate="2016-06-27T22:33:13.950" UserId="12241" />
  <row Id="13666" PostId="12446" Score="0" Text="Thanks. It looks like a good study, and I've up-voted accordingly. I am still looking for a more explicit supervised learning representation of the target variable though" CreationDate="2016-06-28T00:11:39.660" UserId="12515" />
  <row Id="13667" PostId="12446" Score="0" Text="@RyanZotti: Take some time to actually read the above reference as there is a precise notion of score." CreationDate="2016-06-28T00:13:26.050" UserId="12241" />
  <row Id="13669" PostId="12441" Score="0" Text="There is something you may want to try - &quot;sample&quot;" CreationDate="2016-06-28T01:23:12.027" UserId="2608" />
  <row Id="13670" PostId="12441" Score="0" Text="What if my scenario does not allow me to do so!" CreationDate="2016-06-28T01:44:15.513" UserId="13100" />
  <row Id="13671" PostId="12441" Score="0" Text="Despite having multiple cores, have you registered your cores for parallel processing?. If you haven't R still uses only a single core for your processing." CreationDate="2016-06-28T03:50:01.997" UserId="2608" />
  <row Id="13672" PostId="12452" Score="0" Text="can you please share a code example of your explanation @Icarus" CreationDate="2016-06-28T04:47:53.983" UserId="20585" />
  <row Id="13673" PostId="12452" Score="0" Text="Do you prefer using spark with map-reduce or just a simple code example ?" CreationDate="2016-06-28T04:54:06.980" UserId="20907" />
  <row Id="13674" PostId="12383" Score="0" Text="Those are escaped quotes; if you remove the `quote` parameter from `read.table` above it would eliminate those - except for the fact that the data contain mismatched quotes somewhere in the document and will fail without it.  Again, there is probably a solve with respect to encoding." CreationDate="2016-06-28T06:12:12.193" UserId="14625" />
  <row Id="13675" PostId="12446" Score="0" Text="This is a link only answer. To make it a viable answer for the site, please summarise the key points that address the question." CreationDate="2016-06-28T06:59:05.243" UserId="836" />
  <row Id="13676" PostId="12443" Score="0" Text="@NeilSlater You are correct. I was more focused on activation, in the sense that positive input values give activation to both of them, even if the resulting activation value is a bit different." CreationDate="2016-06-28T07:43:05.660" UserId="20920" />
  <row Id="13677" PostId="12443" Score="0" Text="The question is if pre-trained RBMs with sigmoid function are valid and good initializers for a deep net with ReLU activation" CreationDate="2016-06-28T07:57:31.760" UserId="20920" />
  <row Id="13678" PostId="12445" Score="0" Text="Depending on your usage but replacing with a mean value would be a good approach as a first step!" CreationDate="2016-06-28T10:22:29.710" UserId="20952" />
  <row Id="13679" PostId="12441" Score="0" Text="If you are going to do more analysis, I would recommend you to use octave. It out performs R. BUT: There are some edge cases where R is better. You can give it a try ;) IMHO" CreationDate="2016-06-28T10:34:54.133" UserId="20952" />
  <row Id="13680" PostId="12457" Score="0" Text="Welcome to Stack Exchange! You don't need to say thank you but if you get a useful answer remember to up vote and accept it if it answers your question." CreationDate="2016-06-28T10:43:59.110" UserId="18843" />
  <row Id="13681" PostId="12445" Score="0" Text="See https://en.wikipedia.org/wiki/Imputation_(statistics)" CreationDate="2016-06-28T11:52:48.627" UserId="13727" />
  <row Id="13682" PostId="12441" Score="0" Text="@karthikbharadwaj below is my code.&#xA;library(&quot;parallel&quot;)&#xA;library(&quot;foreach&quot;)&#xA;library(&quot;doParallel&quot;)&#xA;&#xA;##leave one cluster for other things than R&#xA;cl &lt;- makeCluster(detectCores() - 1)&#xA;registerDoParallel(cl, cores = detectCores() - 1)" CreationDate="2016-06-28T12:27:03.080" UserId="13100" />
  <row Id="13684" PostId="12463" Score="0" Text="Can you provide more details - how do you extract features from the text, what classifier are you using,..?" CreationDate="2016-06-28T13:51:54.993" UserId="676" />
  <row Id="13685" PostId="12465" Score="0" Text="Do you have a sample of code or a library to use for that? Is it also able to save the Model in memory and have multiple calls at the same time?" CreationDate="2016-06-28T14:36:01.267" UserId="20948" />
  <row Id="13686" PostId="12447" Score="0" Text="If the idea is to use k nearest neighbors (makes sense when the # data points is not very high) why not directly determine the target using kNN (based on whatever attributes are available)instead of first imputing the missing attributes and then using a regression model?" CreationDate="2016-06-28T14:38:25.047" UserId="13686" />
  <row Id="13687" PostId="12457" Score="0" Text="Can you clarify what do you mean by *mathematically incorrect*? That would help the community provide a better answer." CreationDate="2016-06-28T14:50:50.530" UserId="13686" />
  <row Id="13688" PostId="12466" Score="0" Text="Maybe this can help:&#xA;http://r-statistics.co/Outlier-Treatment-With-R.html" CreationDate="2016-06-28T18:08:19.537" UserId="16686" />
  <row Id="13691" PostId="12467" Score="0" Text="Thank you for your answer. When I say &quot;mathematically&quot; correct, I mean that RF and GBM search interactions and these interactions are added to a non-linear model. So, it seems weird for me to add these interactions in my GLM in order to improve the bias. However, I would like to know if I can do that with interactions proposed by &quot;interact.gbm&quot;." CreationDate="2016-06-28T15:51:56.700" UserId="20961" />
  <row Id="13692" PostId="12434" Score="0" Text="can you please give an example of when removing a redundant feature produces a classifier that has less performance power?" CreationDate="2016-06-29T03:08:58.520" UserId="20635" />
  <row Id="13693" PostId="12404" Score="0" Text="@DanLevin I still dont get how &quot;the reduction of entropy in A when given B&quot; is the same as &quot;the reduction of entropy in B when given A&quot;" CreationDate="2016-06-29T03:24:36.963" UserId="20635" />
  <row Id="13694" PostId="12404" Score="0" Text="A &amp; B might have different entropy. However, the mutual information among them is the same. The reduction in entropy, H(B)−H(B|I)= I(A;B)=I(B;A)=H(A)-H(A|B)" CreationDate="2016-06-29T06:34:56.460" UserId="13727" />
  <row Id="13695" PostId="12468" Score="0" Text="I'm voting to close this question as off-topic because *again* this is a *programming question*, contains nothing that could be called data science, and there is an entire stack exchange site, stackoverflow.com, *full* of people wanting to help with programming questions." CreationDate="2016-06-29T06:36:23.557" UserId="471" />
  <row Id="13696" PostId="12434" Score="0" Text="If in redundant you mean exactly the same, than you can safely remove them. If they are different, consider a case of n features with high MI among them and low MI with the concept. If you'll remove them, you'll lose the MI with the concept. For example, let say there is a basic feature x and all the other are x with probability p and the concept with probability (1-p), independently." CreationDate="2016-06-29T06:39:35.633" UserId="13727" />
  <row Id="13699" PostId="12351" Score="0" Text="I tried the `encoding` and with no success. In the end I solved it by changing the encoding on DB level, so it comes to Python clean." CreationDate="2016-06-29T08:39:45.140" UserId="17290" />
  <row Id="13703" PostId="12468" Score="0" Text="I think R questions are generally all on-topic for this SE, even if they are also viable for SO and often better there. I'm reluctant to close such a question here." CreationDate="2016-06-29T13:16:38.577" UserId="21" />
  <row Id="13704" PostId="12487" Score="0" Text="The linked question does indeed involve aggregating counts by date, but, it also involves pivoting the resulting rows to column values." CreationDate="2016-06-29T13:20:02.187" UserId="21" />
  <row Id="13705" PostId="12404" Score="0" Text="@Armon Safai: Define what you mean with intuition. Mutual information is defined to be symmetric. BTW, what is counter-intuitive when we say that the entropy of X given Y is equal to the entropy of X minus the mutual information of X and Y; and the entropy of Y given X is equal to the entropy of Y minus the mutual information of X and Y? (see my edit above)" CreationDate="2016-06-29T15:05:31.603" UserId="16213" />
  <row Id="13713" PostId="12479" Score="0" Text="Thank you. I will try to explain my data in two more specific examples: 1. imagine column 1 is participants names of cuase, from column2 - 8, each column's value range from 1 (least important) - 5 (most important)" CreationDate="2016-06-29T16:32:08.063" UserId="16679" />
  <row Id="13714" PostId="12479" Score="0" Text="so in the above example, all numbers are discrete." CreationDate="2016-06-29T16:32:54.157" UserId="16679" />
  <row Id="13715" PostId="12479" Score="0" Text="in the 2 example: column 9 for example, since this is a multiple choice question. row 1 chose &quot;very successful&quot; as an answer, however, row 2 chose &quot;needs improvement&quot;" CreationDate="2016-06-29T16:36:06.203" UserId="16679" />
  <row Id="13716" PostId="12479" Score="0" Text="so in the 2 example, we have a single column which it contains multiple strings as potential answers" CreationDate="2016-06-29T16:38:18.360" UserId="16679" />
  <row Id="13717" PostId="12479" Score="0" Text="I guess I will add another example: for column 10-15, this is another type of multiple choice questions. However this time, each column represents one answer. column 10 only allows string values &quot;very true&quot;, column 11 only allows string &quot;somewhat true&quot;" CreationDate="2016-06-29T16:49:29.407" UserId="16679" />
  <row Id="13718" PostId="12479" Score="0" Text="really appreciate the help" CreationDate="2016-06-29T16:49:42.920" UserId="16679" />
  <row Id="13719" PostId="12479" Score="0" Text="@Jing Please edit the question with these clarifications. Also give an example like Icarus did in his answer." CreationDate="2016-06-29T17:25:31.340" UserId="15527" />
  <row Id="13720" PostId="12479" Score="0" Text="@K3---rnc, sorry I am still trying to figure out how to format a data sample like lcarus did. But I will modify my question, thanks for the reminding." CreationDate="2016-06-29T17:40:31.890" UserId="16679" />
  <row Id="13721" PostId="12479" Score="0" Text="@Jing you set it as a code block; indent it four spaces." CreationDate="2016-06-29T18:03:02.690" UserId="15527" />
  <row Id="13723" PostId="12468" Score="0" Text="@HamidehIraj What should the result look like?" CreationDate="2016-06-29T19:04:46.863" UserId="15202" />
  <row Id="13724" PostId="12468" Score="0" Text="The results are the binary variables such as java, r,... filled correctly with zero and one." CreationDate="2016-06-29T19:25:30.957" UserId="3151" />
  <row Id="13726" PostId="12496" Score="0" Text="you may want to look at my recent response to this post - http://datascience.stackexchange.com/questions/12437/what-algorithm-is-used-to-extract-keywords-from-unstructured-texts" CreationDate="2016-06-29T20:40:22.020" UserId="6478" />
  <row Id="13727" PostId="12496" Score="0" Text="@BrandonLoudermilk thanks for some headsup. I will read that paper. Moreover, can any ML algorithm be used to give keywords for the data which is already present?" CreationDate="2016-06-29T21:06:35.467" UserId="21007" />
  <row Id="13728" PostId="12496" Score="0" Text="Can you add a few rows of sample data so we can see what u are working with?" CreationDate="2016-06-29T21:08:35.247" UserId="6478" />
  <row Id="13729" PostId="12496" Score="0" Text="Here is some idea of how is my data. Category: A, Description: &quot;Some description of how the product is&quot; . So I have many products under category A and also have many categories(B,C,D). Test data: (Input: Category, and description). Output: Suggest keywords.." CreationDate="2016-06-29T21:17:25.843" UserId="21007" />
  <row Id="13730" PostId="12479" Score="0" Text="@K3---rnc I think I got it worked now, thanks!!!" CreationDate="2016-06-29T21:28:16.323" UserId="16679" />
  <row Id="13732" PostId="12450" Score="1" Text="I'm not sure if you've tried this but &quot;fuzzy&quot; matching can be very difficult with addresses and is certainly not as simple as you mention.  For instance, &quot;321 NW First ST&quot; has a lot of very poor matches that are only and edit distance of 1 away: 320, 322, 323, 324, 325, 326, 327, 328, 329, 121, 221, 421, 521, 621, 721, 821, 921, 021, 301, 311, 331, 341, 351, 361, 371, 381, 391.  Also N, NE, SW, W are all within an edit distance and 1 of the pre-direction NW. Also first--&gt;1st is far in edit distance but is semantically similar as are abbreviations of suffixes, etc" CreationDate="2016-06-29T22:57:25.330" UserId="9420" />
  <row Id="13733" PostId="12468" Score="0" Text="Feels like an [x-y-problem](http://meta.stackexchange.com/questions/66377/what-is-the-xy-problem/233676#233676) - I don't think you need regular expressions and loops to get the final result." CreationDate="2016-06-30T00:41:08.843" UserId="15202" />
  <row Id="13734" PostId="12496" Score="0" Text="@JayPatel Does the Category depends on the description of the product?" CreationDate="2016-06-30T03:20:57.087" UserId="17116" />
  <row Id="13735" PostId="12496" Score="0" Text="@krishnaPrasad Yes it does." CreationDate="2016-06-30T04:01:08.367" UserId="21007" />
  <row Id="13736" PostId="12355" Score="0" Text="Thank you. Solved with the parameters and all works fine now." CreationDate="2016-06-30T04:47:39.100" UserId="8752" />
  <row Id="13737" PostId="12360" Score="1" Text="works now on separate notebooks and also with parameters updated. Thank you." CreationDate="2016-06-30T04:48:48.527" UserId="8752" />
  <row Id="13738" PostId="12487" Score="0" Text="Correct, I referred only to the counting part. The full computation can be also done using aggregation and transposing." CreationDate="2016-06-30T05:36:48.543" UserId="13727" />
  <row Id="13739" PostId="12505" Score="0" Text="Adding a bit about the different between GA and ensemble method. GA is used to find a model. Ensemble methods are used (some time to construct) and combine models." CreationDate="2016-06-30T06:01:36.467" UserId="13727" />
  <row Id="13740" PostId="12452" Score="0" Text="I'm familiar with spark with map-reduce and simple java code :) you can share both code examples.it will be best practice to me  @ Icarus" CreationDate="2016-06-30T06:22:14.553" UserId="20585" />
  <row Id="13741" PostId="12493" Score="0" Text="It is the parameters that `caret` doesn't support tuning of, that have to be tuned. Specifically, `scale_pos_weight` for handling imbalance. But nevermind that for now. Do you think I would have to re-adjust any of these parameters in the (near or distant) future given the nature of the data remains the same?" CreationDate="2016-06-30T06:24:47.530" UserId="13450" />
  <row Id="13742" PostId="12495" Score="0" Text="sounds interesting could you please elaborate on this" CreationDate="2016-06-30T07:08:04.060" UserId="18315" />
  <row Id="13743" PostId="12507" Score="0" Text="Welcome to the site. That is a really good first answer :)" CreationDate="2016-06-30T07:23:40.370" UserId="11097" />
  <row Id="13744" PostId="12507" Score="0" Text="Thank you @Dawny33" CreationDate="2016-06-30T07:28:40.107" UserId="21024" />
  <row Id="13746" PostId="12493" Score="0" Text="Perfect! SMOTEd some of the data, _much_ better results now!" CreationDate="2016-06-30T09:54:24.963" UserId="13450" />
  <row Id="13747" PostId="12516" Score="0" Text="can i get that that 50k tweets data?@Brandon Loudermilk" CreationDate="2016-06-30T12:29:17.877" UserId="20585" />
  <row Id="13748" PostId="12516" Score="0" Text="I think point #5 from the linked post is really important. A lot of people want to perform &quot;sentiment analysis&quot; on data, but that's really just throwing around a term. @DilipBobby you should ask yourself _why_ you want to perform sentiment analysis - what meaning do you want from this analysis - and then prepare the kind of data you need." CreationDate="2016-06-30T12:56:24.943" UserId="13450" />
  <row Id="13749" PostId="12516" Score="0" Text="@DilipBobby - the 50k corpus represents a sizable investment in time and money; it is unlikely that my previous employer will readily share this IP.  You should check on opendata.stackexchange.com for twitter data sources, like this one: http://opendata.stackexchange.com/questions/1545/twitter-open-datasets" CreationDate="2016-06-30T13:08:16.580" UserId="6478" />
  <row Id="13750" PostId="12516" Score="0" Text="thank you @Brandon Loudermilk" CreationDate="2016-06-30T13:23:41.030" UserId="20585" />
  <row Id="13751" PostId="12505" Score="0" Text="So for example, would you say that gradient descent is to a linear model as genetic algorithm is to ... { some other type of model }? In other words it would be inaccurate to say that you have a &quot;trained genetic algorithm&quot; (as a model artifact/output as a result of training) in the same way that you might say you have a trained a linear model?" CreationDate="2016-06-30T14:39:53.273" UserId="12515" />
  <row Id="13752" PostId="12495" Score="0" Text="I added a bit more, explanation, hope it helps!" CreationDate="2016-06-30T15:23:06.837" UserId="21006" />
  <row Id="13753" PostId="12505" Score="1" Text="Genetic algorithm is different from gradient descent in the sense that the mutation in genetic algorithm is random, and the update of target in gradient descent is based on math properties." CreationDate="2016-06-30T15:26:44.040" UserId="12237" />
  <row Id="13754" PostId="11903" Score="0" Text="@GameOfThrows Is there any lib where I can find the cross-correlation? [numpy has one](http://docs.scipy.org/doc/numpy/reference/generated/numpy.correlate.html), but I suspect it is not what you were talking about." CreationDate="2016-06-30T15:51:47.993" UserId="14153" />
  <row Id="13755" PostId="12498" Score="1" Text="All colour schemes are a compromise. We can't tell you what the &quot;best&quot; is without you telling us *your* criteria for &quot;bestness&quot;. What questions are being asked of the map? Do you want a value of 10 to be perceived as half as bad as a value of 20? Then linear scale. Do you want to stretch the contrast as much as possible to emphasise the spatial pattern? Then use quantiles. Every graphic is an answer - make sure the question is well posed." CreationDate="2016-06-30T16:28:27.193" UserId="471" />
  <row Id="13756" PostId="12207" Score="1" Text="67k records of how many features, i.e. how wide?" CreationDate="2016-06-30T16:47:20.173" UserId="15527" />
  <row Id="13757" PostId="12463" Score="0" Text="F1 score punishes false negatives and false positives equally. So, if your classes are not uniformly distributed, you should train with equalizing weights. In sklearn, this would mean using `class_weight=balanced` for whatever model you are using." CreationDate="2016-06-30T16:53:32.640" UserId="16853" />
  <row Id="13758" PostId="12423" Score="0" Text="I was surprised to see &quot;convolutional neural networks&quot; in your list. These neural networks were made for image classification, and I know they have been extended for things like text. But I see wikipedia says they have been used for Go. Impressive stuff!" CreationDate="2016-06-30T17:19:54.617" UserId="16853" />
  <row Id="13759" PostId="9885" Score="0" Text="Why not https://d3js.org/? It sounds like you have very particular requirements. D3 makes it extremely easy to work with SVG." CreationDate="2016-06-30T17:31:16.387" UserId="16853" />
  <row Id="13760" PostId="8660" Score="0" Text="Have you tried contacting the authors? Maybe they have something available online." CreationDate="2016-06-30T17:35:39.060" UserId="16853" />
  <row Id="13762" PostId="12479" Score="0" Text="@lcarus  Thank you, this will certainly help me a lot, but when it comes to choose a distance measurement. since all the data are discrete, would that be any issues? Is K-means pretty meaningless in this situation? Thanks again" CreationDate="2016-06-30T18:03:46.107" UserId="16679" />
  <row Id="13763" PostId="12423" Score="1" Text="Yeah, they're useful when spatial closeness is important. For example, a pawn on a square next to a queen" CreationDate="2016-06-30T18:21:40.633" UserId="12515" />
  <row Id="13764" PostId="12445" Score="0" Text="@Emre What if I build different models with just the data I have? So I have one model for when I have 3 parameters only, then another model for when I have 6 parameters, etc." CreationDate="2016-06-30T19:37:28.113" UserId="16712" />
  <row Id="13765" PostId="12445" Score="0" Text="You can do that. What fraction of your data is missing; is it random, or does it follow a pattern? It would help to update your question with relevant details." CreationDate="2016-06-30T19:56:17.537" UserId="381" />
  <row Id="13766" PostId="12445" Score="0" Text="@Emre It is &quot;random&quot;, when the iPhone sends probe requests and is near all Wi-Fi routers then I get all the data, but the iPhone is only sending probe requests when you are looling for Wi-Fis within the Settings of the phone or once every random time when you are not connected to Wi-Fi or when the phone is &quot;sleeping&quot;. So I suppose it is random." CreationDate="2016-06-30T19:58:59.993" UserId="16712" />
  <row Id="13767" PostId="12110" Score="3" Text="Yes, a deconvolution layer performs also convolution! That is why transposed convolution fits so much better as name and the term deconvolution is actually misleading." CreationDate="2016-06-30T20:47:57.480" UserId="14578" />
  <row Id="13768" PostId="11940" Score="0" Text="Could you provide the actual class labels and an illustrative text for each class? Science is the details..." CreationDate="2016-06-30T21:04:03.620" UserId="6478" />
  <row Id="13770" PostId="12524" Score="0" Text="Worth reinforcing that the `argmax` function is *not* used to optimise anything - it is not really possible, because it is not differentiable. Instead it is being used to output a predictive value that we are interested in. There are ways to use it if for instance we want to meta-search parameters - then we might use accuracy on a cross-validation data set as a metric for the search." CreationDate="2016-07-01T06:58:28.573" UserId="836" />
  <row Id="13771" PostId="12447" Score="0" Text="Missing a very common approach that can go with the mean/median approach - adding a (typically 0 or 1 valued) column that explicitly records whether the data was originally present. Sometimes absence of the value is a predictive feature by itself." CreationDate="2016-07-01T07:02:28.090" UserId="836" />
  <row Id="13773" PostId="12527" Score="0" Text="If questions are unclear you should add a comment and wait for the poster to clarify." CreationDate="2016-07-01T07:22:12.463" UserId="471" />
  <row Id="13774" PostId="12519" Score="0" Text="This is unclear as to what outcome you are interested in and what your data looks like. Example data and some data summaries would be useful. Without some knowledge of the data your question is basically unanswerable and we may as well suggest you add the phase of the moon to the square root of the Dow Jones index multiplied by your data." CreationDate="2016-07-01T07:23:33.027" UserId="471" />
  <row Id="13775" PostId="11096" Score="0" Text="@bl0b Consider accepting my answer if it has helped you solve your problem." CreationDate="2016-07-01T07:25:27.570" UserId="18540" />
  <row Id="13776" PostId="12513" Score="0" Text="What does your data look like? Do you have separate sets of line coordinates for each activity plus the transport mode and the person ID? Something like {&quot;bike&quot;, x,y,x,y....} {&quot;car&quot;,x,y,x,y} etc? I suspect average journey speed would give you a pretty good classifier. Have you done some basic exploratory analysis like that? Or is your data just raw coordinates with no ground truth activity you can use for training a model?" CreationDate="2016-07-01T07:29:16.853" UserId="471" />
  <row Id="13777" PostId="12528" Score="1" Text="It's just a matter of adding an appropriate regularization term to ensure related terms have similar embeddings. It's especially easy if you use the matrix factorization model. In any case, you'll have to write some code, but it should not be that hard. Welcome to DataScience.SE!" CreationDate="2016-07-01T07:42:16.180" UserId="381" />
  <row Id="13778" PostId="12513" Score="0" Text="@Spacedman: I've added an example from my data, you can see I have lat/long for each leg , start and ending time and type of movement. My problem with average speed is that a car and a bus a really similar and I can distinguish them only using number of stops and stops positions. Btw I'm looking for a library/framework to do so" CreationDate="2016-07-01T08:49:07.253" UserId="21035" />
  <row Id="13779" PostId="12524" Score="0" Text="@Xing Wang in the testing phase you don't get as output only 1 predicted value? why i have to do argmax ?" CreationDate="2016-07-01T09:30:15.227" UserId="21043" />
  <row Id="13780" PostId="12524" Score="0" Text="@NeilSlater sorry meta serach you mean build a tree of possibile paramteres and get the best parameter ? and you want build this graph using the output of a cross-validation ?" CreationDate="2016-07-01T09:31:51.790" UserId="21043" />
  <row Id="13781" PostId="12524" Score="1" Text="@Xing Wang i m not sure as ouput you get a vector of value and after you make the argmax ... in the testing phase i suppose to give a value and get as output 1 value... i dont get your point. After i learned all the W parameters after i should have as ouput a value." CreationDate="2016-07-01T09:40:26.123" UserId="21043" />
  <row Id="13782" PostId="12524" Score="1" Text="@T-student: Not usually a tree, but a vector of numeric options where you do not know how to set them to get best generalisation from your model. In logistic regression your search space for meta params is likely to be low dimensions, maybe just the single value for a regularisation param. So you would likely do a very simple search for the best accuracy (or F1 score, or logloss, or area under ROC, whichever metric you think best represents good solutions to your problem) using a cross-validation set, and just try a range of regularisation param values, training for each one in turn." CreationDate="2016-07-01T09:44:45.297" UserId="836" />
  <row Id="13784" PostId="12530" Score="0" Text="If a row of A, say  $A_{j}$ does not fit in RAM the # columns of A ($n$)  will be large as against the # rows ($m$). In this case $C_{m \times k}$ can be quite small ( say $100 \times 100$) and will occupy just $4 \times 10^{4}$ bytes assuming float32 entries. Or am I missing something?" CreationDate="2016-07-01T10:33:32.153" UserId="13686" />
  <row Id="13785" PostId="12530" Score="0" Text="Sorry I misread that $m$ and $k$ were large. Still if $A$ and $B$ are both sparse you may not need worry as much about size of $n$." CreationDate="2016-07-01T10:38:12.163" UserId="836" />
  <row Id="13786" PostId="12524" Score="0" Text="@T-student Could you explain how you get a vector of value after the argmax?  Code snippets are welcomed. For the test phase, you could see the predict function in the whole code http://deeplearning.net/tutorial/code/logistic_sgd.py. The predict function receives 10 samples, and give the predict value for each sample." CreationDate="2016-07-01T17:52:14.630" UserId="12237" />
  <row Id="13787" PostId="12537" Score="0" Text="Yes that works, thank you. (I did this a few years ago for rollings counts in time.) Definitely is an improvement over what I was doing currently." CreationDate="2016-07-01T18:45:52.393" UserId="21079" />
  <row Id="13788" PostId="12537" Score="0" Text="Doing this on my dataset reduces it to around 0.6 billion pairwise comparisons. So in the original it is about 1.6 billion, my bins is around 0.8 billion. (Also my binning approach does not get missing around the edges, because the edges are overlapping.) I think I just got lucky with the bins in my formulation though. Some overly simplified back-of-the-napkin math suggests 5 bins for my data is the optimal." CreationDate="2016-07-01T19:26:52.933" UserId="21079" />
  <row Id="13789" PostId="12537" Score="0" Text="If you use the binning approach wouldn't having bin sizes of 6 years be optimal? And then shift them 3 years each. That way all true pairs always share a bin and there is the least unwanted overlap possible. Whatever way you do it, the end result should be the same however so one of your implementations is bugged :)" CreationDate="2016-07-01T19:29:52.443" UserId="14904" />
  <row Id="13790" PostId="12537" Score="0" Text="Is it possible you have duplicate pairs with your binning approach because the pairs appear in two bins due to the overlap?" CreationDate="2016-07-01T19:31:34.260" UserId="14904" />
  <row Id="13791" PostId="12537" Score="0" Text="When I say 5 bins I mean literally splitting the data up into 5 separate sets that have overlaps of years on the ends (sorry, usings bins and sets synonymously is confusing here). I don't mean a bin of 5 years." CreationDate="2016-07-01T19:32:28.730" UserId="21079" />
  <row Id="13792" PostId="12537" Score="0" Text="Yes I understand, I think bins of 6 years are most appropriate, but what I meant is that if you have row1 and row2 that are both in the edges of two bins, do they appear twice in your pairs list? That could explain the 0.6 and 0.8b difference" CreationDate="2016-07-01T19:34:01.827" UserId="14904" />
  <row Id="13793" PostId="12537" Score="1" Text="Yes my original approach will have redundant calculations, hence why taking it all the way down to simply running bins of 6 years is not optimal. (I'm agreeing that your suggestion is better than my original approach to the problem.) By the time I figure out how to reduce those redundant comparisons I should be left with what your suggestions is anyhow." CreationDate="2016-07-01T19:38:12.480" UserId="21079" />
  <row Id="13794" PostId="12463" Score="0" Text="I am curious about what are the labels of the classification. More problem specific solution might be proposed if the problem is fully described." CreationDate="2016-07-01T19:46:17.547" UserId="12237" />
  <row Id="13795" PostId="12541" Score="0" Text="Anything that doesn't require extracting expensive features; something that works with FFTs and MFCCs, such as a modern neural network should be fine. The training complexity is not an issue in your case." CreationDate="2016-07-01T20:19:13.463" UserId="381" />
  <row Id="13796" PostId="12541" Score="0" Text="Could give some example of the classes? Is your problem similar to speech recognition?" CreationDate="2016-07-01T20:24:42.323" UserId="12237" />
  <row Id="13797" PostId="12541" Score="0" Text="Yes, similar to speech recognition, but discovering such things as &quot;car engine&quot;, &quot;scream&quot;, &quot;human steps&quot;, &quot;dog barks&quot; etc." CreationDate="2016-07-01T20:32:07.553" UserId="15315" />
  <row Id="13798" PostId="12534" Score="0" Text="What language you prefer? I estimate writing this function should not take much effort (in c language)." CreationDate="2016-07-01T20:33:45.420" UserId="12237" />
  <row Id="13799" PostId="12524" Score="0" Text="@William: The OP's code is processing a matrix of all predictions for a data set. So it is taking argmax in one axis only, and ending up with a vector of class labels - one for each example input vector in the dataset. In other words, it's processing a batch." CreationDate="2016-07-01T20:37:52.503" UserId="836" />
  <row Id="13800" PostId="12524" Score="0" Text="To: NeilSlater. Thank you for the explanation on @T-student 's question." CreationDate="2016-07-01T20:52:25.853" UserId="12237" />
  <row Id="13801" PostId="12528" Score="0" Text="@Emre what do you mean by &quot;add an appropriate regularization term to ensure related terms have similar embeddings&quot;? I understand regularization and I also understand matrix factorization, but I don't understand how you're  referring to them in this context. I'm also not sure what you mean by &quot;embeddings&quot; either." CreationDate="2016-07-01T20:58:00.767" UserId="12515" />
  <row Id="13802" PostId="12528" Score="0" Text="When you find a matrix factorized topic model, you need to add a term to the objective function to constrain or encourage the document vectors to have the desired property. There is usually a regularization term for sparsity or energy. LJB would simply need another regularization term. The document vector is the embedding of the document in the topic space. For more on language embeddings look up [word2vec](https://en.wikipedia.org/wiki/Word2vec)." CreationDate="2016-07-01T21:03:48.383" UserId="381" />
  <row Id="13803" PostId="12542" Score="0" Text="My question was about building one model (like linear regression) on many subsets not building many different models on subsets. But, I suppose your ensemble approach is valid. When you are &quot;building on more data&quot;, essentially you are building on many small subsets on each node, and I wasn't sure whether that would result in most optimal model....I think as some other posters (William and Ryan Zotti) suggests that either there are no guarantees of optimal model or depending upon what learning algorithm you use accuracy maybe same" CreationDate="2016-07-01T21:14:18.647" UserId="21082" />
  <row Id="13804" PostId="12549" Score="1" Text="Welcome to DataScience.SE!" CreationDate="2016-07-01T22:01:05.360" UserId="381" />
  <row Id="13805" PostId="12550" Score="0" Text="Ah alright, thank you very much" CreationDate="2016-07-01T23:31:54.033" UserId="21086" />
  <row Id="13806" PostId="12553" Score="5" Text="Anything which says `Learn x in y days/hours` is complete bogus. I'm 3 years into this domain, and I still think I haven't even scratched the surface, let alone `learning` data science." CreationDate="2016-07-02T03:08:24.450" UserId="11097" />
  <row Id="13807" PostId="12553" Score="1" Text="You don't know programming and the word &quot;maths&quot; doesn't even appear in your post so I'd say forget about becoming a data scientist in 100 hours. You have a years of work ahead." CreationDate="2016-07-02T03:24:30.967" UserId="381" />
  <row Id="13808" PostId="12553" Score="2" Text="Start with Andrew Ng's Coursera class. If that's too difficult study programming or maths as appropriate." CreationDate="2016-07-02T04:08:56.773" UserId="381" />
  <row Id="13810" PostId="12533" Score="0" Text="I can't afford 32 but I can afford 16. However, I noticed that it is too slow. Do you think I should try some values between 16-32 or stick with 16?" CreationDate="2016-07-02T08:21:44.217" UserId="17484" />
  <row Id="13811" PostId="12533" Score="0" Text="I would try and time some values. Every epoch should be around the same time so that won't take too long. Try 17 first to see if it's faster or slower because I'm interested in this, given that this power of 2 depends on GPU and/or backend of Keras. But I think just filling it to the brim is likely best" CreationDate="2016-07-02T08:23:54.177" UserId="14904" />
  <row Id="13812" PostId="12542" Score="0" Text="It's not necessarily true that you are building models on subsets. For example, you may perform SGD in a distributed way, computing partial gradients across subsets, but this isn't the same as many regressions. SGD doesn't guarantee an optimal model, but this isn't a function of being distributed. You can solve a linear regression with the normal equation in a distributed way too -- no multiple models there either. That's optimal." CreationDate="2016-07-02T09:39:24.470" UserId="21" />
  <row Id="13813" PostId="12534" Score="0" Text="@William I'll be running these libraries on a small cluster / HPC workstation, so I require libraries with high performance." CreationDate="2016-07-02T10:09:45.537" UserId="21075" />
  <row Id="13814" PostId="12556" Score="0" Text="Can I time these functions to the precision that cudaEvents can ?" CreationDate="2016-07-02T13:14:57.477" UserId="21075" />
  <row Id="13815" PostId="12556" Score="0" Text="I am not sure about the precision bt in python you can definitely time these functions." CreationDate="2016-07-02T13:24:26.177" UserId="20982" />
  <row Id="13816" PostId="12548" Score="0" Text="Thank you @RyanZotti for your reply. I am using data sourced from monthly backups as I currently do not have access to the Live database (as it sourced externally) so it is not in real-time data. If I was to use time of day/ day of week as predictor would I need to have the data in a single table or could I use it as is and link to the &quot;ParkingArea&quot; table to get the MaxSpaces for each area? Thank you for GBM seems like it will do the job!" CreationDate="2016-07-02T15:06:03.610" UserId="21080" />
  <row Id="13817" PostId="12548" Score="0" Text="Caret and scikit-learn require your dataset comes in a single dataframe. So yes, one way would be to make your dataset into a temporary table and then use something like Pandas (Python) or RMySQL (R) to import and convert the temporary table into a dataframe. ... Either way you're going to have to do some SQL transformations because the data is not structured in such a way that your model would be able to use it as is (you'll need to aggregate by time)" CreationDate="2016-07-02T15:23:33.653" UserId="12515" />
  <row Id="13818" PostId="12553" Score="0" Text="The question is so different to before, really you would have been better off creating a new one, rather than editing the existing one so radically. My attempt at an answer now makes absolutely no sense for the new question :-(" CreationDate="2016-07-02T16:13:13.047" UserId="836" />
  <row Id="13819" PostId="12553" Score="0" Text="I'm sorry :( I'll reedit again." CreationDate="2016-07-02T16:25:34.273" UserId="21090" />
  <row Id="13820" PostId="12555" Score="0" Text="Thanks Neil.  Sorry, I'm new to stackexchange and am unfamiliar with how things work." CreationDate="2016-07-02T16:29:40.133" UserId="21090" />
  <row Id="13821" PostId="12553" Score="0" Text="Thanks for the edit to be back to the original question. Marking up the edits and suggesting your own answer in the question is still maybe a bit confusing. I will make another edit, trying to keep it the same question overall as it is now, but also keeping it to Stack Exchange style as best I can. If you don't like or agree with anything I have done, you can rollback (look at the revisions) or apply your own edits to make it closer to what you intend." CreationDate="2016-07-02T21:05:03.377" UserId="836" />
  <row Id="13822" PostId="12553" Score="0" Text="I hope I haven't changed your &quot;voice&quot; too much, and it still expresses what you wanted to ask." CreationDate="2016-07-02T21:33:10.563" UserId="836" />
  <row Id="13823" PostId="12555" Score="2" Text="BTW I read some of the other Quora answers, and think some of them are more useful than the 10-day lesson plan which has risen to the top there, and which IMO is flawed as I explain in my answer. The other answers are worth a look through and some of the resources linked worth bookmarking. However, following up on all of them would of course take much longer than time currently set aside." CreationDate="2016-07-02T21:45:07.440" UserId="836" />
  <row Id="13824" PostId="12555" Score="0" Text="I couldn't agree more. Very nice answer ! Can't upvote twice, sorry !" CreationDate="2016-07-02T21:59:34.547" UserId="5177" />
  <row Id="13825" PostId="12559" Score="0" Text="Thank you for the answer! :)" CreationDate="2016-07-03T03:04:32.053" UserId="21090" />
  <row Id="13826" PostId="12541" Score="0" Text="SVM should work too." CreationDate="2016-07-03T04:41:20.140" UserId="8752" />
  <row Id="13827" PostId="12564" Score="0" Text="Possible duplicate of [Machine Learning Steps](http://datascience.stackexchange.com/questions/10085/machine-learning-steps)" CreationDate="2016-07-03T11:12:42.847" UserId="11097" />
  <row Id="13828" PostId="12550" Score="0" Text="@Dawny33 no hard feeling, but I still don't understand why you rejected the edit. I just believe that  what I'm saying states the same thing as Emre but with clearer. :)" CreationDate="2016-07-03T12:33:14.997" UserId="5177" />
  <row Id="13829" PostId="12550" Score="0" Text="@Emre what do you think ? I think your answer is great but it just need some clarifications." CreationDate="2016-07-03T12:34:48.927" UserId="5177" />
  <row Id="13830" PostId="12550" Score="0" Text="@eliasah many of your edits are putting lots of words in the answerer's mouth. They may or may not be right; they shouldn't be an edit. Write your own answer, or comment." CreationDate="2016-07-03T13:11:05.943" UserId="21" />
  <row Id="13831" PostId="12550" Score="0" Text="Ok. Thanks @SeanOwen" CreationDate="2016-07-03T13:12:06.980" UserId="5177" />
  <row Id="13841" PostId="12571" Score="0" Text="that worked perfectly! thank you!" CreationDate="2016-07-03T19:48:57.653" UserId="21124" />
  <row Id="13846" PostId="12540" Score="0" Text="I don't know if it is really necessary I'm using the caret package and if I'm not wrong the feature selection is already done by the Train() function" CreationDate="2016-07-04T07:57:37.600" UserId="19065" />
  <row Id="13847" PostId="12520" Score="0" Text="No didn't try without Tf-idf or bi-gram, I'm gonna try thanks.&#xA;And I'm gonna remove less low frequency words that's a good idea as well" CreationDate="2016-07-04T07:58:51.153" UserId="19065" />
  <row Id="13848" PostId="12463" Score="0" Text="The labels are basically two things: &quot;Requests&quot; and &quot;Incident&quot; about some softwares" CreationDate="2016-07-04T08:00:16.817" UserId="19065" />
  <row Id="13849" PostId="12548" Score="0" Text="Thank you very Ryan. You have been very helpful. I may be back in contact if I get stuck but should be able to progress." CreationDate="2016-07-04T08:22:34.260" UserId="21080" />
  <row Id="13850" PostId="12582" Score="0" Text="(+1) for the Lev. distance metric.  nltk comes with a ready-made implementation. Cosine similarity isn't a good string-similarity measure IMHO  :)" CreationDate="2016-07-04T08:29:51.697" UserId="11097" />
  <row Id="13851" PostId="12524" Score="0" Text="@William the optimization part i think will output W matrix and b.  after this your test is just doing  x*W + b = y   ... y is your output vector where you need to use the argmax ? :S" CreationDate="2016-07-04T09:50:26.747" UserId="21043" />
  <row Id="13852" PostId="12533" Score="0" Text="Are you sure that batch size doesn't influence the quality of learning? I remember reading some blogs/papers(?) where they said that smaller batches produce noisier gradients than larger batches, but noise can be useful to get out of local minimas. Not sure if/how this applies to LSTMs though." CreationDate="2016-07-04T09:54:53.080" UserId="676" />
  <row Id="13853" PostId="12582" Score="0" Text="I agree that it's much worse than the Levenshtein distance but if you need fuzzy matching between 2 datasets of millions it can actually do that in a reasonable time due to needing some tricks plus matrix multiplication" CreationDate="2016-07-04T11:48:59.540" UserId="14904" />
  <row Id="13854" PostId="12533" Score="0" Text="Not entirely convinced, haven't had enough experience myself but that is what I read. I can see the gradients being less stable so I might be off." CreationDate="2016-07-04T11:50:33.143" UserId="14904" />
  <row Id="13855" PostId="12387" Score="0" Text="Edited my answer to include some simple examples." CreationDate="2016-07-04T12:16:47.243" UserId="2576" />
  <row Id="13856" PostId="12592" Score="0" Text="I do aware about the form of overfitting you mention in your PS. In this question, I used cross-validation only to evaluate a given model, not to compare between models or to tuning any parameters. And actually it is really my first attempt." CreationDate="2016-07-04T15:32:19.880" UserId="21145" />
  <row Id="13857" PostId="12592" Score="0" Text="What I meant is that you cannot know if you overfit only with these results. Score on training set are _systematically_ higher (besides uncommon distribution of data) than on testing data. It doesn't involve overfitting necessarily." CreationDate="2016-07-04T15:37:44.030" UserId="20759" />
  <row Id="13858" PostId="12595" Score="0" Text="Regularisation and dealing with missing data are two different things. Could you either explain how the title relates to your question, or perhaps edit it to summarise what you are asking?" CreationDate="2016-07-04T16:48:22.783" UserId="836" />
  <row Id="13859" PostId="12595" Score="0" Text="Dealing with missing data is, indeed, a regularisation of the model. I changed the title anyway to avoid confusion." CreationDate="2016-07-04T17:26:11.320" UserId="18682" />
  <row Id="13860" PostId="12600" Score="0" Text="Welcome to the site!  I edited the question to make it more clear. Feel free t roll it back if needed :)" CreationDate="2016-07-05T06:08:30.520" UserId="11097" />
  <row Id="13861" PostId="12600" Score="0" Text="@Dawny33 Thanks." CreationDate="2016-07-05T06:10:37.560" UserId="21162" />
  <row Id="13862" PostId="12601" Score="0" Text="Thanks, that was very helpful. Any more resources you could add, in general towards maths based analysis of problems in ML. Thanks again." CreationDate="2016-07-05T06:26:05.097" UserId="21162" />
  <row Id="13864" PostId="3759" Score="0" Text="http://datascience.stackexchange.com/questions/12564/how-do-you-define-the-steps-to-explore-the-data?lq=1" CreationDate="2016-07-05T08:53:33.123" UserId="21024" />
  <row Id="13865" PostId="12535" Score="0" Text="Hi, I can't test because I am using an old verion of `tm` (upgrading would break my code). Basically, you wan't to use your own custom function within `tm_map`. For that you need to encapsulate it within `content_transformer`. Cf for instance https://rstudio-pubs-static.s3.amazonaws.com/66739_c4422a1761bd4ee0b0bb8821d7780e12.html and `tm_transformer`help. Now your function should be something like `mytxtfoo &lt;-content_transformer(function(txt){paste(mystemfunction(strsplit(txt, &quot; &quot;)[[1]]))})`" CreationDate="2016-07-05T09:41:39.047" UserId="18722" />
  <row Id="13868" PostId="12186" Score="0" Text="@AN6U5: I have added code for a function that will generate sample data showing the problem." CreationDate="2016-07-05T11:42:51.573" UserId="20509" />
  <row Id="13871" PostId="12519" Score="0" Text="The main focus is to predict the occurrences of Stale File Handle.The log files consists of the error messages generated with time stamps, source node and few others. On first pass, i tried to use hierarchical clustering to see if all the 'abnormal' states(of time, grouped by occurrences per hour ) clustered together. But that becomes a problem because there are certain messages like 'connect to'/'connecting to' which are more predominant than the other and therefore hclustering without scaling results in bias towards the counts with maximum variance." CreationDate="2016-07-05T15:35:36.720" UserId="21045" />
  <row Id="13872" PostId="12527" Score="0" Text="I did use the Box-Cox. But isn't box cox used for normalizing the distribution of a uni-variate data. I am looking for a transformation that can be applied across the multivariate data, so as to remove bias. I did use the log(x) transformation. But i guess this question is more subjective, pertaining to the particular dataset i have. I am just having a hard time determining which transformation is the best." CreationDate="2016-07-05T15:40:31.353" UserId="21045" />
  <row Id="13873" PostId="12535" Score="0" Text="Hey Eric, thanks for your answer, it seems to be exactly what I was looking for. Now I just need some time to fully understand the RSLP function that I'm working with to try your suggestion." CreationDate="2016-07-05T18:31:28.217" UserId="21044" />
  <row Id="13874" PostId="6508" Score="0" Text="I'd second that, try using PCA first. Then apply a k-means afterwards." CreationDate="2016-07-05T20:41:37.917" UserId="16538" />
  <row Id="13875" PostId="12593" Score="1" Text="I remove outliers most often when I'm visualizing, as the outliers or tails have a habit obscuring the interesting part of the plot. Determining these points is usually simple." CreationDate="2016-07-05T21:50:16.360" UserId="381" />
  <row Id="13876" PostId="12619" Score="0" Text="This question really focuses on how SAS works, rather than an underlying statistical issue, so may not be on-topic here - have a look at our [help/on-topic] for more information. I'm not sure, but this might be a better fit for Data Science SE than it is for Cross Validated." CreationDate="2016-07-05T21:45:48.187" UserId="11096" />
  <row Id="13877" PostId="12607" Score="0" Text="I am using 10 fold cross validation and the countvectorizer consistently performs better. Thank you for this answer! I will look into it. I remove stopwords, both the countvectorizer as the tfidf vectorizer improve a specific amount when I remove them." CreationDate="2016-07-05T15:30:53.000" UserId="21169" />
  <row Id="13878" PostId="12620" Score="1" Text="Look up [CRFs](https://en.wikipedia.org/wiki/Conditional_random_field) and [information extraction](https://en.wikipedia.org/wiki/Information_extraction). Welcome to DataScience.SE!" CreationDate="2016-07-05T23:52:36.083" UserId="381" />
  <row Id="13881" PostId="12626" Score="0" Text="thank you for edits @Dawny33" CreationDate="2016-07-06T06:45:09.853" UserId="20585" />
  <row Id="13882" PostId="12628" Score="0" Text="(+1) for also explaining about the validation set :)" CreationDate="2016-07-06T07:26:50.760" UserId="11097" />
  <row Id="13883" PostId="12628" Score="0" Text="@Hima Varsha  Thank you ,now I have clear view :) some of my questions are not yet answered.If possible plz find some time to answer them :) Good day" CreationDate="2016-07-06T07:59:42.163" UserId="20585" />
  <row Id="13884" PostId="12628" Score="0" Text="So the use of dividing the dataset into training and test is to predict using our model and test using the test set. This will help us to check precisions of our model. Test dataset should not be used to fit the model to prevent bias" CreationDate="2016-07-06T08:04:09.190" UserId="21024" />
  <row Id="13885" PostId="12628" Score="0" Text="@DilipBobby Is there anything more specific I can help you with?" CreationDate="2016-07-06T08:12:47.553" UserId="21024" />
  <row Id="13886" PostId="12628" Score="0" Text="@Hima Varsha  plz share your answer (http://datascience.stackexchange.com/questions/12584/can-we-use-stop-words-while-using-multinomial-navies-theorm)  2) http://datascience.stackexchange.com/questions/12622/does-the-training-set-of-one-topic-will-be-useful-to-predicate-the-sentiment-for" CreationDate="2016-07-06T08:20:09.857" UserId="20585" />
  <row Id="13888" PostId="12628" Score="0" Text="@DilipBobby Sure, i'll check into those. Can you mark this question as answered so that it will help others?" CreationDate="2016-07-06T08:43:52.253" UserId="21024" />
  <row Id="13889" PostId="12631" Score="0" Text="okay.so if i want to get the sentiment on the presidential elections of usa then I need to train the model with that kind of the tweets with correct sentiments and feed the model with that kind of the election tweets only ! Right ?" CreationDate="2016-07-06T10:37:56.150" UserId="20585" />
  <row Id="13890" PostId="12631" Score="0" Text="Preferably, yes. Then you can get the most accurate results." CreationDate="2016-07-06T10:43:10.257" UserId="21024" />
  <row Id="13891" PostId="12438" Score="0" Text="Have you already implemented the basic python code and tried or do you need help with that?" CreationDate="2016-07-06T13:04:29.027" UserId="21024" />
  <row Id="13899" PostId="12635" Score="0" Text="Distributed databases, cluster computing. What are your specific problems, bottlenecks? Welcome to DataScience.SE!" CreationDate="2016-07-06T17:46:35.650" UserId="381" />
  <row Id="13900" PostId="12630" Score="0" Text="Welcome to DataScience.SE! You can consult [this report](http://www.cim.mcgill.ca/%7Eanqixu/pub/2TBN.TRCIM1030.pdf), and use [the accompanying code](https://github.com/anqixu/2tbn). There is enough (pseudo-)code floating around that you should be able to help yourself." CreationDate="2016-07-06T17:53:51.987" UserId="381" />
  <row Id="13901" PostId="12393" Score="0" Text="Sorry, I do not quite understand your thoughts. What I am thinking is if I was given a route task,  like from location A, pick up a child and go along with street B to the intersection C and turn left and continue along the Street D until the destination.&#xA;&#xA;And then I got the real (latitude, longitude) points that how a person run this task, how good this person finish the routed task? use the similarity to describe how good he finish the task." CreationDate="2016-07-06T22:13:07.243" UserId="16756" />
  <row Id="13902" PostId="11652" Score="0" Text="What did you end up doing?" CreationDate="2016-07-06T22:16:14.513" UserId="14703" />
  <row Id="13903" PostId="12640" Score="1" Text="There's not quite enough information provided to answer this question. Raw time-series are often transformed into rectangular data-sets in order for regression models to be applied to them.  Is this what you are proposing doing? Otherwise, time series analysis is usually achieved via smoothing, removal of periodic components (de-seasoning), and forecasting.  ARIMA is a routine that achieves all of the above procedures in a tidy form, but is not strictly regression." CreationDate="2016-07-07T04:23:10.507" UserId="9420" />
  <row Id="13904" PostId="12631" Score="0" Text="okay!should I maintain same amount of data in my pos,neg,neu training model? or can I go with what I have? I'm have 2k pos,2k neu,7k neg labelled tweets." CreationDate="2016-07-07T05:05:27.853" UserId="20585" />
  <row Id="13905" PostId="12631" Score="0" Text="Need not necessarily. You can try with that and hopefully the model will not overfit for the negative tweets. Give it a shot if you have the data and check the results before taking the next step." CreationDate="2016-07-07T05:17:11.907" UserId="21024" />
  <row Id="13906" PostId="12631" Score="0" Text="How to eliminate over fitting problem of a model.Neg set dominates the model :( even if I maintain same size." CreationDate="2016-07-07T05:39:30.623" UserId="20585" />
  <row Id="13907" PostId="12631" Score="0" Text="Can you train on a bigger dataset?" CreationDate="2016-07-07T05:42:27.690" UserId="21024" />
  <row Id="13908" PostId="12631" Score="0" Text="not yet , working with count of tweets that I previously mentioned in the comment" CreationDate="2016-07-07T06:37:53.080" UserId="20585" />
  <row Id="13909" PostId="12393" Score="0" Text="Did you already read about string edit distance?" CreationDate="2016-07-07T08:55:13.047" UserId="2576" />
  <row Id="13910" PostId="12646" Score="0" Text="Worked perfectly! Thx." CreationDate="2016-07-07T11:29:18.220" UserId="15064" />
  <row Id="13911" PostId="12640" Score="0" Text="@AN6U5 does the graph help?" CreationDate="2016-07-07T16:06:55.990" UserId="21211" />
  <row Id="13912" PostId="12640" Score="0" Text="Are you simply trying to forecast the time series? If not, what are you trying accomplish?" CreationDate="2016-07-07T16:32:21.997" UserId="9420" />
  <row Id="13913" PostId="12648" Score="1" Text="Welcome to DataScience.SE! You are running into the difference between [feature _selection_](https://en.wikipedia.org/wiki/Feature_selection) (selecting a subset of the original features) and [feature _extraction_](https://en.wikipedia.org/wiki/Feature_extraction) (deriving new ones)." CreationDate="2016-07-07T17:33:59.357" UserId="381" />
  <row Id="13914" PostId="12638" Score="0" Text="Welcome to DataScience.SE! I'm not sure what your &quot;state&quot; contains (location or condition?), but it sounds like logistic regression is a good place to start." CreationDate="2016-07-07T17:38:52.620" UserId="381" />
  <row Id="13915" PostId="12640" Score="0" Text="@AN6U5 yeah forecasting is the goal." CreationDate="2016-07-07T17:44:38.060" UserId="21211" />
  <row Id="13916" PostId="12654" Score="0" Text="Thanks for the link to the code, that's really interesting.  I'm going to take a look and see if it makes sense." CreationDate="2016-07-07T20:30:17.377" UserId="19161" />
  <row Id="13917" PostId="12655" Score="0" Text="I can answer the titular question but I don't follow the example. You want the forecast the weather using the historical location but not the actual temperature at that location?" CreationDate="2016-07-07T21:21:08.860" UserId="381" />
  <row Id="13919" PostId="12655" Score="0" Text="@Emre I do want to use the actual temperature at that location, but suppose I want to predict the mean temperature on November 1st 2016, then I don't have the temperature on October 31st 2016 yet." CreationDate="2016-07-07T21:33:37.653" UserId="6486" />
  <row Id="13923" PostId="12630" Score="0" Text="Thank you for your comment. I also read the paper, but what I need is structure learning, which is not mentioned by it, I guess." CreationDate="2016-07-08T00:24:53.313" UserId="21193" />
  <row Id="13924" PostId="12393" Score="0" Text="I think string edit distance may not apply for this one, because two routes may be similar but the points we got are totally different because they are discrete points on the route." CreationDate="2016-07-08T00:27:36.983" UserId="16756" />
  <row Id="13925" PostId="12654" Score="0" Text="So the f_classif version of the f_oneway function and the scipy version of the f_oneway function are coded slightly differently and are not directly comparable.  f_classif assumes more than one category and will treat features in each class as levels in a variable. Scipy assumes more than one level and treats each column (feature) as a level.  So it seems to me that f_classif is not a real ANOVA.  It also seems like ANOVA is discouraged for binary classification.  Is it better to just use rank-sum tests to perform feature selection in binary (or really any amount) classification cases?" CreationDate="2016-07-08T00:29:21.820" UserId="19161" />
  <row Id="13926" PostId="12654" Score="0" Text="I prefer $L_1$ or elastic net feature selection so I am unable to answer that question, unfortunately." CreationDate="2016-07-08T00:35:52.303" UserId="381" />
  <row Id="13928" PostId="12662" Score="0" Text="It's a little unclear to me, can you give some small samples of data?" CreationDate="2016-07-08T08:50:20.360" UserId="14904" />
  <row Id="13929" PostId="12662" Score="0" Text="@Jan van der Vegt : I edited to give you samples of data but I don't care about this problem in particular, I just don't know how to describe my problem to Google" CreationDate="2016-07-08T09:00:20.660" UserId="21251" />
  <row Id="13930" PostId="12393" Score="0" Text="Could you please give us a more specific example of two routes from your real data which you would like to compare in terms of distance?" CreationDate="2016-07-08T09:05:00.197" UserId="2576" />
  <row Id="13931" PostId="12592" Score="0" Text="This is a good point. A good example would be using the median as your prediction. This model cannot overfit. It has only one degree of freedom. Yet, it will give slightly higher score for training than test in any normal, finite dataset." CreationDate="2016-07-08T09:53:10.980" UserId="16853" />
  <row Id="13932" PostId="12590" Score="0" Text="I would just add that if the OP can choose between two gradient boosting trees, one of which is using shorter trees than the other, I would go with the one using shorter trees. It is harder to overfit when you have less degrees of freedom. Plus, [Occam's Razor](https://en.wikipedia.org/wiki/Occam%27s_razor). :)" CreationDate="2016-07-08T09:54:33.543" UserId="16853" />
  <row Id="13933" PostId="10571" Score="0" Text="`SGDClassifier` does not support kernels. If the OP wants linear SVM, then I would recommend first trying `LinearSVR`. It is much faster than `SVR` because it solves the problem using a linear regression library, and global minimum is guaranteed (unlike gradient descente)." CreationDate="2016-07-08T10:03:30.390" UserId="16853" />
  <row Id="13934" PostId="12654" Score="0" Text="I probably shouldn't say ANOVA discouraged for binary classification, but with just two classes it is the same as doing a t-test.  Anyways, I've accepted your answer." CreationDate="2016-07-08T14:21:14.213" UserId="19161" />
  <row Id="13935" PostId="10072" Score="0" Text="I just wanted to return and comment on how applicable NAB seems to my problem. The only drawback I can see is that this is only for univariate (one column) time-series anomaly detection, but what about multivariate (many columns)? Thank you for this suggestion, I am going to push it to the shortlist for solution candidates." CreationDate="2016-07-08T15:36:48.457" UserId="1406" />
  <row Id="13936" PostId="12356" Score="0" Text="Thank you, @Lothar, this is helpful. The references I have above seem to be at a very high level with minimal technical material; Do you know of any published comparisons of GE Predix's capabilities (algorithm performance, comparisons, etc) in literature? Something like [Numenta's NAB results](https://arxiv.org/ftp/arxiv/papers/1510/1510.03336.pdf) would be great." CreationDate="2016-07-08T15:46:05.893" UserId="1406" />
  <row Id="13937" PostId="12671" Score="0" Text="I'd suggest reading a survey such as [this one](http://www.sciencedirect.com/science/article/pii/S0950705113001044)." CreationDate="2016-07-08T16:10:53.610" UserId="381" />
  <row Id="13938" PostId="12662" Score="0" Text="maybe `aggregation` is the word you are looking for.." CreationDate="2016-07-08T16:52:03.500" UserId="6550" />
  <row Id="13940" PostId="12672" Score="0" Text="http://blog.yhat.com/posts/recommender-system-in-r.html" CreationDate="2016-07-08T19:31:53.927" UserId="19161" />
  <row Id="13941" PostId="12648" Score="0" Text="Glad to be here! So PCA falls into feature selection right? Because feature extraction varies from field by field,application to application, if I'm not wrong?" CreationDate="2016-07-08T23:46:22.690" UserId="21233" />
  <row Id="13942" PostId="11669" Score="0" Text="Note that topic model attributes a stochastic mixture of the topics to each document, not _one_ topic." CreationDate="2016-07-08T23:47:12.990" UserId="381" />
  <row Id="13943" PostId="12650" Score="0" Text="I have a bad habit, but I like to confirm things, so you're saying PCA does in fact changes values of data right? Does this ever cause any significant loss ever?" CreationDate="2016-07-08T23:48:12.760" UserId="21233" />
  <row Id="13944" PostId="12648" Score="0" Text="No, feature extraction, because the principal components returned by PCA are _functions_ of the input features, not usually a subset of them. I think this will become more clear if you apply PCA to some data and see what the principal components are." CreationDate="2016-07-08T23:48:54.703" UserId="381" />
  <row Id="13945" PostId="12650" Score="0" Text="Of course it can cause significant loss. Any dimensionality reduction can cause loss, or we would always reduce data to 0 dimensions." CreationDate="2016-07-09T01:42:40.617" UserId="924" />
  <row Id="13946" PostId="12681" Score="1" Text="Already explained in the Wikipedia article on random forests: https://en.wikipedia.org/wiki/Random_forest#Tree_bagging, https://en.wikipedia.org/wiki/Random_forest#From_bagging_to_random_forests.  I suggest you read standard references before asking -- it'll help you to ask a better question, or let you answer your own question.  There's little point in us duplicating material already available on standard resources (like Wikipedia)." CreationDate="2016-07-09T02:01:09.487" UserId="8560" />
  <row Id="13947" PostId="12370" Score="0" Text="Any contribution would be helpful!" CreationDate="2016-07-09T06:46:46.373" UserId="16250" />
  <row Id="13948" PostId="12685" Score="1" Text="Normalisation is often not required by the specific data but required by the general type of model. For instance neural networks work better when inputs are normalised to mean 0, sd 1 distribution. Similar issue for k-means clustering. Unless you are in the happy state where input data is already under the target distribution by chance, then you improve accuracy by transforming it. So it is not clear why you want to see some sample data?" CreationDate="2016-07-09T06:49:44.423" UserId="836" />
  <row Id="13949" PostId="12650" Score="0" Text="@Anony-Mousse if we did that then there would be no data to learn from, and reducing too many dimensions may lead to subsequent overfitting." CreationDate="2016-07-09T08:38:31.100" UserId="21233" />
  <row Id="13950" PostId="12648" Score="0" Text="@Emre Ok. So PCA is feaure extraction? Wow, I guess you learn something new everyday!" CreationDate="2016-07-09T08:41:57.527" UserId="21233" />
  <row Id="13951" PostId="12650" Score="0" Text="Therefore, every dimensionality reduction will be lossy at some point (or not reduce dimensionality anymore). Proof complete." CreationDate="2016-07-09T09:39:23.840" UserId="924" />
  <row Id="13952" PostId="10571" Score="0" Text="Appreciate your comment. Could you elaborate on why kernel support is an issue?" CreationDate="2016-07-09T10:25:00.390" UserId="15361" />
  <row Id="13953" PostId="10571" Score="0" Text="From [the documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier), `The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM.` Same thing for `SGDRegressor`. `SGDRegressor` is equivalent to using `SVR(kernel='linear')`. If that is what OP wants, that's great. I was under the impression he wanted to use SVM with a kernel. If that is not the case, I would recommend he first tries `LinearSVR`." CreationDate="2016-07-09T13:37:09.213" UserId="16853" />
  <row Id="13955" PostId="9431" Score="0" Text="Great answer, thanks!" CreationDate="2016-07-09T17:34:44.730" UserId="21288" />
  <row Id="13956" PostId="12687" Score="0" Text="So the classifier would predict all negative elements when tested? Whatever the classifier model class, your estimate from a test set would show &quot;this classifier is much the same as guessing negative in all cases&quot;. I doubt there is any default other than the arithmetic one where $\frac{0}{0}$ is not defined." CreationDate="2016-07-09T19:27:00.957" UserId="836" />
  <row Id="13957" PostId="12687" Score="0" Text="@NeilSlater Yes, I am talking about a &quot;classifier&quot; which &quot;predicts&quot; always just the most common class (the negative one, in this case)." CreationDate="2016-07-09T19:33:38.883" UserId="8820" />
  <row Id="13958" PostId="339" Score="0" Text="Kaggler here. I generally use R for data exploration, visualization, and feature engineering with data.table and ggplot2 and then pipe my data over to python for modeling using scikit-learn." CreationDate="2016-07-09T20:47:21.800" UserId="18246" />
  <row Id="13959" PostId="12619" Score="0" Text="@Silverfish, Thanks for the help. But, I'm not getting any views or response here? What can I do?" CreationDate="2016-07-09T22:28:28.517" UserId="2647" />
  <row Id="13960" PostId="12619" Score="0" Text="Have a look at [this help page](http://datascience.stackexchange.com/help/no-one-answers)" CreationDate="2016-07-09T22:30:20.863" UserId="11096" />
  <row Id="13961" PostId="12619" Score="0" Text="Thanks, I'll try to update as much I can. Meanwhile, IF you think the question is worth answering, please up-vote it so it becomes visible." CreationDate="2016-07-09T22:33:03.617" UserId="2647" />
  <row Id="13962" PostId="4914" Score="0" Text="I'm working on solving this via [Machine Learning Problem Bible (MLPB)](https://github.com/ben519/MLPB), a github repo of minimal example ML problems and solutions with searchable tags like [regression], [classification], [sparse-data], etc." CreationDate="2016-07-10T01:16:06.857" UserId="18246" />
  <row Id="13965" PostId="2627" Score="0" Text="how did you solved this one? i am trying create 3d using d3 and topjson." CreationDate="2016-07-10T10:39:59.713" UserId="21306" />
  <row Id="13966" PostId="12699" Score="0" Text="For JSON file reading and getting summary, see the following links:http://stackoverflow.com/questions/2617600/importing-data-from-a-json-file-into-r, http://www.tutorialspoint.com/r/r_json_files.htm" CreationDate="2016-07-10T15:36:16.210" UserId="17116" />
  <row Id="13967" PostId="12696" Score="2" Text="hi Neil, yes, I will try. please see new answer in a few minutes." CreationDate="2016-07-10T18:42:50.917" UserId="9197" />
  <row Id="13968" PostId="12705" Score="0" Text="So is it safe to say that a convoluted layer won't output activations in a way that distorts the orientation of the image? And that a fully connected layer at the end should be capable of differentiating top-half convolutional activations from bottom half convolutional activations? If the orientation doesn't get lost, I could see how the network should be able to learn from unaltered images" CreationDate="2016-07-10T20:26:03.827" UserId="12515" />
  <row Id="13970" PostId="759" Score="1" Text="This is a very comprehensive answer, but I'm surprised that one aspect seems to be missing.  Cross validation is a vital component of most DS projects and typically requires a random sample, which can make reproducibility difficult.  I suggest that you briefly touch upon using the same seed for random generators in order to be able to reproduce results regardless of statistical variation.  Thanks!" CreationDate="2016-07-10T21:37:32.117" UserId="9420" />
  <row Id="13971" PostId="759" Score="0" Text="@AN6U5: Thank you for kind words! I agree - I missed that aspect (+1). Please feel free to update my answer, adding relevant brief information on cross-validation." CreationDate="2016-07-10T21:48:39.693" UserId="2452" />
  <row Id="13972" PostId="2627" Score="0" Text="@DadhichiTripathi, Sorry. I couldn't create the 3D bars on the map at that point. It was a school project and I was running out of time, so  I ended up using circle markers instead. Sorry, I couldn't be of much help here." CreationDate="2016-07-10T22:44:12.397" UserId="2647" />
  <row Id="13973" PostId="12705" Score="0" Text="Multiple convolutional layers with max pooling or strided convolution tend to lose resolution - of where detected features are exactly - as they get deeper. But they don't otherwise distort things. So the feature maps from the last convolutional layer should still effectively separate features detected in top of image from same features detected in the bottom." CreationDate="2016-07-11T05:18:53.717" UserId="836" />
  <row Id="13974" PostId="12710" Score="0" Text="Derp.  I just needed to specify `eta=1` to remove the effect of shrinkage." CreationDate="2016-07-11T05:51:00.330" UserId="18246" />
  <row Id="13975" PostId="12675" Score="0" Text="Maybe you can add some more details? Yes, we try training data of various sizes. Which preprocessing have you tried? which classifiers? Thanks!" CreationDate="2016-07-11T07:22:47.687" UserId="20884" />
  <row Id="13977" PostId="12679" Score="0" Text="Congrats, I didn't even get past the login screen without the system crashing. (I dumped it and went with Windows and Theano, which worked fine)" CreationDate="2016-07-11T07:44:18.177" UserId="676" />
  <row Id="13978" PostId="12700" Score="1" Text="Conter-example to your first part : I can predict 4 classes with one feature, let's say the age : baby : [0, 3[, child : [3, 13[, teenager : [13, 18[, adults = [18 and above[.&#xA;This is because the tree might definitely have more than one parent node and two leaves." CreationDate="2016-07-11T08:06:08.390" UserId="20759" />
  <row Id="13979" PostId="12673" Score="1" Text="The `nnetar` function allows independent predictor variables. Read the help file." CreationDate="2016-07-11T08:20:55.190" UserId="8646" />
  <row Id="13980" PostId="11696" Score="0" Text="I was just returning to this question having learnt about Actor-Critic, and how it offsets costs/benefits between pure Value based vs Policy based methods. I think this answer comes closest to explaining this detail, but it is covered very well in David Silver's lecture: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf (lecture 7, it is also available on YouTube - https://www.youtube.com/watch?v=KHZVXao4qXs&amp;index=7&amp;list=PL5X3mDkKaJrL42i_jhE4N-p6E2Ol62Ofa )." CreationDate="2016-07-11T08:32:17.407" UserId="836" />
  <row Id="13981" PostId="170" Score="0" Text="Sorry for the late reply. To *gain* information is to *reduce* overall entropy; so they're basically the same concept. [Take a look](https://en.wikipedia.org/wiki/Decision_tree_learning#Information_gain) at the definition of &quot;entropy&quot; and &quot;information gain&quot;." CreationDate="2016-07-11T10:54:22.917" UserId="84" />
  <row Id="13983" PostId="12673" Score="0" Text="@RobHyndman: Thanks! I must have missed that. I will revisit it!" CreationDate="2016-07-11T11:49:34.967" UserId="13413" />
  <row Id="13984" PostId="11805" Score="0" Text="I think the collaborative filtering is the right way to go with this as well. If you are really concerned that the number of employees with access is a problem simply normalize your view count by the number of employees with access to it. 50 views with 10 employees with access=5 views per employee, 50 views with 100000=.0005 views per employee. And then run your collaborative filtering on that. Of course these views would be specific to an individual user. I don't think that's strictly necessary since you are looking for correlations in the numbers, and scale won't matter much." CreationDate="2016-07-11T12:40:23.597" UserId="16538" />
  <row Id="13985" PostId="12712" Score="0" Text="What is the goal of the clustering? Do you have a distance function the suits your goal? I suggest using https://en.wikipedia.org/wiki/DBSCAN It is a connectivity based clustering algorithm. Since tables related by key is data that have connect components dbsacn should be suitable." CreationDate="2016-07-11T13:02:39.060" UserId="13727" />
  <row Id="13986" PostId="12693" Score="0" Text="hi Neil, yes - please see rewording above" CreationDate="2016-07-11T13:56:22.220" UserId="9197" />
  <row Id="13987" PostId="12708" Score="1" Text="To add to what Ryan said, you might actually take away the natural freedom that the decision tree algorithm has in determining splits by converting to a categorical. E.g. let's say the best spilt for your problem is at  25  for age. If you pre- split and create a 20-30 bucket you will take away the option of splitting at 25." CreationDate="2016-07-11T14:31:44.407" UserId="13686" />
  <row Id="13989" PostId="12717" Score="0" Text="I think it is an interesting question, but the general case is likely possible but the performance will degrade. Also converting greyscale to RGB would also be worse in a similar way. In both cases this is because the colour channel information is being used by the filters separately. In specific cases, the impact of this could be small enough you could get away with it . . . as usual the only way to be sure is to try. I'd suggest trying with grayscale images converted to RGB first to get a sense of whether the accuracy loss is acceptable." CreationDate="2016-07-11T14:42:00.267" UserId="836" />
  <row Id="13990" PostId="12717" Score="0" Text="Would you then recommend to use pre-train model for MNIST digit (grayscale) instead? But digit data would not represent more complex shapes... And can you elaborate the last part about color information?" CreationDate="2016-07-11T14:49:59.767" UserId="21337" />
  <row Id="13991" PostId="12717" Score="0" Text="oh, i did not expand your comment. Ok, so maybe I will just try with averaging and see what I get. Thanks!" CreationDate="2016-07-11T14:52:55.970" UserId="21337" />
  <row Id="13992" PostId="10394" Score="0" Text="Can anyone answer this question?  - http://datascience.stackexchange.com/questions/12619/are-sas-data-storage-options-designed-for-big-data" CreationDate="2016-07-11T16:57:14.627" UserId="2647" />
  <row Id="13994" PostId="6896" Score="0" Text="I'm trying to research if using Microsoft R Enterprise is beneficial in terms of scalability, performance, speed etc. when compared to SAS at the Enterprise level. To help this, can you answer this question? &#xA;http://datascience.stackexchange.com/questions/12619/are-sas-data-storage-options-designed-for-big-data" CreationDate="2016-07-11T17:00:50.290" UserId="2647" />
  <row Id="13995" PostId="11583" Score="0" Text="This kind of thing is much easier done in a higher-level framework like Spark. Hadoop is too low level. Did you solve the problem?" CreationDate="2016-07-11T17:40:55.150" UserId="381" />
  <row Id="13996" PostId="6308" Score="1" Text="Which version of Spark are you using, and did the answer solve your problem? Please accept it if so." CreationDate="2016-07-11T17:48:49.667" UserId="381" />
  <row Id="13997" PostId="12673" Score="0" Text="@RobHyndman: It seems I was using v. 6.2, but this feature is in newer versions. Thanks for the direction, I'm a big fan of your work!" CreationDate="2016-07-11T17:52:32.610" UserId="13413" />
  <row Id="13999" PostId="12694" Score="1" Text="For the command line: https://github.com/dpmcmlxxvi/clistats https://github.com/codahale/ministat" CreationDate="2016-07-11T23:10:23.353" UserId="381" />
  <row Id="14000" PostId="12722" Score="0" Text="Thanks for the answer, @Anwar. I'm looking for an analytical calculation rather than an empirical observation." CreationDate="2016-07-12T00:55:39.890" UserId="19172" />
  <row Id="14001" PostId="12712" Score="0" Text="Goal is to be able to group similar databases(graphs). Assume that there would be four different types of databases and we have to group them logically so as to identify the pattern of each group." CreationDate="2016-07-12T03:16:20.127" UserId="7702" />
  <row Id="14002" PostId="12700" Score="0" Text="@IgorOA Thanks for pointing it out, you are totally right, I missed it. I have tried creating a decision tree based on your comment but unable to accommodate same features on multiple node in tree, have a look at my [github repo](https://github.com/krishnaiitd/eagleicode/tree/master/R) R code and fancy decision tree are attached with random sample dataset." CreationDate="2016-07-12T03:36:58.997" UserId="17116" />
  <row Id="14003" PostId="12712" Score="0" Text="What is your definition of similarity among databases?" CreationDate="2016-07-12T06:10:52.780" UserId="13727" />
  <row Id="14004" PostId="12712" Score="0" Text="Based on : Number of tables(nodes), its joining column(edge) and overall structure" CreationDate="2016-07-12T06:26:25.380" UserId="7702" />
  <row Id="14005" PostId="12697" Score="0" Text="Feature importance is selected by the entropy values in Decision trees.  Not sure for RFs.  Entropy averaging?" CreationDate="2016-07-12T06:31:49.303" UserId="11097" />
  <row Id="14006" PostId="12719" Score="0" Text="You should probably ask this on on stats.stackexchange.com - its a statistics question and that's where the statistics experts are." CreationDate="2016-07-12T07:06:48.873" UserId="471" />
  <row Id="14007" PostId="10695" Score="0" Text="Is the vocabulary of the models same?" CreationDate="2016-07-12T07:46:19.067" UserId="21024" />
  <row Id="14008" PostId="12406" Score="0" Text="My objective is to model impact of online rating and review sentiment on price of smartphone. I have multiple smartphone with almost similar configuration. My client is smartphone seller. He wants to know how much discount i should give on my product if my online review or rating is better or worse than competing products. Also, please note that a buyer will not read thousands of reviews. He will skim through a)few recent reviews and will look at b)average rating." CreationDate="2016-07-12T07:47:17.287" UserId="13100" />
  <row Id="14009" PostId="12406" Score="0" Text="My challenge is mostly for creating a feature to model for a), as each review will have sentiment score. I want to give higher weightage to review sentiment from first page and significantly low weightage to review sentiment from page 2 on wards. My problem is to quantify the weights that should be given to reviews in page1 and reviews from page 2 onwards. Can you suggest any idea on how to proceed?" CreationDate="2016-07-12T07:50:55.653" UserId="13100" />
  <row Id="14010" PostId="11134" Score="0" Text="Have you tried doc2vec instead?" CreationDate="2016-07-12T08:04:27.230" UserId="21024" />
  <row Id="14011" PostId="12719" Score="0" Text="@Spacedman I don't think why this is not on-topic here. It is on-topic on CV alright, but there's no reason to migrate it there when it is on-topic and can be answered here :)" CreationDate="2016-07-12T08:41:00.767" UserId="11097" />
  <row Id="14012" PostId="12719" Score="0" Text="Can it be answered here? Given that cross-posting is frowned on and there's an order of magnitude more activity on CV, it is much more likely to get a good answer there. The question to ask is not &quot;is it on-topic on DS?&quot; but &quot;is it off-topic on CV?&quot;. It's not off-topic on CV, so that's a better place for it. DS is not the right place for non stats experts to try and get statistical advice." CreationDate="2016-07-12T09:19:20.757" UserId="471" />
  <row Id="14015" PostId="12723" Score="1" Text="Can you give details on : 1. Whether you are using GPU on your Mac currently 2. What's the desired training time that you want? . A couple of hours is not unusual to train typical deep networks." CreationDate="2016-07-12T14:12:56.600" UserId="13686" />
  <row Id="14016" PostId="12723" Score="0" Text="The TensorFlow binaries for Mac currently only support CPU, not GPU (https://www.tensorflow.org/versions/r0.9/get_started/os_setup.html). The desired time is as fast as possible. I accept that it will always probably take hours, but I'd like to reduce the time as much as possible" CreationDate="2016-07-12T14:24:17.740" UserId="12515" />
  <row Id="14017" PostId="12723" Score="0" Text="Maybe you can check if you can compile tensorflow for your Mac gpu. Compiling is quite easy as per my experience compiling on AWS for the g2.8xlarge instance" CreationDate="2016-07-12T14:35:25.453" UserId="13686" />
  <row Id="14018" PostId="12723" Score="0" Text="Ok, I'll check that out" CreationDate="2016-07-12T15:14:45.927" UserId="12515" />
  <row Id="14020" PostId="12747" Score="0" Text="Thanks. I am trying for a real time or near real-time solution. The log files is huge. So, if I need to replace the  new line character for a large log file, that will be a time consuming operation." CreationDate="2016-07-12T19:02:21.513" UserId="21352" />
  <row Id="14022" PostId="12198" Score="0" Text="@TrầnĐứcNhuận You can use random search to find an almost optimal set of hyperparameters for your model." CreationDate="2016-07-12T19:52:15.873" UserId="20544" />
  <row Id="14023" PostId="12725" Score="0" Text="Thanks for your reply. I am randomizing starting parameters, still I am facing this issue." CreationDate="2016-07-12T20:32:10.720" UserId="13518" />
  <row Id="14024" PostId="12725" Score="0" Text="How? Are you setting your [random seed](https://en.wikipedia.org/wiki/Random_seed)?" CreationDate="2016-07-12T20:54:04.347" UserId="381" />
  <row Id="14025" PostId="12753" Score="0" Text="What's your definition of &quot;tracking system&quot;? I can think of several: surveillance, computer vision, CRM, bugs, etc.?" CreationDate="2016-07-12T21:13:20.903" UserId="381" />
  <row Id="14026" PostId="12753" Score="0" Text="@Emre Actually I am using a circuit board with a beacon, the circuit board will sent out the position of the beacon. Now I have another type of circuit board and I would like to know how to compare the data from these two board. For example the amount of accuracy...etc." CreationDate="2016-07-12T21:43:34.687" UserId="21389" />
  <row Id="14027" PostId="12749" Score="0" Text="So it will be just fine? Not error (nan/infinity)?" CreationDate="2016-07-12T22:49:23.897" UserId="9465" />
  <row Id="14028" PostId="12749" Score="0" Text="I heard about Hessian-free optimization, I thought that's just about using an approximated Hessian instead of the real one?" CreationDate="2016-07-12T22:50:44.910" UserId="9465" />
  <row Id="14029" PostId="9373" Score="0" Text="Off-topic: Pylearn2 is now unmaintained. You may want to look at Lasagne. You can combine between Theano and Lasagne seamlessly. For example: https://github.com/alrojo/RNNProteins/blob/master/train.py" CreationDate="2016-07-12T23:42:15.127" UserId="9465" />
  <row Id="14030" PostId="12749" Score="1" Text="Yes I think most techniques estimate Hessian rather than attempt to robustly calculate it. I don't know why, but would assume because it is hard or impossible to construct the backpropagation as second order effects will entangle terms e.g. $\frac{\partial^2}{\partial x\partial y}$ where x and y are params from different layers." CreationDate="2016-07-13T06:35:44.297" UserId="836" />
  <row Id="14031" PostId="12697" Score="2" Text="Go from the top and do cross-validation to observe the score. In doubt, select fewer features." CreationDate="2016-07-13T06:36:55.640" UserId="723" />
  <row Id="14032" PostId="12750" Score="0" Text="Can you plot the data? What do the false positives look like on the plot? How did you select the threshold (n*MAD) that flags an outlier?" CreationDate="2016-07-13T07:06:11.033" UserId="676" />
  <row Id="14033" PostId="12745" Score="1" Text="Have you looked at what's on the curriculum of *actual* data science major courses? What specialism are you interested in - business, environment, health? I'd suggest you get a solid degree in something conventional and then do a one-year data science Masters postgrad course. Is that a possibility for you?" CreationDate="2016-07-13T07:08:20.373" UserId="471" />
  <row Id="14034" PostId="12755" Score="0" Text="Do you have details about the number of samples written by a single person? Is it 200 together or by each?" CreationDate="2016-07-13T08:51:44.763" UserId="21024" />
  <row Id="14035" PostId="12749" Score="1" Text="@rilut: I have updated the answer, as I think your confusion stems from the fact that Newton's method would not work on the ReLU transfer function. You are correct about that. But once you combine ReLU with an objective function (even something as simple as a single neuron and mean square error objective), it will start to work." CreationDate="2016-07-13T09:20:18.853" UserId="836" />
  <row Id="14036" PostId="12749" Score="0" Text="Awesome. Just awesome." CreationDate="2016-07-13T09:27:35.323" UserId="9465" />
  <row Id="14037" PostId="12725" Score="0" Text="No, I am not setting random seed. Each time parameters are initialized randomly. That's why the result seems strange." CreationDate="2016-07-13T09:50:24.863" UserId="13518" />
  <row Id="14038" PostId="12730" Score="0" Text="Method is converging, it is not diverging. Learning rate is not the issue. Different learning rates converging to same local minima (with different random initializations) is the problem." CreationDate="2016-07-13T09:53:36.330" UserId="13518" />
  <row Id="14039" PostId="12730" Score="0" Text="As the cost function is non-convex, I thought it is expected for the method to converge to different local minimas. But it is converging to same local minima. So I am concerned if there is some problem with my code or something." CreationDate="2016-07-13T09:54:50.367" UserId="13518" />
  <row Id="14040" PostId="12730" Score="0" Text="So if the initial points are same and if you are in the same local valley, it might happen. Can you increase the learning rate by a good amount and check just to check if you will end up at the same point or not? Increasing the learning rate should push the point outside the local valley." CreationDate="2016-07-13T10:02:46.313" UserId="21024" />
  <row Id="14041" PostId="12723" Score="0" Text="See   https://github.com/tensorflow/tensorflow/issues/25 for how to compile for AWS" CreationDate="2016-07-13T10:33:43.610" UserId="13686" />
  <row Id="14042" PostId="12718" Score="0" Text="Thanks for your intuition. I feel that at least it's not a totally bad idea to try. And comments from you and Neil really help me guess how much time I should invest. I will go with 2nd option and see." CreationDate="2016-07-13T10:51:16.437" UserId="21337" />
  <row Id="14052" PostId="12755" Score="0" Text="It is 200 together (i.e., two samples per person)." CreationDate="2016-07-13T12:16:19.423" UserId="21396" />
  <row Id="14053" PostId="12758" Score="0" Text="i edited original question,please explain is this correct?thank you" CreationDate="2016-07-13T12:37:18.693" UserId="10423" />
  <row Id="14054" PostId="9557" Score="0" Text="Pylearn2 is not active anymore. Keras, lasagne and blocks are better option for contribution." CreationDate="2016-07-13T12:40:52.467" UserId="13518" />
  <row Id="14055" PostId="12763" Score="0" Text="So if the nature of the data-set coming-in remains consistent throughout, there is nothing new that the model can learn?" CreationDate="2016-07-13T12:49:30.023" UserId="13450" />
  <row Id="14056" PostId="12755" Score="0" Text="Do you just want a person to handwriting match? Or a ranking giving the highest priority to the ones with the maximum match?" CreationDate="2016-07-13T12:51:58.567" UserId="21024" />
  <row Id="14057" PostId="12763" Score="0" Text="If the data doesnt change and if you are happy with the accuracy of the current model, I see no point in retraining it." CreationDate="2016-07-13T12:52:52.323" UserId="21024" />
  <row Id="14058" PostId="12763" Score="0" Text="@Aayush, Maybe you can use the incoming data as validation set and check your current model." CreationDate="2016-07-13T12:54:20.283" UserId="21024" />
  <row Id="14060" PostId="12763" Score="0" Text="Haha, can you make it answered so that it will help others?" CreationDate="2016-07-13T12:56:24.780" UserId="21024" />
  <row Id="14061" PostId="12763" Score="0" Text="Still too early to accept, but I will. Thanks!" CreationDate="2016-07-13T12:57:54.553" UserId="13450" />
  <row Id="14062" PostId="12755" Score="0" Text="Just a match. E.g, if I have person_1_writing_sample_1, person_1_writing_sample_2, person_2_writing_sample_1, and person_2_writing_sample_2, I want to match the two former and the two latter." CreationDate="2016-07-13T13:04:21.937" UserId="21396" />
  <row Id="14063" PostId="12719" Score="0" Text="I went head a reposed on CV." CreationDate="2016-07-13T13:48:06.113" UserId="12947" />
  <row Id="14066" PostId="12730" Score="0" Text="Initial points are not the same. I have randomized the initialization." CreationDate="2016-07-13T14:54:50.807" UserId="13518" />
  <row Id="14067" PostId="12719" Score="0" Text="There's a similar question here the [answer](http://stats.stackexchange.com/a/199881/37263) to which might be useful to you." CreationDate="2016-07-12T14:18:57.267" UserId="13686" />
  <row Id="14068" PostId="12758" Score="0" Text="Does the training data contain labels as well? and what are these labels? And before these questions what is the training data about and what do you want to predict?" CreationDate="2016-07-13T10:51:49.967" UserId="21414" />
  <row Id="14069" PostId="12758" Score="0" Text="yes it has labels, labels are: city, store, month, ..etc , and target labels are: category1, category2,...category40,  my predictions are if the category present in the store are not( 0 or 1), like binary classification, i build model, i trained model with all months data, i want predictions of particular month." CreationDate="2016-07-13T11:20:16.650" UserId="10423" />
  <row Id="14070" PostId="12758" Score="0" Text="http://mlr-org.github.io/mlr-tutorial/devel/html/multilabel/index.html" CreationDate="2016-07-13T11:24:50.530" UserId="10423" />
  <row Id="14071" PostId="12758" Score="0" Text="If you have trained model already may I know what did you use for learning. For ex Artificial Neural Network, oneVsAll ,K-means ,KNN etc.? Since you have trained parameters ,you can use them to get predictions by sending in X-Matrix(input month) to get the prediction value and test its f-score/accuracy by partitioning your train data. for example forward propagation with trained parameters will give you predictions when ANN is used" CreationDate="2016-07-13T11:26:19.387" UserId="21414" />
  <row Id="14072" PostId="12758" Score="0" Text="if i want next month predictions, i pass the New Data to model like &quot;city,store, in month label i pass only 8&quot;, is this correct?" CreationDate="2016-07-13T11:34:12.687" UserId="10423" />
  <row Id="14073" PostId="12755" Score="0" Text="Try k-means with 100 clusters. You should be able to find a library for it in every language." CreationDate="2016-07-13T18:50:47.337" UserId="381" />
  <row Id="14074" PostId="12725" Score="0" Text="Please post a minimal working example." CreationDate="2016-07-13T19:43:14.660" UserId="381" />
  <row Id="14076" PostId="9796" Score="0" Text="If you solved your problem, you can accept your own answer." CreationDate="2016-07-13T19:45:56.033" UserId="381" />
  <row Id="14077" PostId="12753" Score="0" Text="It would help if you updated your question with these and other relevant details. An example would be great! The more we know, the better we can help. Welcome to DataScience.SE!" CreationDate="2016-07-13T19:47:04.300" UserId="381" />
  <row Id="14078" PostId="12753" Score="0" Text="@Emre Thank you! I added an example, hope that makes my question more understandable." CreationDate="2016-07-13T20:35:04.700" UserId="21389" />
  <row Id="14079" PostId="12753" Score="0" Text="How are the devices tracking your iPhone, and are the returned co-ordinates elements of a time series?" CreationDate="2016-07-13T22:18:58.357" UserId="381" />
  <row Id="14080" PostId="12753" Score="0" Text="@Emre Yes. And supposed I placed a beacon on the back of my iphone, both A and B will capture the signal emitted by the beacon." CreationDate="2016-07-13T23:12:00.847" UserId="21389" />
  <row Id="14081" PostId="12753" Score="0" Text="How long are the time series you are comparing in terms of points? Are the samples regularly spaced? These are the kinds of details you should have mentioned in your post." CreationDate="2016-07-13T23:20:06.367" UserId="381" />
  <row Id="14082" PostId="12753" Score="0" Text="@Emre I think it will be around 1000 points. Both A and B has the same sample rate." CreationDate="2016-07-14T00:00:10.127" UserId="21389" />
  <row Id="14083" PostId="12753" Score="0" Text="Can you run the capturing devices simultaneously?" CreationDate="2016-07-14T00:11:37.050" UserId="381" />
  <row Id="14084" PostId="12753" Score="0" Text="@Emre Sure, data from A has the same time stamp as data from B" CreationDate="2016-07-14T00:18:21.753" UserId="21389" />
  <row Id="14085" PostId="12160" Score="1" Text="Welcome to DataScience.SE! Look into Gaussian process regression and Bayesian time series forecasting." CreationDate="2016-07-14T00:37:25.880" UserId="381" />
  <row Id="14086" PostId="12755" Score="0" Text="I hasten to add that you can't run a clustering algorithm (like k-means) with free-form text; you need to featurize it first, and you want [stylometric features](https://en.wikipedia.org/wiki/Stylometry) in particular. See e.g. [Using Machine Learning Techniques for Stylometry](http://www2.tcs.ifi.lmu.de/~ramyaa/publications/stylometry.pdf), [Conversationally-inspired Stylometric Features for Authorship Attribution in Instant Messaging](http://www.dcs.gla.ac.uk/~vincia/papers/stylometry-MM-2012.pdf) or [Adversarial Stylometry](https://www.cs.drexel.edu/~sa499/papers/adversarial_stylometry.pdf)" CreationDate="2016-07-14T00:55:34.313" UserId="381" />
  <row Id="14087" PostId="12755" Score="0" Text="Thanks! I will try that." CreationDate="2016-07-14T02:05:17.853" UserId="21396" />
  <row Id="14088" PostId="12773" Score="0" Text="can share any code example @Hima Varsha" CreationDate="2016-07-14T06:37:58.173" UserId="20585" />
  <row Id="14089" PostId="12765" Score="1" Text="What happens to the older batches, say, in case of boosted decision trees where I don't really have neural-nets type of back-propagation methods to work with?" CreationDate="2016-07-14T07:05:50.583" UserId="13450" />
  <row Id="14090" PostId="12773" Score="0" Text="umm, I have not come across codes for second option I suggested. There are research papers though which do sentiment analysis on Twitter with 3 classes - positive, negative and neutral. I am sure there will be datasets as well. Would you want that?" CreationDate="2016-07-14T07:12:39.273" UserId="21024" />
  <row Id="14091" PostId="12738" Score="0" Text="What do you mean by complete information in this case?" CreationDate="2016-07-14T07:22:36.697" UserId="14904" />
  <row Id="14092" PostId="12773" Score="0" Text="yeah ! it will be helpful @Hima Varsha" CreationDate="2016-07-14T08:25:05.973" UserId="20585" />
  <row Id="14093" PostId="12775" Score="3" Text="This question seems more relevant in Open Data community, can you ask there?" CreationDate="2016-07-14T12:33:35.323" UserId="21024" />
  <row Id="14094" PostId="12773" Score="0" Text="http://www-nlp.stanford.edu/courses/cs224n/2009/fp/3.pdf this actually contains datasets, codes and loads of references. Also, there is http://crowdsourcing-class.org/assignments/downloads/pak-paroubek.pdf which contains all 3 classes." CreationDate="2016-07-14T12:41:41.657" UserId="21024" />
  <row Id="14095" PostId="12774" Score="0" Text="I am indeed using the imbalanced-learn package, and that's what i got from the code. i just wanted to make sure that i correctly understood it. thanks for your help :)" CreationDate="2016-07-14T13:09:18.180" UserId="21433" />
  <row Id="14096" PostId="12772" Score="0" Text="thanks! i am aware of the way SMOTE works, and my question was only with regard to the inner working of its Python implementation in the package imbalanced-learn. thanks for the description though." CreationDate="2016-07-14T13:28:34.290" UserId="21433" />
  <row Id="14097" PostId="12776" Score="2" Text="I am not sure I agree with the premise. The number of splits that are possible doesn't affect how good they are, and this measure of feature importance depends on the number of times some _one_ split on that feature was chosen as the best" CreationDate="2016-07-14T13:54:39.617" UserId="21" />
  <row Id="14098" PostId="12776" Score="0" Text="@SeanOwen Let me be a little more specific. I am using xgboost `get_fscore()`. My understanding of the `get_fscore()` is as a result of reading this http://stackoverflow.com/questions/33652224/what-does-get-fscore-of-a-ml-model-in-xgboost-do. Therefore, in this case, it is implied that the more splits, the more important the feature right?" CreationDate="2016-07-14T14:24:38.240" UserId="20995" />
  <row Id="14099" PostId="12776" Score="0" Text="... the more splits from that feature that were chosen as the best. The number of possible splits on that feature doesn't really matter for that. A continuous feature has basically infinite possible splits; a binary feature just 1, but, the binary feature's split could be better in most cases and be chosen as the split that goes in the tree more often." CreationDate="2016-07-14T15:50:14.123" UserId="21" />
  <row Id="14100" PostId="12776" Score="0" Text="Exactly. So it is not a great feature importance measure that could do with improving and so brings me back to &quot;Is there a known method on how to compare the feature importance of continuous and categorical features?&quot;" CreationDate="2016-07-14T16:29:27.930" UserId="20995" />
  <row Id="14101" PostId="12780" Score="0" Text="I'd set up neural network and disconnect the neurons corresponding to the missing inputs as necessary." CreationDate="2016-07-14T19:20:45.543" UserId="381" />
  <row Id="14102" PostId="12780" Score="0" Text="impute the missing vals?" CreationDate="2016-07-14T20:12:53.817" UserId="6478" />
  <row Id="14103" PostId="12780" Score="0" Text="Could impute the missing values...Would have to make my own imputer though, the imputer for scikit-learn is not very smart (mean, median, etc.) Not sure I want to just replace every missing value with the average." CreationDate="2016-07-14T20:44:41.313" UserId="21462" />
  <row Id="14104" PostId="12778" Score="1" Text="That is a good article with really helpful visualizations.  Any time patients are involved, data is often scarce and so the model/ feature selection becomes really important." CreationDate="2016-07-14T21:17:33.853" UserId="19161" />
  <row Id="14105" PostId="11225" Score="0" Text="There are some great answers, I just want to add something specific to medicine.  Depending on the field, you can still publish results from ~20 patients, especially if the field is young (see texture analysis of medical images)." CreationDate="2016-07-14T21:46:45.740" UserId="19161" />
  <row Id="14106" PostId="12776" Score="0" Text="I think you're quite misunderstanding this measure of feature importance then.. not sure how else to explain it to you." CreationDate="2016-07-14T21:57:34.263" UserId="21" />
  <row Id="14107" PostId="12783" Score="0" Text="Neural nets generally train best when you have equal numbers of training cases for all your possible outcomes.  If 98% of your data is just one class, your NN apparently found that it can minimize its error by just guessing that one class for everything.  The easiest hack would be to balance your dataset by just copying the data in the rare categories until you have an approximately balanced dataset.  A more sophisticated solution would be to introduce class weights: http://stackoverflow.com/questions/35155655/loss-function-for-class-imbalanced-binary-classifier-in-tensor-flow" CreationDate="2016-07-14T23:16:04.977" UserId="18416" />
  <row Id="14108" PostId="12785" Score="0" Text="Possible duplicate of http://datascience.stackexchange.com/questions/11091/how-does-deep-learning-helps-in-detecting-multiple-objects-in-single-image" CreationDate="2016-07-15T06:37:10.220" UserId="836" />
  <row Id="14109" PostId="12738" Score="0" Text="Hi Jan, I have made an attempt at more completely explaining what I mean." CreationDate="2016-07-15T06:41:23.743" UserId="18843" />
  <row Id="14110" PostId="12738" Score="0" Text="Then I did understand it correctly, I will write a half answer now" CreationDate="2016-07-15T06:44:51.280" UserId="14904" />
  <row Id="14111" PostId="12780" Score="1" Text="What kind of model are you intending to make? When used to predict, do you need it to work with limited parameters same as your early experiments? Or will the final model assume that users will always input all the parameters you have determined are interesting?" CreationDate="2016-07-15T06:46:21.513" UserId="836" />
  <row Id="14112" PostId="12785" Score="0" Text="One answer adds as a footnote: &quot;There are potentially other approaches, such as using a more sophisticated network (maybe an RNN) to help guide the classifier on best places to check.&quot; - that's precisely what I'd like to find out about. Doing some sort of sliding window or ROI pre-processing is obvious, but I actually have a NN which does no pre-processing, but I don't understand how it does it. (In-house code, so I can't share here, unfortunately)" CreationDate="2016-07-15T06:49:37.600" UserId="21478" />
  <row Id="14113" PostId="12785" Score="1" Text="That was my answer, but I haven't studied the area well enough to know how it could work, or what images it would be viable in. In some cases, such as OCR on cheques, you can use non-ML image processing techniques to isolate boxes etc. In any case, if a specific approach interests you, you should re-word your question to narrow it down. That would stop it being a duplicate, too." CreationDate="2016-07-15T06:55:11.240" UserId="836" />
  <row Id="14115" PostId="12786" Score="0" Text="memory error as my understanding in 32 bit cpu only handle upto 2 gb for one process after that they will throw memory error" CreationDate="2016-07-15T07:28:13.447" UserId="21481" />
  <row Id="14116" PostId="12786" Score="1" Text="Try [dask](http://dask.pydata.org/) first. If that fails use a real database or Spark." CreationDate="2016-07-15T07:46:56.647" UserId="381" />
  <row Id="14117" PostId="12775" Score="0" Text="[UCI dataset repository](https://archive.ics.uci.edu/ml/datasets.html?format=&amp;task=reg&amp;att=&amp;area=&amp;numAtt=greater100&amp;numIns=100to1000&amp;type=&amp;sort=nameUp&amp;view=table) has a couple of datasets that match your requirements. You could always sample larger wide data sets." CreationDate="2016-07-15T07:57:31.277" UserId="15527" />
  <row Id="14118" PostId="12789" Score="0" Text="Sorry, I've reworded the question to focus on images rather than digits, but that seems a useful link." CreationDate="2016-07-15T08:36:19.843" UserId="21478" />
  <row Id="14119" PostId="12789" Score="0" Text="That is actually a very different problem, which I will answer in a bit" CreationDate="2016-07-15T08:39:02.137" UserId="14904" />
  <row Id="14120" PostId="12789" Score="0" Text="Potentially useful source material for an answer here: https://github.com/kjw0612/awesome-deep-vision#object-detection" CreationDate="2016-07-15T08:44:14.750" UserId="836" />
  <row Id="14121" PostId="12789" Score="0" Text="Let's say there were 2 cats and 1 dog, would you want to know there were 2 cats and 1 dog or that there is at least a cat and a dog in the picture? These are very different questions/networks again" CreationDate="2016-07-15T08:54:21.843" UserId="14904" />
  <row Id="14122" PostId="12788" Score="1" Text="The classification_report function isn't very configurable.. usually I just write my own functions for printing classification results - call the appropiate metric functions from sklearn.metrics and print their results.." CreationDate="2016-07-15T09:13:20.177" UserId="676" />
  <row Id="14123" PostId="12754" Score="0" Text="What package are you using? I cannot find the `forecast()` function in the [ARIMA class.](http://statsmodels.sourceforge.net/0.6.0/generated/statsmodels.tsa.arima_model.ARIMA.html#statsmodels.tsa.arima_model.ARIMA)" CreationDate="2016-07-15T09:26:07.690" UserId="16853" />
  <row Id="14124" PostId="12776" Score="0" Text="I do understand this measure. It works well enough for comparing two continuous features. What I was asking was is there a way of adapting or normalising the split count to compensate for one split vs near-infinite split. It seems there is not!" CreationDate="2016-07-15T09:31:05.497" UserId="20995" />
  <row Id="14125" PostId="12662" Score="0" Text="Aggregate Output Learning was the right therm, thanks @Valentas." CreationDate="2016-07-15T09:37:42.990" UserId="21251" />
  <row Id="14126" PostId="12662" Score="0" Text="http://www.cs.cmu.edu/~jfolson/documents/musicantd-AggregateLearning.pdf  Here is the paper giving the definition" CreationDate="2016-07-15T09:38:17.867" UserId="21251" />
  <row Id="14128" PostId="12789" Score="0" Text=". . . or where each cat and dog was in the image - by way of bounding boxes or object masks. Or converting image to semantics like http://cs.stanford.edu/people/karpathy/deepimagesent/ . . . I *think* the OP would be happy to see the bounding box version." CreationDate="2016-07-15T10:28:50.577" UserId="836" />
  <row Id="14132" PostId="12776" Score="0" Text="Let us [continue this discussion in chat](http://chat.stackexchange.com/rooms/42537/discussion-between-sean-owen-and-josh)." CreationDate="2016-07-15T11:50:08.337" UserId="21" />
  <row Id="14133" PostId="12794" Score="0" Text="Yes I know that LDA is unsupervised. However I wanna know if my add-on cleaning filter makes my methodology as supervised?&#xA;If no, I wanna know the supervised solutions in my case. Thanks." CreationDate="2016-07-15T10:35:21.327" UserId="21488" />
  <row Id="14134" PostId="12802" Score="0" Text="This is time series data, and I want to be able to rely on other features given there is one feature missing." CreationDate="2016-07-15T15:00:07.790" UserId="17772" />
  <row Id="14135" PostId="12794" Score="0" Text="Thanks for clarifying.  I've added a few edits to help direct @Hima Varsha original answer to your remaining question.  Thanks!" CreationDate="2016-07-15T16:41:09.447" UserId="9420" />
  <row Id="14136" PostId="12807" Score="0" Text="thanks for the answer. Could I ask a follow up? Would you be able to tell me or point me towards some information for doing the above to a dataset that has two subgroups, and applying the centering and normalization to the entire dataset with respect to only one of the groups?" CreationDate="2016-07-15T18:37:56.143" UserId="21430" />
  <row Id="14137" PostId="12807" Score="0" Text="Read the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) and [code](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/preprocessing/data.py#L446) related to sklearn's StandardScaler" CreationDate="2016-07-15T18:39:51.327" UserId="381" />
  <row Id="14138" PostId="12807" Score="0" Text="from my understanding normalization consisted of $${x_{i} - min(x)}  \over {max(x) - min(x)}$$ from that documentation, the standard scalar calculates the standard deviation and the mean and then applys it. isn't that different?" CreationDate="2016-07-15T18:49:32.420" UserId="21430" />
  <row Id="14140" PostId="12808" Score="0" Text="Do you mean that you want to find the functional form y = x^2, or that you want to have a model that will (however it works) given some number x, output the correct answer, which in this case is y = x^2?" CreationDate="2016-07-15T19:12:20.830" UserId="18416" />
  <row Id="14141" PostId="12780" Score="0" Text="The model needs to predict `y` well across the historical data and the new data. One of the parameters, `x1`, is a time, and I need to model `y` as a function of this time parameter, y(x1), and fit an exponential to it for *each* run. y(x1) is the goal, but to improve y(x1)'s predicted form, I hope to incorporate more than just `x1,x2` by using `x3,x4,...` as these new measurement channels become used. As the experiment continues, new measurement channels will probably be added." CreationDate="2016-07-15T19:13:09.787" UserId="21462" />
  <row Id="14142" PostId="12807" Score="0" Text="Yes, it is. Thank you for mentioning it; I've edited my answer." CreationDate="2016-07-15T20:12:39.707" UserId="381" />
  <row Id="14143" PostId="12810" Score="1" Text="Welcome to DataScience.SE! Please provide examples of the types of data, and errors you encounter, along with the corresponding corrections. Do you have any restrictions on the tools or approaches you are allowed to use?" CreationDate="2016-07-15T20:20:11.617" UserId="381" />
  <row Id="14144" PostId="12806" Score="0" Text="http://stackoverflow.com/questions/33072449/extract-document-topic-matrix-from-pyspark-lda-model" CreationDate="2016-07-15T20:25:32.220" UserId="381" />
  <row Id="14145" PostId="12811" Score="0" Text="Are you wedded to Shogun? If not, you will find [plenty](https://aimatters.wordpress.com/2016/01/16/solving-xor-with-a-neural-network-in-tensorflow/) [of](https://gist.github.com/pannous/2b8e2e05cf05a630b132) [tutorials](https://github.com/nivwusquorum/tensorflow-deepq/blob/master/notebooks/XOR%20Network.ipynb) for newer libraries like tensorflow." CreationDate="2016-07-15T20:33:28.220" UserId="381" />
  <row Id="14146" PostId="10130" Score="0" Text="Welcome to DataScience.SE! When you make your prediction, what inputs do you intend to use other than the user ID?" CreationDate="2016-07-15T21:09:47.557" UserId="381" />
  <row Id="14147" PostId="12811" Score="0" Text="I tried with PyBrain, but it's slow with huge data sets. But Shogun is older as improved, thst I guess, that could be a good choise. But it's some complicated" CreationDate="2016-07-15T21:31:59.723" UserId="19058" />
  <row Id="14148" PostId="12811" Score="0" Text="For big data you should be using a neural network library with GPU support, such as tensorflow or theano, but this should not be an issue for a simple XOR function. IF you want simplicity, look at keras." CreationDate="2016-07-15T21:36:23.560" UserId="381" />
  <row Id="14149" PostId="12780" Score="0" Text="My Worry about Imputing the missing values is that it might dilute the information when the values are not missing." CreationDate="2016-07-15T21:36:57.240" UserId="21462" />
  <row Id="14150" PostId="12780" Score="0" Text="If you are looking at &quot;pointwise estimates&quot; of partially observed random variables, imputing the missing values with mean might be a valid approach. If the data is identically and independently distributed throughout Year 1, 2 ... mean is least likely to introduce bias.&#xA;&#xA;Or you can model the problem using a probabilistic graphical model, in which case you can marginalize out the missing variables. In this kind of modeling, you also take the spread of the variable into account while predicting y. So missing variables with high variance might not dilute the information." CreationDate="2016-07-16T01:07:56.873" UserId="10446" />
  <row Id="14151" PostId="12806" Score="0" Text="Thank you. Yes, I found that and implemented that. But I want to learn the theory and implement it for my Python friends." CreationDate="2016-07-16T10:53:28.110" UserId="21503" />
  <row Id="14152" PostId="10999" Score="0" Text="This is a fantastic, under-reported tool." CreationDate="2016-07-17T06:22:58.977" UserId="16928" />
  <row Id="14154" PostId="12833" Score="0" Text="&quot;Hence the kernel of you 1x1 convolutions have shape [1, 1, 3].&quot;. What? There seems to be a bigger misunderstanding of convolutions. I thought if a  convolution kernel has shape [1, 1, 3], then one would say it is a 1x1x3 convolution? So 1x1 convolution is only about the output, not about the kernel?" CreationDate="2016-07-18T03:28:35.573" UserId="8820" />
  <row Id="14156" PostId="12833" Score="0" Text="For me `kernel = filter`, do you agree?&#xA;&gt;&gt; &quot;So 1x1 convolution is only about the output, not about the kernel? &#xA;Not at all. A `3x3` convolution can have an arbitrary output shape. &quot; Indeed, if padding is used and `stride=1` then the `output shape = input shape`.&#xA;&gt;&gt; &quot;I thought if a convolution kernel has shape [1, 1, 3], then one would say it is a 1x1x3 convolution?&quot;&#xA;No, I have never heared someone talking about `3x3x512` convolutions. However all convolution-filters I have seen have a third spatial dimension equal to the number of feature-maps of the input layer." CreationDate="2016-07-18T05:51:08.820" UserId="13662" />
  <row Id="14157" PostId="12833" Score="0" Text="For reference, have a look at the `Convolution Demo` of Karpathies CS321n course: http://cs231n.github.io/convolutional-networks/#conv. Or at the tensorflow API: https://www.tensorflow.org/versions/r0.9/api_docs/python/nn.html#conv2d&#xA;Filters are supposed to have shape  `[filter_height, filter_width, in_channels, out_channels]`." CreationDate="2016-07-18T05:55:00.337" UserId="13662" />
  <row Id="14158" PostId="12821" Score="0" Text="You would like sentence wise classification problems?" CreationDate="2016-07-18T06:44:26.390" UserId="21024" />
  <row Id="14160" PostId="12833" Score="0" Text="May I add the thing with &quot;1x1 convolutions are 1 x 1 x number of channels of the input&quot; to your answer? This was the source of my confusion and I keep forgetting this." CreationDate="2016-07-18T07:17:57.117" UserId="8820" />
  <row Id="14165" PostId="12837" Score="0" Text="Thanks, that sounds reassuring. But I have scan through Scikit-Learn's Neural Network Regressor page and I couldn't catch any parameter to define which kind of output is required. Similarly, I cannot see any hint how could I feed multiple training data sets (one for the image itself and another with the masks of recognized patterns) into the model.&#xA;&#xA;Besides, I have read a handful of papers and tutorials about Convolutional NNs, but these tend to be too challenging for me at the moment (the methods involve Keras, Tensorflow and other unknown libraries)." CreationDate="2016-07-18T09:50:05.960" UserId="21560" />
  <row Id="14166" PostId="12837" Score="0" Text="Scikit-learn has a very limited neural network offering, you will not be able to do these things with it unfortunately. Without convolutional neural networks this is going to be very challenging. My advice would be to invest more time in CNNs and learn Keras (it's a lot less difficult than it looks at first)." CreationDate="2016-07-18T09:51:48.327" UserId="14904" />
  <row Id="14167" PostId="12837" Score="0" Text="An alternative is to learn **n** different classifiers where **n** is the size of your output vector. This approach suffers from not really learning the dependencies between the outputs but does allow you to use sklearn only" CreationDate="2016-07-18T09:53:17.067" UserId="14904" />
  <row Id="14168" PostId="12840" Score="0" Text="Please make a reproducible example that we can cut and paste and run. Also, state which packages you are using, because this looks like you are using a &quot;plotly&quot; package of some sort. Also also, try posting to stackoverflow unless you have a real data science question." CreationDate="2016-07-18T13:07:59.320" UserId="471" />
  <row Id="14170" PostId="12834" Score="0" Text="Nice! Regarding the features, so if I knew that all your siblings were married at 24, and had their children at 26. The general population with marriages average at 25 at some normal distribution. I don't necessarily know what age you were married (that's what I would be predicting). One year suggests that you were married at 38, the other suggests that you were married at 24. Is it right to say that it's more probable that you were married at 24?" CreationDate="2016-07-18T15:37:09.780" UserId="13768" />
  <row Id="14172" PostId="12843" Score="0" Text="thanks for your suggestion. But when I add minlen=2, it is not creating any rules. i.e., set of 0 rules" CreationDate="2016-07-18T16:31:01.487" UserId="18013" />
  <row Id="14173" PostId="12838" Score="2" Text="You might want to look at Think DSP.  The PDF is online at the publisher's website for the book: http://greenteapress.com/wp/think-dsp/" CreationDate="2016-07-18T16:31:12.283" UserId="18416" />
  <row Id="14175" PostId="12833" Score="0" Text="Sure, go ahead!" CreationDate="2016-07-18T17:29:43.143" UserId="13662" />
  <row Id="14176" PostId="12838" Score="0" Text="What types of signals?" CreationDate="2016-07-18T17:33:05.540" UserId="381" />
  <row Id="14177" PostId="12853" Score="0" Text="linear algebra? cause i have taken statistics courses in college" CreationDate="2016-07-18T17:40:35.347" UserId="21573" />
  <row Id="14178" PostId="12853" Score="1" Text="Linear algebra is a must. Take some machine learning courses too." CreationDate="2016-07-18T18:11:40.697" UserId="381" />
  <row Id="14179" PostId="12824" Score="0" Text="Do you mean I can train model on publicly available dataset which does categorization of short text and apply that to my dataset! How that will work since I have different classes. Or are you suggesting I should try find similar data set and classes for categorization? It's a smart idea though in general." CreationDate="2016-07-18T19:00:09.627" UserId="19147" />
  <row Id="14180" PostId="12821" Score="0" Text="@HimaVarsha Not sure what you mean? do you mean semantic analysis of text?" CreationDate="2016-07-18T19:03:19.817" UserId="19147" />
  <row Id="14182" PostId="12854" Score="0" Text="Welcome to DataScience.SE! Is this a homework question?" CreationDate="2016-07-18T19:34:10.603" UserId="381" />
  <row Id="14184" PostId="12854" Score="0" Text="no. i'm just playing around. I'm a NN newb, so I'm not sure if this sort of thing is in standard texts." CreationDate="2016-07-18T19:49:10.533" UserId="21576" />
  <row Id="14185" PostId="12824" Score="0" Text="In editing the question you have added new information and changed the scope of the question. I now see that you are seeking to turn your unsupervised training data into supervised training data by determining the target variable and then attempting to feed this back into your supervised learning model.  I will add an adendumm to address this." CreationDate="2016-07-18T19:53:25.207" UserId="9420" />
  <row Id="14186" PostId="12824" Score="0" Text="I am also mentioning that I'm newbie :) What do you mean I have 'unsupervised training data' ? Do you mean my problem naturally falls in to unsupervised learning and I'm trying to force it to supervised learning?" CreationDate="2016-07-18T19:58:11.847" UserId="19147" />
  <row Id="14187" PostId="12824" Score="0" Text="Yeah.  There's no problem with adding target data that is hand created, but scalability becomes an issue.  Mechanical turks can help, but this can cost a bit of money.  Semi-supervised learning may be the solution you are after." CreationDate="2016-07-18T20:04:14.267" UserId="9420" />
  <row Id="14188" PostId="12833" Score="0" Text="@MartinThoma, I think you added the comment on the wrong paragraph. The first paragraph descripes the shapes of the input/output layer. Although they have spatial dimension `1 x 1` in your example, this is not directly related to the `1 x 1` convolution. I think the comment would fit better in the second paragraph, which talks about the convolution itself." CreationDate="2016-07-18T20:24:12.517" UserId="13662" />
  <row Id="14189" PostId="12833" Score="0" Text="You're right. However, it is your answer. Go ahead and change it to whatever you think is good. I only wanted to have this stated explicitly as this was the part which was important for me to read." CreationDate="2016-07-18T20:28:42.403" UserId="8820" />
  <row Id="14190" PostId="12857" Score="0" Text="Thanks for your answer. I assume  the &quot;activation function&quot; for the output layer just be the identity?" CreationDate="2016-07-18T21:22:22.597" UserId="21576" />
  <row Id="14191" PostId="12857" Score="0" Text="@nbren12: Yes for regression you would normally just have linear output layer, no activation function." CreationDate="2016-07-18T21:23:32.340" UserId="836" />
  <row Id="14192" PostId="12857" Score="0" Text="all your points are well taken, thank you. Is there any good literature on using NNs for regression rather than classification? most of the tutorials online are about classifying MNIST digits or doing super simple linear regression. For instance, when can a multilayer network with nonlinear activation functions do a decent job fitting a linear function?" CreationDate="2016-07-18T22:52:25.000" UserId="21576" />
  <row Id="14193" PostId="12838" Score="0" Text="@J.O'BrienAntognini Thanks! This seems like a good one to start with." CreationDate="2016-07-18T23:59:52.573" UserId="13100" />
  <row Id="14194" PostId="12838" Score="0" Text="@Emre Depending on the type of problem you are trying to solve, you may be able to utilize advance signal processing algorithms like: wavelets, shearlets, curvelets, contourlets, bandlets, for time-frequency analysis you may also apply Fourier Analysis and Convolution. This helps in feature extraction, just in case you are stuck in feature engineering and cannot find pattern in features." CreationDate="2016-07-19T00:03:50.167" UserId="13100" />
  <row Id="14195" PostId="12838" Score="1" Text="How about [Sparse Image and Signal Processing: Wavelets, Curvelets, Morphological Diversity](http://www.multiresolutions.com/sparsesignalrecipes/) or [Harmonic and Applied Analysis: From Groups to Signals](http://www.springer.com/us/book/9783319188621)? No python/R code, but solid theory. The thing is, the current trend in machine learning is away from using complex features like these to feature learning, so you might not find a lot of books that cover both topics. The O'Reilly book is a good suggestion." CreationDate="2016-07-19T05:43:13.530" UserId="381" />
  <row Id="14196" PostId="12857" Score="0" Text="@nbren12: If you have a linear function to fit, you would probably use simple linear regression to fit it - or you could use a single layer network with no activation function, which is equivalent. NNs work OK at regression, just use linear output layer and suitable objective function such as mean square error. Other aspects of any tutorial would remain the same. You still need to worry about over-fitting for example, and search for good architecture and other meta-params, and there is a bewildering array of clever designs/choices to learn about." CreationDate="2016-07-19T06:34:15.737" UserId="836" />
  <row Id="14197" PostId="12843" Score="0" Text="Try decreasing the minimum support and confidence. The higher you set those limits, the fewer rules will be found. This was basically the reason why you found only rules with empty antecedents before. Looking at the low support values in your sample output, I think you'll have to go quite low to get rules of size two. Try starting with 1e-4 or 1e-5 and work your way up from there." CreationDate="2016-07-19T07:23:27.227" UserId="21567" />
  <row Id="14198" PostId="12843" Score="0" Text="before going ahead I would like to hear one more thing. As you see the data, there are duplicate id with different codes. I want to know that before converting the data into transaction type, should I change the code variable into string as I did using dplyr function and then make the df into transactions. Because I doubt that I should convert the df directly to the transaction type and apply the algo." CreationDate="2016-07-19T07:44:10.173" UserId="18013" />
  <row Id="14199" PostId="12843" Score="0" Text="I see. Converting your data from transactional format is a common pitfall. I'm not an expert on R, but have used `dplyr` before and now that you mention it your conversion does look a bit sketchy. It's a bit hard to tell though not seeing your `.csv`. I'd recommend you take a look at example 4 of the transaction class [documentation](https://cran.r-project.org/web/packages/arules/arules.pdf) which seems to be just what you want. The documentation of R is sometimes a bit hard to navigate which can make it scary for beginners. Hang in there!" CreationDate="2016-07-19T07:54:12.783" UserId="21567" />
  <row Id="14200" PostId="12862" Score="0" Text="There is no time (or steps) in this &quot;algorithm&quot;. It is a simple algebraic expression, so the paper is inapplicable. But thanks for the reference! The paper was fun to read!" CreationDate="2016-07-19T08:22:47.497" UserId="16853" />
  <row Id="14201" PostId="12838" Score="0" Text="@Emre Thanks for your input!" CreationDate="2016-07-19T08:32:36.393" UserId="13100" />
  <row Id="14202" PostId="12855" Score="0" Text="so then what to do there? decision trees- or what? also--&gt; how about SVM and kernels- do people still use this? how about decision logic?" CreationDate="2016-07-19T08:34:38.647" UserId="8285" />
  <row Id="14203" PostId="12843" Score="0" Text="Thank you, I can see your point." CreationDate="2016-07-19T09:05:18.797" UserId="18013" />
  <row Id="14204" PostId="12775" Score="0" Text="IOk I will ask it there also I will post the list of all the datasets that I get here!" CreationDate="2016-07-19T10:30:50.443" UserId="21449" />
  <row Id="14205" PostId="12782" Score="0" Text="Your method is great, however synthetic dataset is not an option." CreationDate="2016-07-19T10:32:11.773" UserId="21449" />
  <row Id="14206" PostId="12865" Score="1" Text="Please summarize what other NLP &amp; sentiment analysis related posts you have read and how they don't answer your specific question... you will get more replies if you have shown due diligence in seeking an answer." CreationDate="2016-07-19T11:17:15.103" UserId="6478" />
  <row Id="14207" PostId="12865" Score="0" Text="This is one of the link : http://stackoverflow.com/questions/37853905/how-to-declare-a-tweet-as-negative-or-neutral-by-using-standford-nlp" CreationDate="2016-07-19T11:30:07.700" UserId="20497" />
  <row Id="14208" PostId="12855" Score="0" Text="When your dataset is only several hundred records it's often best to use linear models to reduce the risk of overfitting. An SVM or decision tree would also work, but you probably wouldn't want an ensemble of decision trees like a random forest or GBM. Yes, people still use SVMs. They work best on small to medium size datasets." CreationDate="2016-07-19T11:35:31.040" UserId="12515" />
  <row Id="14209" PostId="12867" Score="0" Text="How many rows do you have available?" CreationDate="2016-07-19T12:19:37.257" UserId="14904" />
  <row Id="14210" PostId="12867" Score="0" Text="Some 110 million rows" CreationDate="2016-07-19T12:30:15.720" UserId="18315" />
  <row Id="14211" PostId="12838" Score="0" Text="@Enthusiast Signal processing can be broad. Do you include Image Processing, Video, Communications? Or could you narrow it down to more specific signal processing topics?" CreationDate="2016-07-19T13:11:36.043" UserId="12527" />
  <row Id="14212" PostId="12865" Score="1" Text="That above is called [cross posting](http://meta.stackexchange.com/q/64068). You're not supposed to do that. You should tell us what material you _read_ through before posting here. There is sufficient literature on sentiment analysis of tweets, like this one [here](http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/). Perhaps start from there and come up with a more specific question when you've implemented something _or_ you have an actual data science related problem so that all of us may also learn something by it." CreationDate="2016-07-19T14:10:36.163" UserId="13450" />
  <row Id="14213" PostId="12867" Score="0" Text="Random forest can handle continuous (x,y,z dimensions) and categorical data.  If you have too many categories (I believe python sklearn can handle more than R) you could try converting to dummy variables.  Also, try using the built in Gini importance to rank the variables and only use the important categories." CreationDate="2016-07-19T15:32:41.040" UserId="19161" />
  <row Id="14214" PostId="12869" Score="0" Text="I have also seen lots of sources on Lasso/Elastic Net for feature selection, but I've often gotten better results just by doing rank-sum tests and taking the variables with best (lowest) p.  Or use the Gini importance." CreationDate="2016-07-19T15:36:18.713" UserId="19161" />
  <row Id="14215" PostId="12869" Score="0" Text="@Hobbes, yes, I've heard of that too. I've often seen variable importances from trees used to rank order variables and then just taking the top N variables. I also haven't seen Elastic Net or Lasso work as well in practice as it's claimed to work in theory" CreationDate="2016-07-19T15:49:03.523" UserId="12515" />
  <row Id="14216" PostId="12857" Score="0" Text="I was just thinking that the most basic test of any regression platform is linear regression, so any fancy nonlinear regression model (e.g. neural network) should, at least, be able to handle linear functions. thanks for all your comments." CreationDate="2016-07-19T16:06:31.820" UserId="21576" />
  <row Id="14217" PostId="12862" Score="0" Text="is this just an arxiv paper?" CreationDate="2016-07-19T16:07:21.417" UserId="21576" />
  <row Id="14218" PostId="12834" Score="0" Text="Well, yes with a normal distribution 24 is closer to the population average of 25 than 38. So without any other data you can say that it is more probable, simply because you already have a distribution in mind." CreationDate="2016-07-19T16:07:35.670" UserId="16538" />
  <row Id="14219" PostId="12862" Score="0" Text="yes, there is an arxiv paper by Alex Graves et al" CreationDate="2016-07-19T16:08:55.517" UserId="21024" />
  <row Id="14220" PostId="12862" Score="0" Text="interesting paper. i do have to agree with ricardo. i'm just trying to learn a function of known inputs, there is no dynamical behavior, so the paper isn't applicable." CreationDate="2016-07-19T16:45:27.877" UserId="21576" />
  <row Id="14221" PostId="12857" Score="0" Text="Making a nonlinear model fit a linear data set won't do as good a job as fitting an inherently linear model. If simple assumptions, such as linearity, are correct and &quot;baked in&quot; to the model class, it will perform much better. You should be able to get similar results for accuracy/loss with care, but the nonlinear model may overfit, and will take longer to train." CreationDate="2016-07-19T16:48:44.783" UserId="836" />
  <row Id="14222" PostId="12834" Score="0" Text="My plan is to collect data to create the distributions for this variable (and several others that I will continue thinking about). I feel like I already added biases such as &quot;all the data I collect is nobility only&quot;, etc... But these biases were based on a historian's opinion of the class of people we should be drawing distributions for. Thanks for the answers! I'll accept it!" CreationDate="2016-07-19T17:59:27.020" UserId="13768" />
  <row Id="14223" PostId="12838" Score="0" Text="@LaurentDuval i am including image, video data along with traditional data such as time series." CreationDate="2016-07-20T02:04:31.490" UserId="13100" />
  <row Id="14224" PostId="12875" Score="0" Text="you mean Latent Dirichlet Algorithm?" CreationDate="2016-07-20T06:00:51.223" UserId="21024" />
  <row Id="14226" PostId="12875" Score="0" Text="@HimaVarsha that would one good starting point to relate to" CreationDate="2016-07-20T06:01:25.170" UserId="21625" />
  <row Id="14227" PostId="12875" Score="0" Text="yes, those answers pretty much do the job I hope." CreationDate="2016-07-20T06:03:17.360" UserId="21024" />
  <row Id="14228" PostId="12874" Score="0" Text="Are you using a library that implements RNNs on top of TensorFlow? If not, what reference to RNN in TensorFlow are you using?" CreationDate="2016-07-20T06:37:46.877" UserId="836" />
  <row Id="14230" PostId="483" Score="0" Text="Sigmoid and Tanh are fine, nonlinearity is wanted to learn more complex interactions and the weights will map these to any range the network deems necessary" CreationDate="2016-07-20T11:35:56.790" UserId="14904" />
  <row Id="14231" PostId="12874" Score="0" Text="My data has inconsistent numbers of time steps, so 5 is actually the maximum. I'm trying to work off of this example of a RNN for dynamic length time series: https://github.com/aymericdamien/TensorFlow-Examples/blob/master/examples/3_NeuralNetworks/dynamic_rnn.py" CreationDate="2016-07-20T16:33:39.637" UserId="21464" />
  <row Id="14232" PostId="12885" Score="0" Text="Welcome to DataScience.SE! Consider modeling your activity with a Beta random variable." CreationDate="2016-07-20T18:18:28.723" UserId="381" />
  <row Id="14233" PostId="12883" Score="1" Text="Mahout is about the worst tool for this, unless you have billions of records. Don't use clustering at all - use association rule mining." CreationDate="2016-07-20T19:45:26.230" UserId="924" />
  <row Id="14234" PostId="12877" Score="0" Text="Maybe it isn't &quot;optimal&quot;? Is anybody using it?" CreationDate="2016-07-20T19:58:01.120" UserId="924" />
  <row Id="14235" PostId="12885" Score="0" Text="@Emre Thanks - can you point to any tutorials on how to use beta random variables for a dummy.  I'm struggling to understand how i'd apply this to my problem." CreationDate="2016-07-20T20:23:24.763" UserId="21642" />
  <row Id="14239" PostId="12885" Score="0" Text="I hope [this](http://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution) helps. You haven't mentioned how you are modeling the features though; an binary variable to indicate whether a feature was introduced that week?" CreationDate="2016-07-21T04:41:08.277" UserId="381" />
  <row Id="14240" PostId="12884" Score="0" Text="Thanks. I wanted to share one more thing, while using LinearSVC I looked at the predictions on my test set and what I observed was that all entries turned out to be classified in one or none of the labels. Any idea why this might be happening?" CreationDate="2016-07-21T05:22:05.693" UserId="21608" />
  <row Id="14242" PostId="12898" Score="0" Text="I think that is an impressive result, but I have only seen other outputs of the style transfer work, not studied it. If you link the original photo and style source in a comment here, I will edit them into the question for you (I'm not going to go searching for matches). I think the both source images are important in case they have some noise element that contributes to the patterns that you do not want." CreationDate="2016-07-21T07:32:56.787" UserId="836" />
  <row Id="14243" PostId="12884" Score="0" Text="@HimanshuRai As far as I know, LinearSVC needs a little bit of tweaking in order to allow for multiclass classification, whereas SVC can do it out of the box. Read this: http://scikit-learn.org/stable/modules/svm.html#multi-class-classification" CreationDate="2016-07-21T07:35:16.037" UserId="8152" />
  <row Id="14244" PostId="12894" Score="0" Text="You have more than 2 weight matrices, due to recursion, plus other things due to internals of LSTM. I suspect lots is going on inside `rnn_cell.BasicLSTMCell`, but I cannot see a full picture from the code you posted - most likely that is because I don't know enough TensorFlow though, so someone else needs to answer" CreationDate="2016-07-21T07:57:53.223" UserId="836" />
  <row Id="14245" PostId="12902" Score="0" Text="any scientific articles? Can it be news for instance? Have you check reuters dataset?" CreationDate="2016-07-21T09:20:12.657" UserId="21024" />
  <row Id="14246" PostId="12902" Score="0" Text="Scientific articles from journals, no news articles, no popular science articles." CreationDate="2016-07-21T09:26:47.503" UserId="21671" />
  <row Id="14247" PostId="12872" Score="1" Text="Can you tell why word2vec is not ideal? Word2vec is designed to capture semantic similarity of words, then why is it not ideal?" CreationDate="2016-07-21T10:07:17.327" UserId="13518" />
  <row Id="14248" PostId="12884" Score="0" Text="Yeah, I wrapped it in a OneVsRest Classifier." CreationDate="2016-07-21T10:26:25.370" UserId="21608" />
  <row Id="14249" PostId="12884" Score="0" Text="Can you tell me in what scenario might an SVM predict no classes?" CreationDate="2016-07-21T10:38:35.930" UserId="21608" />
  <row Id="14250" PostId="12906" Score="0" Text="You don't backpropagate to the *input* layer because that doesn't give you gradients that you can do anything with. So to clarify, you mean to use this in the case `l-1` layer is *not* the input layer?" CreationDate="2016-07-21T12:01:05.587" UserId="836" />
  <row Id="14251" PostId="12906" Score="0" Text="I don't understand your notation, but essentially you backprop each set of feature map deltas (at each point) via the convolutional kernel exactly the same as a MLP layer, and accumulate them for each point in the input due to overlap." CreationDate="2016-07-21T12:08:54.210" UserId="836" />
  <row Id="14252" PostId="12885" Score="0" Text="I literally have no idea what I am doing, hence the ask for tutorials.  I have no idea where to start." CreationDate="2016-07-21T12:26:23.343" UserId="21642" />
  <row Id="14253" PostId="12905" Score="0" Text="Is the purpose to learn an unknown DFA from observation of data, or to encode a known DFA as a RNN, or something else?" CreationDate="2016-07-21T13:19:35.170" UserId="836" />
  <row Id="14254" PostId="12906" Score="0" Text="@Neil Slater Yeah, I would like to use this in the case `l-1` layer is from another layer. In my implementation, objects (with different parameters) instantiated from the class `ConvPoolLayer` can be trained together. That is, the output of the previous layer might be used as the input of current layer. So I need to backprop the error to the input layer." CreationDate="2016-07-21T14:03:16.157" UserId="21676" />
  <row Id="14255" PostId="12909" Score="0" Text="You could just take one step further generic-ness and look at https://en.wikipedia.org/wiki/Mathematical_model - most ML models will match that definition, even if they do not match &quot;statistical model&quot; (although I think almost all trained unsupervised or supervised ML models would be considered statistical models)." CreationDate="2016-07-21T14:19:22.893" UserId="836" />
  <row Id="14256" PostId="12907" Score="0" Text="What if signups were in the 100's?  The above data was dummy data." CreationDate="2016-07-21T14:34:48.183" UserId="21642" />
  <row Id="14257" PostId="12905" Score="0" Text="Mainly, to learn unknown DFA from data; but, unlike the RNN approach which &quot;interprets&quot; RNN as a DFA, I would yield a DFA as output of the network." CreationDate="2016-07-21T14:49:22.143" UserId="14955" />
  <row Id="14258" PostId="12885" Score="0" Text="Maybe you need to step back and get a book on statistics? Larry Wasserman's _All of Statistics_ isn't bad." CreationDate="2016-07-21T15:37:53.437" UserId="381" />
  <row Id="14259" PostId="12872" Score="0" Text="Yes, that's why I provided the examples in the question. To reiterate: 'hot' and 'cold' are antonyms, but they're more similar (according to word2vec) than 'hot' and 'warm' which are much closer to synonyms. 'Hot' can also mean 'popular' (e.g., a &quot;hot item&quot;) but again &quot;hot&quot; and &quot;cold&quot; are closer to each other than &quot;hot and &quot;popular&quot; are. So word2vec is not ideal because it scores antonyms (which are semantic opposites) as more similar than synonyms (which are semantically equivalent)." CreationDate="2016-07-21T16:02:27.633" UserId="6403" />
  <row Id="14260" PostId="9713" Score="0" Text="OOB can be used to get an idea of the test error. OOB measures the performance of the model by taking a data point from the training set, and making a prediction using only the trees which does not contain that specific data point. If accuracy is still what you need you can calculate it from the confusion matrix" CreationDate="2016-07-21T17:07:58.177" UserId="20429" />
  <row Id="14262" PostId="12917" Score="1" Text="Welcome to DataScience.SE! What is the distribution of classes over the training data? Can you upload the data to github or something?" CreationDate="2016-07-21T17:43:54.983" UserId="381" />
  <row Id="14263" PostId="12917" Score="0" Text="@Emre Thank you! Unfortunately I can't upload the raw data. The distribution from class labels 1 through 5 is 20%, 20%, 20%, 30% and 10%, which I think is fairly balanced, no?" CreationDate="2016-07-21T18:00:33.370" UserId="21690" />
  <row Id="14264" PostId="12917" Score="0" Text="Turns out the predictions are much better using SVC than RandomForest with 100 trees. Still not predicting in all categories, but predicts in three and the recall is decent." CreationDate="2016-07-21T18:30:46.953" UserId="21690" />
  <row Id="14265" PostId="12919" Score="0" Text="Support vector machine implemented in TensorFlow: https://github.com/AidanGG/tensorflow_tmva/wiki/Support-Vector-Machine" CreationDate="2016-07-21T19:29:36.540" UserId="836" />
  <row Id="14266" PostId="12905" Score="0" Text="How much of the system do you have observed in your data? Do you have full knowledge of the state and input string at each step? If not, what do you have?" CreationDate="2016-07-21T19:36:17.740" UserId="836" />
  <row Id="14269" PostId="12707" Score="0" Text="You could look at out of bag testing. To see the performance of trees where data points were not used in the construction of that specific tree." CreationDate="2016-07-21T20:34:03.413" UserId="20429" />
  <row Id="14270" PostId="12918" Score="0" Text="Never used ggplot in python. But in R, you want to use `geom_histogram(bins=30)`, not `binwidth`, which refers to the width of each bin and cannot be used in combination with `bins`. (By default, `bins=30` by the way,)" CreationDate="2016-07-21T20:34:05.400" UserId="16853" />
  <row Id="14271" PostId="12917" Score="0" Text="Are you working with text data?" CreationDate="2016-07-21T20:40:06.677" UserId="20429" />
  <row Id="14272" PostId="12918" Score="0" Text="As Python's ggplot is built on top on matplotlib I would expect that bins=X parameter should be working but it is not." CreationDate="2016-07-21T21:10:54.063" UserId="21692" />
  <row Id="14273" PostId="12917" Score="0" Text="No text. Some categorical responses (converted to dummy variables), some ordinal responses (left alone), and some continuous responses. I've scaled all of them with sklearn.preprocessing.scale. Many NaN values that I've used pd.fillna(0) with. I don't think interpolating/imputing makes sense." CreationDate="2016-07-21T21:42:26.070" UserId="21690" />
  <row Id="14274" PostId="12922" Score="0" Text="There's not much to go on here, but you might want to try using a random forest.  I think of them as a sort of Swiss Army knife of machine learning.  They do an okay job at lots of different kinds of problems, and in the absence of much information they're a good place to start.  Once you have a model, you can try omitting different features to see if the performance of your model improves, decreases, or remains the same.  That might help you to identify extraneous features." CreationDate="2016-07-21T23:01:40.957" UserId="18416" />
  <row Id="14275" PostId="12907" Score="0" Text="If you can have the sign ups broken down per day you could try using CausalImpact from Google or Probablistic Programming  https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers to see what the effect is." CreationDate="2016-07-21T23:07:15.040" UserId="15361" />
  <row Id="14276" PostId="12927" Score="0" Text="FYI  [How to install Theano on Anaconda Python 2.7 x64 on Windows?](http://stackoverflow.com/q/33687103/395857)" CreationDate="2016-07-21T22:56:47.450" UserId="843" />
  <row Id="14277" PostId="12927" Score="1" Text="Have a look at our [help/on-topic] to see what is considered on-topic here - this seems to be a question exclusively about installing software rather than an underlying statistical issue" CreationDate="2016-07-21T23:03:17.513" UserId="11096" />
  <row Id="14278" PostId="12927" Score="1" Text="Do feel free to migrate this to the stackexchange where you think this might be more appropriate!" CreationDate="2016-07-21T23:08:39.833" UserDisplayName="gradstudent" />
  <row Id="14279" PostId="12927" Score="0" Text="This would probably be more appropriate for StackExchange's Data Science forum, which is still technically in Beta but gets a lot of traffic. They cater more to the programming side of machine learning, which is probably why there are tons of posts there about the ins and outs of Theano." CreationDate="2016-07-22T00:26:58.290" UserId="18544" />
  <row Id="14280" PostId="12890" Score="0" Text="Thx. I will check it." CreationDate="2016-07-22T04:41:52.347" UserId="17933" />
  <row Id="14281" PostId="12877" Score="0" Text="What does CURE stand for?" CreationDate="2016-07-22T06:22:03.107" UserId="8820" />
  <row Id="14282" PostId="8694" Score="0" Text="cross validate as usual" CreationDate="2016-07-22T07:06:23.207" UserId="8237" />
  <row Id="14284" PostId="12898" Score="0" Text="original photo(the second photo in the link):  http://m.v4.cc/News-1500016.html  style source:http://www.ivsky.com/tupian/shuimo_fengjing_v3890/pic_119419.html  Thank you.@NeilSlater" CreationDate="2016-07-22T08:18:17.537" UserId="21658" />
  <row Id="14285" PostId="12773" Score="0" Text="hello can you help me to get answer for this question" CreationDate="2016-07-22T09:01:08.907" UserId="20585" />
  <row Id="14286" PostId="12935" Score="0" Text="Sorry I need to revise my question, I mean the coordinate on the new PC coordinate system ( like in your example, it probably will become (x1,0) on direction (1,1) and (1,-1)" CreationDate="2016-07-22T12:23:04.560" UserId="16380" />
  <row Id="14287" PostId="12871" Score="0" Text="Are you normalising the tf-idf vectors? Removing stop words? Cleaning the text? What is the distribution of the data (i.e. class labels)?" CreationDate="2016-07-22T16:16:14.230" UserId="20429" />
  <row Id="14288" PostId="12863" Score="0" Text="There are many goods in dataset. How can we label every goods in dataset?" CreationDate="2016-07-23T02:43:51.700" UserId="21236" />
  <row Id="14289" PostId="12950" Score="1" Text="Thank you, and welcome to DataScience.SE! Please post the gist of it here because blogs come and go, and we like our answers to be self-contained." CreationDate="2016-07-23T04:58:38.880" UserId="381" />
  <row Id="14290" PostId="12877" Score="0" Text="https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm" CreationDate="2016-07-23T05:12:28.687" UserId="14357" />
  <row Id="14291" PostId="12877" Score="0" Text="https://en.wikipedia.org/wiki/CURE_data_clustering_algorithm  &#xA;&#xA;CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases&#xA;&#xA;Wikipedia introduce partitioning, and labeling data on disk&#xA;&#xA;but, I doubt how to implement and wonder why they said it's optimal for large database as the complexity is rather expensive" CreationDate="2016-07-23T05:20:26.623" UserId="14357" />
  <row Id="14293" PostId="12930" Score="0" Text="k-means (and many other clustering algorithms) require the data to be *continuous* vectors. None of your attributes is really appropriate for this. `log(amount)` may be suitable, but certainly the merchant id is not a continuous variable..." CreationDate="2016-07-23T07:35:09.760" UserId="924" />
  <row Id="14294" PostId="12930" Score="0" Text="The bad news is that clustering such data does never work. All you'll get out is statistical nonsense. You sure can get results by massaging your data into numbers, but probably a fortune teller will give you better clusters." CreationDate="2016-07-23T07:36:58.213" UserId="924" />
  <row Id="14295" PostId="12957" Score="0" Text="The answer should depend strongly on what do you want to do with your data and why are your data missing. My first concern would be that observations without missing data could be a very biased sample of your population. Anyway, if you just want to ignore missing data, usually R functions has a parameter to ignore it." CreationDate="2016-07-23T15:47:16.183" UserId="21748" />
  <row Id="14296" PostId="12954" Score="2" Text="Yes, it's possible. Google News does this. Get started." CreationDate="2016-07-23T20:58:15.987" UserId="924" />
  <row Id="14297" PostId="6446" Score="0" Text="Please have a look at rf.pred, testing$target. I got similar error when i ended up in passing a NULL value target class(due to wrong slicing) to confusion matrix. Note: I'm not able to post a comment." CreationDate="2016-07-23T03:00:03.427" UserId="21697" />
  <row Id="14298" PostId="12960" Score="0" Text="have a look at this answer for a general answer- http://datascience.stackexchange.com/a/12736/21024" CreationDate="2016-07-24T13:30:41.937" UserId="21024" />
  <row Id="14299" PostId="12396" Score="0" Text="1) This kind of looks like one of Andrew NG's homework questions... is it? 2) What is the F subscript?" CreationDate="2016-07-24T15:50:39.667" UserId="9420" />
  <row Id="14300" PostId="12396" Score="0" Text="Also, 3) Is the &quot;out of the page arrow&quot; meant to be a vector dot product? 4) Why are you taking the lengths of vectors before squaring them? Its the same thing as just squaring them and will slow your code down." CreationDate="2016-07-24T15:57:34.763" UserId="9420" />
  <row Id="14302" PostId="12967" Score="0" Text="I actually did the pyenchant route a while back.  It works very well." CreationDate="2016-07-24T16:45:39.567" UserId="17309" />
  <row Id="14304" PostId="12962" Score="1" Text="Thanks! I'll give it a try and see how it goes!" CreationDate="2016-07-25T01:34:29.773" UserId="21750" />
  <row Id="14305" PostId="12953" Score="0" Text="Thanks for the detailed answer. Apprecate it. I will try to add few more points.&#xA;I did not standardize the data and I will work on doing it.&#xA;&#xA;1) The reason I kept userid is because, it will help me assocaiate back userid with the cluster in which user is classified. Is this incorrect approach?&#xA;&#xA;&#xA;2) For merchant, I have merchant categories. So probably, will replace merchant id with merchant category" CreationDate="2016-07-25T02:00:11.237" UserId="21707" />
  <row Id="14306" PostId="12953" Score="0" Text="3) I will create categories of amount. Eg - Cheap, moderate, expensive, very_expensive&#xA;&#xA;&#xA;4) Day of Purchase in my data, is actually day of week. I am also planning to add is_weekday and is_weekend vatiables. Brilliant explanation of turning time into two spatial dimension. I will follow that." CreationDate="2016-07-25T02:00:27.083" UserId="21707" />
  <row Id="14307" PostId="12950" Score="1" Text="Thanks. I have added the gist. Sorry, I am new to this process." CreationDate="2016-07-25T04:55:53.633" UserId="21733" />
  <row Id="14309" PostId="12863" Score="0" Text="Well, the first approach I talk about doesn't require labeling at all. If it is practical depends on what you mean by 'many'. Many 100s, and you'll have no problem fitting a time series model to each. You might even get away with it if it's many 1000s. Much more than that, probably no..." CreationDate="2016-07-25T07:08:22.943" UserId="21567" />
  <row Id="14311" PostId="12951" Score="0" Text="Thank you Robert. Is there any link between the kernel density and the frequencies ?" CreationDate="2016-07-25T08:21:44.813" UserId="19212" />
  <row Id="14312" PostId="12951" Score="0" Text="And more importantly, what is the kernel density ?" CreationDate="2016-07-25T08:36:58.770" UserId="19212" />
  <row Id="14313" PostId="12971" Score="0" Text="Thank you for you response. Your notes on ARIMA are helpful and I try to keep them in mind. In case of cross validation and comparing the two models, for the current datasets, using variety of parameters I can tell which algorithm is performing better and so on. The problem is that non of those datasets could represent the real data (streaming data) and that's why I figured it would be better to ask others in the area of data science and machine learning to share their thoughts and experiences." CreationDate="2016-07-25T13:48:49.903" UserId="20544" />
  <row Id="14314" PostId="12971" Score="0" Text="But if you have streaming data then you can turn this into testing data by saving the stream.  The nice part about time series is that you can always create a testing data set out of any data set.  You just need to save some of the data and then roll back time." CreationDate="2016-07-25T13:58:54.620" UserId="9420" />
  <row Id="14315" PostId="12907" Score="0" Text="Thanks for the suggestion - I'll take a look" CreationDate="2016-07-25T14:13:41.013" UserId="21642" />
  <row Id="14316" PostId="12885" Score="0" Text="Thanks Emre ... Going to jump into a few books and also look at some data science courses to get my head wrapped around this.  I thought maybe there was an established algorithm to solving this stuff, but the more I look into it, the more I realize there is not one right answer." CreationDate="2016-07-25T14:15:07.763" UserId="21642" />
  <row Id="14317" PostId="12954" Score="0" Text="Manually labelling them is *not* impossible. It will either take a long time if done by one person, or cost money if done by many. Have you costed this process if performed by Amazon Mechanical Turk workers?" CreationDate="2016-07-25T16:22:08.870" UserId="471" />
  <row Id="14319" PostId="9885" Score="0" Text="Have you tried gephi?" CreationDate="2016-07-25T21:03:25.157" UserId="4968" />
  <row Id="14320" PostId="12951" Score="0" Text="Kernel density is like a histogram but with smoothed relative frequencies. See here  http://stats.stackexchange.com/a/43234/77852" CreationDate="2016-07-25T21:09:45.673" UserId="21737" />
  <row Id="14322" PostId="12981" Score="0" Text="Currently, after some discussion with my colleagues we only want to cluster the news article into business and non-business. Do simply clustering still works?" CreationDate="2016-07-26T04:15:01.393" UserId="20974" />
  <row Id="14323" PostId="12997" Score="0" Text="The noise is AWGN noise, I can smooth the signal but at the end I am looking for one number and it turns out it does not matter if I want to use mean command whether I smooth the signal before or not." CreationDate="2016-07-26T06:56:03.647" UserId="21810" />
  <row Id="14324" PostId="12999" Score="0" Text="how did you get the dataFeatures? Did you combine all the features(Feat1, Feat2..) into a list or so? and what is Y?" CreationDate="2016-07-26T08:46:53.227" UserId="21024" />
  <row Id="14325" PostId="12999" Score="0" Text="@HimaVarsha I've edited the question to include that :)" CreationDate="2016-07-26T08:48:26.420" UserId="17772" />
  <row Id="14328" PostId="12999" Score="0" Text="What does the Y mean?" CreationDate="2016-07-26T08:55:01.237" UserId="21024" />
  <row Id="14329" PostId="12999" Score="0" Text="@HimaVarsha Y is dataLabel - I've also added that to the question" CreationDate="2016-07-26T09:03:34.067" UserId="17772" />
  <row Id="14333" PostId="12871" Score="0" Text="Hadn't normalized. It didn't change much. Yes, I had removed all the stop words and cleaned the text, lemmatized, stemmed, almost all the basic processes." CreationDate="2016-07-26T11:28:36.573" UserId="21608" />
  <row Id="14334" PostId="12951" Score="0" Text="Thanks a lot Robert. You were really helpful" CreationDate="2016-07-26T11:43:13.547" UserId="19212" />
  <row Id="14335" PostId="12955" Score="0" Text="Are quartiles really a good representation of _degrees_ of risk with the kind of distribution seen?" CreationDate="2016-07-26T13:52:53.963" UserId="13450" />
  <row Id="14336" PostId="13001" Score="0" Text="Thanks Ricardo, I have been stuck on this for longer than I care to admit :) Is there any way to map encoded labels back to original labels(integers) after prediction?" CreationDate="2016-07-26T13:56:04.570" UserId="17772" />
  <row Id="14337" PostId="13001" Score="0" Text="@gbhrea I am a little confused. But I edited my answer. Please see if it helps..." CreationDate="2016-07-26T14:44:43.553" UserId="16853" />
  <row Id="14338" PostId="13001" Score="0" Text="Ricardo - sorry I confused myself there - what I meant to say was is there any way to map encoded features back to their original labels?" CreationDate="2016-07-26T14:57:51.753" UserId="17772" />
  <row Id="14340" PostId="13001" Score="0" Text="@gbhrea Ah okay, see my updated answer then :)" CreationDate="2016-07-26T15:30:47.870" UserId="16853" />
  <row Id="14342" PostId="12986" Score="0" Text="Some papers I've come across (e.g., [Deep Learning with Limited Numerical Precision](http://www.jmlr.org/proceedings/papers/v37/gupta15.pdf)) indicate that 8 bits are not enough for adequately training neural networks, but someone might devise a general approach that overturns this conclusion, so it's good for the hardware to be ahead of the software. [Here](https://arxiv.org/abs/1605.06402)'s one attempt. Welcome to DataScience.SE!" CreationDate="2016-07-26T17:48:36.507" UserId="381" />
  <row Id="14343" PostId="13005" Score="0" Text="Welcome to DataScience.SE! I think you need more information; e.g., an itemization of the sales, and knowledge about the types of items each salesperson sells. Is this really all you have?" CreationDate="2016-07-26T18:19:48.400" UserId="381" />
  <row Id="14344" PostId="12955" Score="1" Text="Yes, quantiles also work on skewed data." CreationDate="2016-07-26T19:03:44.080" UserId="924" />
  <row Id="14347" PostId="12784" Score="0" Text="Thanks. Ultimately, I decided to go with difference metrics (Jaccard, etc.)." CreationDate="2016-07-26T19:44:28.927" UserId="21396" />
  <row Id="14348" PostId="13005" Score="0" Text="I wish. There are other data point like &quot;Type&quot;, &quot;Sales Method&quot;, etc... but these are all aggregated at the same level that &quot;# of Sales&quot; is as well." CreationDate="2016-07-26T20:14:00.360" UserId="21829" />
  <row Id="14354" PostId="13010" Score="0" Text="What do you mean `Big datasets`?  What is the their average size?" CreationDate="2016-07-27T06:51:05.023" UserId="11097" />
  <row Id="14355" PostId="13017" Score="0" Text="The issue is that character distribution is a bad key for anagrams: dict key should be unique, so all anagrams, except the last one will be flushed." CreationDate="2016-07-27T07:19:49.080" UserId="941" />
  <row Id="14356" PostId="13017" Score="1" Text="No, it won't; the value is a list of all the anagrams with the same key (distribution). Isn't identity of the character distribution precisely what constitutes an anagram?" CreationDate="2016-07-27T07:29:54.243" UserId="381" />
  <row Id="14357" PostId="13022" Score="0" Text="Isn't this just a standard use case for a stacked chart like [this](https://betterfigures.files.wordpress.com/2013/09/ldeaths5.png)?" CreationDate="2016-07-27T07:48:16.113" UserId="381" />
  <row Id="14358" PostId="12905" Score="0" Text="I've a structurally complete set of strings with respect to target language/automaton.&#xA;Then, all the needed states are &quot;observable&quot;." CreationDate="2016-07-27T09:41:28.187" UserId="14955" />
  <row Id="14360" PostId="13017" Score="0" Text="You are right, in &quot;dict of lists&quot; case it will work, indeed. Didn't think about such implementation." CreationDate="2016-07-27T10:47:52.533" UserId="941" />
  <row Id="14362" PostId="13029" Score="0" Text="Please also let me know if there is an error in my pseudocode" CreationDate="2016-07-27T14:35:31.463" UserId="8820" />
  <row Id="14363" PostId="13032" Score="0" Text="Vitaly, you hit the nail the head on your apology.  This question is too vague for anyone to give you meaningful, insightful help.  Scikit-learn is perfectly suited for data on the scale of GB, but unless you can give a better description of what you're hoping to classify, and why a particular set of characteristics may be predictive, it's tough to comment further." CreationDate="2016-07-27T17:31:28.770" UserId="8041" />
  <row Id="14364" PostId="13028" Score="0" Text="Welcome to DataScience.SE! What software exactly do you want to be running on your local server to read the model?" CreationDate="2016-07-27T17:52:33.753" UserId="381" />
  <row Id="14365" PostId="12684" Score="0" Text="Have you already see the Hunspell package from R? You can download a Spanish dictionary and use with it for spelling corrections.&#xA;You can also check github to see if anyone has any project in this area for the Spanish language." CreationDate="2016-07-27T18:00:04.160" UserId="21044" />
  <row Id="14367" PostId="13028" Score="0" Text="@Emre, Thanks for the welcome. I have a &quot;decision&quot; server written in pure Java that I'd like to load the model into and query. I'm not opposed to using any libraries, so long as the model can be loaded by and run in the JVM." CreationDate="2016-07-27T18:23:38.257" UserId="21853" />
  <row Id="14368" PostId="13028" Score="0" Text="Obviously you need a machine learning library that can read your model parameters. I have no practical experience with them, but there's a good chance [DL4J](http://deeplearning4j.org/image-data-pipeline.html#record) and maybe [H2O](https://github.com/h2oai/sparkling-water) will do what you want. What kind of machine learning models are you considering?" CreationDate="2016-07-27T18:35:36.937" UserId="381" />
  <row Id="14369" PostId="13032" Score="0" Text="@j.a.gartner there is reason to believe that certain words/n-grams in text fields are associated with triggering the binary parameter (say, B). As to why a set of characteristics may be predictive, that's one part of what I'm trying to find out -- which combinations of characteristics, if any, predict the desired B. The total number of columns in the data frame is not very large, about 5 to 10, and my question was mostly about meaningfully transforming them into a shape on which I could act with random forests." CreationDate="2016-07-27T20:28:15.223" UserId="21854" />
  <row Id="14370" PostId="13040" Score="1" Text="I think this is a bit of a lost cause - sorry. There is no point smoothing the data since you don't have noise in your measurements. Setting an upper limit for predictions is just arbitrary, and using non-linear regression could fit your data perfectly with a sufficiently high-order polynomial, but would be hopeless at predicting (generalising) a trend. With the data that you have, the best you could hope for is (perhaps) a vaguely increasing trend over time, but any predictions would have a large error." CreationDate="2016-07-27T20:42:43.927" UserId="17509" />
  <row Id="14371" PostId="5838" Score="0" Text="@sylvia  - I have similar problem to solve. I had posted it as a separate question. Could you give some suggestions on how you solved it?&#xA;http://datascience.stackexchange.com/questions/12930/clustering-users-based-on-buying-behaviour?noredirect=1#comment14294_12930&#xA;&#xA;My other doubt is for K means, did you group the records by customer? Meaning did each row represented a transaction or it represented aggregated purchases of that customer till date." CreationDate="2016-07-28T02:45:02.137" UserId="21707" />
  <row Id="14372" PostId="5838" Score="0" Text="@logc - Glad to have your suggestion as well" CreationDate="2016-07-28T02:45:26.513" UserId="21707" />
  <row Id="14374" PostId="13049" Score="0" Text="doesn't your third answer (&quot;should I assume no hashing ...&quot;) contradicts the second one (&quot;should I enumerate bit length for each feature...&quot;)?! if some features are more &quot;important&quot; and &quot;diverse&quot; they might need a higher hashing bit-length representation." CreationDate="2016-07-28T08:34:01.270" UserId="21596" />
  <row Id="14375" PostId="13040" Score="0" Text="Your data consists of only time and the variable you want to predict? Do you have many observations? Before applying an ARIMA model, I would start to find trends in your data. Maybe they play worse in the winter than the summer? Maybe they play better on Fridays? You have to try to add more variables into the mix." CreationDate="2016-07-28T10:58:35.713" UserId="16853" />
  <row Id="14376" PostId="13034" Score="0" Text="Thanks for the advice, but your points do not really address my specific questions. I had even explicitly mentioned stop word removal and stemming in the question. Also, I know word2vec, but I am following a different trail here." CreationDate="2016-07-28T11:00:50.053" UserId="21858" />
  <row Id="14377" PostId="13040" Score="0" Text="If you have a lot of observations, you could try to reframe the problem a little, and use a [quantile regression](https://en.wikipedia.org/wiki/Quantile_regression) to see if there are patterns within which you can predict with higher confidence. For instance, maybe you can predict the next score very well if it has been increasing. Maybe start with something simpler like a logistic regression to tell you whether in the next game score will go up or down. But anyhow, you should to try to add more variables like what kind of game or championship it is, what players are playing, against whom..." CreationDate="2016-07-28T11:03:38.487" UserId="16853" />
  <row Id="14380" PostId="13056" Score="0" Text="This is perfect. Thanks a lot !" CreationDate="2016-07-28T14:08:29.993" UserId="21881" />
  <row Id="14381" PostId="13048" Score="0" Text="Thanks for the link" CreationDate="2016-07-28T14:33:21.670" UserId="13736" />
  <row Id="14382" PostId="12551" Score="0" Text="I managed to reduce the number of variables using some simple rules and I could train all random forests. Like you said, naive bayes was less accurate but was faster to train. Thanks for your answer!" CreationDate="2016-07-28T14:49:32.147" UserId="4978" />
  <row Id="14383" PostId="9824" Score="0" Text="Can the two parts be trained jointly?" CreationDate="2016-07-28T14:50:02.307" UserId="21895" />
  <row Id="14384" PostId="13057" Score="1" Text="Just to clarify, you want to find probabilities of a response that only occurs 1% of the time using logistic regression?  Or do you still want to use random forest?  Also, did the 10% sample do poorly in cross-validation?" CreationDate="2016-07-28T15:18:04.353" UserId="19161" />
  <row Id="14385" PostId="13057" Score="0" Text="@Hobbes I want to keep using random forests. Yes, the classification acuracy in test set using the balanced dataset was 20% better." CreationDate="2016-07-28T15:21:13.017" UserId="4978" />
  <row Id="14386" PostId="13058" Score="0" Text="Did it work on Spark 1.6.x? Refer to the [version compatibility list](https://github.com/metreta/spark-orientdb-connector/blob/master/docs/version_list.md). And welcome to DataScience.SE!" CreationDate="2016-07-28T18:04:40.457" UserId="381" />
  <row Id="14387" PostId="13054" Score="0" Text="I _am_ doing that, this is an additional measure. Thanks, though." CreationDate="2016-07-28T18:28:14.380" UserId="13450" />
  <row Id="14388" PostId="13057" Score="2" Text="Have you considered a different probability threshold for classification purposes (maybe everything p&gt;.10 instead of p&gt;.50)?  I would recommend also examining precision and recall in a situation like this (including ROC/AUC/F1/Brier Score).  In my experience trees don't bode to well with major imbalance and trying to turn a classification result into a modeled probability is a tough task." CreationDate="2016-07-28T18:29:07.883" UserId="14913" />
  <row Id="14389" PostId="13057" Score="0" Text="I used the ROC curve to choose the probability threshold for both cases in my simulations. And I examined the KS." CreationDate="2016-07-28T18:34:30.803" UserId="4978" />
  <row Id="14391" PostId="12684" Score="0" Text="Yes I have used Hunspell, which is written in many languages as well (C++, R, .NET, Java, etc.) I tried them almost all (even the paid license) and the correction level is very low, we tested less than 30% effective, our was 93% on the same wordset. also I have not seen any resources on github yet. Thank you," CreationDate="2016-07-28T20:44:50.097" UserId="21276" />
  <row Id="14392" PostId="12938" Score="0" Text="Thank you very much for your input. This is very helpful!" CreationDate="2016-07-28T21:05:41.550" UserId="21242" />
  <row Id="14393" PostId="10957" Score="0" Text="Where does your `rts.fut` come from? How can we confirm this if you don't give us a reproducible example? Also there are many more R experts on StackOverflow than there are here, so for simple R questions you will do better posting there." CreationDate="2016-07-28T22:08:26.843" UserId="471" />
  <row Id="14395" PostId="13054" Score="0" Text="If you'd use an ml classifier like the SGD from the Scikit-learn it will create a model that would contain your &quot;ranks&quot; after it is trained. No need to manually calculate them. So may be I misunderstood your endeavor and you want to redesign a classifier algorithm itself?" CreationDate="2016-07-28T22:31:00.497" UserId="15361" />
  <row Id="14396" PostId="6727" Score="0" Text="If the answers solve your problem please mark the one you like." CreationDate="2016-07-28T22:57:32.083" UserId="381" />
  <row Id="14398" PostId="10510" Score="1" Text="I wanted to add that while one-hot encoding zip will work just fine, a zip code is a content rich feature, which is ripe for value-added feature engineering. So you should think about the things it could add to your data if you inner join it to other zip code data sets. States can be extracted, latitude and longitudes can be extracted, average summer high temperatures, days per year of rain, gun ownership, socio-economics, upward mobility, average home values, average education level, degree of urbanization, ...  The list goes on and on, so zip can be highly rich in adding information." CreationDate="2016-07-29T03:54:41.363" UserId="9420" />
  <row Id="14399" PostId="13072" Score="1" Text="What do you mean by a `data science companies`?  My company delivers food, but we do some pretty serious DS stuff here :)" CreationDate="2016-07-29T08:09:31.487" UserId="11097" />
  <row Id="14400" PostId="13072" Score="0" Text="This kind of work is often done by external consultancy companies (at least here in EU), but they tend to offer whole package of services, therefore you don't find often pure Data Science companies (but they exist). So i'd say look for those and ask how much Data Science they do." CreationDate="2016-07-29T08:42:23.970" UserId="17290" />
  <row Id="14401" PostId="13074" Score="0" Text="Thank you for your reply. No. It is not good at all. It miss cluster many points. it clusters about half of points in one cluster and the half in another cluster." CreationDate="2016-07-29T09:13:01.473" UserId="12345" />
  <row Id="14402" PostId="13074" Score="0" Text="Ok, how about hierarchical clustering ? You will have to cut the tree at 2 clusters." CreationDate="2016-07-29T09:15:37.193" UserId="21755" />
  <row Id="14404" PostId="13071" Score="0" Text="But this will not replace the values in original dataframe and I want to replace the original values by the mean values in the original dataframe" CreationDate="2016-07-29T09:48:50.700" UserId="15412" />
  <row Id="14405" PostId="13074" Score="0" Text="k-means with 2 clusters does not work for this dataset, because samples in visual section A can be closer to any centroid in B than they are to a centroid in A." CreationDate="2016-07-29T10:29:29.850" UserId="836" />
  <row Id="14406" PostId="13036" Score="0" Text="I am not the OP, but wondering, do you know if it is possible to apply data augmentation to text in order to extend the dataset? (as is done to images when training CNNs)" CreationDate="2016-07-29T12:03:35.917" UserId="16853" />
  <row Id="14407" PostId="13072" Score="0" Text="It would help to know what country you are from and what your background is." CreationDate="2016-07-29T12:56:00.007" UserId="16853" />
  <row Id="14408" PostId="13082" Score="2" Text="Neither is better. But if you have some goals in mind, you might be able to get some advice on what balance has worked towards those goals. So please explain about your specific goals for studying ML." CreationDate="2016-07-29T15:23:46.483" UserId="836" />
  <row Id="14409" PostId="13036" Score="0" Text="It stands to reason that it ought work, but I've never done this myself.  In the case of the IMDB dataset, you might try using the Yelp dataset to expand the language base; it's still text associated with a rating, so it may increase your flexibility to 'sarcasm' words." CreationDate="2016-07-29T15:46:10.663" UserId="8041" />
  <row Id="14410" PostId="13071" Score="1" Text="@Nain &#xA;`df = df.groupby(['Hospital_ID', 'District_ID']).mean()`" CreationDate="2016-07-29T16:46:43.577" UserId="20722" />
  <row Id="14411" PostId="13059" Score="0" Text="By merge, do you mean performing JOIN operations or appending one file to another?" CreationDate="2016-07-29T16:51:57.967" UserId="20722" />
  <row Id="14412" PostId="13040" Score="0" Text="Thanks for your input @RicardoCruz -- I think I'll start looking at logistic regression and then check out quantile regression.&#xA;&#xA;Would binary variables like is_friday (as you suggested) potentially increase accuracy, or does regression require a continuous value as a feature?" CreationDate="2016-07-29T16:53:46.057" UserId="21862" />
  <row Id="14413" PostId="13071" Score="0" Text="Rohan's method is the only way; you can not aggregate in place." CreationDate="2016-07-29T17:14:42.803" UserId="381" />
  <row Id="14414" PostId="13059" Score="0" Text="JOIN operation . Appending isn't that costly." CreationDate="2016-07-29T17:21:32.873" UserId="15412" />
  <row Id="14415" PostId="13089" Score="0" Text="How did you create the dataframe in the first place? Are the &quot;NAN&quot; values literally three character strings. If not, what are they?" CreationDate="2016-07-29T17:52:36.493" UserId="381" />
  <row Id="14416" PostId="13089" Score="0" Text="The data is provided in form a CSV file by one of my client. And 'NAN' appeared as values in the columns for which no information was provided in the CSV. 'NAN' just represents that no info about that column is available." CreationDate="2016-07-29T17:56:15.237" UserId="15412" />
  <row Id="14417" PostId="13091" Score="0" Text="I want to convert Hospital_names and States to float values." CreationDate="2016-07-29T18:08:35.550" UserId="15412" />
  <row Id="14418" PostId="13059" Score="1" Text="Can you hold at least one of them in RAM? If so, you can use iterate over the second frame in chunks to do your join, and append the results to a file in a loop." CreationDate="2016-07-29T18:08:42.053" UserId="21940" />
  <row Id="14419" PostId="13090" Score="0" Text="I want to convert the Hospital_name and State values (which are strings) to float because the model I am going to define is dependent on Hospital_name too." CreationDate="2016-07-29T18:09:56.457" UserId="15412" />
  <row Id="14420" PostId="13091" Score="0" Text="There's no standard for converting a sting to a float. You'll have to define your own mapping, for example `{'Delhi' : 0, 'Chennai' : 1}`, and use [map] (http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html)" CreationDate="2016-07-29T18:12:57.867" UserId="21940" />
  <row Id="14421" PostId="13090" Score="0" Text="You will have to use a mapping. If you want some nlp related advice, maybe you can use some word embeddings like word2vec and convert word to a vector." CreationDate="2016-07-29T18:19:58.873" UserId="21024" />
  <row Id="14422" PostId="13059" Score="0" Text="AFAIK it is not possible in Python. You could use Spark with Hive. You can load data and run SQL like queries on it." CreationDate="2016-07-29T18:20:10.257" UserId="20722" />
  <row Id="14423" PostId="13095" Score="0" Text="Thank you. No, this is not my only figure. I have many figures like this and visualization will take so much space in my work. Therefore, I need a good algorithm to work as visualization. Yes, DBSCAN with Epsilon=0.05 works well but  single linkage  is so poor in this issue. What is the point in selecting Epsilon? Btw, Do you know any open source tool that has OPTICS ?" CreationDate="2016-07-29T22:01:28.250" UserId="12345" />
  <row Id="14424" PostId="13095" Score="0" Text="OPTICS works well for me. I use the ELKI version, which is the only feature-complete version I know. It's open source." CreationDate="2016-07-29T22:57:33.150" UserId="924" />
  <row Id="14425" PostId="13040" Score="0" Text="@Chris, you have a problem that sounds to me as difficult as the stock market. Stock markets look almost random, because if they were easily predictable, everybody would try to predict them. Your problem looks just as difficult, which is why I suggested classification/logistic. It is my understand that is a little easier to predict whether it goes up or down (and you should buy or sell) than the specific price. This [wikipedia article](https://en.wikipedia.org/wiki/Stock_market_prediction) seems to corroborate that (search for &quot;[2]&quot;)." CreationDate="2016-07-30T10:19:58.183" UserId="16853" />
  <row Id="14426" PostId="13040" Score="0" Text="With regard to quantile regressions, if your goal with this is just to learn machine learning, I would avoid them. They are very interesting optimization problems where you optimize for a given quantile, instead of for the average error (or the MSE). So, you get confidence levels. You'll probably only find them implemented in R. They are very interesting models, but most people do not know about them. It is something we use where I work. You can google them a bit, but probably not worth your time if you just want to learn machine learning." CreationDate="2016-07-30T10:26:22.083" UserId="16853" />
  <row Id="14427" PostId="12110" Score="0" Text="Why do you say &quot;no padding&quot; in Figure 1, if actually input is zero-padded?" CreationDate="2016-07-30T13:06:04.790" UserId="11148" />
  <row Id="14428" PostId="13047" Score="0" Text="Thank you. So, how is PCA analysis helpful to determine best contributing variables? Or is it used for a different purpose?" CreationDate="2016-07-30T15:31:46.670" UserId="13736" />
  <row Id="14429" PostId="13103" Score="0" Text="Thank you so much for your reply! This is great information and the example is very helpful. I think the stock market analogy is very true, and I'll start reading more about data science/machine learning in that field." CreationDate="2016-07-30T16:11:07.327" UserId="21862" />
  <row Id="14430" PostId="13099" Score="0" Text="If the dependent variable is different you can not compare the R2." CreationDate="2016-07-30T19:09:41.453" UserId="21737" />
  <row Id="14431" PostId="13099" Score="0" Text="The dependent variable is the same.  The study area is different though." CreationDate="2016-07-30T21:07:21.640" UserId="15032" />
  <row Id="14432" PostId="13062" Score="0" Text="Thanks Thomas.  What are your thoughts about reorganizing the dataset so that each row represents the total sales by salesperson where the features are the sales people (values the # of times each person was on team).  So for Fred, y = 36 for example." CreationDate="2016-07-31T02:22:30.663" UserId="21829" />
  <row Id="14433" PostId="13107" Score="0" Text="I didn't get you. If the document belongs to C, why wouldn't you want to tag it with C during training?" CreationDate="2016-08-01T07:09:06.297" UserId="21024" />
  <row Id="14434" PostId="13047" Score="0" Text="Determining best contributing variables is not PCA purpose, neither a easy task to perform hereafter. Its aim is to 1. get the direction explaining as much variance within the data as possible 2. Eventually reduce the dimension of your data by keeping the _most contributing_ new variables (which are a combinaison of your initial one) [Doc](https://en.wikipedia.org/wiki/Dimensionality_reduction#Principal_component_analysis)" CreationDate="2016-08-01T08:35:12.730" UserId="20759" />
  <row Id="14435" PostId="13028" Score="0" Text="If I may ask, what do you mean by &quot;exporting a model and loading it&quot;. How do you actually perform your export ?" CreationDate="2016-08-01T08:42:57.790" UserId="5177" />
  <row Id="14436" PostId="13118" Score="5" Text="I didn't read his book but I doubt there is such a quest for a master algorithm since the No Free Lunch theorem have been introduced ([here](https://en.wikipedia.org/wiki/No_free_lunch_theorem) and [here](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)). It looks more like a marketing strategy rather than a scientific day-to-day quest (I'm not sure many scientists will agree that &quot;_the race is on to invent the ultimate learning algorithm: one capable of [...] doing anything we want, before we even ask_&quot; and &quot;_that enables [...] computers to program themselves_&quot;)" CreationDate="2016-08-01T08:46:40.050" UserId="20759" />
  <row Id="14438" PostId="13121" Score="0" Text="Hello, thanks for your response. I updated the question with a small example. Would you update your answer to show how would you apply this to the example I provided? Thanks." CreationDate="2016-08-01T10:18:56.670" UserId="21992" />
  <row Id="14439" PostId="13099" Score="0" Text="`R^2` for the second model is not that high imho... it only &quot;explains&quot; (or better covers) ~50% of the variance, right ?" CreationDate="2016-08-01T10:42:50.647" UserId="21995" />
  <row Id="14440" PostId="13121" Score="0" Text="I understand it a lot better after your edit. But why would want a single vector? why can't you use all the words(vectors) instead for classification?" CreationDate="2016-08-01T11:11:21.980" UserId="21024" />
  <row Id="14441" PostId="13121" Score="0" Text="I use all the word vectors. But I need to represent every tweet in a way they all have the same size. For the classifier, all of the examples must have the same size." CreationDate="2016-08-01T11:13:26.133" UserId="21992" />
  <row Id="14442" PostId="13121" Score="0" Text="Have you tried using a vectorizer and doing fit_transform for the tweets?" CreationDate="2016-08-01T11:18:26.457" UserId="21024" />
  <row Id="14443" PostId="8923" Score="0" Text="I think they stopped developing Mocha and MXNet is the way to go forward. See malmaud's comment here: https://github.com/pluskid/Mocha.jl/issues/157" CreationDate="2016-08-01T13:31:17.993" UserId="22000" />
  <row Id="14444" PostId="13124" Score="0" Text="You can just type in R `?summary.lm`. The documentation tells you the statistics name of each word, and you can then use wikipedia to learn more about each." CreationDate="2016-08-01T13:32:12.950" UserId="16853" />
  <row Id="14445" PostId="13124" Score="0" Text="Thanks Ricardo but i just want to know what experts says about this specific summary ." CreationDate="2016-08-01T14:10:48.447" UserId="21384" />
  <row Id="14446" PostId="13124" Score="6" Text="Ah, this specific summary! $\beta_0=4.2$ and $\beta_1=15.5$. Your $\beta_1$ is statistically significative (t-test &lt;&lt; 0.05). $R^2=0.99$ meaning 0.99 of the variance in the dependent variable is explained by your regression. Finally, the F-test is statistically significative as well. In other words, it looks like a good regression." CreationDate="2016-08-01T14:37:49.727" UserId="16853" />
  <row Id="14447" PostId="13123" Score="1" Text="If you have an error message, you should post it; it most likely has important info in helping to debug the situation." CreationDate="2016-08-01T15:55:21.857" UserId="8041" />
  <row Id="14448" PostId="10836" Score="0" Text="Me too. I also looking for a reference/citation on TimeDistributedDense" CreationDate="2016-08-01T16:30:51.030" UserId="9465" />
  <row Id="14449" PostId="13114" Score="0" Text="So you want to filter the users by keyword, then break down their topical distributions? If so, obviously you will need to infer the topics of the documents they read." CreationDate="2016-08-01T18:41:27.963" UserId="381" />
  <row Id="14450" PostId="12780" Score="0" Text="Tried it...I guess my data is such that mean doesn't actually work very well. I'm going to need a better imputation method." CreationDate="2016-08-01T19:06:03.003" UserId="21462" />
  <row Id="14451" PostId="13132" Score="0" Text="Thank you for the answer, except from these two are there any other ? It seems strange to me that only these two exist." CreationDate="2016-08-01T19:54:02.817" UserId="14560" />
  <row Id="14452" PostId="13123" Score="0" Text="i'm trying to import csv contents into pandas dataframes and then  converting it into spark data frames....but it is showing error something like &quot;Py4JJavaError&quot;   An error occurred while calling o28.applySchemaToPythonRDD.&#xA;: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient" CreationDate="2016-08-02T04:59:02.320" UserId="21998" />
  <row Id="14453" PostId="13123" Score="0" Text="and my code was--&gt;  from pyspark import SparkContext&#xA;from pyspark.sql import SQLContext&#xA;import pandas as pd&#xA;sqlc=SQLContext(sc)&#xA;df=pd.read_csv(r'D:\BestBuy\train.csv')&#xA;sdf=sqlc.createDataFrame(df)   ----&gt; Error" CreationDate="2016-08-02T05:01:07.863" UserId="21998" />
  <row Id="14454" PostId="13029" Score="0" Text="I think a good place to start to understand this would be this recent paper : http://jmlr.org/proceedings/papers/v32/silver14.pdf" CreationDate="2016-08-02T06:47:12.493" UserId="9577" />
  <row Id="14456" PostId="13123" Score="0" Text="Welcome to DataScience.SE! Please edit your original post instead of adding comments." CreationDate="2016-08-02T07:54:02.360" UserId="381" />
  <row Id="14457" PostId="13107" Score="0" Text="If the document belongs to C I want it to be tagged as such but what about D, E and F? I don't know if the document belongs to D or doesn't belong to it. If I'll choose to tag the document to D (meaning that the output of class D will be 1) than it will be wrong, if I'll choose to tag the document as if it doesn't belong to D (meaning of class D will be 0) it will wrong as well. I'm not sure about class D so in such case I don't want the network to change its weights as a result of class D." CreationDate="2016-08-02T08:03:57.913" UserId="17443" />
  <row Id="14458" PostId="13107" Score="0" Text="Did I made myself clear now?" CreationDate="2016-08-02T08:04:18.620" UserId="17443" />
  <row Id="14459" PostId="13107" Score="0" Text="Are you referring to multilabel classification?" CreationDate="2016-08-02T08:44:29.143" UserId="21024" />
  <row Id="14460" PostId="13107" Score="0" Text="Yes, just like in the title" CreationDate="2016-08-02T08:57:13.457" UserId="17443" />
  <row Id="14461" PostId="13145" Score="0" Text="As Kaggle requires you to agree to T&amp;Cs before downloading, this is not going to be easy to work around." CreationDate="2016-08-02T10:49:54.233" UserId="836" />
  <row Id="14462" PostId="13115" Score="0" Text="I found [slides by David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Resources_files/AlphaGo_IJCAI.pdf) who explained that the improved the policy network, the value network, the search; and the hardware by switching from GPU to TPU (tensor processor units)." CreationDate="2016-08-02T14:04:02.893" UserId="21982" />
  <row Id="14463" PostId="13028" Score="0" Text="@eliasah, It may be that my understanding of how machine learning models work. From working with the data-scientist on our team, I've come under the impression that once a model is trained, you can export that model to a framework-independent format.&#xA;&#xA;Am I incorrect in thinking that a model can be thought of as a mathematical function? (Takes an input to its predict method, returns output.) If training the model is simply refining a mathematical function, what prevents that function from being evaluated in a different language?" CreationDate="2016-08-02T14:28:24.480" UserId="21853" />
  <row Id="14464" PostId="13028" Score="0" Text="The only export available now is the PMML format and I'm not sure it's the case for all the algorithms. But still what you are trying to do isn't very clear to me. How do you load your model, technically ? Are you re-training the same algorithm each time ?" CreationDate="2016-08-02T14:31:39.213" UserId="5177" />
  <row Id="14465" PostId="13028" Score="0" Text="@eliasah, loading the model is what I'm trying to figure out. I understand how to load a model into a Spark context and query it. That scenario shares similarities with querying a database service. What I'm trying to do is load a model into native Java data structures so the query time can be much faster." CreationDate="2016-08-02T16:04:42.550" UserId="21853" />
  <row Id="14466" PostId="13150" Score="0" Text="I don't know this method, and I doubt many do, but the paper says &quot;Our scheme of mRMR does not intend to select features that are independent of each other. Instead, at each step, it tries to select a feature that minimizes the redundancy and maximizes the relevance. For real data, the features selected in this way will have more or less correlation with each other. However, our analysis and experiments show that the joint effect of these features can lead to very good classification accuracy.&quot; Why don't you just try it?" CreationDate="2016-08-02T17:43:09.550" UserId="381" />
  <row Id="14467" PostId="13152" Score="0" Text="Thank you very much for your answer! I learned a lot with it. But I still can't solve my problem. I understand the binomial model I can find  new estimates for the coefficients using this approach. But when I'm using random forest is it still possible practically?" CreationDate="2016-08-02T17:53:01.153" UserId="4978" />
  <row Id="14468" PostId="13153" Score="0" Text="Cheers, thanks a lot :)" CreationDate="2016-08-02T20:16:54.283" UserId="22034" />
  <row Id="14469" PostId="13160" Score="0" Text="Thanks for the inputs. Can you mention some famous `anomaly detection` algorithms (maybe from the point of view of `Java` or `Python`) that seems relevant?" CreationDate="2016-08-03T05:15:17.480" UserId="22045" />
  <row Id="14473" PostId="13160" Score="0" Text="I have edited my answer to reflect the same." CreationDate="2016-08-03T05:49:15.323" UserId="21414" />
  <row Id="14476" PostId="13150" Score="0" Text="@Emre thanks for your comment, I tried it. So, do you interpret that sentence as that one combination of the variables could be selected by the algorithm? Because my interpretation was that mRMR selects features which are good *per se* and that taken together are good to classify better. However my problem, which I haven't proved that is equivalent, is that I need to select features which are relevant *when added/subtracted one with another*. In your opinion, is that the same problem?" CreationDate="2016-08-03T08:03:07.977" UserId="133" />
  <row Id="14478" PostId="13145" Score="0" Text="phiver You are correct. I cannot download the data without logging in. How did you know that this code only works for cites where you do not  have to be logged on." CreationDate="2016-08-03T09:32:03.347" UserId="18811" />
  <row Id="14479" PostId="13164" Score="0" Text="Thanks, that seems to have done it. Especially the larger number of epochs. I had just assumed that because the MNIST dataset converges to a high degree of accuracy on a small number of epochs (98%+ in 3 epochs!) that the Iris dataset, being simpler would also have such quick convergence" CreationDate="2016-08-03T13:21:49.613" UserId="3313" />
  <row Id="14480" PostId="13150" Score="0" Text="@Emre what I am searching for is like this: http://www.nature.com/articles/srep30672 . I will try to use this (the paper was published last week)" CreationDate="2016-08-03T16:10:52.950" UserId="133" />
  <row Id="14481" PostId="13115" Score="0" Text="And there is a [talk by Fan Hui and Aja Huang](https://www.youtube.com/watch?v=LX8Knl0g0LE) about the progress." CreationDate="2016-08-03T17:17:17.243" UserId="21982" />
  <row Id="14482" PostId="13162" Score="0" Text="Thanks :) That comment was added 7 days ago, after this question.  After some digging I have asked Tensorflow team in Google group why they definition of LSTM cell is differs from literature LSTM cell.. and they have added that comment :)" CreationDate="2016-08-03T17:59:07.820" UserId="21319" />
  <row Id="14483" PostId="13062" Score="0" Text="What do you mean by &quot;no interaction effects&quot;?  Because it would seem there will be given that sales teams can be mixed (e.g. Fred is part of both teams 1 and 3 above though his partners were different)." CreationDate="2016-08-03T18:29:08.487" UserId="21829" />
  <row Id="14484" PostId="13062" Score="0" Text="No interaction effects means that, for example, John and Wayne don't create more or less value together than apart. A negative interaction effect might occur because they don't like each other, so they hinder each others efforts. A positive interaction effect might occur if they help each other out more than they do other team members." CreationDate="2016-08-03T19:37:18.670" UserId="6403" />
  <row Id="14485" PostId="13174" Score="0" Text="Both the OP and the comment already link to that package. Did it work for you with Spark 2.0?" CreationDate="2016-08-03T21:23:51.167" UserId="381" />
  <row Id="14486" PostId="13176" Score="0" Text="So we need the neural network to be operational even with encrypted weights. It sounds like what you're describing just encrypts data when it's stored, but to actually make use of the data you have to decrypt it and ship it to the client, which is not actually what I want to do. The neural network needs to work without being decrypted." CreationDate="2016-08-03T21:34:31.040" UserId="22075" />
  <row Id="14487" PostId="13176" Score="0" Text="No, that's the thing - you don't need to decrypt it to use it - that why it's called &quot;Transparent.&quot; I set up my own TDE encryption long ago and haven't had to mess with it at all. I wrote VB.net code to import the data and operate on the weights, display them in a GUI, etc. without once having to take the encryption into account or mess with it. I forgot it's even there. The data would be obfuscated, however, if I were a user without sufficient permissions tried to access it." CreationDate="2016-08-03T23:01:26.000" UserId="18544" />
  <row Id="14488" PostId="13176" Score="0" Text="More clarification:any decryption occurs under the hood,so the neural net algorithms that access the weights don't even have to take TDE into account - as long as I've got the right permissions. I can change the decrypted values any way I like or display them etc. They only appear encrypted to unauthorized users; since I have the right permissions, I never see or operate on the encrypted values. Your neural net will work the same. I could also authorize the program to act as a middle man and display the decrypted data to unauthorized users just as seamlessly, should the use cases call for it." CreationDate="2016-08-03T23:10:14.657" UserId="18544" />
  <row Id="14489" PostId="13180" Score="1" Text="Great answer, I would just add that it depends a bit on the particular distribution of data that you are dealing with and whether you are removing outliers.  Normal data will look very good if you standardize it.  However, a uniform distribution might look much better with min/max normalization. Further, a log normal distribution with sigma=10 might hide much of the interesting behavior close to zero if you min/max normalize it." CreationDate="2016-08-04T04:17:57.153" UserId="9420" />
  <row Id="14490" PostId="13176" Score="0" Text="We can't trust the user with the unencrypted weights though. Let's say Alice wants to use Bob's neural network, but Bob can't afford to rent out a server to crunch Alice's data. The weights for the NN are intellectual property, so Bob doesn't want to share the weights with Alice. Bob sends Alice some sort of holomorphic encrypted neural network so Alice can run the network on her own machine, without being able to derive the weights. Here Alice would need to be an &quot;authorized user&quot; that sees the weights in cleartext, i.e. Bob must trust Alice. Is there a trust-less way to do this?" CreationDate="2016-08-04T04:34:17.747" UserId="22075" />
  <row Id="14491" PostId="13176" Score="0" Text="You could send Alice a TDE-encrypted database to run on her own machine, but only grant access permissions to a front-end application that limits her to viewing only the information you want, including hiding the weights. In that case, she won't able to view the data as clear text because she'll lack the proper permissions. This isn't a foolproof solution if Alice has sysadmin rights on her computer - see http://sqlblog.com/blogs/jonathan_kehayias/archive/2010/05/06/the-database-as-intellectual-property.aspxt as to why - so it would help to use secondary encryption functions in addition to TDE" CreationDate="2016-08-04T06:25:09.653" UserId="18544" />
  <row Id="14492" PostId="13176" Score="1" Text="If you're an admin who can manage the Windows OS-level permissions for these neural net users, you just have to lock down those so they can't hack the encrypted database you've installed on their system. If they have admin rights and you don't, I don't think there's any foolproof way of encrypting data on another user's machine that a determined, knowledgeable user can't hack in time. As far as out-of-the-box solutions go, however, this is a reasonable option to consider, one that at least throws a lot of roadblocks in the way of anyone who wants to hack your neural weights." CreationDate="2016-08-04T06:32:16.533" UserId="18544" />
  <row Id="14493" PostId="13189" Score="0" Text="Does that change the formulation of the LSTM itself in any way?" CreationDate="2016-08-04T08:37:35.813" UserId="23097" />
  <row Id="14494" PostId="13189" Score="0" Text="Or does it change the way that the LSTMs are linked together?" CreationDate="2016-08-04T08:37:56.220" UserId="23097" />
  <row Id="14495" PostId="13189" Score="0" Text="Sorry, I'm not an expert, moreover in NLP. Maybe others can chime in." CreationDate="2016-08-04T08:41:05.313" UserId="9465" />
  <row Id="14496" PostId="13189" Score="0" Text="This looks like an implementation of seq2seq in Keras, maybe there's some insight of that in this https://github.com/farizrahman4u/seq2seq" CreationDate="2016-08-04T08:50:03.547" UserId="9465" />
  <row Id="14497" PostId="13189" Score="0" Text="I will look at it and see if it helps. Thanks" CreationDate="2016-08-04T08:56:24.157" UserId="23097" />
  <row Id="14498" PostId="13124" Score="1" Text="@RicardoCruz This'd make a good answer :)" CreationDate="2016-08-04T10:52:31.213" UserId="11097" />
  <row Id="14500" PostId="13181" Score="0" Text="Define &quot;reasonable&quot;? Is your goal to get to an accuracy that could be used in a production system? Is your goal some other thing? How many classes are there?There are some variations in pre-training and semi-supervised training that could save you effort, so could you clarify whether your concern is in the effort *labelling* images, or simply sourcing any image. Finally, how clean and simple are your target images? Images where lighting and pose are fixed will be easier to train than &quot;real world&quot; photographs with the sneakers being worn." CreationDate="2016-08-04T13:27:41.187" UserId="836" />
  <row Id="14501" PostId="13181" Score="0" Text="Yes, this will be used in production. I currently don't know how many classes there will be since I don't know how many different sneaker types there are in the image library. My best guess would be on the order of 50-100, but the courser the description of the sneaker, the less the classes (e.g. air-jordan vs. air-jordan-ultrafit). Unfortunately, the image library is a mix of sneakers being worn and sneakers posed as fixed items with a white backdrop." CreationDate="2016-08-04T14:18:28.387" UserId="14150" />
  <row Id="14502" PostId="13194" Score="0" Text="I have already performed a quick test using TensorFlow's Inception-v3. The best it could do is give me a very course classification, such as &quot;running shoe,&quot; but I need something a little more granular, such as &quot;air-jordan-ultrafit.&quot; This is why I'm building a new training set to use with Inception." CreationDate="2016-08-04T14:23:31.480" UserId="14150" />
  <row Id="14506" PostId="12984" Score="0" Text="More info: http://datascience.stackexchange.com/questions/8286/are-there-any-tools-for-feature-engineering?rq=1" CreationDate="2016-08-04T18:27:13.593" UserId="19161" />
  <row Id="14507" PostId="13184" Score="0" Text="I could have worded that better, but the net-net is &quot;use qnorm/pnorm&quot;." CreationDate="2016-08-04T18:48:18.000" UserId="6554" />
  <row Id="14508" PostId="13195" Score="0" Text="Also you have set max depth to `None`. The trees will be built until node purity is achieved or until all leaves contain less than min_samples_split samples. With a few million data points this can lead to very deep and large trees." CreationDate="2016-08-04T19:50:34.950" UserId="20429" />
  <row Id="14509" PostId="13180" Score="0" Text="@AN6U5 - Very good point. I honestly didn't think too much about the impacts of the scaling on different underlying distributions/outliers. Might give it a read up this morning!" CreationDate="2016-08-04T20:32:59.650" UserId="21702" />
  <row Id="14511" PostId="13201" Score="0" Text="for my second question,you said μ and x(i) must be same dimension,but when you are calculating μ , you are using a formula which is find an average of samples and it's giving to you an average ,you know.How do I subtract x-μ then?" CreationDate="2016-08-04T21:21:35.707" UserId="20926" />
  <row Id="14512" PostId="13124" Score="1" Text="From experience, not from theory, $R^2=.99 $ is an unusually large result.  Here is the point: the p-value is calculated under the assumption that the residuals follow a certain pattern, and without checking that you cannot trust the p-value. You have only 14 observations, so a few extreme outliers would easily invalidate the assumptions needed to compute the p-values. Plot the pairs (comp$Minutes, residuals) and check if there are a few large residuals." CreationDate="2016-08-05T00:23:02.367" UserId="7763" />
  <row Id="14513" PostId="13152" Score="0" Text="Sure. The computation is about over-sampling, and how to relate the model in the manufacured sample and in the real sample. If works for binary variables, I've never use it in other case, so I have not thought of the corresponding correction in cas of multinomial response (trees are great to model multinomial targets)." CreationDate="2016-08-05T00:36:04.060" UserId="7763" />
  <row Id="14514" PostId="13179" Score="0" Text="Excellent comment. It should be added that for some problems one or the other might make more sense regardless of computational concerns. For instance, if your goal is to study word vectors to find relations between words or if you want to generate a text based on a word-topic, then you have to go with word-based RNN. And, conversely, there probably are problems where char-based RNN is the way to go. It also depends on what the user is trying to do." CreationDate="2016-08-05T04:30:09.173" UserId="16853" />
  <row Id="14515" PostId="13179" Score="0" Text="I did not understand your last comment: &quot;Char-based RNN LM (...) fall short when it comes to making actual sense.&quot; I haven't seen a Word-based RNN making sense either. Why did you isolate char-based models here?" CreationDate="2016-08-05T04:30:27.910" UserId="16853" />
  <row Id="14516" PostId="13195" Score="0" Text="I think you have an error in your code.. you first fit the classifier on Xtrain, then you call cross_val_score on Xtest. cross_val_score retrains the classifier on chunks of Xtest! So you're not testing the original classifier trained on Xtrain. To see how it did, just call classifier.predict   on Xtest instead of cross_val_score." CreationDate="2016-08-05T05:15:54.113" UserId="676" />
  <row Id="14517" PostId="13227" Score="1" Text="Is it possible to set a `seed` or `random_state`? If yes, then that should solve the issue :)" CreationDate="2016-08-05T08:18:04.230" UserId="11097" />
  <row Id="14519" PostId="13227" Score="0" Text="@Dawny33 what are these things?" CreationDate="2016-08-05T08:23:25.043" UserId="8013" />
  <row Id="14520" PostId="13174" Score="0" Text="No luck with Spark 1.6.x or 2.0.0 yet. OrientDB support is also looking at the issue for me. As previously stated, that connector on Github does not suite my needs since it won't work from Java. They are however promising Java support within a couple of months." CreationDate="2016-08-05T08:37:46.200" UserId="21896" />
  <row Id="14521" PostId="13227" Score="1" Text="Some algorithms are (partly) stochastic, thus randomly choosing value to explore the space of solutions. However, computers cannot generate random numbers but pseudo-random numbers that are simulated based on an initial seed. If you fix it, you should get the same results each time. There is no randomness in the SVM, thus I would guess it is in the splitting part" CreationDate="2016-08-05T08:38:24.577" UserId="20759" />
  <row Id="14522" PostId="13227" Score="0" Text="@Dawny33 so the partitioning is leading to this right?" CreationDate="2016-08-05T08:40:03.973" UserId="8013" />
  <row Id="14523" PostId="13227" Score="0" Text="@Rishika Yes. So, pl read up on how to set a `random_state` or a `seed` in Matlab, similar to Python :)" CreationDate="2016-08-05T08:47:00.470" UserId="11097" />
  <row Id="14525" PostId="13124" Score="1" Text="What @VictorZurkowski said. I did not actually &quot;answer&quot; the question because it would involve mentioning all those things. Anyhow, [here](http://stats.stackexchange.com/a/16395/35537) is a list of the typical assumptions when hypothesis-testing a linear regression. Since you only have one variable, you can plot the regression for that variable and see if the residuals (difference between your line and the points) make sense. The difference should be uniform across the variable. See [here](http://www.theanalysisfactor.com/linear-models-r-plotting-regression-lines/) how to plot it." CreationDate="2016-08-05T10:45:23.787" UserId="16853" />
  <row Id="14526" PostId="13232" Score="0" Text="Thanks for that. My goal is to do exactly what you said - figure out what classifier algorithm would be best suited for my problem. I guess I'm confused in terms of the documentation of SKLearn: http://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html#grid-search   (under 'nested cross-validation')" CreationDate="2016-08-05T11:26:22.533" UserId="23090" />
  <row Id="14527" PostId="13226" Score="0" Text="I thought grid search was solely for optimizing hyper parameters? How would I use gridsearch in conjunction with something else to figure out the best classifier algorithm (i.e. SVR vs. RandomForest)?" CreationDate="2016-08-05T11:31:26.083" UserId="23090" />
  <row Id="14528" PostId="13179" Score="0" Text="I've updated the ambiguous ending." CreationDate="2016-08-05T11:33:41.203" UserId="22012" />
  <row Id="14529" PostId="13232" Score="0" Text="To test the performance of the best-selected model, would I do a final cross-validation on the whole dataset? Or should I split my dataset into train/test BEFORE nested CV, run nested CV on the train, and then fit the best model on the train data and test on test?" CreationDate="2016-08-05T11:38:50.723" UserId="23090" />
  <row Id="14531" PostId="13226" Score="0" Text="Yes. For each combinations of hyper-parameters GridSearchCV  makes folds and calculates the scores (mean squared error in your case) on the left-out data. Thus each combination of hyper-parameters gets its own mean score. The &quot;optimization&quot; is just choosing the combination with the best mean score. You can extract those mean scores and compare them directly for various models." CreationDate="2016-08-05T11:47:27.110" UserId="9085" />
  <row Id="14532" PostId="13232" Score="0" Text="Sorry for the comment barrage. So my *final* model would be: `best_idx = np.where(np.mean(cv,1).min())[0];&#xA;final_m = GridSearchCV(models[best_idx], params[best_idx]);&#xA;final_m.fit(X,y)`" CreationDate="2016-08-05T11:59:41.807" UserId="23090" />
  <row Id="14533" PostId="13231" Score="0" Text="You cshould not &quot;guess&quot; the training and prediction speed of an algorithm. you should make benchmark by yourself to see how fast training is on your own dataset. Once you model built (with problems such as overfitting solved), prediction is another step." CreationDate="2016-08-05T12:24:50.940" UserId="21825" />
  <row Id="14534" PostId="13232" Score="0" Text="Building off what you said, this was what I was going for with built-in SKLearn functions (gives the same as your answer): `for model, param in zip(models, params):&#xA;    clf = GridSearchCV(model, param)&#xA;    my_score = cross_val_score(clf, X, y, scoring='mean_squared_error')&#xA;    my_scores.append(my_score)`" CreationDate="2016-08-05T12:27:18.750" UserId="23090" />
  <row Id="14535" PostId="13236" Score="0" Text="Would a multi-aget system make things easier? https://en.wikipedia.org/wiki/Multi-agent_system" CreationDate="2016-08-05T13:15:56.613" UserId="2576" />
  <row Id="14536" PostId="13236" Score="0" Text="I can see how each agent can represent a network node.&#xA;Would every single node employ it's own rnn?" CreationDate="2016-08-05T13:44:12.823" UserId="23137" />
  <row Id="14537" PostId="13235" Score="0" Text="You're basically repeating what I wrote in my question; this is not an answer to my question" CreationDate="2016-08-05T13:48:23.153" UserId="8820" />
  <row Id="14538" PostId="13231" Score="0" Text="@ManuH I know that for the implementation I used it was too slow for kNN.  But I can only guess that this is an intrinsic problem of the algorithm,  which cannot be solved in this problem domain (e. g.  By heavy dimensionality reduction).  This is the reason why I ask for papers: I want to know what others have tried." CreationDate="2016-08-05T13:55:17.327" UserId="8820" />
  <row Id="14539" PostId="13239" Score="0" Text="Please read the question in bold in the text I wrote." CreationDate="2016-08-05T13:57:51.467" UserId="8820" />
  <row Id="14540" PostId="13236" Score="0" Text="In a multi-agent system each agent is in charge of learning its own model of the world, depending on the on the information they receive by means of their sensors." CreationDate="2016-08-05T14:47:15.273" UserId="2576" />
  <row Id="14541" PostId="13236" Score="0" Text="Ah, I see. So one agent would learn timing dependencies and another the categorical ones. The shared system would be able to learn the nonlinear complexities?" CreationDate="2016-08-05T14:59:58.383" UserId="23137" />
  <row Id="14542" PostId="13217" Score="0" Text="Thank you so much for your help. I had seen them both, unfortunately they have one drawback in common: they are summary data, they don't provide day-to-day information, it's rolled up to yearly data. I've even found quarterly, but i need raw data with day granularity - time even." CreationDate="2016-08-05T15:08:14.870" UserId="23127" />
  <row Id="14543" PostId="8458" Score="0" Text="For sklearn, you can map the string variables to numbers by `df['string_feature'].map(my_map)` then use the random forest" CreationDate="2016-08-05T15:31:06.970" UserId="21955" />
  <row Id="14544" PostId="13235" Score="0" Text="Here is one such paper using a variant of SVM. http://robotics.stanford.edu/~koller/Papers/Gao+Koller:ICCV11.pdf" CreationDate="2016-08-05T15:41:30.620" UserId="9848" />
  <row Id="14546" PostId="13244" Score="0" Text="Are you trying to extract the width and height of every brick in the image?  Or are you trying to extract the mean width and height of all the bricks in a single image?  Based on your examples, it looks like there is substantial variation in the widths of the bricks in a single image." CreationDate="2016-08-05T16:51:38.707" UserId="18416" />
  <row Id="14547" PostId="13244" Score="0" Text="There is only one height in and two width. I'm trying to extract both width values." CreationDate="2016-08-05T16:53:38.067" UserId="20487" />
  <row Id="14548" PostId="13244" Score="0" Text="I see.  Are you approaching this problem with NNs just to get practice training them?  Because there are easier ways of solving this than using a NN." CreationDate="2016-08-05T16:55:34.817" UserId="18416" />
  <row Id="14549" PostId="13244" Score="0" Text="Also, how big is your training set?" CreationDate="2016-08-05T16:55:50.270" UserId="18416" />
  <row Id="14550" PostId="13244" Score="0" Text="I try using NNs as I want to generalize it to different kinds of textures. Currently exploring possibilities to solves this. My current NN was trained with 300k + images." CreationDate="2016-08-05T16:59:04.943" UserId="20487" />
  <row Id="14551" PostId="13244" Score="0" Text="If you're just looking to solve the problem and you don't care how it's done, I would suggest finding the color of the brick by taking the brightness of all pixels, and finding the pixel with median brightness.  Then use the color of that pixel.  That will work as long as the bricks occupy more area than the mortar.  To find the spacing between the bricks, take a bunch of horizontal (for width) or vertical (for height) slices, and calculate the autocorrelation of each one.  Add up all the autocorrelations and you'll see a strong peak at the height (or two peaks at the widths)." CreationDate="2016-08-05T17:04:33.227" UserId="18416" />
  <row Id="14552" PostId="13244" Score="0" Text="That would be a solution for the special Case.  Plus the features are actually more complex ( 2 colors for the Brick 2 for mortar, roughness and the sizes ). I would like to make the nns work. Even if the output is only roughly right I could use it for further refinement." CreationDate="2016-08-05T17:26:53.430" UserId="20487" />
  <row Id="14554" PostId="13201" Score="0" Text="@Mus, The centroid should also be a co-ordinates in a space that is the same dimension as each example" CreationDate="2016-08-05T18:00:26.897" UserId="23110" />
  <row Id="14555" PostId="13244" Score="0" Text="A CNN *should* be able to do this task. It is possible you have a bug in data prep, a poor choice of architecture or meta-params, or a bug in implementation. To assess that, you would need to share some of your code and data - not sure how feasible it is to get help here, as it starts to get heavily involved. But perhaps you could share the archtecture and which library you are using as a starting point. Also your data prep - how you are normalising the images and preparing data for training." CreationDate="2016-08-05T19:56:46.577" UserId="836" />
  <row Id="14556" PostId="13244" Score="0" Text="@NeilSlater I've updated my post with more information. I didn't normalize or preprocess my images at all as they are rendered, showing a 1x1 m section of a wall. Pixel values in range 0..1. Outputs all range in 0..1 uniformly distributed." CreationDate="2016-08-05T20:07:21.993" UserId="20487" />
  <row Id="14557" PostId="12403" Score="0" Text="How about using a genetic algorithm to tune the parameters?" CreationDate="2016-08-04T19:35:12.460" UserId="23119" />
  <row Id="14558" PostId="8528" Score="0" Text="Thanks for sharing this post. I am using your model for the time-decay attribution modeling. Can you please tell me what is the unit for 't' and 'tbc' since both are time and the values in the data are integers. I appreciate your help. Thanks in advance." CreationDate="2016-08-05T03:40:53.860" UserId="23129" />
  <row Id="14559" PostId="13207" Score="0" Text="Multivariate regression typically refers to a regression model with more than 1 outcome, which is not true in this case" CreationDate="2016-08-05T23:16:14.150" UserId="12206" />
  <row Id="14560" PostId="13227" Score="1" Text="@Dawny33 thanks a lot! It worked!!!!" CreationDate="2016-08-06T02:42:01.330" UserId="8013" />
  <row Id="14561" PostId="13217" Score="0" Text="@LearnByReading Um.. when I looked into dataset from those sites, incidents are recorded almost day-to-day. Do you find something per incident?" CreationDate="2016-08-06T03:36:59.173" UserId="22054" />
  <row Id="14563" PostId="13251" Score="0" Text="Thank you so much. By supervised, I mean does GLRT construct a model from historical data and learn distributions/parameters from them and then is applied to a new test record to decide about hypothesis? Because I see that this is used for change point detection. If it works just online and does not need to be integrated with other methods and data, it should raise many false positive in detecting that change point. Does it have any assumption or threshold selection which makes it to be improper in settings that data distributions change periodically?" CreationDate="2016-08-06T05:51:08.047" UserId="12345" />
  <row Id="14564" PostId="13251" Score="0" Text="(G)LRTs require knowledge of the likelihood function; not estimates from the data. I suppose you could do that but I don't know what guarantees there are for its power. If you want to find out more the key word is _empirical_ likelihood ratio; cf. e.g. [Empirical likelihood ratio test for the change-point problem](http://www.sciencedirect.com/science/article/pii/S0167715206002537), or [Estimation and hypothesis testing in nonstationary time series](http://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=7266&amp;context=rtd)." CreationDate="2016-08-06T06:07:29.390" UserId="381" />
  <row Id="14565" PostId="13251" Score="0" Text="Thank you for the link. Can you let me know what are the inputs of that function and how obtained (obtain from a training data)? However, overall I mean that does this GLRT is parametric and need to set some thresholds or any prior assumption about data distribution?" CreationDate="2016-08-06T06:23:16.753" UserId="12345" />
  <row Id="14566" PostId="13242" Score="0" Text="Can you tell me how can I use them with pyspark in windows ?&#xA;I am new to pyspark btw." CreationDate="2016-08-06T06:36:35.480" UserId="23138" />
  <row Id="14567" PostId="13227" Score="0" Text="@Rishika Awesome. Added an answer which can help future users with this problem :)" CreationDate="2016-08-06T09:35:05.117" UserId="11097" />
  <row Id="14568" PostId="13201" Score="0" Text="Ok , I got that , thanks a lot" CreationDate="2016-08-06T10:48:11.140" UserId="20926" />
  <row Id="14570" PostId="13251" Score="0" Text="The inputs to the GLRT are the likelihood functions, the hypotheses, and the ratio threshold." CreationDate="2016-08-06T18:11:53.007" UserId="381" />
  <row Id="14571" PostId="13251" Score="0" Text="Thank you. Does selection of these inputs vary in different datasets? I mean for different data distribution/pattern we should change these inputs?" CreationDate="2016-08-06T19:14:46.960" UserId="12345" />
  <row Id="14572" PostId="13259" Score="0" Text="What do you mean by &quot;differentiating in related numbers&quot;; the model parameters? If so, do you know what family they come from, e.g., GMM?" CreationDate="2016-08-06T19:30:24.457" UserId="381" />
  <row Id="14575" PostId="13259" Score="0" Text="Thank you so much. No, I do not have any assumption and knowldge about distribution family. These are density plot of two vectors. as we see there is difference in these densities. Now, I mean be able to compute a statistical feature(value) for these 2 vectors in which helps us in relatively clustering two vectors points (2 clusters). Features like mean, mode, median, skewness (I have tested these but don't yield desired result)" CreationDate="2016-08-06T20:10:50.823" UserId="12345" />
  <row Id="14576" PostId="13251" Score="0" Text="The likelihood is a function of the observed outcomes and distribution parameters; you just take the variates as fixed." CreationDate="2016-08-06T20:40:35.227" UserId="381" />
  <row Id="14577" PostId="13260" Score="2" Text="What research have you done?" CreationDate="2016-08-06T21:00:36.820" UserId="381" />
  <row Id="14578" PostId="13207" Score="0" Text="Oops you are correct, I will change my answer." CreationDate="2016-08-06T22:22:44.430" UserId="14904" />
  <row Id="14579" PostId="13262" Score="0" Text="Thank you. But Kolmogorov-Smirnov  test just say that these are from the same family or not. As you mentioned I want to extract feature like moment. I have tested all moments but haven't get desired result." CreationDate="2016-08-06T23:25:04.400" UserId="12345" />
  <row Id="14580" PostId="13262" Score="0" Text="Sorry Mostafa. KS applies to convergence of the empirical cumulative distribution function and a test for its limit. It doesn't apply to your problem. I'll write some points as answer." CreationDate="2016-08-07T00:35:16.980" UserId="7763" />
  <row Id="14581" PostId="13263" Score="0" Text="Thank you so much Victor. I think that you have mentioned completely the right point as it sounds logical based on the figures. But if possible, please let me know how can I implement it in Matlab for a vector as I'm not so good at statistic." CreationDate="2016-08-07T07:53:08.693" UserId="12345" />
  <row Id="14582" PostId="13264" Score="0" Text="An algorithm three orders of magnitude faster than Apriori is FP-growth. Don't know about R, but a fast Python 3 implementation is at least in [Orange3-Associate package](https://github.com/biolab/orange3-associate/)." CreationDate="2016-08-07T10:49:15.860" UserId="15527" />
  <row Id="14583" PostId="13255" Score="0" Text="I think that, when you have a large number of features w.r.t. a small number of observation you should **always** run a feature selection algorithm before applying any classification method. That is because, the more feature (predictors) you have, the higher is the probability that **by chance** one of these features is correlated with the outcome variable, leading to sub-optimal results. Here (http://rsta.royalsocietypublishing.org/content/367/1906/4237) for example, it is said that **as a rule of thumb**, the number of features should be no more than one fifth of the number of observations." CreationDate="2016-08-07T10:50:14.780" UserId="133" />
  <row Id="14584" PostId="12842" Score="0" Text="There is no such concept... How can this assign a length of 1 when the value is in quotes? I tried all your workarounds, but none of them are working." CreationDate="2016-08-07T11:39:48.010" UserId="13395" />
  <row Id="14586" PostId="13033" Score="0" Text="Machine learning on text data is not my area, but the data encoding you describe does not seem the most natural: you encode each n-gram as a sparse vector where each index represents a different word, rather than (what I would try first) encoding each *document* as a sparse vector, where each index represents a different n-gram. Whatever clustering or duplicate detection algorithm you use should result in merging subsequent n-grams, thus indirectly you go back to the alternative (traditional?) document-by-ngram representation." CreationDate="2016-08-07T12:57:41.643" UserId="6550" />
  <row Id="14587" PostId="13264" Score="0" Text="@K3---rnc Yeah, FP-growth is pretty good. But, as the OP asked for a rough overview, I gave the simpler one as an example. You're free to add an answer with the FP-growth algo. though :)" CreationDate="2016-08-07T14:52:32.427" UserId="11097" />
  <row Id="14588" PostId="13263" Score="0" Text="Sorry, my Matlab skills are not any good. :(" CreationDate="2016-08-07T15:15:36.977" UserId="7763" />
  <row Id="14589" PostId="13238" Score="0" Text="Which version of spark are you using ?" CreationDate="2016-08-07T17:03:33.450" UserId="5177" />
  <row Id="14590" PostId="13263" Score="0" Text="Any other language is welcomed. I mean the technical approach." CreationDate="2016-08-07T17:26:44.343" UserId="12345" />
  <row Id="14591" PostId="12842" Score="0" Text="I have edited the answer in response to your comment." CreationDate="2016-08-07T18:42:25.987" UserId="21566" />
  <row Id="14592" PostId="13242" Score="0" Text="It's not really much different in Windows. The arguments to pyspark are still the same, you'll just have a slightly different way of setting the suggested environment variable. Possibly check [this question](http://datascience.stackexchange.com/questions/6169/how-to-run-a-pyspark-application-in-windows-8-command-prompt) for more, or post a separate question about running pyspark under Windows." CreationDate="2016-08-07T23:02:21.230" UserId="23199" />
  <row Id="14593" PostId="12727" Score="0" Text="I may be wrong, but using line breaks in something that is meant to be CSV-parseable, without escaping the multi-line column value in quotes, seems to break the expectations of most CSV parsers. This looks like some special format as well, as indicated by the double-asterisk at the start of that multi-line row (and the inconsistent trailing double-asterisk later) -- which will definitely break all CSV parsers. You may have better luck forking the code for one of the parser libraries, tweaking it to better fit this alternate format, then using it in a similar way as the answer proposed below." CreationDate="2016-08-07T23:18:45.247" UserId="23199" />
  <row Id="14594" PostId="2515" Score="0" Text="@PabloSuau You can check Andrew Ng's Machine Learning class on Coursera, week 10, he explains why the convergence can be faster than both SGD and batch GD. To be more precise: it should always be as fast as SGD, but sometimes it should be even faster in practice." CreationDate="2016-08-07T23:19:12.810" UserId="2544" />
  <row Id="14595" PostId="12747" Score="0" Text="The univocity parser is probably assuming those columns are properly escaped with double-quotes. If creating your own parser library based on code for one of the existing CSV ones isn't an option, and preprocessing it isn't either, there's really no clean way around this. What happens if an input split boundary falls right through the middle of one of these multi-line messages?" CreationDate="2016-08-07T23:30:07.167" UserId="23199" />
  <row Id="14596" PostId="12089" Score="0" Text="This link doesn't work anymore. Might be helpful to note which institution offered it and who the instructor was, to help narrow it down within the Coursera search results." CreationDate="2016-08-07T23:38:49.790" UserId="23199" />
  <row Id="14597" PostId="13242" Score="0" Text="I am using iPython with spark, do I have to create an environment variable PYSPARK_SUBMIT_ARGS ? And whenever I start pyspark using the following command :&#xA;pyspark --packages com.databricks:spark-csv_2.11:1.4.0  &#xA;and then use 'sc' then it shows spark is not defined. But normally when I start pyspark, it does not show any error regarding 'sc'." CreationDate="2016-08-08T04:26:43.247" UserId="23138" />
  <row Id="14598" PostId="13238" Score="0" Text="@eliasah I am using spark 2.0.0" CreationDate="2016-08-08T04:35:38.780" UserId="23138" />
  <row Id="14599" PostId="13277" Score="0" Text="Is that the performance on the testing data set?  Do also post the performance on the training dataset.  (Check out the concept of `Validation Curves` too)" CreationDate="2016-08-08T04:42:05.683" UserId="11097" />
  <row Id="14600" PostId="13277" Score="0" Text="@Dawny33 no this is training data set, i have not used used this model for prediction yet." CreationDate="2016-08-08T04:47:40.330" UserId="8013" />
  <row Id="14601" PostId="13277" Score="3" Text="Then do plot of validation curve of your training error and testing error with varying feature set sizes. If the training error is doing really good, while your testing error is very less, then maybe you're overfitting." CreationDate="2016-08-08T04:52:34.090" UserId="11097" />
  <row Id="14602" PostId="13277" Score="1" Text="@Dawny33 you mean if my training error is low and my testing error is high then I am over-fitting. Right?" CreationDate="2016-08-08T04:53:41.797" UserId="8013" />
  <row Id="14603" PostId="13277" Score="1" Text="Yes. Exactly. Do check out the concept of `Validation Curves` too :)" CreationDate="2016-08-08T04:56:39.823" UserId="11097" />
  <row Id="14604" PostId="13281" Score="0" Text="I'm a simple data guy. If I see a `cross validation` advice, I give a +1  :D" CreationDate="2016-08-08T07:23:50.730" UserId="11097" />
  <row Id="14605" PostId="13281" Score="0" Text="@Dawny33 I have updated the question" CreationDate="2016-08-08T07:29:14.667" UserId="8013" />
  <row Id="14606" PostId="13277" Score="0" Text="By publishing those confusion matrices, it changes the question quite a bit. I'd say there is no evidence of over-fitting from the graphs. You should bear in mind that almost all models will over or under fit slightly, but that isn't usually a concern - getting best possible generalisation in your model is the real goal. To achieve that for your data you will want to look into k-fold cross-validation (as suggested in my answer)." CreationDate="2016-08-08T07:31:59.057" UserId="836" />
  <row Id="14607" PostId="13280" Score="0" Text="I also have time complexity not just space problem!" CreationDate="2016-08-08T08:40:39.593" UserId="23201" />
  <row Id="14608" PostId="13283" Score="0" Text="I have one column in the data set which is categorical and has more than 1000 different Levels or categories." CreationDate="2016-08-08T09:09:47.643" UserId="23198" />
  <row Id="14609" PostId="13273" Score="0" Text="If your categorical features exhibit an order, you transform these features by assigning a number to each level. If it is not the case, you could add one feature per value, and assign a binary value to it. This way, those feature would be orthogonal." CreationDate="2016-08-08T09:15:18.470" UserId="21825" />
  <row Id="14612" PostId="13283" Score="0" Text="Then I'd first make sure you really need 1000 categories (there is really business process with 1000 categories?), then I'd merge some together. Also validate if the column is useful at all (&quot;feature selection&quot;). After that, you can do one-hot encoding. Or if you aim only at certain category, try &quot;One-vs-All&quot; approach." CreationDate="2016-08-08T09:40:31.527" UserId="17290" />
  <row Id="14613" PostId="13280" Score="1" Text="So what is the size of your data set?" CreationDate="2016-08-08T09:50:46.550" UserId="6550" />
  <row Id="14616" PostId="13280" Score="0" Text="t's a social network, so we expecting it grows daily. So we will supose to have many many users." CreationDate="2016-08-08T10:25:10.167" UserId="23201" />
  <row Id="14617" PostId="13273" Score="0" Text="The categorical Feature has more than 1000 different categories and does not exhibit any order. I guess the above Approach would then make the task computationally expensive." CreationDate="2016-08-08T12:29:36.960" UserId="23198" />
  <row Id="14619" PostId="12110" Score="1" Text="By the way: It is called transposed convolution now in TensorFlow: https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#conv2d_transpose" CreationDate="2016-08-08T14:08:48.023" UserId="8820" />
  <row Id="14622" PostId="13286" Score="0" Text="Haven't you fixed the random number generator seed somewhere with the 'rng' function?" CreationDate="2016-08-08T14:35:13.830" UserId="9814" />
  <row Id="14623" PostId="13282" Score="0" Text="http://www.sciencedirect.com/science/article/pii/S0167865508000524" CreationDate="2016-08-08T14:38:30.487" UserId="9814" />
  <row Id="14624" PostId="12678" Score="0" Text="I tried to solve this by implementing a MLP. I first labeled curves as positive or negative (for an event occurring), and then used a sample of 1400 data points from the curve as the input to a 1400 node input layer. Even leaving out the goal of &quot;predicting what time an event would occur,&quot; I was unable to get well above 50% accuracy using this method. On the one hand, I was trying to build a model for my data, but I was also equally interested in finding IF there was a good model to fit my data. I have concluded there may not a good model for my sparse data, but I will update as I test more." CreationDate="2016-08-08T14:44:57.817" UserId="21182" />
  <row Id="14629" PostId="13273" Score="0" Text="it will indeed be memory expensive. I would try a kernel-trick ( seems to work with [Kmeans](https://www.mathworks.com/matlabcentral/fileexchange/26182-kernel-kmeans) ) with an approahc similar to the MKL, the value of  the kernel for the categorical part being zero if the category is not the same, one otherwise. not sure my thought is clear." CreationDate="2016-08-08T15:50:36.790" UserId="21825" />
  <row Id="14630" PostId="12089" Score="1" Text="Hi Brian, the Coursera Specialization has changed a few months ago, now you can access trough the Big Data Specialization from UC San Diego in the third course, but is a simple introduction with a initial tutorial for start to know Spark https://www.coursera.org/specializations/big-data . Also you have the Edx Course mentioned in other answer that is a part of spark series https://www.edx.org/course/introduction-apache-spark-uc-berkeleyx-cs105x" CreationDate="2016-08-08T16:22:50.037" UserId="18973" />
  <row Id="14631" PostId="13294" Score="0" Text="I think you're looking for a columnar database. look at [this](https://en.wikipedia.org/wiki/List_of_column-oriented_DBMSes) wikipedia page for a list." CreationDate="2016-08-08T17:03:47.120" UserId="10517" />
  <row Id="14632" PostId="9536" Score="0" Text="The convolutional neural networks (CNNs) that Martin mentioned are designed specifically with this issue in mind. The use of layers of partially connected receptive regions across many layers acts as a sort of efficient information filter that allows you to process millions of pixels without using millions of neurons. The algorithms is based on the means by which humans and cats use few optic pathway neurons to process huge amount of visual information and leads to the same sort of cost savings." CreationDate="2016-08-08T17:29:35.780" UserId="18544" />
  <row Id="14633" PostId="8662" Score="0" Text="@Shawn documentation says they use bootstrapping and I think that's true: https://github.com/mwaskom/seaborn/blob/master/seaborn/categorical.py#L1449" CreationDate="2016-08-08T18:06:18.680" UserId="23228" />
  <row Id="14634" PostId="13303" Score="0" Text="Welcome to DataScience.SE! If you need to ask the difference supervised learning and online learning, I think we need to ask _you_ a question: what is your goal?" CreationDate="2016-08-08T18:16:51.290" UserId="381" />
  <row Id="14635" PostId="13303" Score="0" Text="I know the difference between supervised and unsupervised LDA but not between different unsupervised LDA implementations. My goal is to use LDA as a dimension skirt reduction technique on a collection of documents and then classify them. I will also use the supervised LDA to compare the results with the unsupervised LDA" CreationDate="2016-08-08T18:19:08.673" UserId="23227" />
  <row Id="14636" PostId="13295" Score="0" Text="Have you considered using an autoregressive model of some kind?  https://en.wikipedia.org/wiki/Autoregressive_model" CreationDate="2016-08-08T19:03:22.057" UserId="17346" />
  <row Id="14637" PostId="12904" Score="0" Text="Broken Link, &quot;Text Analytic Tools for Semantic Similarity&quot;." CreationDate="2016-08-08T19:34:15.353" UserId="2742" />
  <row Id="14638" PostId="13295" Score="0" Text="I don't really understand what you want and why. But maybe compression algorithms can help somehow." CreationDate="2016-08-08T19:48:05.633" UserId="723" />
  <row Id="14641" PostId="13312" Score="1" Text="Welcome to DS stack exchange!  Thanks for checking back and posting your solution.  I guess in this case you want the actual rolling average over a full year, but I would typically use an exponentially weighted moving average for this type of problem.  Check out [http://statsmodels.sourceforge.net/](http://statsmodels.sourceforge.net/) and there might be a way to apply a flat function rather than a weight in ARIMA or EWMA." CreationDate="2016-08-09T01:08:57.323" UserId="9420" />
  <row Id="14642" PostId="13309" Score="0" Text="Thanks for the reply! Yeah I was wondering if you could go deeper into the explanation of how the trees are stacked to get the set of rules that justifies every decision. Given each tree has different weights in the final model, different splits, etc., how is this final decision tree generated?" CreationDate="2016-08-09T01:40:09.513" UserId="15768" />
  <row Id="14645" PostId="13286" Score="0" Text="@rcpinto But i am getting a stable result? Still do i need ti fix rng?" CreationDate="2016-08-09T03:09:12.693" UserId="8013" />
  <row Id="14646" PostId="13313" Score="0" Text="This qn belongs in the OpenData site :)" CreationDate="2016-08-09T04:50:47.687" UserId="11097" />
  <row Id="14647" PostId="13315" Score="1" Text="What are the testing errors of both?  Nothing can be said by just looking at the training errors. Maybe `rng(1)` is better than `rng(2)` or maybe it is overfitting." CreationDate="2016-08-09T05:48:31.843" UserId="11097" />
  <row Id="14648" PostId="13315" Score="0" Text="@Dawny33 So does this mean that there is overfitting? A good model should have the same accuracy for any rng right?" CreationDate="2016-08-09T05:52:22.610" UserId="8013" />
  <row Id="14649" PostId="13316" Score="0" Text="yes I know. But if I get different accuracy which accuracy should I take? Or should I average the accuracies ?" CreationDate="2016-08-09T05:56:09.483" UserId="8013" />
  <row Id="14650" PostId="12904" Score="0" Text="@xtian Thank you, just fixed it." CreationDate="2016-08-09T05:57:03.970" UserId="21024" />
  <row Id="14651" PostId="13118" Score="0" Text="Also the definition of an algorithm is very broad. Given an infinite amount of data generated by an unknown stochastic algorithm, you can enumerate all algorithms from simplest to most complex and, after seeing more and more data, recover the original algorithm more and more accurately http://www.sciencedirect.com/science/article/pii/089054019290050P." CreationDate="2016-08-09T05:58:57.833" UserId="6550" />
  <row Id="14652" PostId="13316" Score="0" Text="if I am not using the rng, then I get a constant accuracy. Does this mean that the model can be used for further classification and is stable?" CreationDate="2016-08-09T05:59:00.080" UserId="8013" />
  <row Id="14653" PostId="13316" Score="0" Text="@Rishika As I have already mentioned under one of your qns before, plot validation curves and take the dataset, which has the best balance of bias and variance (Also, refer to this [answer](http://stats.stackexchange.com/a/222312/84191)).  **Yes**, this means that your model is stable and has fitted the data nicely." CreationDate="2016-08-09T06:02:30.400" UserId="11097" />
  <row Id="14656" PostId="13317" Score="0" Text="made a small change in the question. Let me know whether still there is over-fitting or not" CreationDate="2016-08-09T07:15:28.183" UserId="8013" />
  <row Id="14657" PostId="13317" Score="1" Text="@Rishika No, I don't think there's any overfitting there. But, that's without changing the rng value, right?" CreationDate="2016-08-09T07:17:17.180" UserId="11097" />
  <row Id="14658" PostId="13321" Score="0" Text="If you find that API thing, pls share." CreationDate="2016-08-09T07:18:24.727" UserId="23246" />
  <row Id="14659" PostId="13317" Score="0" Text="yes. This is without changing rng" CreationDate="2016-08-09T07:22:02.310" UserId="8013" />
  <row Id="14660" PostId="13317" Score="1" Text="@Rishika The model looks good to me, then :)" CreationDate="2016-08-09T07:25:52.940" UserId="11097" />
  <row Id="14663" PostId="9785" Score="0" Text="I implemented the above algorithm and came across a question: **Why is softmax used?**&#xA;Let me show you an example of two normalization functions: def softmax(w, t = 1.0): # Source: https://gist.github.com/stober/1946926 e = np.exp(w / t) return e / np.sum(e) def normalization(w): return w / np.sum(w) a = np.array([.0002, .0001, .01, .03]) print normalization(a) print softmax(a, t=1) Let's compare the outputs: [ 0.00496278 0.00248139 0.24813896 0.74441687] [ 0.24752496 0.24750021 0.24996263 0.25501221] As we can see, softmax gives .03 roughly the same probability as compared to .0001 (which is" CreationDate="2016-08-09T08:13:01.173" UserId="23247" />
  <row Id="14664" PostId="13323" Score="3" Text="I doubt it. But a suggestion: the area of machine learning that studies performance in machine learning is called [empirical risk minimization](https://en.wikipedia.org/wiki/Empirical_risk_minimization). You can try to add those keywords to your searches." CreationDate="2016-08-09T10:10:13.353" UserId="16853" />
  <row Id="14665" PostId="19" Score="0" Text="This is an excellent question. As denoted by the variety of answer, the definition is... undefined" CreationDate="2016-08-09T10:13:15.123" UserId="21825" />
  <row Id="14666" PostId="13324" Score="2" Text="Depends heavily.  For me, it might be identifying sarcasm in online reviews.  For others, maybe different.  So, completely opinion based :)" CreationDate="2016-08-09T10:19:09.000" UserId="11097" />
  <row Id="14667" PostId="13324" Score="0" Text="Good point. I should've been more specific in the question, but I guess I'm also still figuring out what it exactly is I want to ask :) I'm specifically interested in finding 'a good bet', i.e. an area that would be very relevant even after some years of gaining expertise in it. Also, it would be very nice to hear some opinions about what kind of things are possible to do for individuals and small companies (I know building a multipurpose robot assistant that Just Works would be wonderful and great business, but that's stuff that whole university departments collaborate on...)." CreationDate="2016-08-09T10:25:26.840" UserId="23250" />
  <row Id="14668" PostId="13325" Score="0" Text="Thanks, Hima, what a great answer! I'll start going through the links :)" CreationDate="2016-08-09T10:26:54.777" UserId="23250" />
  <row Id="14669" PostId="13325" Score="0" Text="can you mark it as answered so that it helps others?" CreationDate="2016-08-09T10:40:34.880" UserId="21024" />
  <row Id="14670" PostId="13325" Score="0" Text="Ah, sure, just beginning to learn how this works :)" CreationDate="2016-08-09T10:54:36.247" UserId="23250" />
  <row Id="14672" PostId="13309" Score="0" Text="There is no final decision tree. Gradient boosting is an ensemble. It computes a prediction by computing a weighted sum over the predictions of all the independent decision trees." CreationDate="2016-08-09T12:09:26.157" UserId="20372" />
  <row Id="14673" PostId="13321" Score="0" Text="you can get facebook page data(posts) using graph api.You can't get personal or person data using graph api.https://developers.facebook.com/docs/graph-api is the api @M John" CreationDate="2016-08-09T12:26:24.727" UserId="20585" />
  <row Id="14674" PostId="13321" Score="0" Text="@M John  sentiment-analysis is something that is done on the english sentences to determine the polarity of that sentences(positive,negative,neutral).facebook posts mostly look like english sentences and the content is longer when compare with twitter.In twitter we can only write 160 chars of message.Hope you get the what I'm saying" CreationDate="2016-08-09T12:38:45.007" UserId="20585" />
  <row Id="14675" PostId="13256" Score="0" Text="Thanks. Right now, I'm simplifying the model by focusing on a single time period, but, I will play around with using a weighted average as a more stable denominator. One question is though, how do I evaluate the performance of my current model against the weighted average model?" CreationDate="2016-08-09T13:02:31.490" UserId="17594" />
  <row Id="14677" PostId="13295" Score="0" Text="I added an experimentation I made today, base on levenshtein distance. It looks promising.&#xA;Also, I edited a bit the introduction so it is hopefully more clear.&#xA;Thank you for your suggestions, I will have a look." CreationDate="2016-08-09T15:14:26.963" UserId="23213" />
  <row Id="14678" PostId="13304" Score="0" Text="I ran it once, during 13 days." CreationDate="2016-08-09T15:28:44.640" UserId="21473" />
  <row Id="14679" PostId="13256" Score="0" Text="To compare two models, do more or less what you are doing when you measure the prediction error holding Company and Product constant, etc. i.e.: 1) choose a sample of known prices, 2) apply both prediction methods (=models) to the known abservation as you would if the prices were, 3) compute the prediction errors for both methods, 4) compare results. Histograms of errors is one way, but hstograms have too much going on; I would use - as a first approximation - a summary statistic, like &quot;root mean squared error&quot;, or something more insensitive to small errors, like..." CreationDate="2016-08-09T16:39:38.083" UserId="7763" />
  <row Id="14681" PostId="13256" Score="0" Text=".. average of max( 0, |price - predicted_price| - 1) (make changes to handle different currencies; this error measure does not put any weight on predictions within one monetary unit of the observed price), or MAPE = average (|price - prediction|/price).  I said &quot;first approximation&quot; because to be fair one would have to consider the complexity of the model; you can leave model complexity aside for now." CreationDate="2016-08-09T16:49:03.070" UserId="7763" />
  <row Id="14682" PostId="13327" Score="0" Text="https://en.wikipedia.org/wiki/Information_retrieval#Precision_at_K" CreationDate="2016-08-09T17:37:43.373" UserId="381" />
  <row Id="14683" PostId="12153" Score="0" Text="Can you explain how to incorporate the event type and nonstationarity with this approach?" CreationDate="2016-08-09T17:40:05.563" UserId="381" />
  <row Id="14684" PostId="13314" Score="1" Text="Can you give more detail? How many classes are you predicting between? How much data are you using for training/cross validation/testing? How big is the input layer? How many hidden layers? What optimization are you using? etc." CreationDate="2016-08-09T18:07:50.437" UserId="9848" />
  <row Id="14685" PostId="13314" Score="0" Text="@JohnYetter Good questions, thanks! I am using small data sets, less than 5000 records each for test and train. 15 inputs to 1 hidden layer to produce 3 classification outputs." CreationDate="2016-08-09T18:10:01.140" UserId="23240" />
  <row Id="14686" PostId="13334" Score="0" Text="Convolutions can work on any data where there is a sensible way of representing each data record as an image (2D array)." CreationDate="2016-08-09T19:59:56.600" UserId="17509" />
  <row Id="14687" PostId="13309" Score="0" Text="Sorry I phrased the question wrongly, how are the final explanation rules generated from an ensemble of decision trees, all with different splits and weights?" CreationDate="2016-08-09T20:09:20.367" UserId="15768" />
  <row Id="14688" PostId="13309" Score="0" Text="Basically I am familiar with gbm, I just am not sure how the explain method is calculated, since I can't find any other literature on this issue." CreationDate="2016-08-09T20:10:42.097" UserId="15768" />
  <row Id="14689" PostId="13338" Score="0" Text="Are you asking how jointly mined digital currencies can be fairly shared between the participants? The question is unclear." CreationDate="2016-08-09T20:37:18.497" UserId="381" />
  <row Id="14690" PostId="13338" Score="0" Text="So digital currencies are emitted to remunerate mining and are then distributed proportionaly to participation?" CreationDate="2016-08-09T20:53:27.740" UserId="23263" />
  <row Id="14691" PostId="13341" Score="0" Text="@MatthewGraves has suggested this be brought here from artificial intelligence site." CreationDate="2016-08-09T21:13:06.927" UserId="23264" />
  <row Id="14692" PostId="12153" Score="0" Text="One periodogram for each event type. I don't think non-stationarity is a problem. As for non-uniform spacing, the daily data should be resampled (aggregated) onto a weekly/bi-weekly/monthly period. If non-stationarity is indeed a problem, differencing." CreationDate="2016-08-09T22:21:43.017" UserId="15527" />
  <row Id="14693" PostId="13336" Score="0" Text="But I'm trying to decide which coupon to offer based on profit maximization, allowing for individual level variation based on demographics and other data." CreationDate="2016-08-09T22:40:16.913" UserId="2723" />
  <row Id="14694" PostId="9768" Score="0" Text="@Tankske- I am having similar problem. Could you please let me know your approach?" CreationDate="2016-08-10T02:16:10.160" UserId="21707" />
  <row Id="14695" PostId="13336" Score="0" Text="I know that in the case of eXtreme Gradient Boosting in either R or Python I can specify a custom objective function. This is just a different tree model, so I should be able to do the same, no?" CreationDate="2016-08-10T03:08:36.580" UserId="2723" />
  <row Id="14696" PostId="13332" Score="0" Text="So essentially, in this paper (for sentiment analysis) it just means simple precision?" CreationDate="2016-08-10T04:29:46.207" UserId="21024" />
  <row Id="14697" PostId="13347" Score="2" Text="You can make use of Spark's [Normalizer](https://spark.apache.org/docs/2.0.0/mllib-feature-extraction.html#normalizer) and, if you are interested in &quot;all-pairs similarity&quot;, [DIMSUM](https://databricks.com/blog/2014/10/20/efficient-similarity-algorithm-now-in-spark-twitter.html)." CreationDate="2016-08-10T06:44:26.990" UserId="381" />
  <row Id="14698" PostId="13344" Score="0" Text="Is there a way to find out, which features the network learned and considered useful? For the forest, it is easy to get a histogram of the importance of every feature, but for a network I do not know of any method." CreationDate="2016-08-10T07:05:30.850" UserId="22080" />
  <row Id="14699" PostId="13344" Score="2" Text="@Merlin1896: It is likely that the network will have created a bunch of weird mixed-up features in the hidden layer, that you will find hard to interpret. Often neural networks are called &quot;black box&quot; algorithms for the reason that they are hard to interpret." CreationDate="2016-08-10T07:28:14.000" UserId="836" />
  <row Id="14700" PostId="13340" Score="2" Text="It would be interesting to hear what the bug was.  I imagine that they likely were using the same seed value each time and the random number generator was thus not really random." CreationDate="2016-08-10T07:41:52.353" UserId="9420" />
  <row Id="14701" PostId="13277" Score="0" Text="@Dawny33 what should be the difference in the training and testing performance percentage? Or what should be the difference in the error rates of training and testing dataset for which I would consider that my model is not overfitting?" CreationDate="2016-08-10T08:31:15.410" UserId="8013" />
  <row Id="14702" PostId="13277" Score="1" Text="@Rishika Depends on the problem statement and the analyst. If it's a considerable huge difference, like about `&gt;=10%`, then it might be overfitting." CreationDate="2016-08-10T08:40:00.810" UserId="11097" />
  <row Id="14704" PostId="13349" Score="0" Text="Mh, I thought that increasing the number of trees would lead to an increase of the bias component of the error (which is somehow counter-balanced by a decreasing variance of the model)." CreationDate="2016-08-10T11:06:05.950" UserId="133" />
  <row Id="14706" PostId="13352" Score="0" Text="Are you dealing with unstructured (informal) data?" CreationDate="2016-08-10T11:31:16.077" UserId="12168" />
  <row Id="14707" PostId="13349" Score="1" Text="I always thought that adding trees could never hurt, but in [The Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/) page 596, they claim that too rich of a forest can introduce extra variability, so I think your answer is still correct, but with a tiny grain of salt :)" CreationDate="2016-08-10T11:43:25.540" UserId="14913" />
  <row Id="14709" PostId="13351" Score="0" Text="@tlorieul When I changed my code to this : clf.ebaluate(X_test,Y_test). This code is throwing an error ''KerasRegressor' object has no attribute 'evaluate''" CreationDate="2016-08-10T12:01:29.607" UserId="15412" />
  <row Id="14710" PostId="13352" Score="0" Text="@ShuaiYuan for example - it can be email or any PDF receipt" CreationDate="2016-08-10T12:03:28.313" UserId="23285" />
  <row Id="14711" PostId="13336" Score="0" Text="@HackR I suppose, but that seems like a more indirect way of doing it. To me it sounds simpler to just use a default loss/objective function (like mean absolute error), and just make your target variable more relevant. By the way, you can use any model, not just tree models." CreationDate="2016-08-10T12:46:47.197" UserId="12515" />
  <row Id="14712" PostId="13349" Score="1" Text="@TBSRounder Thanks for the link. The author is discussing too many things in such a short piece of text, so it's hard to distill him, especially confusing because he talks about tree depth and tree number at the same time. But, with regard to B (=#trees), he is saying that more trees makes the resulting model closer to the average tree, and, of course, the average tree may be biased, so you may end up with a biased model. I never claimed otherwise. I just said that B, by itself, does not increase bias. It just makes your model more robust (=reduces variance)." CreationDate="2016-08-10T13:03:03.170" UserId="16853" />
  <row Id="14713" PostId="13349" Score="0" Text="Ah yes, It was a little confusing, thanks for the clarification.  For some reason I thought the author was talking about B, not tree depth." CreationDate="2016-08-10T13:12:44.073" UserId="14913" />
  <row Id="14714" PostId="13340" Score="0" Text="I would guess some failure to treat split evenly, either pre-sending or post sending (e.g. accidentally assign some responses to A when in fact they were in B). If this also applied to regular AB tests, it would be a very serious bug indeed, making the use of the whole product questionable." CreationDate="2016-08-10T13:13:03.950" UserId="836" />
  <row Id="14715" PostId="13275" Score="0" Text="thank you very much" CreationDate="2016-08-10T13:39:17.257" UserId="22068" />
  <row Id="14716" PostId="13351" Score="0" Text="@Nain Sorry, my bad, I did not read properly your code, I edited my answer, using `Y_test` instead of `res` is the answer.&#xA;And as said by @NeilSlater, you shouldn't fit on the test data in any cases, you should fit on the train data, compute the score on the train data to check if the model was fitted correctly and compute the score on the test data and compare it to the training score to check if it has overfitted or not." CreationDate="2016-08-10T13:43:02.127" UserId="23283" />
  <row Id="14717" PostId="13300" Score="0" Text="I have a particular structure that I want to impose on the network, so I do not want to randomly drop inputs as you suggest." CreationDate="2016-08-10T14:01:11.260" UserId="23195" />
  <row Id="14718" PostId="12110" Score="2" Text="Thanks for this very intuitive answer, but I'm confused about why the second one is the 'stride two' case, it behaves exactly like the first one when kernel moves." CreationDate="2016-08-10T14:01:13.950" UserId="23294" />
  <row Id="14719" PostId="13355" Score="0" Text="Ok that makes it clearer, thanks :) I've read about using combined evidence with bayesian classifiers as a technique for missing data, but don't understand it too well - do you know anything about this?" CreationDate="2016-08-10T15:09:21.107" UserId="17772" />
  <row Id="14720" PostId="13217" Score="0" Text="I searched a few cities and didn't find any, but then I looked for some more major cities and found the exact thing I was looking for (Chicago, IL) for those of you who are also looking. Thank you very much, I'll keep the question open for a couple of days and if nobody else posts an answer, I'll choose yours. Thanks!" CreationDate="2016-08-10T15:21:38.780" UserId="23127" />
  <row Id="14721" PostId="13282" Score="0" Text="thank you for your response but I don't think this would solve this issue because the time series samples for example for sensor1 have mixed values and I don't know how to compare 'OFF' values with numeric values when I want to measure the distance between Time Serie i and Time Serie j" CreationDate="2016-08-10T15:25:02.120" UserId="23103" />
  <row Id="14722" PostId="13282" Score="0" Text="Sorry, I misunderstood your question. I thought that you were working with mixed variable types, but it seems that your problem is that a *single* variable may assume more than one type in different time steps." CreationDate="2016-08-10T15:26:35.113" UserId="9814" />
  <row Id="14723" PostId="13332" Score="0" Text="What's &quot;simple&quot; precision?" CreationDate="2016-08-10T16:10:42.693" UserId="381" />
  <row Id="14724" PostId="13339" Score="0" Text="Tutorial looks great and I didn't know about iPyWidgets. Thanks for providing multiple options!" CreationDate="2016-08-10T16:20:37.977" UserId="23262" />
  <row Id="14725" PostId="13300" Score="0" Text="You could then call layer.set_all_params() with the parameters you want? http://lasagne.readthedocs.io/en/latest/modules/layers/helper.html#lasagne.layers.set_all_param_values" CreationDate="2016-08-10T17:01:26.217" UserId="21182" />
  <row Id="14726" PostId="13300" Score="0" Text="I have explained why this won't work for my purposes in the original post." CreationDate="2016-08-10T18:39:19.500" UserId="23195" />
  <row Id="14727" PostId="13340" Score="0" Text="I added the bug explanation, which totally makes sense with respect to what happened to me.  I am running a third AA testing that hopefully will at last give me the dull results I expect" CreationDate="2016-08-10T21:56:22.683" UserId="21473" />
  <row Id="14728" PostId="13359" Score="0" Text="This seems to be a signal processing problem.  Could you either resample your signal to be relative to your signal features, or vary the size of your signal features?  Then you could convolve the signal features with the test signal and retrieve an impulse where there are similarities." CreationDate="2016-08-10T22:14:48.403" UserId="19161" />
  <row Id="14729" PostId="13357" Score="0" Text="What is a spectrum integral? What is its measure units?" CreationDate="2016-08-10T23:08:14.300" UserId="15361" />
  <row Id="14730" PostId="13357" Score="0" Text="@Diego We measure the spectrum of our laser pulse and get something that looks like [this](https://www.thorlabs.com/images/TabImages/QTH10_Spectrum_780.gif). Integrating it just takes the area under the curve. We are trying to maximize the area under the curve as it means we have more light" CreationDate="2016-08-10T23:15:49.067" UserId="23301" />
  <row Id="14731" PostId="13357" Score="0" Text="Thanks. Do you believe that the spectral components are linearly independent, that is actuating one motor will affect only the light of a certain wavelength? If they are - then you could work with them one by one. If not - maybe monte-Carlo optimization is the keyword for you." CreationDate="2016-08-10T23:43:30.933" UserId="15361" />
  <row Id="14732" PostId="13294" Score="0" Text="Can't you just install 64GB of RAM and push your table there?" CreationDate="2016-08-10T23:53:29.313" UserId="15361" />
  <row Id="14733" PostId="13294" Score="0" Text="Let's say i'm willing to install 64GB RAM, i'm looking for a DB\Soloution that will allow to easly &quot;push&quot; the table there. Currently i'm trying Oracle's &quot;in_memory&quot; option following @phiver's comment." CreationDate="2016-08-11T06:44:44.367" UserId="23211" />
  <row Id="14734" PostId="13358" Score="0" Text="Where did you get this formula for implementation?" CreationDate="2016-08-11T07:41:24.607" UserId="2750" />
  <row Id="14735" PostId="13282" Score="0" Text="exactly this is the point that some variables have a space feature with numeric and some non numeric values that are not missing." CreationDate="2016-08-11T08:20:54.580" UserId="23103" />
  <row Id="14736" PostId="13363" Score="0" Text="How about a simple forecast model? Not sure if that is available in knime." CreationDate="2016-08-11T08:26:58.610" UserId="10517" />
  <row Id="14737" PostId="13332" Score="0" Text="Sorry for that. I mean prec@1 is essentially the precision i'll obtain if I use scikit's metrics library and calculate for multiclass classification?" CreationDate="2016-08-11T09:03:58.073" UserId="21024" />
  <row Id="14738" PostId="13358" Score="0" Text="I'm taking Andrew Ng's Machine Learning [Course](https://www.coursera.org/learn/machine-learning) .Week 8 and Lecture  - Dimensionality Reduction .Why did you ask that?Is it wrong?This formula." CreationDate="2016-08-11T09:15:06.910" UserId="20926" />
  <row Id="14739" PostId="13362" Score="0" Text="I misread that. You're right that ResNet doesn't use any uniform initialization at all. I stand corrected." CreationDate="2016-08-11T10:24:35.737" UserId="9465" />
  <row Id="14740" PostId="10738" Score="0" Text="Hi.. I have a similar problem where I'd like to use x% of information and  unsure how to do this? I intend to use the [IPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA) to do this I can leave the n_components=None but how do I then Decide what are the features that have x% of the data ?" CreationDate="2016-08-11T15:14:28.517" UserId="17487" />
  <row Id="14741" PostId="13375" Score="0" Text="Thanks for the reply ! I am currently using ensembles methods such as Random Forest, and Gradient Boosted Trees. The reason I did not mention the them, as I wanted to know how good the approach is agnostic to the type of algorithm." CreationDate="2016-08-11T18:10:00.463" UserId="12858" />
  <row Id="14742" PostId="13375" Score="0" Text="About the sample space, don't you think that can be handled with giving weights to the observations ?  building some sort of time notion." CreationDate="2016-08-11T18:10:59.317" UserId="12858" />
  <row Id="14743" PostId="406" Score="0" Text="Maybe this whitepaper could be useful.&#xA;https://www.knime.org/files/knime_bigdata_energy_timeseries_whitepaper.pdf&#xA;It is about modelling time series including seasonality. -- Rosaria" CreationDate="2016-08-11T10:14:19.740" UserId="23317" />
  <row Id="14744" PostId="13376" Score="0" Text="It looks like your X-train and y_train are still pandas dataframes, but they should be Numpy arrays.  Could that be it?" CreationDate="2016-08-11T20:19:14.367" UserId="19161" />
  <row Id="14745" PostId="13376" Score="0" Text="@Hobbes, even after converting them to numpy arrays, the same error is present" CreationDate="2016-08-11T21:17:01.140" UserId="15412" />
  <row Id="14746" PostId="13370" Score="0" Text="The page you mention introduces various generalities about dimensionality reduction. Would you mind summing up what applies to the question above? Thanks a lot!" CreationDate="2016-08-12T04:10:11.550" UserId="18790" />
  <row Id="14748" PostId="13377" Score="0" Text="Maybe `plt.tight_layout()` can help before `plt.savefig()`" CreationDate="2016-08-12T08:54:54.907" UserId="23345" />
  <row Id="14749" PostId="13390" Score="0" Text="What do you think the basics of DL are?  CLASSICAL MODELS" CreationDate="2016-08-12T09:51:21.447" UserId="11097" />
  <row Id="14750" PostId="13373" Score="0" Text="Excuse a neophyte, please. You must have a very special dataset for it to come labeled, yes? Supervised labeling is by nature costly and slow." CreationDate="2016-08-12T10:13:09.987" UserId="2742" />
  <row Id="14751" PostId="13352" Score="1" Text="Regular expressions" CreationDate="2016-08-12T10:36:11.233" UserId="2742" />
  <row Id="14752" PostId="13377" Score="0" Text="I'm getting max() arg is an empty sequence. I think that plt.tight_layout() only works when you have subplots." CreationDate="2016-08-12T12:50:06.463" UserId="23330" />
  <row Id="14753" PostId="12745" Score="0" Text="Just my experience - as a data scientist, you will be often part of some kind of &quot;Business Intelligence&quot; department. Plus you will spend ~70% of time cleaning and transforming data from (usually) SQL databases. Therefore the info major makes more sense for me (and because you already study the &quot;hard&quot; stuff)." CreationDate="2016-08-12T13:33:05.527" UserId="17290" />
  <row Id="14754" PostId="13352" Score="0" Text="@xtian added examples. We don't have all examples of texts. There can be any kind of text." CreationDate="2016-08-12T13:55:34.793" UserId="23285" />
  <row Id="14755" PostId="10843" Score="0" Text="Your explanation is better than wacax's in the referred link, thank you" CreationDate="2016-08-12T15:00:14.803" UserId="23358" />
  <row Id="14756" PostId="13392" Score="0" Text="Thanks!! You are the best." CreationDate="2016-08-12T15:18:53.427" UserId="15412" />
  <row Id="14757" PostId="13392" Score="0" Text="One more thing. After calling res = model.predict(X_test) in the above code. If i want to check the score for the model by calling model.score(y_test, res), then there also must be changed something otherwise the same error as above will be thrown?" CreationDate="2016-08-12T15:32:52.013" UserId="15412" />
  <row Id="14758" PostId="13373" Score="0" Text="@xtian Cost of supervised labelling and time it takes is significantly dependent on the problem. Lets say you had ML model that predicted when someone walks in a dealership will he buy the car or not (given person attributes) ? Your labeled data collection is relatively fast in this case. In a day, you might get 100+ labeled samples." CreationDate="2016-08-12T16:41:14.653" UserId="12858" />
  <row Id="14759" PostId="13309" Score="0" Text="Judging from the explantion and signature in the documentation I think that it returns a list of list of decisions in each tree. explanation['explanation'][0] from the example refers then to the path that the record takes in the first tree. But I am not familiar with Turi." CreationDate="2016-08-12T20:50:10.253" UserId="20372" />
  <row Id="14760" PostId="13395" Score="0" Text="The combination of reinforcement learning and DL also becomes also popular in game AI (go, Mario, flappy bird)" CreationDate="2016-08-12T22:28:11.807" UserId="20372" />
  <row Id="14761" PostId="11122" Score="0" Text="I am not familiar with genome analysis. What is a &quot;sample&quot;? Is it a single record? And what is the &quot;tissue&quot;?" CreationDate="2016-08-12T22:56:25.873" UserId="20372" />
  <row Id="14762" PostId="13399" Score="0" Text="By kernel, do you mean svm?" CreationDate="2016-08-13T04:53:38.653" UserId="1426" />
  <row Id="14763" PostId="13352" Score="0" Text="If you have a dictionary of products the problem becomes trivial" CreationDate="2016-08-13T05:33:11.330" UserId="816" />
  <row Id="14764" PostId="13352" Score="0" Text="@AlexeyGrigorev I don't have product names." CreationDate="2016-08-13T06:04:52.403" UserId="23285" />
  <row Id="14765" PostId="13399" Score="0" Text="That is also certainly an option but I was referring to the more general [kernel trick][https://en.m.wikipedia.org/wiki/Kernel_method]." CreationDate="2016-08-13T07:53:49.857" UserId="20372" />
  <row Id="14766" PostId="13352" Score="0" Text="Then get them. There are a couple of datasets with names available online or you can create these datasets yourself by scraping the web" CreationDate="2016-08-13T15:46:45.537" UserId="816" />
  <row Id="14770" PostId="13407" Score="0" Text="Maybe you could be a little more specific: which equation are you actually talking about? You mean every equation? (in this case, well it's a math course you're looking for) Only the ones with probabilities? With matrix operations?" CreationDate="2016-08-14T08:56:08.177" UserId="23283" />
  <row Id="14771" PostId="13417" Score="0" Text="Not a movie, but there is a TV series named &quot;numb3rs&quot;, where each episode features a different mathematical model used by the protagonist." CreationDate="2016-08-14T10:49:19.303" UserId="13518" />
  <row Id="14773" PostId="13118" Score="0" Text="Slightly off topic but there's a lot of interesting talk by people like Demis Hassabis and Elon Musk about finding a general purpose learning algorithm such as our brains (may) use ." CreationDate="2016-08-14T15:26:32.083" UserId="5144" />
  <row Id="14774" PostId="13407" Score="0" Text="@tlorieul i have hyperlinked this in the question text. Find below the link however. http://wak2.web.rice.edu/bio/My%20Reprints/Concomitantvariablelatentclassmodels.pdf" CreationDate="2016-08-14T16:18:28.387" UserId="13100" />
  <row Id="14775" PostId="13421" Score="0" Text="So if I do this conversation, I will still have to convert features into numerical values?" CreationDate="2016-08-14T18:55:53.417" UserId="23403" />
  <row Id="14776" PostId="12432" Score="0" Text="I would try to reinstall theano" CreationDate="2016-08-14T21:34:43.010" UserId="14994" />
  <row Id="14777" PostId="13412" Score="0" Text="Ok. How can we prove it? What if we average every corresponding point from all examples? i.e average all first predicted points together, so forth, then average of all N points." CreationDate="2016-08-15T04:31:10.783" UserId="23383" />
  <row Id="14778" PostId="13425" Score="0" Text="Possibly related: http://www.kashyaptodi.com/data/SketchploreDIS2016.pdf but I don't think it will be the same thing as grid.io. In part, the problem being solved is combinatorial search, so ML components (if any) are just part of the mix." CreationDate="2016-08-15T06:14:35.800" UserId="836" />
  <row Id="14779" PostId="13420" Score="0" Text="all values are present when training the model, I'm looking for different techniques to deal with missing features in testing data" CreationDate="2016-08-15T08:41:00.777" UserId="17772" />
  <row Id="14780" PostId="13412" Score="0" Text="Prove what, that minimizing squared error leads to the MLE? I think most texts or online resources can explain that best; it's not something to show in a comment. What would averaging do that you're not already doing by computing root-_mean_-squared-error? or, what are you objecting to there?" CreationDate="2016-08-15T09:20:00.560" UserId="21" />
  <row Id="14781" PostId="13370" Score="0" Text="yes; it shows how to use SVD for recommendation" CreationDate="2016-08-15T14:49:10.857" UserId="21995" />
  <row Id="14782" PostId="13407" Score="0" Text="Yes, I have seen it but I wanted to know if you didn't understand any of these equations or if there are some that you find more difficult then others." CreationDate="2016-08-15T14:58:00.300" UserId="23283" />
  <row Id="14783" PostId="13434" Score="0" Text="Thanks for the answer. Do you have any ideas on how to explore different growth rate scenarios? In your example, the growth rate is stable throughout the 12 months, but what if you could grow faster in the beginning and then stabilize?" CreationDate="2016-08-15T15:33:12.500" UserId="23417" />
  <row Id="14784" PostId="13434" Score="0" Text="I added an example with a stronger first growth." CreationDate="2016-08-15T17:10:06.827" UserId="14904" />
  <row Id="14785" PostId="10705" Score="1" Text="If this were Python and not R, then this answer might be sensible. Wrong language." CreationDate="2016-08-15T18:30:42.140" UserId="1138" />
  <row Id="14787" PostId="12222" Score="0" Text="It is unclear how you measure &quot;importance&quot;. Random forests provide a measure of variable importance, decision tree does not provide a sorted list of variable importance afaik." CreationDate="2016-08-15T19:42:44.507" UserId="133" />
  <row Id="14788" PostId="9863" Score="0" Text="This post should give you a good start https://danijar.com/variable-sequence-lengths-in-tensorflow/" CreationDate="2016-08-15T15:47:48.317" UserId="8011" />
  <row Id="14789" PostId="13309" Score="0" Text="That makes sense, then you would say there's no weighting or combining of all the trees to get the general decision rules for the gbm model as a whole?" CreationDate="2016-08-16T00:52:58.003" UserId="15768" />
  <row Id="14790" PostId="13430" Score="0" Text="Sorry, I cannot see the point in building separate networks for both the training instances and the labels while there is a possibility to feed these in a single network in the fitting phase which does the job anyway. I can see that merging is a possibility but not its advantage over &quot;non-merging&quot;." CreationDate="2016-08-16T07:59:03.677" UserId="21560" />
  <row Id="14791" PostId="13444" Score="0" Text="Responses to this question are likely to be based on opinions, so I've suggested it be closed." CreationDate="2016-08-16T07:59:28.143" UserId="471" />
  <row Id="14792" PostId="13430" Score="0" Text="How do you feed them in the fitting phase? The inputs are always seperate, you cannot use your convolution layer on your labels so these layers need to be merged somehow." CreationDate="2016-08-16T08:00:52.987" UserId="14904" />
  <row Id="14793" PostId="13444" Score="0" Text="Well, ok. Sorry." CreationDate="2016-08-16T08:04:41.693" UserId="23441" />
  <row Id="14794" PostId="13430" Score="0" Text="In Keras `model.fit()` accepts both X and y for fitting and `model` in this case can be an &quot;non-merged&quot; model as well. Pretty much like other model types in Sklearn for example." CreationDate="2016-08-16T08:13:34.190" UserId="21560" />
  <row Id="14795" PostId="13430" Score="1" Text="Labels might be a poorly chosen name from my side, let's say you have a picture and the annotation with that picture, and you want to classify if that combination is about cats or not, then you have two types of input, and one binary output. To get the synergy between them you will have to merge the layers somewhere. Another example is where you have two pictures, one from the top and one from the bottom that you have to classify together" CreationDate="2016-08-16T08:16:30.350" UserId="14904" />
  <row Id="14796" PostId="13430" Score="0" Text="I've added a picture that hopefully makes things a bit clearer what I meant" CreationDate="2016-08-16T08:19:36.710" UserId="14904" />
  <row Id="14797" PostId="13448" Score="0" Text="As I wrote, I _do_ know sigmoid and tanh works, my question was how to select this or that or another for certain layers (it's not entirely clear either which one to apply for an output layer, because, for example, simple step and sigmoid both seem good for classification, so what?).&#xA;&#xA;Anyway, thanks for the link." CreationDate="2016-08-16T08:45:45.220" UserId="21560" />
  <row Id="14798" PostId="10705" Score="0" Text="oops! thanks @B_Miner. I'm not deleting this answer as it might be helpful for others that will make the same mistake and think we're talking about python.." CreationDate="2016-08-16T08:49:43.610" UserId="16050" />
  <row Id="14799" PostId="13430" Score="0" Text="I think worth emphasising that merging is not a &quot;per network&quot; thing, but is essentially combining two layers into a single virtual layer. This *enables* combinations of architecture components within a network. It is not dependent on having different input types, but does help solve that problem (&quot;I want an RNN because that's best for word sequences, but I also want a CNN because that is best for images - if only there was some way of combining them . . .&quot;)" CreationDate="2016-08-16T08:50:01.770" UserId="836" />
  <row Id="14800" PostId="13430" Score="0" Text="How do you evaluate the performance of the member models respectively? How do you know which branch is weak if the merged model underperforms? And how do you define the output/input layer node number at merging phase? This seems independent of the number of the final output layer, doesn't it?" CreationDate="2016-08-16T08:50:13.393" UserId="21560" />
  <row Id="14801" PostId="13430" Score="1" Text="@Hendrik: There aren't &quot;component models&quot;, there is only one model. It is a complex one, enabled by the layer merging feature. You evaluate it as you do for any single model - i.e. with a metric against a hold-out test data set (in the image/words example with data comprising images, associated partial text and the next word as the label to predict). If you want, you can inspect the layers within the model to see what they are doing - e.g. the analysis of CNN features can still be applied to the convolutional layers." CreationDate="2016-08-16T08:52:03.287" UserId="836" />
  <row Id="14802" PostId="13430" Score="0" Text="Yes it's about merging layers, not networks. Different layers are added into one layer and all of it together is one network" CreationDate="2016-08-16T08:57:05.930" UserId="14904" />
  <row Id="14803" PostId="13430" Score="0" Text="But why would you do this &quot;image-tag&quot; biclassification in this cumbersome way? It seems me a two-step process of &quot;conventional&quot; prediction (with any proper model, ANN or not): one step is to multiclassify the sentences (for mining topic) and then a biclassification to decide if the image is related to the topic. What is the additional advantage of this complex networks? More accuracy?" CreationDate="2016-08-16T09:04:20.987" UserId="21560" />
  <row Id="14804" PostId="13430" Score="0" Text="Yes, better accuracy. There are all kinds of correlations between inputs that belong together that can be captured if you do it within one network. You are throwing away extra information if you don't do it in one network" CreationDate="2016-08-16T09:05:56.757" UserId="14904" />
  <row Id="14806" PostId="13365" Score="0" Text="I'm trying to predict how many will happen..." CreationDate="2016-08-16T11:17:05.720" UserId="23278" />
  <row Id="14807" PostId="13448" Score="0" Text="As mentionned above, for a hidden layer, dont use either, rather use some sort of rectifier. For the classification output you should use softmax, not tanh or sigmoid. But if your output is some other variable between -1 and 1 use tanh, if it is some other variable between 0 and 1 use sigmoid." CreationDate="2016-08-16T12:52:39.093" UserId="23416" />
  <row Id="14808" PostId="13440" Score="1" Text="If you can't use the TO convention because there are intervening numeric variables that should be left as is, you can define a macro for all your string variables (or a selected subset) using the SPSSINC SELECT VARIABLES extension command.  The INTO subcommand could use TO, but the variable names might be inconvenient.  However, if the input variables have variable labels, those are propagated to the new variables." CreationDate="2016-08-16T13:18:04.700" UserId="20834" />
  <row Id="14809" PostId="13430" Score="0" Text="@Hendrik: Specifically in the example, you don't know what the &quot;Image Representation&quot; vector should be in order to be salient to the description. Having this feature being learned, as opposed to being in a constructed pipeline, seems to work well as a strategy (similar arguments hold for why CNNs seem to work better at extracting image features automatically compared to visual bag-of-words or Sobel filters etc - yes you can use those alternatives and performance may be good enough, but the deep CNN will often give better performance, if you have the data)" CreationDate="2016-08-16T14:08:41.297" UserId="836" />
  <row Id="14810" PostId="13430" Score="0" Text="I see, gentlemen. But how do you feed more than one test data set into one (merged) network to fit and predict? I mean, in the example above one set with the images and one with the annotation texts?In Keras both fitting and prediction accept only one X data set, whereas in our case we should feed X1 and X2 as well." CreationDate="2016-08-16T14:31:21.117" UserId="21560" />
  <row Id="14811" PostId="13430" Score="0" Text="I'll update my answer" CreationDate="2016-08-16T14:59:29.587" UserId="14904" />
  <row Id="14812" PostId="13465" Score="1" Text="It's also unnecessary to assign the result from fit back onto clf, it is an inplace method which changes your clf object" CreationDate="2016-08-16T16:55:26.727" UserId="14904" />
  <row Id="14813" PostId="13467" Score="1" Text="Thank you very much, I did not know about the SNAP library. This seems a handy option. BTW would you now out of curiosity if SNAP scales with Spark?" CreationDate="2016-08-16T18:09:51.740" UserId="14560" />
  <row Id="14814" PostId="13467" Score="0" Text="Also, about the graphLab me too, do not know if it is totally free, I read somewhere that you can get granted 1 year full, for academic purposes only. Btw I had found this interesting podcast about Dato if would be of interest [O'reilly podcast](https://www.oreilly.com/ideas/building-and-deploying-large-scale-machine-learning-applications)" CreationDate="2016-08-16T18:28:39.557" UserId="14560" />
  <row Id="14815" PostId="13467" Score="1" Text="I wouldn't invest in graphlab now that [Apple bought it](http://www.geekwire.com/2016/exclusive-apple-acquires-turi-major-exit-seattle-based-machine-learning-ai-startup/)." CreationDate="2016-08-16T18:32:17.723" UserId="381" />
  <row Id="14816" PostId="13455" Score="0" Text="I suggest looking into Spark's [GraphFrames](http://graphframes.github.io). It's Scala (not a bad thing!) but uses the familiar dataframe paradigm." CreationDate="2016-08-16T18:33:12.567" UserId="381" />
  <row Id="14817" PostId="13357" Score="0" Text="Investigate [active learning](https://en.wikipedia.org/wiki/Active_learning_(machine_learning)) and [Bayesian optimization](https://en.wikipedia.org/wiki/Bayesian_optimization)." CreationDate="2016-08-16T18:37:41.603" UserId="381" />
  <row Id="14818" PostId="13455" Score="0" Text="@Emre Yes indeed, I had a quick look. I can see that supports some Python, however, I suppose that the functionalities are less than using Scala. Btw, if you know, just to make sure, the licensing for GraphFrames is totally free? even for commercial purposes?" CreationDate="2016-08-16T18:46:04.507" UserId="14560" />
  <row Id="14819" PostId="13467" Score="0" Text="@Emre Thank you, nice input" CreationDate="2016-08-16T18:47:45.287" UserId="14560" />
  <row Id="14820" PostId="13455" Score="0" Text="It's published under the [Apache License](https://github.com/graphframes/graphframes/blob/master/LICENSE) ([FAQ](http://www.whitesourcesoftware.com/whitesource-blog/top-10-apache-license-questions-answered/))" CreationDate="2016-08-16T19:37:49.107" UserId="381" />
  <row Id="14821" PostId="13445" Score="0" Text="Fit the time series to a model, and cluster the model parameters." CreationDate="2016-08-16T19:45:10.313" UserId="381" />
  <row Id="14823" PostId="10417" Score="0" Text="Hi, could you give me more details about how to retrieve output matrix (syn1 in your example) from trained w2v embedding model? I think the w2v dropped the output matrix when finish training." CreationDate="2016-08-16T20:09:39.550" UserId="23459" />
  <row Id="14824" PostId="10417" Score="0" Text="base on my understanding, your answer of 2nd question is to reconstruct the output matrix, is that correct?" CreationDate="2016-08-16T20:56:21.843" UserId="23459" />
  <row Id="14825" PostId="13465" Score="1" Text="what version of python are you using?" CreationDate="2016-08-16T20:56:42.360" UserId="2549" />
  <row Id="14826" PostId="13445" Score="0" Text="@ emre thank you for your response. I just can't seem to find a way of finding the right modeling framework for such context. So you have a specific method in mind?" CreationDate="2016-08-16T21:48:50.593" UserId="23440" />
  <row Id="14827" PostId="13472" Score="0" Text="@ Pete thank you for your response.this seems to me an interesting avenue to explore.I will get back to you when I finish reading" CreationDate="2016-08-16T21:49:40.233" UserId="23440" />
  <row Id="14828" PostId="13445" Score="0" Text="I'd use a [neural network](http://romisatriawahono.net/lecture/rm/survey/machine%20learning/Langkvist%20-%20Deep%20Learning%20for%20Time%20Series%20Modeling%20-%202014.pdf)." CreationDate="2016-08-16T21:51:15.613" UserId="381" />
  <row Id="14829" PostId="13466" Score="0" Text="Thanks! I indeed use python 3..." CreationDate="2016-08-16T21:57:08.710" UserId="23452" />
  <row Id="14830" PostId="13445" Score="0" Text="That's indeed a very good paper.  I am a newbie to deep learning. It will be difficult for me to implement it in python. Do you know of  good resources for a beginner(books moods github..)?" CreationDate="2016-08-16T22:24:44.630" UserId="23440" />
  <row Id="14831" PostId="13455" Score="0" Text="@Emre Thank you very much and especially for you valuable answer, GraphFrames seems a legit solution." CreationDate="2016-08-16T22:26:39.307" UserId="14560" />
  <row Id="14833" PostId="13445" Score="0" Text="Start [here](https://www.tensorflow.org/versions/r0.10/get_started/index.html), then try [these time series tutorials](https://github.com/tgjeon/TensorFlow-Tutorials-for-Time-Series)." CreationDate="2016-08-16T23:03:46.060" UserId="381" />
  <row Id="14834" PostId="13445" Score="0" Text="Thank you so much for your advices. So once the model parameters are learned it's possible to cluster using similarity measure like Euclidean distance?  Is it still a valid metric for such features representation?" CreationDate="2016-08-16T23:48:31.010" UserId="23440" />
  <row Id="14835" PostId="13477" Score="0" Text="[TensorBoard: Graph Visualization](https://www.tensorflow.org/versions/master/how_tos/graph_viz/index.html). [Example](http://blog.altoros.com/visualizing-tensorflow-graphs-with-tensorboard.html)." CreationDate="2016-08-16T23:53:53.560" UserId="381" />
  <row Id="14836" PostId="13479" Score="0" Text="http://www.benjaminmbrown.com/2016/02/tutorial-how-to-build-real-time-data-visualization-with-d3-crossfilter-and-websockets-in-python-by-example/" CreationDate="2016-08-17T02:19:44.910" UserId="381" />
  <row Id="14837" PostId="13444" Score="0" Text="I don't know about the field but you can take a look at this: http://www.uva.nl/en/research/phd/phd-vacancies/item/16-348-phd-candidate-in-multimedia-analytics-for-forensic-investigations.html" CreationDate="2016-08-17T06:27:55.920" UserId="3151" />
  <row Id="14838" PostId="13444" Score="0" Text="You can also ask your question on Researchgate." CreationDate="2016-08-17T06:28:57.500" UserId="3151" />
  <row Id="14839" PostId="13482" Score="0" Text="Hi Tak, thank you for the reply. My model is on my Github. The link is https://github.com/dalalkrish/Attribution-Model. Your suggestions would be highly appreciated. I have read about Markov chains model on R-bloggers I guess there is package in R that serves the purpose. However, the model that I have used is a probabilistic model and it is also dependent only on the response value." CreationDate="2016-08-17T06:29:42.930" UserId="23129" />
  <row Id="14841" PostId="13462" Score="0" Text="It doesn't work unfortunately." CreationDate="2016-08-17T07:39:41.937" UserId="21560" />
  <row Id="14842" PostId="13463" Score="0" Text="It doesn't seem to work: no error message but no output either. Weird." CreationDate="2016-08-17T07:40:22.173" UserId="21560" />
  <row Id="14843" PostId="13430" Score="0" Text="Thank you, that is a very helpful addition with the multiple input syntax! In prediction it is supposed to be the following, right? `model1.predict_classes([X1, X2])`" CreationDate="2016-08-17T07:54:20.407" UserId="21560" />
  <row Id="14844" PostId="13430" Score="0" Text="Yes, that is right" CreationDate="2016-08-17T07:55:09.290" UserId="14904" />
  <row Id="14845" PostId="13389" Score="0" Text="@Richard, sentiment analysis for 2 classes is also a well received binary classification problem, did you try it with your datasets?" CreationDate="2016-08-17T08:41:44.247" UserId="21024" />
  <row Id="14846" PostId="13484" Score="0" Text="Does this help: http://stats.stackexchange.com/questions/21807/evaluation-measure-of-clustering-without-having-truth-labels?" CreationDate="2016-08-17T08:42:08.970" UserId="1244" />
  <row Id="14847" PostId="13488" Score="1" Text="Thanks a lot! You made me a complete guidance." CreationDate="2016-08-17T08:50:12.487" UserId="23201" />
  <row Id="14849" PostId="13028" Score="0" Text="I don't think DL4J or H2O would be relevant here. I'll answer below." CreationDate="2016-08-17T10:39:07.837" UserId="21" />
  <row Id="14853" PostId="12110" Score="0" Text="Personally, I prefer to see it as a convolution with fractional stride. In the cited paper, there is no real deconvolution in the sense of &quot;reversing the effect of a convolution&quot;. The only objective is to learn the up-sampling kernels. In that case, both &quot;deconvolution&quot; and &quot;transposed convolution&quot; are not correct." CreationDate="2016-08-17T14:54:47.423" UserId="23487" />
  <row Id="14854" PostId="13498" Score="0" Text="Why do you specifically want to use a neural network?" CreationDate="2016-08-17T14:56:52.623" UserId="15543" />
  <row Id="14855" PostId="13498" Score="0" Text="@YCR To predict the probability of getting effected by the disease." CreationDate="2016-08-17T15:06:54.680" UserId="23486" />
  <row Id="14856" PostId="13417" Score="0" Text="Numb3rs was ridiculous. He'd have five or six robbery locations, then go &quot;I'm going to use spatial statistics&quot; and then pinpoint the front door of the next robbery. Which is of course where the next robbery happened. I've not seen Moneyball but whenever I've seen stats in a movie it's always been followed by me face-palming (p&lt;0.01)." CreationDate="2016-08-17T15:14:23.887" UserId="471" />
  <row Id="14857" PostId="13498" Score="0" Text="I mean compared to more classic models like logistic regression or xgboost." CreationDate="2016-08-17T15:18:35.483" UserId="15543" />
  <row Id="14858" PostId="13498" Score="0" Text="Your previous [question](http://stackoverflow.com/questions/38959144/how-to-build-a-prediction-app-using-neural-networks-in-r) had been put on hold. You may want to reformulate it, BTW." CreationDate="2016-08-17T15:20:35.717" UserId="15543" />
  <row Id="14859" PostId="13467" Score="1" Text="I actually used SNAP only for simple experiments on medium-size graphs so I do not know if it scales with Spark but they have a QA system to ask your question. I suggest to contact THEM directly. @PhilipC." CreationDate="2016-08-17T16:25:58.800" UserId="8878" />
  <row Id="14860" PostId="13445" Score="0" Text="That's where things get hairy, because clustering is subjective, and rescaling the features will change the clusters with a metric like the Euclidean distance. I suggest just trying the various clustering algorithms and looking into [metric learning](http://ai.stanford.edu/~ang/papers/nips02-metric.pdf). But finding this embedding (time series representation) to make clustering feasible in the first place is the hard part, so don't fret!" CreationDate="2016-08-17T17:44:46.833" UserId="381" />
  <row Id="14861" PostId="13498" Score="0" Text="If you've never done this before start with a simple model like logistic regression, at least to get a baseline. Look up &quot;[cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics))&quot;, and &quot;[classification loss functions](https://en.wikipedia.org/wiki/Loss_functions_for_classification)&quot;." CreationDate="2016-08-17T18:36:07.557" UserId="381" />
  <row Id="14862" PostId="13504" Score="0" Text="Statistics, algorithms, and data structures. Forget Hadoop." CreationDate="2016-08-17T19:39:58.297" UserId="381" />
  <row Id="14863" PostId="13503" Score="0" Text="@Murtuza_07 Let me know if the solution works. In that case, if you could accept the answer I'd be very thankful." CreationDate="2016-08-17T21:10:29.863" UserId="23495" />
  <row Id="14864" PostId="13510" Score="3" Text="Yes, this happens routinely with classification. Can you explain the problem?" CreationDate="2016-08-17T22:23:04.310" UserId="381" />
  <row Id="14865" PostId="13309" Score="0" Text="Looking at it more, it seems like there is an explanation generated for each row, and that there is some general method for obtaining an explanation from the holistic model. The explanation['explanation'][0] seems to call the explanation for the first observation, not the first tree?" CreationDate="2016-08-17T22:33:25.787" UserId="15768" />
  <row Id="14867" PostId="13352" Score="0" Text="@AlexeyGrigorev thanks. After some research and thoughts it seems to be the only way." CreationDate="2016-08-18T06:04:11.610" UserId="23285" />
  <row Id="14868" PostId="13514" Score="0" Text="Thanks!  So you mean to say starting from training the model to deployment, everything will be done in Python? In that case i believe product or e-commerce companies will not using R for machine learning. Instead they will be using Python as Python is more suitable for automation in a software company and in e-commerce." CreationDate="2016-08-18T06:39:17.910" UserId="13100" />
  <row Id="14869" PostId="13510" Score="0" Text="Yes, this happens routinely with extrapolation. Same question as above" CreationDate="2016-08-18T06:42:21.333" UserId="12527" />
  <row Id="14870" PostId="13514" Score="0" Text="They probably won't care what language you do your prototyping in, but they will probably expect the finished product to be in Java or python. They might make an exception if you really need a library that only exists in another language." CreationDate="2016-08-18T06:57:05.933" UserId="381" />
  <row Id="14871" PostId="13514" Score="0" Text="Can a model built in R be 'translated' to Python language? Also, by prototyping do you mean actual model development wherein a model is finalized or you mean kind of one odd randomforest or svm model just to see if it works or not?" CreationDate="2016-08-18T07:00:15.783" UserId="13100" />
  <row Id="14872" PostId="13504" Score="0" Text="There is nice article about this topic - https://www.linkedin.com/pulse/data-science-taught-universities-here-why-maciej-wasiak. As others mentioned - SQL skills level ninja are necessary. ~70% of your time you will be getting and cleaning data. And without nice clean dataset, all Python/R skills are useless." CreationDate="2016-08-18T07:00:58.573" UserId="17290" />
  <row Id="14873" PostId="13504" Score="4" Text="Employers don't hire you, or keep you on board, because of your skills, but because they *like* you. So improve your likeability." CreationDate="2016-08-18T07:06:33.580" UserId="14588" />
  <row Id="14875" PostId="13514" Score="0" Text="Yes, since python is a general purpose language. In the worst case, you won't find a ready-made library and you'll have to write your own. You might find that the company implements its own version of an algorithm for which libraries exist. Maybe theirs is faster, or it is tweaked to better suit their needs, etc. Prototyping is the tinkering you do before writing production code." CreationDate="2016-08-18T07:13:32.670" UserId="381" />
  <row Id="14876" PostId="13514" Score="0" Text="Thanks! Although i feel &quot;predictive modelling markup language&quot; or PMML can bridge the gap between prototyping in one language and deployment in another, still doing it in one language is still better. Thanks a lot for your useful suggestions and comments!!" CreationDate="2016-08-18T07:18:19.937" UserId="13100" />
  <row Id="14877" PostId="13514" Score="1" Text="I was about to say a service-oriented architecture also helps bridge technologies. PMML is a bit enterprisey; I haven't used it, but yours is a Java shop, the mother enterprise languages, so you never know..." CreationDate="2016-08-18T07:21:37.230" UserId="381" />
  <row Id="14878" PostId="13513" Score="8" Text="Python is a good compromise: it provides many (non-standard) library for datascience (pandas, scikit,...) and many industrial process are already coded in python." CreationDate="2016-08-18T07:59:53.153" UserId="21825" />
  <row Id="14880" PostId="13519" Score="0" Text="Do you want to have 1000 separate matrix objects or have them in a list?" CreationDate="2016-08-18T09:41:23.450" UserId="21990" />
  <row Id="14881" PostId="13520" Score="0" Text="Hi Jan and thank you for the answer. In the second command with the lapply, the function will take as argument the vector c(1:0000) right ? &#xA;&#xA;Moreover, after running this command single will be a list containing the 1000 matrices. So, if I need to extract lets say the first matrix, I will have to use single[[1]] ? (I am sorry I am not very familiar with lists)&#xA;&#xA;Lastly, do you know how this could be done with matrices ? My idea would be to create the 1000 matrices and then put them all in the diagonal of a huge matrix using the command &quot;adiag&quot;. What do you think ?" CreationDate="2016-08-18T10:05:57.287" UserId="19212" />
  <row Id="14882" PostId="13520" Score="0" Text="Also now 21 **is** divided by 7. Your way in this case is really smart. But what if A had 20 rows ?" CreationDate="2016-08-18T10:12:45.117" UserId="19212" />
  <row Id="14883" PostId="13520" Score="0" Text="Yes, the access to the elements is exactly as you say. You probably could put them in one large matrix. I will have a look at it, never done it myself. And regarding the last comment, you mean if A had 20 columns? It would be up to you to make sure when you create the matrix, it has correct size. So if you had 41 elements, R would fill the missing element with the first element again - try `matrix(c(1:41), nrow=2)`" CreationDate="2016-08-18T10:18:17.153" UserId="21990" />
  <row Id="14884" PostId="13520" Score="0" Text="No I mean, that now you put in the first row the 21 column-elements of A and in the second row you put  the 7 column-elements of B times 3 (so 3x7=21). that way the number of elements of each row match. But what if A had 20 columns ? Then you would need to put 2 times the column-elements of B and then put the rest 6 (so 2x7+6=20). Right ? Then I guess it should be done like matrix(c(A[x,],rep(B[x,],times=2),B[x,1:6]),...). Correct ?" CreationDate="2016-08-18T10:23:46.773" UserId="19212" />
  <row Id="14886" PostId="13520" Score="0" Text="yh that should work :) you can for example make a working example have two simple 3x3 matrices A and B and play around with the ideas and see what works. That will teach you the best  (also I was checking on toy example if the code works)...regarding the adiag - it will make 42 x 21000 matrix, that is 6.7Mb worth of data..what you gonna do with it? :D" CreationDate="2016-08-18T10:26:16.273" UserId="21990" />
  <row Id="14887" PostId="13520" Score="0" Text="Actually it would be 2000x21000 matrix :P. I could burn my laptop :P. Thank you :)" CreationDate="2016-08-18T10:29:21.150" UserId="19212" />
  <row Id="14888" PostId="13520" Score="0" Text="Yh you right, I found solution, will edit my answer." CreationDate="2016-08-18T10:30:02.020" UserId="21990" />
  <row Id="14889" PostId="13521" Score="0" Text="thank you! Printing and reading it now." CreationDate="2016-08-18T10:57:20.717" UserId="23492" />
  <row Id="14890" PostId="13521" Score="0" Text="Thanks for the paper! However, equation 2 and section 2 tells about boosting using tree models. I am asking about linear regression boosters..." CreationDate="2016-08-18T11:04:35.453" UserId="23492" />
  <row Id="14891" PostId="13521" Score="0" Text="Not sure I follow your comment, but XGBoost uses regression trees. If you can, do share a specific example with which your query can be illustrated." CreationDate="2016-08-18T11:25:12.160" UserId="18540" />
  <row Id="14892" PostId="13522" Score="0" Text="Thanks for your response!! It makes sense as well." CreationDate="2016-08-18T11:28:53.337" UserId="13100" />
  <row Id="14893" PostId="13525" Score="1" Text="I can recommend `PRML` by Bishop, but it's not an optimization-only book." CreationDate="2016-08-18T11:40:51.583" UserId="11097" />
  <row Id="14894" PostId="13525" Score="0" Text="@Dawny33 I am actually looking for a book which is dedicated to optimization and has codes available in Python. Bishop's book has optimization for individual techniques and is not learning method agnostic. ALso it does not have Python code!" CreationDate="2016-08-18T11:50:46.633" UserId="13100" />
  <row Id="14895" PostId="13525" Score="0" Text="@Dawny33 although some genius created code of own in Python, however it misses many chapters like SVM and others. https://github.com/masinoa/machine_learning" CreationDate="2016-08-18T11:54:42.883" UserId="13100" />
  <row Id="14897" PostId="13521" Score="0" Text="Sure, if you read this tutorial: http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3885826/ you will see that Gradient Boosting Machine can use different boosts, like trees or linear regression. The last one can be done with XGBoost by setting the 'booster' parameter to 'gblinear'. My question is how the specific gblinear works in detail. I havre edited the question to add this. Thanks." CreationDate="2016-08-18T12:09:13.640" UserId="23492" />
  <row Id="14898" PostId="13520" Score="0" Text="One last question. If I want to access the (3,2) element from the first matrix in the &quot;single&quot; list, how should I do it ? Or if I want to access the 2nd column of the first matrix in the &quot;single&quot; list, how should I do it ?" CreationDate="2016-08-18T12:29:58.767" UserId="19212" />
  <row Id="14899" PostId="13520" Score="0" Text="If you go single[[1]] it subsets the whole thing there and then behaves as usual matrix. So single[[1]][3,2] will return element in the first matrix, 3rd row, 2nd column. single[[1]][,2] returns second column :)" CreationDate="2016-08-18T12:33:08.333" UserId="21990" />
  <row Id="14900" PostId="13514" Score="2" Text="@Enthusiast don't forget that you can run R under python using RPy2 (for example) so you may end up (as I did in a previous job) running models written in R through python so that they can be presented through a web interface via django." CreationDate="2016-08-18T13:35:54.057" UserId="23527" />
  <row Id="14901" PostId="13514" Score="0" Text="@MD-Tech i understand that model can be built using RPy and once done you will end up with a model that has .rds format. To deploy that in production environment you used RPy? or if i ask the question correctly, can you please elaborate the architecture from model building till deployment with a bit more detail for benefit of all?" CreationDate="2016-08-18T14:03:20.443" UserId="13100" />
  <row Id="14902" PostId="13522" Score="0" Text="Although true, none of the above actually answers the question. Getting the data reduces 99% of the times to querying a database or reading `.csv` files - to which aim R is actually the best suitable tool on the market. Candidates availability: that there are more Java programmers than R programmers does not imply that you have to discard a R candidate if you have one. It doesn't really matter how the scientist performs their exercises as long as they deploy readable code that can be run by some servers (or any other thing the company is running)." CreationDate="2016-08-18T14:09:33.390" UserId="10372" />
  <row Id="14903" PostId="13514" Score="2" Text="We built the model in plain text .r files that were loaded into the R interpreter to test (and to facilitate building). Whilst this was being built and tested we built a python django project with a section that referenced RPy2 and created RPy2 objects. These objects were then used to load the R files in the same way as you would load them in the interpreter so that we could access the functions that wrapped the model. We could then pass data from the database to R via python. The python layer gave us the web frontend with django and control over the database etc.." CreationDate="2016-08-18T14:13:18.683" UserId="23527" />
  <row Id="14904" PostId="13526" Score="0" Text="Thanks for sharing your perspective and experience!! This is helpful. From your second last paragraph, i assume you are talking about scikit-learn? or did you mean RPy? Care to elaborate?" CreationDate="2016-08-18T14:14:27.957" UserId="13100" />
  <row Id="14905" PostId="13514" Score="1" Text="@Enthusiast The results of the model were returned by the R within RPy2 and presented in the front end in various guises, mostly graphs." CreationDate="2016-08-18T14:14:38.543" UserId="23527" />
  <row Id="14906" PostId="13526" Score="1" Text="I simply mean that whatever you are doing in R, there is most likely a similar Python package that does the same job. Pandas covers most of the things that `data.table` offers; scikit-learn, as you mentioned, is another example, but there is many more according to the case at hand." CreationDate="2016-08-18T14:18:45.293" UserId="10372" />
  <row Id="14907" PostId="13514" Score="0" Text="I think you should have added this as response, so that people can upvote your response as it is useful one. Nevertheless thank you!! I couldn't understand &quot; model in plain text .r files that were loaded into the R interpreter to test (and to facilitate building)&quot; part. By model, did you mean something like linear regression or neural network or did you mean a bar chart or some other visualization? If it is former, can you share some link or source with example? My understanding is that for deploying a model, first it has to e built and tested before productization and exported in .rds format" CreationDate="2016-08-18T14:23:08.097" UserId="13100" />
  <row Id="14908" PostId="13526" Score="0" Text="This makes things clear. Thank you!!" CreationDate="2016-08-18T14:24:17.563" UserId="13100" />
  <row Id="14909" PostId="13514" Score="2" Text="@Enthusiast It was a Bayesian network for finance but I can't say more than that. The model was written in straight R. Just plain text; I was editing it in Vim whenever I needed to, and it was &quot;deployed&quot; by loading the R code, as text, into RPy2 using source(&quot;our_code.r&quot;) on the RPy2 objects. It was done this way so that we could live edit the model. This isn't an answer to this question; its an answer to one that hasn't been asked ;)" CreationDate="2016-08-18T14:32:27.310" UserId="23527" />
  <row Id="14910" PostId="13399" Score="0" Text="thanks, do you know of any Python implementation by chance?" CreationDate="2016-08-18T14:58:20.947" UserId="1426" />
  <row Id="14911" PostId="13525" Score="0" Text="This book is math heavy, i am looking for something which is code heavy." CreationDate="2016-08-18T14:58:34.957" UserId="13100" />
  <row Id="14912" PostId="13399" Score="0" Text="If you do not have to many datapoints you could compute the kernel beforehand with a pairwise metric (http://scikit-learn.org/stable/modules/metrics.html#metrics). Otherwise use the svm implementation in sklearn. That's the easy route." CreationDate="2016-08-18T15:07:23.397" UserId="20372" />
  <row Id="14913" PostId="4904" Score="0" Text="So, this is more to do with unstructured data?" CreationDate="2016-08-18T15:18:18.097" UserId="13100" />
  <row Id="14914" PostId="13522" Score="0" Text="Of course you should not discard the candidate. The person is much more important than the tool. Their team may learn R and the candidate can learn Java/Python. But it will take time which in means money." CreationDate="2016-08-18T15:35:43.257" UserId="23521" />
  <row Id="14915" PostId="13522" Score="0" Text="The point I certainly disagree is that it doesn't mind the language. When the only member of the team who knows R is no holidays and they need to make changes the boss won't be happy. Or just ask the team &quot;Oh great, we need to learn a new language just because the new one does things this way&quot;. May be server administration is another department and new types of server need some new analysis, procedures, etc. May be you need green light from IT security to use a new language." CreationDate="2016-08-18T15:40:03.320" UserId="23521" />
  <row Id="14916" PostId="13526" Score="1" Text="Exactly what I do. Research in R, once that is finished, translate to python to integrate into the codebase. But @Enthusiast whether you can do the same in that company depends on its culture. Most people use the programming language their boss uses. And Python isn't hard to learn." CreationDate="2016-08-18T15:51:58.053" UserId="23536" />
  <row Id="14917" PostId="13530" Score="1" Text="Not having any domain expertise in risk scoring, logistic regression is where I would start too. My educated guess is that the important thing here is feature engineering -- access to relevant information -- and historical data to train your model. Once you have a signal, the model usually sorts itself out." CreationDate="2016-08-18T18:02:10.733" UserId="381" />
  <row Id="14918" PostId="13518" Score="0" Text="You should know better what it does than we do since your computer's sitting on MATLAB's source code, but I imagine it is referring to method explicated in [section 3.2 here](http://mlg.eng.cam.ac.uk/pub/pdf/QuiRasWil07.pdf), and elsewhere (Wahba (1990, ch. 7), and in Poggio and Girosi (1990, eq. 25))." CreationDate="2016-08-18T18:25:07.963" UserId="381" />
  <row Id="14920" PostId="13409" Score="0" Text="If would be useful if you could provide what the original data look like. It is unclear whether you want to act on the original data frame or on the one that you obtain after the `value_counts()`. In the latter case you could just rename the entries that satisfy your condition: what in particular have you found difficult to achieve?" CreationDate="2016-08-18T19:02:07.920" UserId="10372" />
  <row Id="14921" PostId="13503" Score="0" Text="What is the purpose of the counter `count`?" CreationDate="2016-08-18T19:04:41.327" UserId="10372" />
  <row Id="14922" PostId="13487" Score="0" Text="What do you mean by *&quot;infer those groupings in the original mutltidimentional feature space?*&quot;" CreationDate="2016-08-18T19:06:26.630" UserId="10372" />
  <row Id="14923" PostId="13502" Score="0" Text="Thank you for the reference. It points out many insightful research findings for clustering and similarity measures. I am I nteressted also in infering rules about time series as multivariate vectors, as to how those groupings can.be used to infer rules associated to each cluster after having stable clusters using clusters membership as class labels." CreationDate="2016-08-18T19:52:37.997" UserId="23440" />
  <row Id="14924" PostId="13487" Score="0" Text="I meant after reaching stable clusters.of multivaruate time series. use cluster memberships as class labels and infer rules associated to their original  features correlations" CreationDate="2016-08-18T19:57:39.137" UserId="23440" />
  <row Id="14925" PostId="13501" Score="0" Text="This is actually a very good solution but nevertheless it puts lots of constraints on the type of recommendation hat one wants to build." CreationDate="2016-08-18T20:03:46.883" UserId="5177" />
  <row Id="14926" PostId="13501" Score="1" Text="I started from the premise in the post (&quot;the program should...&quot;) that he is going to use a matrix factorization model." CreationDate="2016-08-18T20:06:32.270" UserId="381" />
  <row Id="14927" PostId="13501" Score="0" Text="I kinda got that idea too. But I just wanted to note that there is limitations for this method which I also use in some context where I don't need to work much on evaluation metrics optimization. I'm even voting it up because I consider it a really good hack." CreationDate="2016-08-18T20:08:57.237" UserId="5177" />
  <row Id="14928" PostId="13487" Score="0" Text="Still, what &quot;rules&quot; do you want to &quot;infer&quot; (what is a &quot;rule&quot;)? Also, *&quot;their original feature correlations&quot;*: what correlations are you meaning? Standard Pearson correlations between variables or anything else?" CreationDate="2016-08-18T20:27:46.393" UserId="10372" />
  <row Id="14929" PostId="13484" Score="0" Text="You want to use the p-value of what, in particular? If would be useful if you could provide an example of the data set you're using. Moreover it's not really clear what you're asking (cluster, correlations X-squared? are different things, how do you relate them all together?)." CreationDate="2016-08-18T20:31:05.833" UserId="10372" />
  <row Id="14930" PostId="13535" Score="0" Text="The input of *any* clustering algorithm is always the initial set of data together with an additional label that, for each data, specifies the cluster they belong to. What in particular do you want to represent?" CreationDate="2016-08-18T20:48:00.807" UserId="10372" />
  <row Id="14931" PostId="13535" Score="0" Text="I want to determine clusters of 'users/addresses' with similar transactional behaviour. I forgot to mention that one of the features will be a categorised tag of the transaction. ie. transactions to gambling sites will be tagged, donations to charities etc." CreationDate="2016-08-18T20:50:23.680" UserId="15961" />
  <row Id="14932" PostId="13535" Score="0" Text="What is the particular problem that prevents you from doing so? Define a distance between points (to calculate similarities according to your data) and define a clustering method of your choice based on that distance." CreationDate="2016-08-18T20:52:12.953" UserId="10372" />
  <row Id="14933" PostId="13535" Score="0" Text="I suppose the problem is that I am struggling to understand the way in which this type of data would be represented in a dimensional space. I understand [this](http://johnloeber.com/docs/kmeans.html) example in a 2d space but can't quite see how the transactional (source/destination) representation would work. Perhaps i am missing something very obvious and should consider an address as undirected/without edges and with only attributes about the node itself." CreationDate="2016-08-18T21:04:46.623" UserId="15961" />
  <row Id="14934" PostId="13535" Score="0" Text="I think you're making confusion among many different things. Is the clustering that you want to achieve or what else? If so, what are the edges and nodes you're talking about? Each point is represented, in your data set, by a set of coordinates (which we have no idea about because you didn't give any data source): cluster the points according to the sub-set of coordinates you want to consider and you should be done already." CreationDate="2016-08-18T22:08:03.433" UserId="10372" />
  <row Id="14935" PostId="13535" Score="0" Text="your right, i have confused myself. i want clustering on bitcoin user/addresses. a node is a user/address, an edge is the transaction from one user/address to another. the transaction will contain features such as category. the user/address will also contain features. ill update my question to show sample data." CreationDate="2016-08-18T22:15:37.190" UserId="15961" />
  <row Id="14937" PostId="13522" Score="0" Text="@GennaroTedesco the code written by the candidate must be maintainable by *other* programmers, while working together and also in some future when the original author will move on. It's not sufficient to have a candidate that knows a tech well, it is still important to consider how easy it will be to hire *another* candidate who knows the tech well when you will need one.  of course, a new piece of niche technology can be introduced if there's a good reason, but there needs to be a *good* reason to outweigh such business risks." CreationDate="2016-08-19T01:01:43.747" UserId="15518" />
  <row Id="14938" PostId="13527" Score="0" Text="Hey man, this worked! Thanks a lot!" CreationDate="2016-08-19T04:58:45.983" UserId="23208" />
  <row Id="14939" PostId="13537" Score="0" Text="There are many methods, see https://en.wikipedia.org/wiki/Outlier#Detection. If your outliers are heavy tailed, perhaps https://en.wikipedia.org/wiki/Median_absolute_deviation would work better." CreationDate="2016-08-19T07:07:43.200" UserId="6550" />
  <row Id="14940" PostId="13487" Score="0" Text="if we consider a sample time serie of multivariate features as a vector T=(t1..tn) s.t.  for each i =1..n we have ti is a p-multidimentional vector defined as ti=(f1,f2..fp) . Then after clustering we will have each sample vector Tj belonging to a given cluster Cl ," CreationDate="2016-08-19T07:47:14.630" UserId="23440" />
  <row Id="14941" PostId="13487" Score="0" Text="---continued-- we are interessted in infering underlying dynamic of change of series in each cluster as rules. the idea is already exisiting inthe literetture [link](https://www.cs.helsinki.fi/u/htoivone/pubs/dmkd97episodes.pdf) for one dimensional time series. I am wondering if there is another related work but for multidivariate case. Mostly discretization of these series is done by SAX or PAA beforehand, the idea is if it is alreadfy done for raw data?" CreationDate="2016-08-19T07:47:33.193" UserId="23440" />
  <row Id="14942" PostId="13375" Score="0" Text="@trailblazer adding trees to your forest is a decent approach I think, never tried it but there should be literature about it. Look for online learning. Algorithm agnostic will not be possible because some algorithms can only learn over the whole set." CreationDate="2016-08-19T07:50:05.913" UserId="14904" />
  <row Id="14943" PostId="13375" Score="0" Text="@trailblazer with regards to the sample space question, that could work for some algorithms but not for others, this again depends on the online learning possibility but you would also need to keep increasing weights or retrain on everything, you cannot retroactively decrease the weight on older samples without retraining" CreationDate="2016-08-19T07:51:37.663" UserId="14904" />
  <row Id="14944" PostId="13518" Score="0" Text="I have checked the source code but I couldn't find the part where the method is implemented. In the comments it doesn't tell anything other than the help section. &#xA;&#xA;The link you provided introduces the Subset of Regressors method, where a subset is supposed to be chosen. 'ActiveSetMethod' refers to the method of choosing the subset. I just couldn't find anything that relates this to 'log likelihood' in the literature. Since log likelihood gives a scalar output I'm assuming it just sorts them in a greedy algorithm somehow." CreationDate="2016-08-19T08:17:12.423" UserId="16609" />
  <row Id="14945" PostId="13518" Score="0" Text="(too long for edit) -- after a little more search I have found the related source code. It should resolve the problem. For future reference, it is located in: &quot;...\R2015b\toolbox\stats\classreg\+classreg\+learning\+gputils\selectActiveSet&quot;" CreationDate="2016-08-19T08:32:08.047" UserId="16609" />
  <row Id="14946" PostId="13513" Score="3" Text="&quot;our data scientists must have good programming skills in Java/Python as we are a internet/mobile organisation and everything we do is online&quot; is a massive non-sequitur - the conclusion *does not* follow from the premise. I suspect the CDS is just trying to get rid of you." CreationDate="2016-08-19T10:00:57.910" UserId="471" />
  <row Id="14947" PostId="13542" Score="0" Text="What are you classifying then? Every classifier will return Yes and all will do very good" CreationDate="2016-08-19T10:06:25.083" UserId="14904" />
  <row Id="14948" PostId="13542" Score="0" Text="@Jan I have a set of attributes &quot;a1&quot; to &quot;an&quot; in my dataset. The attribute &quot;an' is either &quot;yes&quot; or &quot;no&quot; . I train the classifier using train data and the classifier labels the test data as either &quot;yes&quot; or &quot;no&quot; and then I compare them with the actual &quot;yes&quot; and &quot;no&quot; to measure performance. Now my question is how to compare the labels when there are no actual &quot;yes&quot; and &quot;no&quot;(all of my data are actually &quot;yes&quot; and I don't have the attributes a1 to an-1 for &quot;no&quot; samples)," CreationDate="2016-08-19T10:25:31.060" UserId="23564" />
  <row Id="14949" PostId="13513" Score="0" Text="@Spacedman it can or cannot be. It happened almost a year ago. As of today if i search for jobs in his company, it clearly asks for Python/JAVA and R is not  even mentioned in job description for data scientist position." CreationDate="2016-08-19T10:35:47.640" UserId="13100" />
  <row Id="14950" PostId="13535" Score="0" Text="Most people are not very intricate with bitcoin transactions, could you show a few lines and explain every column in one or a few lines?" CreationDate="2016-08-19T10:37:48.670" UserId="14904" />
  <row Id="14951" PostId="13535" Score="0" Text="So the in and out-degree are part of a transaction? So they refer to earlier transactions?" CreationDate="2016-08-19T10:54:46.737" UserId="14904" />
  <row Id="14952" PostId="13535" Score="0" Text="Sorry i didn't make that part clear, the degrees and page rank hold information about the address within the network as a whole, not specific to that transaction. So they take into account previous transactions as well." CreationDate="2016-08-19T10:59:02.743" UserId="15961" />
  <row Id="14953" PostId="13543" Score="1" Text="That is completely wrong in all means." CreationDate="2016-08-19T12:35:12.283" UserId="10372" />
  <row Id="14954" PostId="13551" Score="0" Text="Thanks alot! That answers my questions perfectly. I thought - somehow - that sklearn's tree-clf stores the predictions in `predictions` by default." CreationDate="2016-08-19T12:35:43.617" UserId="23452" />
  <row Id="14955" PostId="13547" Score="3" Text="R code can be easily run by almost all the tools available out there in the market. Java is almost of no use for data science." CreationDate="2016-08-19T12:37:31.250" UserId="10372" />
  <row Id="14956" PostId="13551" Score="0" Text="It has never seen your test set yet so it wouldn't know what to predict on, and packages generally do not put anything in global variables but return their values via the functions calls (in this case the predict function of your classifier object)" CreationDate="2016-08-19T12:37:32.037" UserId="14904" />
  <row Id="14957" PostId="13551" Score="0" Text="I am a frequent stata user and python is totally new to me. Your answer helps me a great deal. Thank you!  Now I only need to understand how to print the decision tree... slowly...but steady." CreationDate="2016-08-19T12:48:25.917" UserId="23452" />
  <row Id="14958" PostId="13551" Score="1" Text="The sklearn package has terrific documentation and a lot of examples, make sure to look at those!" CreationDate="2016-08-19T12:49:50.350" UserId="14904" />
  <row Id="14959" PostId="13547" Score="1" Text="@GennaroTedesco JAVA is useful for coding in bigdata tools though. So partly useful for querying data." CreationDate="2016-08-19T13:14:02.840" UserId="13100" />
  <row Id="14961" PostId="13554" Score="0" Text="In this case i know for sure that role was for modelling as it asked for machine learning skills and specified list of techniques." CreationDate="2016-08-19T13:53:39.480" UserId="13100" />
  <row Id="14962" PostId="13554" Score="0" Text="They could still be doing that inside those technologies though using Java/Python libraries, something like H20 or MLlib spring to mind." CreationDate="2016-08-19T13:59:12.107" UserId="23578" />
  <row Id="14964" PostId="13556" Score="0" Text="I don't know of a specific dataset for this task but it should be trivial to artificially generate some test cases with augmentations of labeled cases" CreationDate="2016-08-19T14:13:29.890" UserId="14904" />
  <row Id="14965" PostId="13556" Score="0" Text="@JanvanderVegt I could think of creating an artificial dataset. However, the only idea to make this with augmentations would be a dataset with bounding boxes / semantic segmentation of high resolution. Then I could scale a big window of the image down or cut a small window of the image out." CreationDate="2016-08-19T14:24:47.607" UserId="8820" />
  <row Id="14966" PostId="13513" Score="0" Text="If he said &quot;must have java/python skills as that's what our codebase is&quot; or &quot;that's what we all like&quot; then it makes sense. But to say &quot;we need Java/Python because we are an internet/mobile company&quot; does not make sense. There are &quot;internet/mobile companies&quot; that only use objective-C, or Javascript, or Lisp, or almost *any* other language. Their reason for wanting Java/Python is *not* and can *not be* because they are an internet/mobile org and everything they do is online." CreationDate="2016-08-19T14:25:38.890" UserId="471" />
  <row Id="14967" PostId="13556" Score="0" Text="Could you be more specific about &quot;influence of architecture&quot;? By describing a real-life example for instance?" CreationDate="2016-08-19T14:27:34.853" UserId="12527" />
  <row Id="14968" PostId="13556" Score="1" Text="Maybe something on this facial recognition database site will help.  I haven't explored it much, but facial recognition seems like a good, real-life instance of when varying distance would be significant. http://www.face-rec.org/databases/" CreationDate="2016-08-19T14:33:34.980" UserId="19161" />
  <row Id="14969" PostId="13556" Score="0" Text="@LaurentDuval Sure. Suppose you want to make semantic segmentation of street / cars / car signs in a dataset similar to [KITTI](http://www.cvlibs.net/datasets/kitti/eval_object.php). Now you have two architectures: One which has two convolutional layers with filters of size 5x5 and another one with three convolutional layers, but with filters of size 3x3 (and more layers, but which are the same; the number of filters adjusted so that the number of parameters is roughly the same). Is there any influence on how well they recognize cars which are far away (or close)?" CreationDate="2016-08-19T14:36:18.030" UserId="8820" />
  <row Id="14970" PostId="13510" Score="0" Text="Here is [an example of a logistic regression](http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html). The type of plant (discrete) is predicted from the plant sepal width and length (both continuous). If you know linear regressions, logistic regressions will probably be very easy for you to learn." CreationDate="2016-08-19T15:41:52.347" UserId="16853" />
  <row Id="14971" PostId="13549" Score="0" Text="I never did much in clustering, but I believe that [Silhouette score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) is the go-to score for kmeans (more [info here](http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient), and [example here](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html))." CreationDate="2016-08-19T15:47:52.070" UserId="16853" />
  <row Id="14972" PostId="13545" Score="0" Text="&quot;Hadoop&quot; in the large includes Spark. You won't be able to do anything with Spark without it because you need storage, compute. But doesn't mean you need to know core Hadoop details." CreationDate="2016-08-19T16:08:19.563" UserId="21" />
  <row Id="14973" PostId="13557" Score="0" Text="xgboost tries to *maximize* the objective function. It sounds like you need to return negative RMSE if you want to *minimize* it." CreationDate="2016-08-19T16:23:16.703" UserId="16853" />
  <row Id="14974" PostId="13557" Score="0" Text="I think is the opposite: it tries to minimize the objective function, which is the error... http://xgboost.readthedocs.io/en/latest/model.html" CreationDate="2016-08-19T16:27:37.777" UserId="23492" />
  <row Id="14975" PostId="13557" Score="1" Text="It is customizable. But you're right, at least in the python API `maximize=False`." CreationDate="2016-08-19T16:35:14.587" UserId="16853" />
  <row Id="14976" PostId="13523" Score="1" Text="http://stats.stackexchange.com/questions/153938/incremental-gaussian-process-regression" CreationDate="2016-08-19T17:46:55.173" UserId="381" />
  <row Id="14978" PostId="13514" Score="1" Text="I'm not sure your statement for Java is accurate any more(unless you have a source supporting it). I know people like to beat up on Java for performance, but it's not as bad as it used to be. And Hadoop has become very popular in recent years, and it's built on Java. Java has the added benefit that you can turn just about any PC into a node without modifying your code at all, so for smaller companies that don't have racks of blade servers, they can mobilize every office PC in off-hours for data mining." CreationDate="2016-08-19T18:24:10.123" UserId="23584" />
  <row Id="14980" PostId="13514" Score="0" Text="It's not about speed or the JVM -- I'm happy to use Scala or Clojure -- but I've got better things to do than write Java boilerplate. It just is not a good fit for data science; its target audience is different. Machine learning engineering maybe, but I wouldn't want to do any prototyping in Java." CreationDate="2016-08-19T18:44:28.917" UserId="381" />
  <row Id="14981" PostId="13513" Score="4" Text="@ManuH If by &quot;non-standard,&quot; you mean, &quot;not in the standard library,&quot; you're correct. But those tools get pretty wide-spread usage, and they're certainly staples of the language. numpy currently has over 100k questions on SO, pandas has 74k. I think you could certainly make a case that they're *industry standards*. (At least on the software development side. I'd hardly call myself a &quot;data scientist.&quot;)" CreationDate="2016-08-19T22:27:09.693" UserId="23586" />
  <row Id="14982" PostId="13562" Score="0" Text="Thank you. I realized that it does not make sense to have an objective function different to the evaluation metric." CreationDate="2016-08-20T00:09:44.360" UserId="23492" />
  <row Id="14983" PostId="13513" Score="1" Text="As a counter example, all the people in my region want C# and Python or SPSS - with the reason probably being that this was what the first analyst turned data scientist used at the company and it is costly to change gears afterwards. Same reason you still occasionally find Cobol code when working as a developer" CreationDate="2016-08-20T04:53:19.773" UserId="10019" />
  <row Id="14984" PostId="13513" Score="0" Text="&quot;Data Scientist&quot; is not well defined term. Data Scientist is basically somebody who can do useful things with data. They don't have to be using machine learning or statistical packages. Somebody might be using Java/Scala/Spark/whatever to manage large amounts of data and get useful insights without any machine learning." CreationDate="2016-08-20T05:35:49.780" UserId="3070" />
  <row Id="14985" PostId="13567" Score="0" Text="Could you clarify why you don't think that you can normalise those features? Presumably they are numerical the same as other features, so you can take mean/sd? Is your concern about having natural measure of distance between locations? If so, does the data cover a small area (with similar values) or is it global?" CreationDate="2016-08-20T07:13:11.417" UserId="836" />
  <row Id="14986" PostId="13567" Score="0" Text="@NeilSlater It's just that intuitively it does not make sense to me to normalize these features. Will the information not be lost if normalized? I have the dataset covering counties of America." CreationDate="2016-08-20T08:15:44.647" UserId="23600" />
  <row Id="14987" PostId="13567" Score="0" Text="What information do you think will be lost? It probably will not be actually lost, but if you explain in your question what your concern is, someone will be able to answer. Not knowing any more, I would just normalise regardless - for fully global values and some problems (where distance between points is important) I might create a 3d cartesian co-ordinates feature from the long/lat." CreationDate="2016-08-20T08:20:41.110" UserId="836" />
  <row Id="14988" PostId="13498" Score="0" Text="With only 10 parameters I would recommend other machine learning algorithm such as decision trees" CreationDate="2016-08-20T09:51:44.973" UserId="14994" />
  <row Id="14989" PostId="13568" Score="0" Text="Thanks Janpreet. Why is that?" CreationDate="2016-08-20T10:18:38.917" UserId="23492" />
  <row Id="14990" PostId="13568" Score="1" Text="This is due to the nature of logarithmic function, let us suppose the correct prediction was 65 but the model underpredicted it to be 45 so |log(45) - log(65)| = 0.3677 (which is proportional to error) while if the model would over predict it to be 75 then |log(85) - log(65)| = 0.26826, we see that the error comes out to be more in case of under prediction, so, more penalty will be applied on a underprediction.                                  PS: My language in last answer was a bit misleading, I have changed that now." CreationDate="2016-08-20T11:04:22.943" UserId="23380" />
  <row Id="14991" PostId="13421" Score="0" Text="No, that wouldn't be necessary then" CreationDate="2016-08-20T11:46:42.487" UserId="15412" />
  <row Id="14993" PostId="13068" Score="0" Text="Thanks for your answer. I've used both and did not think that they provided CNN algorithms? I just searched the documentation for both for the word convolutional and nothing came up. Do they actually have this functionality?" CreationDate="2016-08-20T12:01:55.060" UserId="2723" />
  <row Id="14994" PostId="13571" Score="0" Text="Yes, also RNN, LSTM. They have many examples in their github" CreationDate="2016-08-20T12:10:33.930" UserId="14994" />
  <row Id="14995" PostId="13567" Score="0" Text="What's your question here? What are you trying to find out from the data? Correlation? Clustering? Classification? Prediction? Interpolation? How is location important to your model?" CreationDate="2016-08-20T12:46:19.147" UserId="471" />
  <row Id="14997" PostId="13564" Score="0" Text="What have you tried? Something as simple as plotting one against the other? `plot(newdata$Weight, newdata$Health)`?" CreationDate="2016-08-20T12:50:57.190" UserId="471" />
  <row Id="14998" PostId="13564" Score="0" Text="@Sumanth Sharma I have somehow rephrased your question. I hope I have kept your initial goals" CreationDate="2016-08-20T12:51:17.483" UserId="12527" />
  <row Id="14999" PostId="13571" Score="0" Text="Ah, yes thanks. I have used mxnet for image classification but didn't think it had CNN algorithms for the same. I see a CNN text classification example. I probably should've specified image classification. Still, perhaps it can be leveraged as such. I will look into it. Thanks! +1" CreationDate="2016-08-20T13:38:13.923" UserId="2723" />
  <row Id="15000" PostId="13168" Score="0" Text="Thank you, this helped a lot!" CreationDate="2016-08-20T14:09:25.020" UserId="22059" />
  <row Id="15002" PostId="13564" Score="0" Text="Also, I'd call your &quot;Health&quot; variable &quot;Sickness&quot; or &quot;BadHealth&quot;, because the larger the value, the *less* healthy the subject is." CreationDate="2016-08-20T16:02:52.287" UserId="471" />
  <row Id="15005" PostId="13522" Score="0" Text="_You_ might have an $x productivity improvement by using R, but it's no help if _they_ have to expend \$2x of effort in changes to their workflow.  Why would they do that, especially if they could hire someone else who might not cost them \$2x?" CreationDate="2016-08-20T16:10:41.000" UserId="23617" />
  <row Id="15007" PostId="13564" Score="0" Text="@Spacedman, i used the above thing which you said but as it is very big data base  i can see the exact picture. How the graph is floating." CreationDate="2016-08-20T17:20:20.260" UserId="21392" />
  <row Id="15008" PostId="13387" Score="0" Text="What type of signal?" CreationDate="2016-08-20T18:18:58.867" UserId="381" />
  <row Id="15009" PostId="13387" Score="0" Text="21 numerical features.. But I am looking for an example which is not related to audio, speech or text.. Just numerical features in (21 columns)  and binary output out..." CreationDate="2016-08-20T18:22:28.847" UserId="20884" />
  <row Id="15011" PostId="13567" Score="0" Text="@Spacedman Please see edit." CreationDate="2016-08-20T18:58:30.223" UserId="23600" />
  <row Id="15012" PostId="13575" Score="0" Text="That is very interesting. Thank you! Could you confirm if these are the formulas for conversion? x = R * cos(lat) * cos(lon), y = R * cos(lat) * sin(lon), z = R *sin(lat)" CreationDate="2016-08-20T19:01:51.217" UserId="23600" />
  <row Id="15013" PostId="13567" Score="0" Text="@NeilSlater Thanks! That does make sense." CreationDate="2016-08-20T19:02:19.710" UserId="23600" />
  <row Id="15014" PostId="13575" Score="0" Text="I don't have access to my code at the moment but it looks right. You don't need the R since you will be standardizing anyway ;)" CreationDate="2016-08-20T19:07:29.107" UserId="14904" />
  <row Id="15015" PostId="13580" Score="2" Text="Just split it twice." CreationDate="2016-08-20T19:32:33.533" UserId="381" />
  <row Id="15016" PostId="13580" Score="0" Text="Wouldn't that be somewhat ugly? I mean there must be a way in scikit-learn for that." CreationDate="2016-08-20T19:33:36.230" UserId="15412" />
  <row Id="15017" PostId="13575" Score="0" Text="Perfect! Thank you." CreationDate="2016-08-20T19:46:50.603" UserId="23600" />
  <row Id="15018" PostId="13576" Score="0" Text="What are your &quot;history of matching patterns&quot; like?" CreationDate="2016-08-20T19:49:51.827" UserId="381" />
  <row Id="15019" PostId="13584" Score="1" Text="Can't you linearize the problem and use a Kalman filter? I skimmed your links and saw no references to control theory." CreationDate="2016-08-20T22:35:21.997" UserId="381" />
  <row Id="15020" PostId="13585" Score="0" Text="Welcome to DataScience.SE! Tell us more about your sensor data, and what you are trying to predict." CreationDate="2016-08-21T00:03:36.527" UserId="381" />
  <row Id="15021" PostId="13585" Score="0" Text="@Emre Thank you :) I have edited the question with additional data." CreationDate="2016-08-21T01:40:48.300" UserId="23632" />
  <row Id="15022" PostId="13534" Score="0" Text="The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). But we have another variable(A) to compare." CreationDate="2016-08-21T04:16:08.637" UserId="15569" />
  <row Id="15023" PostId="13484" Score="0" Text="@gennaro-tedesco I cluster data and expect result(clusters) will be similar to attribute A. how should I measure similarity between attribute A and clusters?" CreationDate="2016-08-21T04:18:39.717" UserId="15569" />
  <row Id="15024" PostId="13584" Score="0" Text="@Emre my understanding of all this is rudimentary at best, but isn't a Kalman filter about using additional inputs to filter noise? I'm trying to set the rudder and sail position based on environmental variables, I don't understand the connection.." CreationDate="2016-08-21T04:44:40.543" UserId="23628" />
  <row Id="15025" PostId="13585" Score="1" Text="Whatever answer you get about specific approaches, you should consider taking a short course on machine learning. ML algorithms are very easy to use in terms of code, but it is tricky for beginners to understand difference between good and bad approaches to use them. Most critically, you need to learn about approaches to cross-validation and testing, so you have toolkit to understand how well your solution is really working." CreationDate="2016-08-21T07:52:36.707" UserId="836" />
  <row Id="15026" PostId="13584" Score="0" Text="See if this thesis helps: [Modeling, control and state-estimation for an autonomous sailboat](http://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-261553)" CreationDate="2016-08-21T08:59:52.443" UserId="381" />
  <row Id="15027" PostId="13587" Score="0" Text="With regards to your first question, backpropagation is to see what weights and inputs influences your loss in what way. In case of max pooling only the max of the neurons influences the output (except for when there is a tie). So only propagate the error to the neuron that had the maximum activation value." CreationDate="2016-08-21T10:53:42.987" UserId="14904" />
  <row Id="15028" PostId="13514" Score="0" Text="The __last__ language that I would try to do data science in would be assembler and even that would come a close heat with VB6 which some people that I know are till trying to use for everything. The again I just ___like___ python." CreationDate="2016-08-21T12:45:00.013" UserId="23647" />
  <row Id="15029" PostId="13587" Score="0" Text="Yes, I understand this and I also said this in a recap at the beginning of my post. But I don't understand how do I &quot;combine&quot; the gradient of the next layer neurons to propagate back. Hope you know what I mean." CreationDate="2016-08-21T13:03:44.353" UserId="23640" />
  <row Id="15030" PostId="13523" Score="0" Text="Thank you! I was hoping for something more like an existing library or something. Unfortunately, I don't think I will have the time to implement these papers." CreationDate="2016-08-21T13:52:09.083" UserId="16812" />
  <row Id="15031" PostId="13564" Score="0" Text="How many points? This would be useful information to edit into your question." CreationDate="2016-08-21T14:31:04.357" UserId="471" />
  <row Id="15032" PostId="13567" Score="0" Text="So maybe you want a regression model with a spatial surface defined by a Gaussian field, or a 2-dimensional parametric spline surface or something like that? Are you just hoping to feed the numbers into an ML algorithm or Random Forests or something?" CreationDate="2016-08-21T14:33:13.153" UserId="471" />
  <row Id="15033" PostId="13588" Score="0" Text="This project has been finished.  Thanks for the update though." CreationDate="2016-08-21T14:42:53.560" UserId="16291" />
  <row Id="15034" PostId="13524" Score="0" Text="That is a great answer. Exactly the kind of insight I was looking for. Thanks!" CreationDate="2016-08-21T15:59:44.000" UserId="23511" />
  <row Id="15035" PostId="13526" Score="0" Text="@GennaroTedesco: &quot;I simply mean that whatever you are doing in R, there is most likely a similar Python package that does the same job&quot;. I actually strongly disagree with this statement. The biggest advantage with R is that 90% of statisticians publish their latest and &quot;greatest&quot; in R, rather than Python. If these methods catch on, they may eventually make their way to Python. But that's also a plus for Python; there are *lots* of R stats packages that are just garbage, while I think Python stats packages are more likely to be the tried and true methods." CreationDate="2016-08-21T17:56:50.247" UserId="13005" />
  <row Id="15036" PostId="13526" Score="0" Text="&quot;R syntax and grammar is extremely complicated. I myself strongly favour R other than anything else but have to however admit that the syntax is not really straightforward and has a very picked learning curve.&quot; Both of these seem to be opinions, but one is dressed as an objective statement and the other opposes it. I'm baffled. I also feel that Python's syntax and idioms are more complicated (OOP emphasis, for one), so I'm doubly confused by this answer." CreationDate="2016-08-21T17:59:18.687" UserId="23658" />
  <row Id="15037" PostId="13513" Score="0" Text="Hey, look at it as an opportunity. Data analytics theory is the same whether you develop your analysis in R or in Python. Just learn both! I would look for a candidate that has a curious mind and won't mind learning a new language (or two, or three)..." CreationDate="2016-08-21T19:34:39.157" UserId="23664" />
  <row Id="15038" PostId="13526" Score="0" Text="@TrevorAlexander I don't understand why the latter statement opposes the former. And yes, they are of course personal opinions." CreationDate="2016-08-21T19:48:37.637" UserId="10372" />
  <row Id="15039" PostId="13589" Score="0" Text="I would advise against bash, for reasons similar to the ones you specified. I personally a python fan, because of it's simplicity and versatility. Creating processes and working with files are also really simple, as these the only requirements you've provided." CreationDate="2016-08-21T19:50:16.520" UserId="13714" />
  <row Id="15040" PostId="13589" Score="0" Text="I did experiment with python a little bit. Calling subprocesses from different packages just is not as clean from python as from bash (with importing different modules and such). Also, I wanted to stay close to the software doing the actual analyses so users familiar with any one of the options wouldn't have to relearn anything in a different language (e.g. they could add their own scripts and call them pretty much as they would within matlab/bash/python). You're right though, it might be worth it for speed and handling of the more complex parts. Maybe I'll give it another shot. Thanks!" CreationDate="2016-08-21T20:16:59.573" UserId="23593" />
  <row Id="15041" PostId="6547" Score="0" Text="Your problem is ill defined. What constitutes an anomaly can have a lot of different meanings. Is it deviation of the mean? Is it certain patterns of behaviour? Different methods apply in each case. You'll need to look into &quot;outlier detection&quot; if the anomaly is deviation from the mean. If you are looking for specific patterns you'd be much better served with a supervised learning algorithm such as neural networks." CreationDate="2016-08-21T20:40:45.750" UserId="23664" />
  <row Id="15042" PostId="13574" Score="0" Text="Do not repost questions: [How to evaluate clusters base on a label?](http://datascience.stackexchange.com/questions/13484/how-to-evaluate-clusters-base-on-a-label)" CreationDate="2016-08-21T20:59:43.710" UserId="924" />
  <row Id="15043" PostId="13484" Score="0" Text="Possible duplicate of [How to evaluate clusters base on an attribute of the dataset?](http://datascience.stackexchange.com/questions/13574/how-to-evaluate-clusters-base-on-an-attribute-of-the-dataset)" CreationDate="2016-08-21T21:00:26.417" UserId="924" />
  <row Id="15044" PostId="13589" Score="0" Text="take a look at `os.system`, that's basically what you get with bash." CreationDate="2016-08-21T21:43:12.577" UserId="13714" />
  <row Id="15045" PostId="13584" Score="0" Text="It's interesting, thanks for sharing. He/she didn't have too much success, and I'm still interested in an answer to my question, though." CreationDate="2016-08-22T03:56:29.147" UserId="23628" />
  <row Id="15046" PostId="13586" Score="0" Text="Try this site on data sets opendata.stackexchange.com" CreationDate="2016-08-22T05:34:20.977" UserId="13727" />
  <row Id="15047" PostId="13374" Score="0" Text="Thanks for providing a deeper insight into possible solutions!&#xA;Can you provide some links to statistical segmentation examples using time series, as I couldn't find any relevant source?&#xA;Moreover, on discussions, I also came across that &quot;Hidden Markov Models&quot; can also be used here, whats your say? Any links?" CreationDate="2016-08-22T08:34:30.197" UserId="23304" />
  <row Id="15048" PostId="13589" Score="0" Text="Perl 5 is also a good choice, designed to be a glue language with bash-like backward compatibility, or resemblances. If you want to control long-running processes started by few selected users via web pages, consider perl/CGI. Old but useful in a data staging environment. Web server configuration might require a lot of learning, though. - It  is less popular, but still possible to construct GUI frontends based on widget libraries (such as Tk) with perl." CreationDate="2016-08-22T08:37:56.740" UserId="14588" />
  <row Id="15049" PostId="13592" Score="0" Text="Note that this is called &quot;one hot encoding&quot;." CreationDate="2016-08-22T09:40:18.527" UserId="21" />
  <row Id="15050" PostId="13592" Score="0" Text="Thank you, lots of documentation once you get the name !" CreationDate="2016-08-22T10:56:54.740" UserId="23679" />
  <row Id="15051" PostId="13589" Score="0" Text="Thanks, I'll look into that!" CreationDate="2016-08-22T11:09:52.670" UserId="23593" />
  <row Id="15052" PostId="13513" Score="1" Text="@jpmc26 Yes that was I meant. Now I realize that even libraries which has not yet reach industry standards could be mentioned (one more argument for python)" CreationDate="2016-08-22T11:30:55.097" UserId="21825" />
  <row Id="15053" PostId="13592" Score="0" Text="Perhaps you can answer your own question?" CreationDate="2016-08-22T11:47:18.907" UserId="15527" />
  <row Id="15054" PostId="13374" Score="0" Text="Actually I provided the link to a paper in the answer I hyeperlinked for you. It's here http://www.lancs.ac.uk/~khaleghi/Publications_files/khaleghi16a.pdf . But 2 points: 1) it's an unsupervised method and you need to modify it to your question as I stated in my answer. 2) The paper is deeply mathematical and if you are not that into theoretical background of time-series analysis then that might be pretty difficult to follow. I would suggest you to have a look at http://web.science.mq.edu.au/~cassidy/comp449/html/ch11s02.html for some insight into DTW plus explanations of HMM usage as u asked" CreationDate="2016-08-22T11:47:40.030" UserId="8878" />
  <row Id="15055" PostId="13374" Score="0" Text="But my favourite tutorial is one published by Springer in which DTW is explained theoretically rigorous but totally understandable. The great point about that tutorial is that it contains subsequence matching which is exactly your problem. I could not copy the link here but if you google &quot;dynamic time warping&quot; one of top 5 results would be a PDF file from Springer. I'd say go for that directly." CreationDate="2016-08-22T11:50:15.167" UserId="8878" />
  <row Id="15056" PostId="13374" Score="0" Text="About HMMs: As you will see in the second link, HMMs are pretty fine for your application but again; it depends on your background. If you are from Machine Learning community go for it otherwise it might be a bit overdeep mathematically. But after all, HMMs are great and do the job like perfect. The best HMM tutorial is by Andrew Moore from Carnegie Mellon. His tutorials are widely read in ML community because they are just great! Here you can find them: https://www.autonlab.org/tutorials/hmm14.pdf" CreationDate="2016-08-22T11:54:04.827" UserId="8878" />
  <row Id="15057" PostId="13596" Score="0" Text="Convolving the images would give you a measure of similarity, and it would be fast.  You could smooth the images first to limit noise effects too, and equalize the images.  The weakness would be the translations though." CreationDate="2016-08-22T14:19:30.467" UserId="19161" />
  <row Id="15058" PostId="13596" Score="0" Text="But what convolution to use? If that is the case isn't using a convolutional network to learn the best kernels to use to distinguish different images better? And then you would use Euclidean distance over the convolutions? Then we get into the (denoising) autoencoder territory correct?" CreationDate="2016-08-22T14:54:55.587" UserId="14904" />
  <row Id="15059" PostId="13596" Score="0" Text="What I'm referring to is actually the cross-correlation.  Here's a reference: http://www.mathworks.com/help/images/examples/registering-an-image-using-normalized-cross-correlation.html" CreationDate="2016-08-22T15:05:18.153" UserId="19161" />
  <row Id="15060" PostId="13602" Score="1" Text="See also [currying](https://mtomassoli.wordpress.com/2012/03/18/currying-in-python/)." CreationDate="2016-08-22T19:42:20.400" UserId="381" />
  <row Id="15061" PostId="9917" Score="0" Text="Here is great article where is mentioned how confidence intervals can be added to prediction models - http://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html" CreationDate="2016-08-23T05:29:07.127" UserId="17290" />
  <row Id="15062" PostId="13607" Score="0" Text="Is the training loss going down?" CreationDate="2016-08-23T07:41:05.017" UserId="14904" />
  <row Id="15064" PostId="12895" Score="0" Text="Its clearly a survival analysis problem because the data is time to failure, with (I guess) censoring when drives have run for some time without failure. ML is just another tool you could use for survival analysis. Your question should be &quot;Machine Learning or Classical Maximum Likelihood or Bayesian methods for Survival Analysis?&quot; Do a lit search and read stuff like: http://www.sciencedirect.com/science/article/pii/S0933365700000531" CreationDate="2016-08-23T11:18:04.740" UserId="471" />
  <row Id="15065" PostId="13608" Score="0" Text="I thought about segment my data first then build model on higher level. but I was just thinking loud and trying to see if there is a way that I am not aware of to actually build 300 models for 300 product,  those 300 product are already referring to the grade level (product category level)." CreationDate="2016-08-23T11:22:22.543" UserId="23699" />
  <row Id="15066" PostId="13583" Score="0" Text="Thank you very much, I did not know about Dask seems a nice framewrok!" CreationDate="2016-08-23T11:45:39.337" UserId="14560" />
  <row Id="15067" PostId="13593" Score="0" Text="Looks like this error: https://github.com/biolab/orange3/issues/1501 You need those Postgre add-ons." CreationDate="2016-08-23T12:37:58.400" UserId="15527" />
  <row Id="15068" PostId="13612" Score="0" Text="Not a proper place? Interesting. Because it [says on the Orange website](http://orange.biolab.si/community/) to ask Orange GUI-related questions on DS.SE. What other discussion forum or mailing list is more appropriate for this kind of questions?" CreationDate="2016-08-23T12:42:32.563" UserId="15527" />
  <row Id="15069" PostId="13608" Score="0" Text="hierarchical time series modeling is what I am looking for, thank you hssay!" CreationDate="2016-08-23T13:21:46.427" UserId="23699" />
  <row Id="15070" PostId="13607" Score="0" Text="No, the training loss also remains constant throughout." CreationDate="2016-08-23T13:21:59.410" UserId="14360" />
  <row Id="15071" PostId="13607" Score="0" Text="You haven't set any validation data or validation_split in your fit call, what would it validate on? Or did you mean test?" CreationDate="2016-08-23T13:24:48.643" UserId="14904" />
  <row Id="15072" PostId="13613" Score="0" Text="But how could you transform the test set to the new feature space? In R, for instance, `prcomp` simply calculates the synthetic features of the given data set, you cannot instruct it to calculate these on the basis of any previous components or data set, can you?" CreationDate="2016-08-23T13:50:02.403" UserId="21560" />
  <row Id="15073" PostId="13620" Score="0" Text="I stem the words to reduct dimensionality. I use gold standard corpus, so my training set is not that large. The current goal is some kind of text classification. With less dimensions I expect to get better results with the same amount of training data so that's why I think I need to correct mispelled words, and remove affixes." CreationDate="2016-08-23T14:52:37.723" UserId="23713" />
  <row Id="15074" PostId="13583" Score="0" Text="An upvote and an &quot;accepted answer&quot; are always a nice way to say &quot;Thank you&quot; ;). No pressure though" CreationDate="2016-08-23T14:53:22.297" UserId="13714" />
  <row Id="15075" PostId="13607" Score="0" Text="That is after experimenting around.  I set validation_split = 0.2 before setting it to None and experimented around with that also." CreationDate="2016-08-23T15:30:11.703" UserId="14360" />
  <row Id="15076" PostId="13607" Score="0" Text="And does it change somewhat around the same area or is it literally constant?" CreationDate="2016-08-23T15:33:02.217" UserId="14904" />
  <row Id="15077" PostId="13583" Score="0" Text="I firmly agree with you, I was the one who upvoted you, the reason that I not accepted your answer is because I leave some room for more recommendations especially for the distributed ML part. Again thank you for your proposal!" CreationDate="2016-08-23T15:40:50.460" UserId="14560" />
  <row Id="15078" PostId="13567" Score="0" Text="@Spacedman I am still working on the feature engineering part but yes I am trying to build a regression model." CreationDate="2016-08-23T15:44:22.083" UserId="23600" />
  <row Id="15079" PostId="13620" Score="0" Text="The 'tm' package allows the creation of a corpus with ability to stem words in the corpus.  This will definitely reduce the dimensions in any document-term-matrix or term-document-matrix you might want to create.  However, it relies on an algorithm to stem, as opposed to a dictionary.  See the tm::stemDocument function for more info.  I believe RTextTools does similarly, but without direct corpus creation." CreationDate="2016-08-23T16:08:30.093" UserId="23420" />
  <row Id="15080" PostId="13607" Score="0" Text="Not literally constant, but varies so little that for all purposes it can be thought of as a constant. 56.4% to 57.10%." CreationDate="2016-08-23T16:16:48.810" UserId="14360" />
  <row Id="15081" PostId="13607" Score="1" Text="Can you fit one batch for a lot of times to see if you can get the training loss to be lower?" CreationDate="2016-08-23T16:18:49.023" UserId="14904" />
  <row Id="15082" PostId="13607" Score="0" Text="Thanks for the tip. However, I have trouble understanding your advice. Do you mean a single batch of data multiple times?" CreationDate="2016-08-23T16:27:14.263" UserId="14360" />
  <row Id="15083" PostId="13618" Score="0" Text="The most popular text mining package in python is called [`ntlk`](http://www.nltk.org/). It has a [stemming module](http://www.nltk.org/api/nltk.stem.html). Is it not good enough for what you want?" CreationDate="2016-08-23T16:44:39.720" UserId="16853" />
  <row Id="15084" PostId="13611" Score="0" Text="What are a few of the vertices connected to &quot;India&quot; in this example? You haven't told us what this graph represents." CreationDate="2016-08-23T17:44:11.817" UserId="381" />
  <row Id="15085" PostId="13607" Score="0" Text="Yeah, intentionally overfitting on that batch to see if your training loss will go down, good tool in diagnosis of what is happening" CreationDate="2016-08-23T18:42:49.437" UserId="14904" />
  <row Id="15086" PostId="13613" Score="1" Text="you can use the `predict(prcomp object, newdata)`.  See `?predict.prcomp`" CreationDate="2016-08-23T19:24:36.070" UserId="14913" />
  <row Id="15087" PostId="13627" Score="0" Text="Hi Philip C. thanks for your response. I will need to make this link analysis using Spark (is one of the requirements). I'm trying to predict which products are associated in this dataset like the &quot;Diapers and Beer&quot; research. &#xA;As I am young in this topic I'm not sure if the dataset have an appropriate schema for this prediction." CreationDate="2016-08-23T19:49:49.070" UserId="23714" />
  <row Id="15088" PostId="13627" Score="0" Text="There are other Graph implementations for Spark, however, If I understand right, you want to make use of Association algorithms like Apriori. Is this what you need?" CreationDate="2016-08-23T19:56:17.307" UserId="14560" />
  <row Id="15089" PostId="13627" Score="0" Text="exactly :) Apriori is good example of the graph that I want to return :)" CreationDate="2016-08-23T20:24:52.033" UserId="23714" />
  <row Id="15090" PostId="13618" Score="0" Text="The best solution would be a configurable, extendable, dictionary-based stemming algorithm that produces meaningful words as stems, while keeping the part of speech. Hunspell is such, for many languages, except for English, since I couldn't find the right dictionary. My second choice would be the Porter stemmer from nltk though, due to it's simplicity." CreationDate="2016-08-23T20:38:54.127" UserId="23713" />
  <row Id="15091" PostId="13589" Score="0" Text="Thanks for the suggestions everyone! I decided to stick with bash and write python or perl helper scripts when things get too complicated" CreationDate="2016-08-23T21:23:09.390" UserId="23593" />
  <row Id="15092" PostId="13600" Score="0" Text="They use this in a supervised manner, in my case there are no labels (although that might be a possibility for some of the photos). Do you think a similar approach could work with a convolutional autoencoder with the sigmoid in the middle, including the added binary 'enforcing' term in the loss function?" CreationDate="2016-08-23T22:57:17.283" UserId="14904" />
  <row Id="15093" PostId="13600" Score="0" Text="Here's the unsupervised version: [Learning Compact Binary Descriptors with Unsupervised Deep Neural Networks](http://www.iis.sinica.edu.tw/~kevinlin311.tw/cvpr16-deepbit.pdf)" CreationDate="2016-08-23T23:16:04.857" UserId="381" />
  <row Id="15094" PostId="13589" Score="0" Text="Yes, lots of correctly-written Perl scripts means job safety like no other." CreationDate="2016-08-23T23:28:23.067" UserId="15527" />
  <row Id="15095" PostId="13607" Score="0" Text="Can you help me with that? How to use only a single batch?" CreationDate="2016-08-24T00:41:12.347" UserId="14360" />
  <row Id="15097" PostId="13607" Score="0" Text="If I understood it correctly, fit the model to a single batch would be done using a loop (for/while)?" CreationDate="2016-08-24T03:49:16.400" UserId="14360" />
  <row Id="15098" PostId="13611" Score="0" Text="@ Emre    here the link http://news-explorer.mybluemix.net/?query=india&amp;type=unconstrained" CreationDate="2016-08-24T04:05:47.890" UserId="20585" />
  <row Id="15099" PostId="13625" Score="0" Text="There's no way to receive the document images instead, is there? Do your OCR results have position information?" CreationDate="2016-08-24T05:14:34.263" UserId="381" />
  <row Id="15100" PostId="13612" Score="0" Text="I guess it depends what you mean by 'GUI-related questions'. I interpret it as workflow construction. Do you interpret it as bugs and issues?" CreationDate="2016-08-24T07:42:56.437" UserId="23708" />
  <row Id="15101" PostId="13633" Score="0" Text="It seems highly unlikely that it would come down to the exact same performances for all 5 feature sets, I would think there was an implementation error" CreationDate="2016-08-24T07:46:20.077" UserId="14904" />
  <row Id="15102" PostId="13633" Score="0" Text="@JanvanderVegt How is there an error that is what I am unable to understand. And if there is any how do I find it out. I am using the toolbox, it is not my implementation ...." CreationDate="2016-08-24T07:49:15.710" UserId="8013" />
  <row Id="15103" PostId="13633" Score="0" Text="It looks like it doesn't change the features used whatsoever, since there should be some perturbations in your performance due to the noise in your feature space and potential over fitting" CreationDate="2016-08-24T07:50:52.427" UserId="14904" />
  <row Id="15104" PostId="13633" Score="0" Text="@JanvanderVegt I have fixed the seed to obtain each of the results btw, Like I have used rng(1), rng(10), rng(158), rng(250) and averaged the results to obtain one single pair of train-test accuracy. Is fixing the seed leading to this?" CreationDate="2016-08-24T07:53:33.563" UserId="8013" />
  <row Id="15105" PostId="13633" Score="0" Text="I don't think the seeds are the problem, it looks like all the runs use feature size 40 (or 8 or whatever), because for different feature sizes you should get different accuracy (albeit small maybe)" CreationDate="2016-08-24T08:00:11.783" UserId="14904" />
  <row Id="15106" PostId="13633" Score="0" Text="@JanvanderVegt I will do it again, to see if anything changes or not. IF I do not get any change, how do I know where the error is?" CreationDate="2016-08-24T08:01:51.140" UserId="8013" />
  <row Id="15107" PostId="13633" Score="0" Text="Do some diagnostics, print the feature dimensions in between the runs to make sure it uses only the number of features you intended. Else I don't know, but it seems off to me" CreationDate="2016-08-24T08:03:50.073" UserId="14904" />
  <row Id="15108" PostId="13633" Score="0" Text="@JanvanderVegt I did these all, as I myself was surprised to see this, as I was expecting some variations too. But still I will check. The number of feature that I had added were done very carefully." CreationDate="2016-08-24T08:04:56.467" UserId="8013" />
  <row Id="15109" PostId="13584" Score="1" Text="@kolosy You might want to have a look at [my explanation of the Kalman filter](https://martin-thoma.com/kalman-filter/)." CreationDate="2016-08-24T09:16:18.847" UserId="8820" />
  <row Id="15110" PostId="13581" Score="3" Text="What data do you have? Which features do you have?" CreationDate="2016-08-24T09:19:02.097" UserId="8820" />
  <row Id="15112" PostId="13620" Score="0" Text="Sorry, just saw your edit that you are working in Python.  Those two packages are for R." CreationDate="2016-08-24T12:43:07.507" UserId="23420" />
  <row Id="15113" PostId="13612" Score="0" Text="Is what Dilip Bobby is asking about above a bug? Where do all other questions belong or, IOW, how does one properly reach the community?" CreationDate="2016-08-24T12:44:21.307" UserId="15527" />
  <row Id="15118" PostId="13640" Score="0" Text="What do you mean by large dataset? Could you specify the dimensionality of dataset? Also, does it take longer time for prediction or generating a model?" CreationDate="2016-08-24T16:33:57.113" UserId="20722" />
  <row Id="15121" PostId="13622" Score="0" Text="Why do I need mapping? I'm perfectly happy assuming an endless ocean around me and just GPS waypoints to head toward." CreationDate="2016-08-24T20:30:12.497" UserId="23628" />
  <row Id="15124" PostId="13477" Score="0" Text="Thanks for pointing out the duplication! :)" CreationDate="2016-08-25T02:12:09.740" UserId="1097" />
  <row Id="15125" PostId="13646" Score="0" Text="As this is likely to require discussion of representation of action choices (so that an action can be selected by the policy), may be work explaining how you are currently representing the available actions in a backgammon move." CreationDate="2016-08-25T06:38:52.427" UserId="836" />
  <row Id="15126" PostId="13640" Score="0" Text="Dimensionality is low. I have four attributes. Two nominal and two numeric." CreationDate="2016-08-25T07:08:15.827" UserId="23735" />
  <row Id="15127" PostId="13652" Score="0" Text="so are row_id and distance the x, y values respectively?" CreationDate="2016-08-25T07:42:06.287" UserId="15961" />
  <row Id="15128" PostId="13653" Score="0" Text="Following resource provides broad information about data science environment, take a look into it for further tools that may help you.&#xA;&#xA;http://www.kdnuggets.com/2016/06/r-python-top-analytics-data-mining-data-science-software.html" CreationDate="2016-08-25T07:59:36.213" UserId="23567" />
  <row Id="15129" PostId="12980" Score="0" Text="I think OpenCV uses BGR" CreationDate="2016-08-25T08:06:56.607" UserId="14360" />
  <row Id="15130" PostId="13646" Score="0" Text="Good point, I will add it to my question" CreationDate="2016-08-25T08:24:34.737" UserId="14904" />
  <row Id="15131" PostId="13646" Score="0" Text="Am I right in understanding from your description that your action representation is &quot;desired state&quot; and that all possible game states are selectable (99%+ of which are not legal moves - e.g. moving pieces backwards, shuffling them to completely different positions etc, all ignoring the actual dice rolled)" CreationDate="2016-08-25T08:45:51.897" UserId="836" />
  <row Id="15132" PostId="13646" Score="0" Text="No, what I'm currently doing is that the Game object passes all possible legal states to the Player object and that the player object evaluates the values of these possible states, and turns them into a probability distribution to sample from in the case of generating training data and picks the highest one in case of playing for real. But the policy network of AlphaGo learns a probability distribution over all moves, which is (seemingly) impossible to do for Backgammon" CreationDate="2016-08-25T08:55:28.397" UserId="14904" />
  <row Id="15133" PostId="13646" Score="0" Text="OK. I think this is a representation issue (and may affect more than just the structure you are using for actions), but don't have the answer." CreationDate="2016-08-25T09:16:16.337" UserId="836" />
  <row Id="15134" PostId="13646" Score="0" Text="I agree and I'm certainly not married to my current representation, I'm looking for inspiration of how to tackle this (I'm not even convinced that it can be tackled)" CreationDate="2016-08-25T09:20:25.593" UserId="14904" />
  <row Id="15135" PostId="13622" Score="0" Text="If your ship does not care about obstacle (other ships), then what you need is a PID, which will solve your problem so far. https://en.wikipedia.org/wiki/PID_controller. I use to program them when I was working with helicopter simulators. If you want the source code I could provide it to you." CreationDate="2016-08-25T09:34:15.780" UserId="23721" />
  <row Id="15136" PostId="13622" Score="0" Text="What you want to do is an automatic pilot. You give to the system a set of waypoints, and then, your ship will automatically follow straight lines between points, changing its heading when get one." CreationDate="2016-08-25T09:48:02.153" UserId="23721" />
  <row Id="15137" PostId="13622" Score="1" Text="Maybe you would like I do some source code implementation into your [ArduSailor](https://github.com/kolosy/ArduSailor) github about these PID's." CreationDate="2016-08-25T10:37:27.617" UserId="23721" />
  <row Id="15138" PostId="13658" Score="0" Text="XML and HDF5 might be candidates." CreationDate="2016-08-25T12:19:08.057" UserId="16213" />
  <row Id="15139" PostId="13620" Score="0" Text="It's not you. I added that after I've seen your answer. Anyway, thanks for sharing your ideas." CreationDate="2016-08-25T12:53:31.110" UserId="23713" />
  <row Id="15140" PostId="13654" Score="2" Text="The problem is very loosely constrained. For instance in your example, he could be wearing a different T-Shirt (or a variety of other clothing), and lighting could be from any angle, and it would still be valid result. So I would expect output from a predictor to be either vague or highly variable or both. I'm not aware of any attempts to solve this problem, so cannot answer properly. I'd guess some extension of a generative model like a GAN or VAE where you could add the silhouette as a constraint might work." CreationDate="2016-08-25T13:07:08.297" UserId="836" />
  <row Id="15141" PostId="13625" Score="0" Text="I do have the images and do the OCR.  I can get positional (x,y coordinates) for words.  How would I use that ?  &lt;br/&gt; One of the problems that I have faced with letters is that due to headers content shifts to page 2 and which is then incorrectly classified as the page of interest." CreationDate="2016-08-25T13:12:48.513" UserId="23155" />
  <row Id="15142" PostId="13660" Score="0" Text="Maybe the dataset you contain is vary varied and the frequencies of the words don't matter to come to the conclusion?" CreationDate="2016-08-25T13:32:42.787" UserId="21024" />
  <row Id="15143" PostId="13660" Score="0" Text="But if that was the case, shouldn't it give a performance on par with the non-use of it? If the frequencies don't account for the conclusion, shouldn't TF-IDF do zilch to the accuracy (as we assume it doesn't really matter either way)?" CreationDate="2016-08-25T13:43:34.193" UserId="22059" />
  <row Id="15144" PostId="13664" Score="4" Text="More importantly, accuracy is a not a differentiable function so you cannot backpropagate through it." CreationDate="2016-08-25T13:45:04.017" UserId="14904" />
  <row Id="15145" PostId="13664" Score="0" Text="@JanvanderVegt Yes, that's a great point" CreationDate="2016-08-25T13:45:47.477" UserId="12515" />
  <row Id="15146" PostId="13664" Score="0" Text="I learnt that in Keras I can put a &quot;custom&quot; evaluation metrics (by custom in this case I mean that no built-in implementation in Keras, like AUC or F1-Score) in the compilation function. I presume that in this case this &quot;custom&quot; metrics will be used/displayed instead of Accuracy everywhere where `show_accuracy` parameter is set to True (like at fitting or in evaluation). Is that correct?" CreationDate="2016-08-25T14:10:29.747" UserId="21560" />
  <row Id="15147" PostId="13664" Score="1" Text="@Hendrik yes you can, just create a `def your_own_metric(y_true, y_pred)` function and pass it to `model.compile(..., metrics=[your_own_metric])`" CreationDate="2016-08-25T15:31:04.990" UserId="9465" />
  <row Id="15148" PostId="13652" Score="0" Text="You'd probably want the actual features as X &amp; Y. For example, let's say your data set was about countries and contained population, GDP, land area, etc as features. You could use population and GDP as your X and Y. The user would be able to see the distances for those dimensions. [This section of the Seaborn docs](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html) has a nice example. Search for &quot;Show different levels of a categorical variable by the color of plot elements&quot; within that page for the relevant image." CreationDate="2016-08-25T16:34:44.343" UserId="23761" />
  <row Id="15149" PostId="13644" Score="2" Text="How does this answer the specifics in the question, namely the fact that there's only 1 in 100,000 &quot;NO&quot; responses? Its just some generic ML statements, a link to a tutorial, and a quote from a statistician." CreationDate="2016-08-25T16:54:24.617" UserId="471" />
  <row Id="15150" PostId="13641" Score="0" Text="What's an &quot;anomaly&quot; here? A &quot;No&quot; where you'd expect a &quot;Yes&quot; and vice-versa?" CreationDate="2016-08-25T16:54:46.923" UserId="471" />
  <row Id="15151" PostId="13660" Score="0" Text="So when you say you don't use tf-idf, what are your features then? Just a simple BOG?" CreationDate="2016-08-25T17:03:50.420" UserId="21024" />
  <row Id="15152" PostId="13660" Score="0" Text="My features are unigrams and bigrams formed from the review texts (including stopwords) without any feature selection other than excluding features occurring only once. I'm not sure what &quot;BOG&quot; stands for." CreationDate="2016-08-25T18:09:01.437" UserId="22059" />
  <row Id="15153" PostId="13644" Score="0" Text="@Spacedman Firstly, the link isn't to a tutorial. It's a list of ML algorithms that are available in R packages. Anomaly detection can be done using many ML algorithms based on data. You have to chose one which suits you! Hence the link. Secondly, about quote, question asks for the best algorithm and quote is the first thing that comes in mind." CreationDate="2016-08-25T18:56:33.260" UserId="20722" />
  <row Id="15154" PostId="13673" Score="0" Text="Did you see the [documented examples](https://spark.apache.org/docs/1.6.2/mllib-linear-methods.html#logistic-regression) or [this](http://stackoverflow.com/questions/30925819/from-dataframe-to-rddlabeledpoint) question? Try something like `trainingData.map(row =&gt; LabeledPoint(row.label, row.features))`" CreationDate="2016-08-25T19:58:50.683" UserId="381" />
  <row Id="15155" PostId="13669" Score="0" Text="I am not sure whether it would make sense as in jupyter notebook, kernel  maintains the state of previously run commands which might not be the case with ipython %run command." CreationDate="2016-08-25T20:20:55.777" UserId="20722" />
  <row Id="15156" PostId="13640" Score="0" Text="How about number of rows? (Asking again!) does it take longer time for prediction or generating a model?" CreationDate="2016-08-25T20:24:15.800" UserId="20722" />
  <row Id="15157" PostId="13673" Score="0" Text="@Emre, Thanks for your suggestion, Yes I have gone above [documentation](https://spark.apache.org/docs/1.6.2/mllib-linear-methods.html#logistic-regression)  and also found relevant [question](http://stackoverflow.com/questions/33551747/logistic-regression-mllib-pyspark-issue-with-multiple-labels), but their feature vector is not Spark Vector, In my case feature is `Spark Vector`, I think this is the issue. `trainingData.map(row =&gt; LabeledPoint(row.label, row.features))` this is **Scala** sentence and corresponding in PySpark I already tried in **case 3**." CreationDate="2016-08-26T01:45:15.817" UserId="17116" />
  <row Id="15159" PostId="13669" Score="0" Text="For a single cell you could simply copy and paste or use IPython magics: 1) %%writefile, see http://stackoverflow.com/questions/21034373/how-to-load-edit-run-save-text-files-py-into-an-ipython-notebook-cell or 2) %save, see http://stackoverflow.com/questions/947810/how-to-save-a-python-interactive-session command." CreationDate="2016-08-26T07:08:03.613" UserId="6550" />
  <row Id="15160" PostId="13681" Score="0" Text="What &quot;details&quot; are you after? The name and address? The telephone number? The field of study? The name of the faculty dean's pet cat? How many university URLs are you starting from? Do you care that some Unis call them &quot;schools&quot; instead of departments? Seriously, this is a massively broad question and you should focus more. Have you looked at some Uni web sites to get an idea of how they might do their faculty detail pages? The faculty might have its own server. Do some research yourself before dumping your problem on us." CreationDate="2016-08-26T07:19:20.857" UserId="471" />
  <row Id="15162" PostId="13658" Score="1" Text="Can't you use R's database drivers and read what you want straight from it? What kind of server are you talking to? MySQL? Postgres? MS-SQL? There's loads of info out there on getting R to talk to them. https://support.rstudio.com/hc/en-us/articles/214510788-Setting-up-R-to-connect-to-SQL-Server-" CreationDate="2016-08-26T07:27:13.053" UserId="471" />
  <row Id="15163" PostId="13666" Score="0" Text="I don't see how this helps, unless there's an R interface to Spark/Parquet/Avro that doesn't involve dumping the data to CSV first? Let's see.. oh yes.. http://stackoverflow.com/questions/30402253/how-do-i-read-a-parquet-in-r-and-convert-it-to-an-r-dataframe but then there's interfaces for most SQL db so why complicate life with a new data base system?" CreationDate="2016-08-26T07:30:39.143" UserId="471" />
  <row Id="15164" PostId="13669" Score="1" Text="@RohanSadale what I want is to work both in my jupyter notebook and in my ipython shell. my notebook is made to be shared and when I want to make the command previously run on my notebook but with a bigger dataset and test some stuff on it, I made it on my ipython shell." CreationDate="2016-08-26T08:10:59.530" UserId="21825" />
  <row Id="15165" PostId="13673" Score="0" Text="You have at least two problems: trying to use logistic regression for a multi-class problem, and mixing spark.mllib classes with the spark.ml API." CreationDate="2016-08-26T08:11:42.870" UserId="21" />
  <row Id="15166" PostId="13669" Score="0" Text="@Valentas your comment should be rewritten as an answer." CreationDate="2016-08-26T08:11:54.490" UserId="21825" />
  <row Id="15167" PostId="13690" Score="1" Text="Out of interest, why are you *classifying* when a rating of 49 would be better than a rating of 85 when the ground truth is 50? Most loss functions for classifiers ignore near misses." CreationDate="2016-08-26T09:35:17.697" UserId="836" />
  <row Id="15168" PostId="13690" Score="0" Text="Hmm my goal of classification is to attempt to let the model predict a class (or a score) that is as close to the ground truth class/score as possible. I don't mean to predict the images to have a better score, but rather a score that is close to its aesthetic quality. So a 49 score, even when not classified as 50, is indeed better than a rating of 85." CreationDate="2016-08-26T09:39:12.153" UserId="13527" />
  <row Id="15169" PostId="13690" Score="1" Text="You should use a regression model then" CreationDate="2016-08-26T09:41:33.390" UserId="836" />
  <row Id="15170" PostId="13690" Score="0" Text="Yes my intention was to use regression, but the framework I use (caffe) has a little difficulty doing regression (that is only to my knowledge, though). Either way, I'm hoping to explore both regression and classification, just that i'm unsure how my model will be skewed by the data." CreationDate="2016-08-26T09:50:20.473" UserId="13527" />
  <row Id="15171" PostId="13660" Score="0" Text="When applying TF-IDF, does your output vocabulary has the same exact dimensions as your input vocabulary?" CreationDate="2016-08-26T09:52:32.043" UserId="16853" />
  <row Id="15172" PostId="13690" Score="0" Text="Example using caffe for regression on images: https://github.com/qiexing/caffe-regression" CreationDate="2016-08-26T10:13:35.587" UserId="836" />
  <row Id="15173" PostId="13641" Score="1" Text="anomaly here is occurrence of &quot;No&quot; when you would expect &quot;Yes&quot; to happen" CreationDate="2016-08-26T10:22:01.317" UserId="13517" />
  <row Id="15174" PostId="13691" Score="4" Text="You (or someone very like you) have already asked this question and had some feedback. Please respond to that and not just repost the question: http://datascience.stackexchange.com/questions/13681/which-machine-learning-algorithm-to-implement-for-web-crawler" CreationDate="2016-08-26T11:27:54.140" UserId="471" />
  <row Id="15175" PostId="13658" Score="0" Text="The problem is that I cannot use R to connect to the database. It is protected such that it can only be accessed within the system and R is not on the system. So I need to transfer it to my laptop." CreationDate="2016-08-26T11:57:36.937" UserId="20372" />
  <row Id="15176" PostId="13658" Score="0" Text="Btw, it's a pilot project. I get that this is not a definite solution. But the question does not need to be about a database." CreationDate="2016-08-26T11:58:25.993" UserId="20372" />
  <row Id="15177" PostId="13666" Score="0" Text="@Spacedman Yes, that's why I said, &quot;I would try using a relational database until your data exceeds 50 GB&quot;. OP said &quot;I am exporting data from an SQL database and importing it into R. ... [and] I am using csv files to save the data&quot; so I figured that he had already considered making a new table on the existing RDBMS and had to save the data elsewhere for some unspecified reason." CreationDate="2016-08-26T13:25:54.170" UserId="12515" />
  <row Id="15178" PostId="13666" Score="0" Text="So you're proposing an SQL dump of the data from the secure DB (a file of basically CREATE TABLE and INSERT INTO statements), transfer that to workstation, import into similar DB system running locally? That would work, and preserve types via the DB schema." CreationDate="2016-08-26T13:44:03.520" UserId="471" />
  <row Id="15179" PostId="13658" Score="0" Text="What *can* you do on the database server or with a database connection? Can you run Python or any other language on the database server just to connect and create a more specialised file which you can transfer to your workstation? Can you dump the data as SQL? Clearly the remote DB is the sticking point in terms of functionality, so we really need to know what capabilities it has. If all you can do is write CSVs, then that's pretty much it, unless you can write the schema metadata as a CSV and parse that..." CreationDate="2016-08-26T13:45:45.730" UserId="471" />
  <row Id="15180" PostId="13694" Score="0" Text="I'm having troubles understanding the purchase_value column as it is inconsistent considering the product_id. Also if we forget a bit about big data and spark, your problem as you define it can be associated to any of the following : recommendation system, similarity measures, frequent pattern matching. Thus making your question quite broad to answer. Would you care reviewing your question for the matter so we can try to help more efficiently ?" CreationDate="2016-08-26T15:41:34.733" UserId="5177" />
  <row Id="15181" PostId="13694" Score="0" Text="hi eliasah, many thanks for your help :) The purchase value just shows the value of the product in that specific purchase it can be manipulate by quantity or promotions. &#xA;&#xA;What I'm trying to get is a algorithm that can anlyze the list of items by each purchase and can extract the products that are purchased together." CreationDate="2016-08-26T15:47:06.053" UserId="23714" />
  <row Id="15182" PostId="13666" Score="0" Text="@Spacedman Yes, that's right" CreationDate="2016-08-26T15:56:37.850" UserId="12515" />
  <row Id="15183" PostId="13684" Score="0" Text="I see. 2nd answer on that link made sense. so any function whose value range +/- Inf can be fitted via GLM in general? is that an assumption here?" CreationDate="2016-08-26T15:59:01.507" UserId="19147" />
  <row Id="15184" PostId="10645" Score="0" Text="I would take a bit of issue with &quot;Whatever combination or transformation that increases your Cross-Validation or Test Set performance then you should use it.&quot;. Blindly trying stuff until something improves your performance metric could result in discovering a relationship that doesn't make sense and causes overfitting. This could really hurt performance on new observations down the road." CreationDate="2016-08-26T16:50:18.450" UserId="14685" />
  <row Id="15185" PostId="13622" Score="0" Text="that'd be phenomenal. Feel free to fork and do as you wish. Just want to warn you, though that because locomotion isn't a direct function of steering here, I think the problem is more complex.. but I could be wrong." CreationDate="2016-08-26T17:22:27.227" UserId="23628" />
  <row Id="15186" PostId="13677" Score="0" Text="_Missing data_ is a common problem in recommender systems; you will find many leads if you search for this term. See if the _matrix completion_ approach applies to your problem. Welcome to DataScience.SE and good luck!" CreationDate="2016-08-26T17:24:55.790" UserId="381" />
  <row Id="15187" PostId="13677" Score="0" Text="Thanks @Emre. The issue is not exactly missing data really. The fact that the data is missing is in fact informative for the problem at hand. In this case it means that some of the users have not given any ratings, as opposed to 'not knowing' what their ratings are. In other words, the fact that I have no ratings for some of the users carries some value in itself, it's just that I'm not sure how to represent this in my dataset. Any ideas? Thanks for the help" CreationDate="2016-08-26T17:38:06.487" UserId="17783" />
  <row Id="15188" PostId="13677" Score="0" Text="You can introduce an additional, boolean variable to indicate that an item has been used, and set the rating to None if the item has not been rated, therefore your table would look like (uid, rating, consumed). You see this scheme with _implicit_ recommender systems." CreationDate="2016-08-26T17:55:48.200" UserId="381" />
  <row Id="15189" PostId="13677" Score="0" Text="Exactly, the only issue then being that I cannot really have numerical continuous values for some data points and 'None' for some other data points. I like the extra Boolean feature, but the problem of the actual 'rating' feature value remains. Thanks for the help!" CreationDate="2016-08-26T18:06:12.727" UserId="17783" />
  <row Id="15190" PostId="13677" Score="0" Text="Why not? That's your missing data." CreationDate="2016-08-26T18:12:00.723" UserId="381" />
  <row Id="15191" PostId="13677" Score="0" Text="I think if I feed this dataset to SVM, for instance, it will blow up in my face :) Especially if I want the 'rating' feature to be a continuous and not categorical feature(?)" CreationDate="2016-08-26T18:14:29.067" UserId="17783" />
  <row Id="15192" PostId="13677" Score="0" Text="So use a more appropriate algorithm. Search for &quot;recommender systems&quot;, &quot;missing data&quot;, and &quot;implicit feedback&quot;. I believe you can still use the matrix factorization approach if you introduce an additional matrix to represent the binary variables. Time to roll up your sleeves and write your own code!" CreationDate="2016-08-26T18:21:25.567" UserId="381" />
  <row Id="15193" PostId="13677" Score="0" Text="Thanks for the help Emre" CreationDate="2016-08-26T18:29:03.270" UserId="17783" />
  <row Id="15194" PostId="13677" Score="0" Text="You can encode the rating as a categorical variable, but I would suggest keeping it continuous and centering or standardizing it because it's  really _ordinal_, not  categorical." CreationDate="2016-08-26T18:30:38.950" UserId="381" />
  <row Id="15195" PostId="13690" Score="0" Text="Thank you for your link. I have finished the classification output, but as I only used a small subset of images (around 1k) for development, the model doesn't generalize well as expected. I'm trying the regression model now using num_output=1 and a EuclideanLoss, hopefully it would perform better." CreationDate="2016-08-27T03:05:07.483" UserId="13527" />
  <row Id="15197" PostId="13673" Score="0" Text="@SeanOwen Yes, I am trying logistic regression for a multi-class problem, but not mixing the `spark.mllib` classes with `spark.ml` API, I have written these  cases separately. For full details of code and error output, please check my [github repo](https://github.com/krishnaiitd/LogisticRegression), Also created README for understanding, could you please check and do let me know where I am missing something." CreationDate="2016-08-27T10:56:12.597" UserId="17116" />
  <row Id="15198" PostId="8587" Score="0" Text="This qustion should probably be moved to http://hsm.stackexchange.com" CreationDate="2016-08-27T11:33:31.167" UserId="16213" />
  <row Id="15199" PostId="8587" Score="0" Text="@aventurin I was actually saying this to myself. I posted this way before I discovered [history of science and mathematics](http://hsm.stackexchange.com/). Should I flag it myself?" CreationDate="2016-08-27T11:41:28.947" UserId="12527" />
  <row Id="15200" PostId="8587" Score="1" Text="@Laurent Duval I do not know how to move it over to HSM. Feel free to do it. Your question sounds very interesting to me. Hope you'll get an answer there." CreationDate="2016-08-27T13:33:25.247" UserId="16213" />
  <row Id="15201" PostId="13645" Score="1" Text="k-means will not work on your data. It's no gold for binary columns. The result you have is typical. A few outliers in 1-element clusters, everything else in one big blob." CreationDate="2016-08-27T15:50:34.830" UserId="924" />
  <row Id="15203" PostId="13531" Score="0" Text="I just now read some sample pages of the book and it appears that the first book you referred is very hands on. It has implementation of gradient descent in the chapters itself. I will look forward for the second book. Thank you very much!!" CreationDate="2016-08-28T04:26:53.003" UserId="13100" />
  <row Id="15204" PostId="13607" Score="0" Text="@JanvanderVegt I was able to do what you asked me to do. I fitted one batch of data for 15 epochs. There is an increase in the accuracy which turns out to be 85.80%. However, the funny thing is it still remains constant throughout the epochs." CreationDate="2016-08-28T05:18:25.047" UserId="14360" />
  <row Id="15206" PostId="13713" Score="2" Text="myLearningAlgorithm(int x) { return x + 1; }" CreationDate="2016-08-28T11:17:37.420" UserId="9123" />
  <row Id="15207" PostId="13716" Score="0" Text="Could you tell more about the problem and the environment you are working in? A casual reader might think that the problem can be fixed by copy-pasting." CreationDate="2016-08-28T13:48:36.480" UserId="20372" />
  <row Id="15208" PostId="13713" Score="1" Text="Do you want to use text input (strings) or numeric input?" CreationDate="2016-08-28T18:49:36.087" UserId="20372" />
  <row Id="15209" PostId="13715" Score="0" Text="I have come to be aware that the words for what I mean are &quot;Natural Language Understanding&quot; and &quot;Natural Language Generation.&quot;  I'm wondering whether there are good examples of using NL**G** on the output." CreationDate="2016-08-28T23:39:46.043" UserId="23855" />
  <row Id="15210" PostId="9529" Score="0" Text="Cross-posted on Stats.SE, Stack Overflow, SO.RU, and DataScience.SE: http://stats.stackexchange.com/q/188291/2921, http://stackoverflow.com/q/34474767/781723, http://ru.stackoverflow.com/q/486133, http://datascience.stackexchange.com/q/9529/8560.  Please [do not post the same question on multiple sites](http://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted." CreationDate="2016-08-29T02:36:59.103" UserId="8560" />
  <row Id="15211" PostId="9301" Score="0" Text="Cross-posted on Stats.SE, SO, and DataScience.SE: http://stats.stackexchange.com/q/186027/2921, http://stackoverflow.com/q/34193685/781723, http://datascience.stackexchange.com/q/9301/8560.  Please [do not post the same question on multiple sites](http://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted." CreationDate="2016-08-29T02:40:53.090" UserId="8560" />
  <row Id="15212" PostId="9202" Score="0" Text="Cross-posted on DataScience.SE and SO: http://datascience.stackexchange.com/q/9202/8560, http://stackoverflow.com/q/34087329/781723.  Please [do not post the same question on multiple sites](http://meta.stackexchange.com/q/64068). Each community should have an honest shot at answering without anybody's time being wasted." CreationDate="2016-08-29T02:45:24.043" UserId="8560" />
  <row Id="15213" PostId="13722" Score="0" Text="In your example you can treat them the same by using the [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)). I suppose you can't use a pre-trained embedding to avoid this problem?" CreationDate="2016-08-29T06:59:19.550" UserId="381" />
  <row Id="15214" PostId="13658" Score="0" Text="Do not mind the capabilities of the server, I am asking about the most common used file type with types." CreationDate="2016-08-29T08:41:09.010" UserId="20372" />
  <row Id="15215" PostId="13666" Score="0" Text="How does these two solutions compare to hdf5?" CreationDate="2016-08-29T08:42:20.423" UserId="20372" />
  <row Id="15216" PostId="9529" Score="0" Text="Closed as duplicate of http://stackoverflow.com/questions/34474767/how-to-select-regression-algorithm-for-noisy-scattered-data but I can't migrate it" CreationDate="2016-08-29T09:31:38.143" UserId="21" />
  <row Id="15217" PostId="9529" Score="0" Text="I'm voting to close this question as off-topic because it's a duplicate of http://stackoverflow.com/questions/34474767/how-to-select-regression-algorithm-for-noisy-scattered-data but isn't migrateable" CreationDate="2016-08-29T09:31:54.047" UserId="21" />
  <row Id="15218" PostId="9202" Score="0" Text="I'm voting to close this question as off-topic because it's a duplicate of http://stackoverflow.com/questions/34135683/how-to-differentiate-categorical-and-ordinal-variables-in-regression-analysis" CreationDate="2016-08-29T09:33:37.913" UserId="21" />
  <row Id="15219" PostId="9301" Score="0" Text="I'm voting to close this question as off-topic because it's a duplicate of http://stackoverflow.com/questions/34193685/how-to-visualize-make-plot-of-regression-output-against-categorical-input-vari" CreationDate="2016-08-29T09:34:07.820" UserId="21" />
  <row Id="15220" PostId="13666" Score="0" Text="I've never used (or heard of) HDF5 until now, so I'm not sure." CreationDate="2016-08-29T10:08:24.100" UserId="12515" />
  <row Id="15221" PostId="13718" Score="0" Text="The problem with treating this as a linear system is a chicken-and-egg situation. Using a `y=x+1` model is telling the system how to count, not having the system *learn* from data. Its the difference between coding a tic-tac-toe game by either coding the winning algorithm or coding a general learning algorithm and playing sample games." CreationDate="2016-08-29T11:14:42.897" UserId="471" />
  <row Id="15222" PostId="13727" Score="0" Text="Can you please put the whole error here so that more insight can be taken out of it ?" CreationDate="2016-08-29T12:18:51.823" UserId="15412" />
  <row Id="15223" PostId="13718" Score="0" Text="@Spacedman, linear regression models do not require that the equation be provided to the algorithm apriori. Rather the equation is inferred from the data.  In this case the linear behavior is so simple that only two records are needed. I'm not proposing that y=x+1 be given, just that two records be given. No chicken and egg problem here." CreationDate="2016-08-29T14:16:25.840" UserId="9420" />
  <row Id="15224" PostId="13699" Score="0" Text="It worked. Thank you. How can I add to the patterns? I will have several patterns and 'JJ', 'JJ', 'NNS' is only one of them." CreationDate="2016-08-29T15:09:18.013" UserId="3151" />
  <row Id="15225" PostId="13732" Score="0" Text="I wanted to do the same with chicken (we have 12 chicken with 8 different breeds) as a private side project, but didn't find the time yet. Hope there'll be some interesting answers. :)" CreationDate="2016-08-29T15:44:16.390" UserId="676" />
  <row Id="15226" PostId="13716" Score="1" Text="How would you know which instances to connect and which to leave disconnected?" CreationDate="2016-08-29T15:57:20.283" UserId="15527" />
  <row Id="15227" PostId="13733" Score="0" Text="Many thanks Raku! Did you if is easy after user groupByKey or combineByKey to detect which products are frequently purchase together? Like creating association rules?" CreationDate="2016-08-29T16:03:07.060" UserId="23811" />
  <row Id="15228" PostId="13721" Score="0" Text="I don't think there is anything like that in Orange yet. You would have to code it externally. Why not submit them an enhancement request: https://github.com/biolab/orange3" CreationDate="2016-08-29T16:05:22.480" UserId="15527" />
  <row Id="15229" PostId="13718" Score="0" Text="The only thing the system &quot;learns&quot;  by fitting a linear model is the intercept and slope. The slope is just the way of saying &quot;add 1&quot; or &quot;add 2&quot; or &quot;add 3.14&quot; for every 1 unit in x. The machine is not learning to count, its just learning what steps to count in. We're still waiting for the OP to clarify what they mean. Do they want to put addition or linearity into the system a priori, or do they want a symbolic learning process that has no inherent idea about &quot;1&quot; and &quot;2&quot; being mathematical concepts." CreationDate="2016-08-29T18:51:39.313" UserId="471" />
  <row Id="15230" PostId="13699" Score="0" Text="You're welcome, to make this work for any patterns just replace the &quot;pattern&quot; variable :  eg. `pattern = ['DT', 'NN']`" CreationDate="2016-08-29T21:21:55.023" UserId="21719" />
  <row Id="15231" PostId="13716" Score="0" Text="I assume that they should be connected in the order they exist in data table (corresponding to feature that is used as X axis). I will have a look for Line Chart, which you mentioned in the answer below. Thanks!" CreationDate="2016-08-29T23:11:12.123" UserId="23851" />
  <row Id="15232" PostId="13735" Score="0" Text="Feature Constructor seems to match my needs, but how can I implement something like this: `new_feature = old_feature - &lt;min value of old feature&gt;`?" CreationDate="2016-08-29T23:24:37.813" UserId="23851" />
  <row Id="15233" PostId="13733" Score="0" Text="Map the transaction list gained by combineByKey to a list whose element is ((item1,item2),1),then reduce it, so you get an RDD with ((item1,item2),together-buy-times). My suggestion is use flatMap while mapping, and use sort to make item1 and item2 are alphabetically sorted." CreationDate="2016-08-30T00:17:29.260" UserId="23601" />
  <row Id="15234" PostId="13736" Score="0" Text="equal_val=True is wrong. Welch's test assume unequal variance." CreationDate="2016-08-30T06:09:37.540" UserId="9123" />
  <row Id="15235" PostId="13743" Score="0" Text="I was using a backprop trainer on a network with 5 layers (46, 30, 15, 5, 1)  and I was using trainer.trainUntilConvergence(maxEpochs = 1000 ). I have changed the network to 3 layers (46, 20, 1) and maxEpochs to 100." CreationDate="2016-08-30T06:19:57.543" UserId="23905" />
  <row Id="15236" PostId="13743" Score="0" Text="5 layers is very computational intensive! Try 3 or even 1 to get a feeling." CreationDate="2016-08-30T06:26:40.350" UserId="9123" />
  <row Id="15237" PostId="13742" Score="0" Text="Hi, Reading batched from the file one batch at a time rather than processing chunks from all data read in memory drastically reduced the time. Thanks!" CreationDate="2016-08-30T07:34:23.563" UserId="23905" />
  <row Id="15238" PostId="13711" Score="0" Text="do you want the frequency of the phrase in the document? If that is the case, you can just use a dictionary counter right?" CreationDate="2016-08-30T07:41:15.577" UserId="21024" />
  <row Id="15239" PostId="13739" Score="0" Text="Thanks, but that version doesn't work either: `TypeError: __init__() takes at least 2 arguments (4 given)`" CreationDate="2016-08-30T07:41:17.443" UserId="21560" />
  <row Id="15240" PostId="13739" Score="0" Text="Then please upload the whole error. I cannot simply think of anything useful from the above line. A proper full error as thrown by your code might give me some insight." CreationDate="2016-08-30T07:47:02.890" UserId="15412" />
  <row Id="15241" PostId="13739" Score="0" Text="Sorry, I cannot publish here the full error message because it far more lengthy than the allowed text limit." CreationDate="2016-08-30T08:12:36.017" UserId="21560" />
  <row Id="15242" PostId="13746" Score="0" Text="The error message does not match your code. You have `tags = set(tags.eval)` in the error message, but `tags = set(tags)` in the pasted code. I don't think you need the `eval` for Keras because you generally are not interacting with the backend, it could be Theano or TensorFlow interchangeably." CreationDate="2016-08-30T08:57:14.550" UserId="836" />
  <row Id="15243" PostId="13746" Score="0" Text="Sorry, corrected, thanks." CreationDate="2016-08-30T09:00:23.957" UserId="21560" />
  <row Id="15244" PostId="13746" Score="0" Text="Hmm, the error message does imply you are getting tensor objects. Perhaps you need the eval after all! If so, your mistake is likely to be using `eval` when you mean `eval()`" CreationDate="2016-08-30T09:08:33.593" UserId="836" />
  <row Id="15247" PostId="13754" Score="0" Text="IIUC, `predictors` returns a `numpy array` which you are referencing to a `pandas Dataframe` object by it's columns which is incorrect as `numpy arrays` do not have the attribute `columns`." CreationDate="2016-08-05T10:31:01.083" UserId="18399" />
  <row Id="15248" PostId="13754" Score="0" Text="Sorry, it was a typo on the code. predictors and outcome are two `pandas DataFrame` with shape `m x n` and `m x 1`. It should be clear now." CreationDate="2016-08-05T10:34:00.090" UserId="133" />
  <row Id="15249" PostId="13755" Score="0" Text="Sorry lanenok, the number of trees is not the default one. I put an example code (and this is true for all the parameters, e.g. `min_samples_split`) because I cannot disclose the data I am working on. However, is it due to the number of trees, plus other parameters, or am I doing some mistakes here?" CreationDate="2016-08-05T11:16:42.747" UserId="133" />
  <row Id="15250" PostId="13754" Score="1" Text="I have come across the same findings some while ago. Could be that this is due to the fact that a number of features are important, but as features can be high or low in the decision tree (as only a random subset are offered when making a split), their importance varies highly from tree to tree, which results in a high standard deviation." CreationDate="2016-08-30T12:22:05.427" UserId="14372" />
  <row Id="15251" PostId="13753" Score="0" Text="Worth explaining what your current cross-validation and testing strategies are. Also, is the test set under your control, or is it held back by an organiser (e.g. Kaggle, ImageNet competition)? This is important, since you will want to match the organiser's train/test sampling strategy, amongst other concerns." CreationDate="2016-08-30T12:24:46.597" UserId="836" />
  <row Id="15252" PostId="13735" Score="0" Text="I don't know. Whoever designed the widget didn't seem to account for this use case. The project seems pretty active, you should submit an enhancement request on their issue tracker: https://github.com/biolab/orange3" CreationDate="2016-08-30T12:40:18.020" UserId="15527" />
  <row Id="15254" PostId="13739" Score="0" Text="Done, I have citated more error lines." CreationDate="2016-08-30T13:18:07.290" UserId="21560" />
  <row Id="15255" PostId="13757" Score="1" Text="You have an incredibly small sample size, so I would not be surprised to see spurious results like this. Moreover, it's not clear based on this question what the difference is between data sets `F` and `S` but it could well be that there's some non-trivial systematic difference between them. I would suggest to increase the sample size by about 10x if possible. If this doesn't resolve the problem then if you could provide more contextual information about the data and a reproducible example then we can explore this further." CreationDate="2016-08-30T14:04:19.973" UserId="2723" />
  <row Id="15256" PostId="13739" Score="0" Text="Can you please run the code again after setting n_jobs=1 ?" CreationDate="2016-08-30T14:20:07.147" UserId="15412" />
  <row Id="15257" PostId="13739" Score="0" Text="Done (appended to the original question)." CreationDate="2016-08-30T14:28:39.530" UserId="21560" />
  <row Id="15258" PostId="13739" Score="0" Text="Here is the catch. There is problem with your reshaping. Try something like this                                                                                                          `(X_train, y_train), (X_test, y_test) = mnist.load_data()&#xA;X_train = X_train.reshape(60000, 784)&#xA;X_test = X_test.reshape(10000, 784)&#xA;X_train = X_train.astype('float32')&#xA;X_test = X_test.astype('float32')&#xA;X_train /= 255&#xA;X_test /= 255`" CreationDate="2016-08-30T14:44:44.853" UserId="15412" />
  <row Id="15259" PostId="13757" Score="0" Text="Regarding sample size, unfortunately there are few observations in the minority class and I am using data which cannot be produced in large quantities. The two datasets are produced with the same technique and by same operators, so I would not expect them to differ systematically (but it may be). Unfortunately I cannot provide contextual information about the data because it is a proprietary dataset, sorry." CreationDate="2016-08-30T14:44:44.830" UserId="133" />
  <row Id="15260" PostId="13761" Score="0" Text="Still getting the same error message." CreationDate="2016-08-30T14:54:38.010" UserId="10902" />
  <row Id="15261" PostId="13761" Score="0" Text="@AbhishekJaiswal Can you show the full code and error with the steps above?" CreationDate="2016-08-30T14:56:49.260" UserId="2723" />
  <row Id="15262" PostId="8598" Score="0" Text="Take a look at ID3 algorithm used to generate a decision tree from a database&#xA;https://en.wikipedia.org/wiki/ID3_algorithm" CreationDate="2016-08-30T08:38:22.387" UserId="23905" />
  <row Id="15263" PostId="13762" Score="1" Text="It just appears to be hashed for privacy. There's probably no reason you'd want to throw away this feature -- just use it as a factor. After all, you can see right off the bat that some of the ID's appear repeatedly, so this is probably an extremely useful feature as it gives you a way to identify which rows correspond to the same individuals." CreationDate="2016-08-30T15:19:36.810" UserId="2723" />
  <row Id="15264" PostId="13759" Score="0" Text="Can you please share the full code, including the suggestions below so that we can see what's going on? It appears to be a missing dependency for `slam` so if installing `slam` doesn't change anything that would be highly surprising." CreationDate="2016-08-30T15:23:01.547" UserId="2723" />
  <row Id="15265" PostId="13762" Score="0" Text="Exactly @Hack-R but in order to make my model memory efficient I have to convert it into something else. How can I do that ?" CreationDate="2016-08-30T15:26:22.803" UserId="15412" />
  <row Id="15266" PostId="13762" Score="0" Text="Hashes are usually the most memory efficient. You can't use that? If not then I would just assign a sequence of integer or character ID's." CreationDate="2016-08-30T15:34:27.520" UserId="2723" />
  <row Id="15267" PostId="13761" Score="0" Text="@AbhishekJaiswal OP?" CreationDate="2016-08-30T15:40:46.287" UserId="2723" />
  <row Id="15268" PostId="13759" Score="0" Text="Also give your R version and the output from `sessionInfo()`" CreationDate="2016-08-30T15:52:22.393" UserId="471" />
  <row Id="15269" PostId="13735" Score="0" Text="thanks, will do:)" CreationDate="2016-08-30T16:55:04.900" UserId="23851" />
  <row Id="15270" PostId="13768" Score="0" Text="It's not necessary.  And yes, you can." CreationDate="2016-08-30T17:30:05.737" UserId="19161" />
  <row Id="15271" PostId="13699" Score="0" Text="I mean I have a list of patterns such as [['JJ', 'JJ', 'NNS'], ['JJ', 'NNS']]" CreationDate="2016-08-30T18:48:55.493" UserId="3151" />
  <row Id="15272" PostId="13772" Score="0" Text="The LSTM network *is* a type of RNN." CreationDate="2016-08-30T21:44:21.510" UserId="836" />
  <row Id="15273" PostId="13736" Score="0" Text="Thanks for pointing out my typo! I ended up using another nonparametric test for the project, but am still curious how to return degrees of freedom." CreationDate="2016-08-30T23:20:19.800" UserId="23701" />
  <row Id="15274" PostId="13736" Score="0" Text="I had the same experience before. I also tried statsmodel, but nothing like that was given." CreationDate="2016-08-31T00:33:27.763" UserId="9123" />
  <row Id="15275" PostId="13751" Score="0" Text="Please guide me to the part where it seems unclear." CreationDate="2016-08-31T03:14:28.320" UserId="23773" />
  <row Id="15276" PostId="13774" Score="1" Text="I think a simple google search would be more useful." CreationDate="2016-08-31T06:50:28.600" UserId="9123" />
  <row Id="15277" PostId="13774" Score="0" Text="@StudentT searched already but can not find nice response" CreationDate="2016-08-31T06:53:22.807" UserId="23940" />
  <row Id="15278" PostId="13774" Score="1" Text="@StudentT why you demotivate me by downvoting my question" CreationDate="2016-08-31T06:56:36.840" UserId="23940" />
  <row Id="15279" PostId="13774" Score="1" Text="They are very different things, not even close. Why not try to read wikis for the topics you mentioned?" CreationDate="2016-08-31T07:02:53.457" UserId="9123" />
  <row Id="15280" PostId="13720" Score="0" Text="You might have better luck on Stack Overflow for questions that are purely R programming.  There's a good response to R questions there." CreationDate="2016-08-31T07:45:26.367" UserId="23944" />
  <row Id="15281" PostId="13749" Score="0" Text="Which version of spark are you using ?" CreationDate="2016-08-31T09:39:10.073" UserId="5177" />
  <row Id="15282" PostId="13749" Score="0" Text="Spark  version 1.6.0" CreationDate="2016-08-31T09:46:12.940" UserId="23811" />
  <row Id="15283" PostId="13749" Score="0" Text="ok why don't you use dataframes ? and spark-csv to read your csv ? is there a constraint on using RDDs ?" CreationDate="2016-08-31T09:46:48.680" UserId="5177" />
  <row Id="15284" PostId="13749" Score="0" Text="Is there any advantage on using Data Frames? I don't have any constraint on using RDDs but I'm getting a little confusing on my code. I only want to &quot;group&quot; all the products based on transaction_ID" CreationDate="2016-08-31T09:54:22.133" UserId="23811" />
  <row Id="15286" PostId="13775" Score="0" Text="+1 Great answer." CreationDate="2016-08-31T11:48:38.350" UserId="20832" />
  <row Id="15287" PostId="10040" Score="0" Text="Which programming language do you use? Did you have the code?&#xA;If possible could you send to me, please?" CreationDate="2016-08-30T15:03:57.933" UserId="23917" />
  <row Id="15289" PostId="13778" Score="0" Text="Spark may or may not be helpful, it's a fine DB, but I don't see a specific resource constraint mentioned in your question that directly relates to something Spark does better (or worse) than other DB's, so I'm not sure how it fits in. It sounds like you need to do a lot of computationally non-intensive operations constantly or in parallel. Incremental training is of course a good idea. It sounds like the GPU is not a limiting factor. If the bottleneck is all of the reading and writing then perhaps what you should focus on is fast I/O via SSD and enough cores for sufficient parallelization." CreationDate="2016-08-31T13:23:40.073" UserId="2723" />
  <row Id="15290" PostId="13778" Score="1" Text="@Hack-R isn't a DB, it is a processing engine. Now to comment on the question, considering the size of your data, using Spark is an overkill. To be clear, training thousands of models won't be done in parallel but sequentially in Spark. Parallelization will be executed on for each model training nevertheless. Also considering the size of your data, you can perfectly train each model in a local manner." CreationDate="2016-08-31T15:01:47.777" UserId="5177" />
  <row Id="15291" PostId="13765" Score="1" Text="Thanks, it worked!" CreationDate="2016-08-31T15:23:04.320" UserId="10902" />
  <row Id="15292" PostId="13778" Score="0" Text="@eliasah That's just a matter of semantics. I've used Spark daily for 2 years and everyone I know refers to it as such. Including the Spark Summit, e.g.  &quot;Not Your Father's Database: How to Use Apache Spark Properly in Your Big Data Architecture&quot; and Google, e.g. &quot;Spark Database - Tools for Modern Applications‎&quot;." CreationDate="2016-08-31T15:29:40.213" UserId="2723" />
  <row Id="15293" PostId="13778" Score="0" Text="It's not. I use spark also daily but semantic how you call it is not the subject here nevertheless. It's not a DB." CreationDate="2016-08-31T15:31:31.927" UserId="5177" />
  <row Id="15294" PostId="13778" Score="0" Text="@eliasah I disagree entirely. But you should contact the Spark Summit and Google to let them know how wrong they are." CreationDate="2016-08-31T15:35:06.743" UserId="2723" />
  <row Id="15295" PostId="13761" Score="0" Text="Hey, it worked after I reinstalled the R. Meanwhile, thanks for all your help. Cheers! :)" CreationDate="2016-08-31T15:38:49.860" UserId="10902" />
  <row Id="15296" PostId="13778" Score="0" Text="That makes us even. I'm not looking for a confirmation from you to decide whether it is a DB or not. Now please if you don't have anything to add to the question, don't waste my time ! Have a nice day." CreationDate="2016-08-31T15:42:35.510" UserId="5177" />
  <row Id="15297" PostId="13775" Score="0" Text="@sergiOrtiz          Thank you" CreationDate="2016-08-31T17:18:46.007" UserId="23940" />
  <row Id="15298" PostId="13774" Score="5" Text="Your questions is equivalent to &quot;*Is there is a big difference between pineapple, Apple Inc. and applause*?&quot;" CreationDate="2016-08-31T17:43:39.353" UserId="8479" />
  <row Id="15299" PostId="13774" Score="0" Text="haha i am new here but this question bothered me alot" CreationDate="2016-08-31T17:45:56.880" UserId="23940" />
  <row Id="15303" PostId="13789" Score="0" Text="How big is your data, since you mention GraphLab?" CreationDate="2016-09-01T04:06:18.423" UserId="381" />
  <row Id="15305" PostId="13721" Score="0" Text="I've been playing around with code and added a new &quot;rank&quot; discretize method, sadly it does not work because final transformation only works with ranges. I tried to propagate the kind of discretize until this point but I don't see how :(" CreationDate="2016-09-01T05:46:28.250" UserId="23872" />
  <row Id="15306" PostId="13789" Score="0" Text="What model are you looking for?" CreationDate="2016-09-01T05:54:38.970" UserId="9123" />
  <row Id="15308" PostId="13739" Score="0" Text="I had already done these steps, but now I have implemented your code version literally - same error remains." CreationDate="2016-09-01T07:47:17.667" UserId="21560" />
  <row Id="15309" PostId="13779" Score="0" Text="I've come across that term, however, it is (in my case) not the term that I was looking for. The cause of the discrepancy is more likely to be characterized by the term [domain adaptation](https://en.wikipedia.org/wiki/Domain_adaptation). But I am looking for a more general term or discipline that addresses the issue that training sets can not be used for their real world test sets." CreationDate="2016-09-01T08:07:10.533" UserId="14372" />
  <row Id="15310" PostId="13793" Score="0" Text="So what if I do not balance my training set, then I feed my algorithm with it and I use the model to predict the imbalanced prediction set? Will it overperform the balanced trained model? Why?&#xA;&#xA;Thanks for your answer." CreationDate="2016-09-01T08:15:19.060" UserId="23567" />
  <row Id="15311" PostId="13782" Score="0" Text="Just do it; that's not a lot of data even for a laptop." CreationDate="2016-09-01T02:32:39.903" UserId="381" />
  <row Id="15312" PostId="5226" Score="0" Text="In case of sckitlearn I have seen that we need to encode the categorical variables, else fit method would throw an error saying ValueError: could not convert string to float" CreationDate="2016-08-31T23:53:27.997" UserId="23972" />
  <row Id="15313" PostId="13784" Score="0" Text="Are these figures in US $? your location will be important too." CreationDate="2016-09-01T09:23:12.203" UserId="23911" />
  <row Id="15314" PostId="13797" Score="0" Text="What you mean by loss function? Thanks in advance!" CreationDate="2016-09-01T09:49:22.900" UserId="23567" />
  <row Id="15315" PostId="13798" Score="0" Text="Why not use the original and official IMDB data instead? It is available at http://www.imdb.com/interfaces." CreationDate="2016-09-01T09:55:21.587" UserId="6550" />
  <row Id="15316" PostId="13798" Score="0" Text="I used that, but when I used it the performance I get from my model is pretty bad. But when I use this, it seems to working well." CreationDate="2016-09-01T09:59:05.560" UserId="21024" />
  <row Id="15317" PostId="13798" Score="0" Text="You should add how you got to this link and show a few example rows" CreationDate="2016-09-01T13:19:06.773" UserId="14904" />
  <row Id="15318" PostId="13802" Score="0" Text="I'm looking for an algorithm where I can provide the maximum diameter  a cluster can have. Still, thanks." CreationDate="2016-09-01T15:55:03.663" UserId="23850" />
  <row Id="15319" PostId="13802" Score="0" Text="Did you even read what I wrote? The maximum distance *is* a diameter." CreationDate="2016-09-01T15:56:04.270" UserId="924" />
  <row Id="15320" PostId="13802" Score="0" Text="Ok, I didn't get that bit I guess. Could you please elaborate? This and also that I need  an algorithm that is a bit more robust to noise." CreationDate="2016-09-01T15:57:37.567" UserId="23850" />
  <row Id="15321" PostId="13802" Score="0" Text="Not much to elaborate here. Just try it." CreationDate="2016-09-01T15:58:17.800" UserId="924" />
  <row Id="15322" PostId="13802" Score="0" Text="I mean what if I need the diameter to be 6.3? What would the height be then?" CreationDate="2016-09-01T16:00:04.453" UserId="23850" />
  <row Id="15323" PostId="13802" Score="0" Text="The height is the maximum distance. So 6.3 of course." CreationDate="2016-09-01T16:02:53.413" UserId="924" />
  <row Id="15324" PostId="13802" Score="0" Text="How do I cut the tree at a height of 6.3? This kind of does not make sense to me. Please explain." CreationDate="2016-09-01T16:03:59.450" UserId="23850" />
  <row Id="15325" PostId="13802" Score="0" Text="You call the cutree function with parameter 6.3" CreationDate="2016-09-01T16:06:36.103" UserId="924" />
  <row Id="15326" PostId="13799" Score="0" Text="Is there an algorithm in this class that is robust to noise?" CreationDate="2016-09-01T16:07:14.837" UserId="23850" />
  <row Id="15327" PostId="13802" Score="0" Text="Ok, is there a package or toolbox or any such resource that you have in mind? Could you please provide me with a link?" CreationDate="2016-09-01T16:08:33.237" UserId="23850" />
  <row Id="15328" PostId="2468" Score="0" Text="Since this answer was written, Vertica added support for machine-learning analytics.  I haven't used it, but here's the [documentation](https://my.vertica.com/docs/7.2.x/HTML/index.htm#Authoring/MachineLearning/MachineLearning.htm%3FTocPath%3DAnalyzing%2520Data%7CMachine%2520Learning%2520for%2520Predictive%2520Analytics%7C_____0)." CreationDate="2016-09-01T16:40:39.717" UserId="23931" />
  <row Id="15329" PostId="13802" Score="0" Text="This is standard functionality. Supposedly *every* tool has this." CreationDate="2016-09-01T18:36:00.720" UserId="924" />
  <row Id="15330" PostId="13814" Score="1" Text="Any bounds would not be strictly based on the number of categories, but the complexity of the decision boundaries. If the boundaries are simple hyperplanes in the input space, then you would need no hidden layers and only one output neuron per category. Most interesting problems are not so simple though." CreationDate="2016-09-02T07:03:50.490" UserId="836" />
  <row Id="15331" PostId="13808" Score="0" Text="Make sure to use a proper IDE that will take care of the indentation for you, that way a problem like this will never happen (or it will at least be visible where it is)" CreationDate="2016-09-02T07:06:02.063" UserId="14904" />
  <row Id="15332" PostId="13816" Score="0" Text="Log loss will work correctly for values other than 0,1 provided the optimiser has been coded to allow for that input. I expect (but don't know) that multiclass logloss on softmax is most usually *not* coded this way and will not work - that's because with 0,1 you can make some really useful simplifications to the maths, and that has a large impact on the code. I have actually coded this at https://github.com/neilslater/ru_ne_ne/blob/master/ext/ru_ne_ne/core_objective_functions.c#L280-L308 (the simple case is 4 lines of code, the complex one is longer and calls out to sub-functions not shown)." CreationDate="2016-09-02T07:24:25.017" UserId="836" />
  <row Id="15333" PostId="13814" Score="0" Text="What does n-int integer input mean? Is it discrete and capped?" CreationDate="2016-09-02T07:42:04.530" UserId="14904" />
  <row Id="15334" PostId="13814" Score="0" Text="Discrete yes, I'm asking for a maximum complexity case here." CreationDate="2016-09-02T08:01:59.550" UserId="24003" />
  <row Id="15335" PostId="13816" Score="0" Text="I can use the cross entropy loss for this right? Also, the number of outputs is currently equal to the number of moves. I think the two ways can work together." CreationDate="2016-09-02T08:05:58.123" UserId="24003" />
  <row Id="15336" PostId="13816" Score="0" Text="Cross entropy loss and log loss are the same. Yes the difference between this and the approach I mentioned with 12 outputs with individual binary classification is that with the cross entropy + softmax solution you output a probability distribution over the moves and with the binary solution you have a probability distribution of yes/no optimal per move." CreationDate="2016-09-02T08:14:42.667" UserId="14904" />
  <row Id="15337" PostId="13819" Score="0" Text="I only want to save the results in HDFS. I don't know if the best option is to convert it into CSV. There is a easily way to export the results of:&#xA; df.groupBy(&quot;Product_ID&quot;).agg(collect_list($&quot;Stock&quot;)).show&#xA;To my directory of HDFS?" CreationDate="2016-09-02T11:14:45.757" UserId="23714" />
  <row Id="15338" PostId="13819" Score="0" Text="If it's only one column you can just use .rdd on your dataframe and save it using .saveTextFile" CreationDate="2016-09-02T11:52:02.707" UserId="14904" />
  <row Id="15339" PostId="13814" Score="1" Text="@davik: &quot;Maximum&quot; complexity would essentially be learning a random class assgnment for each discrete input. Which would probably require a huge network, and be about as useful a concept as the simple hyperplanes, because the concept of generalisation would not apply, and you may as well just store a lookup table of input to output. Really what you are asking for does not exist. The best size of network depends critically on the nature of your problem, and the advice is usually to try stuff and see what works." CreationDate="2016-09-02T13:18:39.290" UserId="836" />
  <row Id="15340" PostId="13819" Score="0" Text="When I save as text file is saving like this:&#xA;[40278043620120628,WrappedArray(5824, 9909, 5824, 9909, 9909, 3504)]&#xA;&#xA;There is any way to save like this:&#xA;[40278043620120628,5824, 9909, 5824, 9909, 9909, 3504]&#xA;???" CreationDate="2016-09-02T14:04:12.863" UserId="23714" />
  <row Id="15341" PostId="13819" Score="0" Text="I don't know Scala but if you want that you have to map the WrappedArray to a normal list somehow" CreationDate="2016-09-02T14:05:23.783" UserId="14904" />
  <row Id="15342" PostId="13813" Score="0" Text="I tried fooling with Facebook Graph API. Not exactly sure how to gather an entire neighborhood's worth of user generated content." CreationDate="2016-09-02T14:06:27.903" UserId="23998" />
  <row Id="15343" PostId="13107" Score="0" Text="So for a document you know for a fact it belongs to A and B, for C you are not sure, but are there also topics you know for a fact it doesn't belong to?" CreationDate="2016-09-02T14:57:24.870" UserId="14904" />
  <row Id="15344" PostId="13822" Score="1" Text="Machine learning could do this but there are much better ways to achieve this with more traditional computer vision techniques, could you post a photo as an example?" CreationDate="2016-09-02T15:01:46.203" UserId="14904" />
  <row Id="15345" PostId="13822" Score="0" Text="@JanvanderVegt I have edited my question to include an example image." CreationDate="2016-09-02T15:16:18.660" UserId="23915" />
  <row Id="15346" PostId="13792" Score="0" Text="It is possible in some cases but as you mention there are other approaches that don't require balanced classes.  This post may be a good read: http://www.svds.com/learning-imbalanced-classes/" CreationDate="2016-09-02T16:22:56.000" UserId="23995" />
  <row Id="15347" PostId="13814" Score="0" Text="I am basically asking how big it needs to be to learn to be a lookup table, this is mainly out of theoretical interest to me" CreationDate="2016-09-02T16:29:08.783" UserId="24003" />
  <row Id="15348" PostId="13816" Score="0" Text="Oh so you are suggesting no softmax? That sounds like a good idea" CreationDate="2016-09-02T16:30:59.670" UserId="24003" />
  <row Id="15349" PostId="13816" Score="0" Text="But doesn't this lose some information about confidence, so if I were to use this network to solve the cube, it would take longer." CreationDate="2016-09-02T16:37:58.083" UserId="24003" />
  <row Id="15350" PostId="13816" Score="0" Text="Not really, every move still has a probability of the chance of being an optimal move. During solving you would just take the move with the highest probability involved." CreationDate="2016-09-02T16:59:33.107" UserId="14904" />
  <row Id="15351" PostId="13822" Score="0" Text="So the sticky notes are all the same shape?" CreationDate="2016-09-02T17:24:11.620" UserId="14904" />
  <row Id="15352" PostId="13822" Score="0" Text="That is correct. They might vary in color, but the shape should remain consistent." CreationDate="2016-09-02T17:31:43.263" UserId="23915" />
  <row Id="15353" PostId="13808" Score="0" Text="Thanks yeah, I like to use Notepad++ just because I have a cheapo laptop and having one light editor is easier - however I will be using an IDE here on out because of exactly the reason you stated." CreationDate="2016-09-02T18:02:52.903" UserId="23997" />
  <row Id="15354" PostId="13827" Score="0" Text="upon further research I believe that it has something to do with the `titles = hxs.xpath('//span[@class=&quot;pl&quot;]')` section, but I'm not positive" CreationDate="2016-09-02T19:42:55.000" UserId="23997" />
  <row Id="15355" PostId="13828" Score="1" Text="Does that &quot;rows: 3 columns:6&quot; mean you have three data points and six explanatory variables? That's going to mess up a linear regression because you are trying to find more variables than you have data points. So I suspect -log(Lik) is Inf because of that.... If -log(Lik)=Inf then log(Lik)=-Inf then Lik=0 - somehow the likelihood is zero...." CreationDate="2016-09-02T22:05:12.477" UserId="471" />
  <row Id="15356" PostId="13776" Score="0" Text="The same question has an answer on [SO](http://stackoverflow.com/questions/39254231/how-to-use-rmagic-in-azure-notebooks/39268025#39268025)." CreationDate="2016-09-03T02:45:34.440" UserId="23945" />
  <row Id="15357" PostId="9021" Score="0" Text="Do you have the standard error for your coefficient of $\text{weight}$?" CreationDate="2016-09-03T04:52:27.330" UserId="24024" />
  <row Id="15358" PostId="11212" Score="0" Text="No, it does not make sense." CreationDate="2016-09-03T07:37:45.287" UserId="24024" />
  <row Id="15359" PostId="11536" Score="0" Text="You are running your model on different data set so it's really not that surprising." CreationDate="2016-09-03T07:42:02.277" UserId="24024" />
  <row Id="15360" PostId="10666" Score="0" Text="What do you mean by &quot;coefficient stability&quot;?" CreationDate="2016-09-03T07:44:33.377" UserId="24024" />
  <row Id="15361" PostId="13832" Score="0" Text="Why down vote ?" CreationDate="2016-09-03T08:25:39.630" UserId="24024" />
  <row Id="15362" PostId="13175" Score="0" Text="When you say &quot;unpack the executable&quot; do you mean if the NN was a C program and you shipped just an executable binary (and no source code) then that would not be a sufficiently obfuscated solution for you?" CreationDate="2016-09-03T08:29:16.157" UserId="471" />
  <row Id="15363" PostId="13834" Score="0" Text="Have you checked wit.ai?" CreationDate="2016-09-03T10:34:05.660" UserId="9123" />
  <row Id="15364" PostId="13828" Score="0" Text="@Spacedman I've added my guess, please take a look." CreationDate="2016-09-03T10:46:23.613" UserId="9123" />
  <row Id="15365" PostId="13828" Score="0" Text="@Spacedman: I expect that &quot;rows:3 columns: 6&quot; is referring to the displayed table - which has 3 rows and 6 columns - and is not a description of the input data. It's just a generic table display widget that happens to be showing the summary data for the question." CreationDate="2016-09-03T11:25:06.640" UserId="836" />
  <row Id="15366" PostId="13814" Score="0" Text="Not something I know much about, but you might start here to cover the associated theory of NN &quot;Capacity&quot;: https://en.wikipedia.org/wiki/Artificial_neural_network#Capacity" CreationDate="2016-09-03T11:38:11.170" UserId="836" />
  <row Id="15367" PostId="13828" Score="0" Text="@minu You need to give us some more info about your data set and the model specification because I suspect something is degenerate about it and its only possible to guess wildly without knowing both." CreationDate="2016-09-03T11:59:30.587" UserId="471" />
  <row Id="15368" PostId="13834" Score="0" Text="Yes @student . However this is not exactly what I am looking for." CreationDate="2016-09-03T13:11:52.783" UserId="24029" />
  <row Id="15369" PostId="13181" Score="0" Text="A very similar question was asked on CV [How few training examples is too few when training a neural network?](http://stats.stackexchange.com/q/226672/12359)." CreationDate="2016-09-03T15:49:56.657" UserId="843" />
  <row Id="15370" PostId="13828" Score="0" Text="3 rows and 6 columns refers to the displayed table of evaluation results. Please find edits in my question with a little more data description." CreationDate="2016-09-03T15:55:48.617" UserId="2647" />
  <row Id="15371" PostId="13835" Score="0" Text="I've added some data description in the question. Is this happening because my observations are too less for the spread (variables) in my data? I haven't copied and pasted the same data over rows. What can I do to make this data viable for modeling?" CreationDate="2016-09-03T16:10:27.370" UserId="2647" />
  <row Id="15372" PostId="13828" Score="0" Text="@Minu, you'll need to notify them by adding @ to their user names." CreationDate="2016-09-03T17:33:26.803" UserId="9123" />
  <row Id="15374" PostId="13827" Score="0" Text="Thanks for the format fix @Jan-van-der-Vegt" CreationDate="2016-09-03T19:38:44.580" UserId="23997" />
  <row Id="15375" PostId="13806" Score="0" Text="What algorithm are you referring to?" CreationDate="2016-09-03T20:27:26.710" UserId="15527" />
  <row Id="15376" PostId="13839" Score="0" Text="Yes, I agree, what I'm asking is precisely how many it takes to memorize" CreationDate="2016-09-03T20:56:54.130" UserId="24003" />
  <row Id="15377" PostId="13839" Score="0" Text="But you don't want it to memorize usually. Plus, there is an infinite number of integers. So if by &quot;all possible inputs&quot; you mean every integer, the answer will be infinite." CreationDate="2016-09-03T21:04:18.487" UserId="924" />
  <row Id="15378" PostId="13612" Score="0" Text="Ah, never mind — I found it: http://orange.biolab.si/forum/" CreationDate="2016-09-03T22:34:08.937" UserId="15527" />
  <row Id="15379" PostId="13839" Score="0" Text="I agree that this is against the usual use case for neural networks. Also I'm limiting the number of bits of input, I'm wondering if there is literature on this somewhere." CreationDate="2016-09-03T22:34:29.000" UserId="24003" />
</comments>