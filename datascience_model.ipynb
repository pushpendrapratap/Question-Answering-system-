{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Python 2: use print only as a function\n",
    "# from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import gensim\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer \n",
    "from sklearn.multiclass import OneVsRestClassifier \n",
    "from sklearn.multioutput import MultiOutputClassifier  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['machine-learning', 'classification', 'evalua...</td>\n",
       "      <td>How to improve an existing (trained) classifier?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'r', 'logistic-regression...</td>\n",
       "      <td>Random Forest, Type - Regression, Calculation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['visualization']</td>\n",
       "      <td>How to analyze which site has most numbers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['bigdata']</td>\n",
       "      <td>Privacy through fake data?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['r', 'data-wrangling']</td>\n",
       "      <td>When to choose character instead of factor in R?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tag  \\\n",
       "0  ['machine-learning', 'classification', 'evalua...   \n",
       "1  ['machine-learning', 'r', 'logistic-regression...   \n",
       "2                                  ['visualization']   \n",
       "3                                        ['bigdata']   \n",
       "4                            ['r', 'data-wrangling']   \n",
       "\n",
       "                                               title  \n",
       "0   How to improve an existing (trained) classifier?  \n",
       "1  Random Forest, Type - Regression, Calculation ...  \n",
       "2         How to analyze which site has most numbers  \n",
       "3                         Privacy through fake data?  \n",
       "4   When to choose character instead of factor in R?  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './datascience_datasets_for_doc_similarity.csv'\n",
    "\n",
    "d_set = pd.read_csv(path, header=None, names=['tag', 'title', 'answers'])\n",
    "d_set.drop(['answers'], axis=1, inplace=True)\n",
    "\n",
    "d_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []                                       # contains values of d_set['tag']\n",
    "for i in d_set['tag']:  \n",
    "    y_labels.append(ast.literal_eval(i))            # to remove unicodeed string \n",
    "y_labels = [j for i in y_labels for j in i] \n",
    "y_labels = list(set(y_labels))\n",
    "\n",
    "# for i in range(len(y_labels)):\n",
    "#     if (y_labels[i].find('-') > -1):\n",
    "#         y_labels[i] = y_labels[i].replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "class lemmatokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.token_pattern = r\"(?u)\\b\\w\\w+\\b\"       \n",
    "#         self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self,doc):                            # here, doc is one string sentence\n",
    "        token_pattern = re.compile(self.token_pattern)\n",
    "        return [self.stemmer.stem(t) for t in token_pattern.findall(doc)]\n",
    "#         return lambda doc: token_pattern.findall(doc) \n",
    "#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_title = CountVectorizer(max_df=0.5,\n",
    "                             min_df=5,\n",
    "                             stop_words='english',\n",
    "                             tokenizer=lemmatokenizer(),\n",
    "                             ngram_range=(1,3))\n",
    "\n",
    "tfidf_vect_title = TfidfVectorizer(smooth_idf=False,\n",
    "                                   max_df=0.5,\n",
    "                                   min_df=5,\n",
    "                                   stop_words='english',\n",
    "                                   tokenizer=lemmatokenizer(),\n",
    "                                   ngram_range=(1,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To make it clear what actually we are doing...\n",
    "\n",
    "### tag                           \n",
    "['r', 'machine-learning', 'ai'] \n",
    "   \n",
    "### Labelencoder() \n",
    "[32, 324, 17]              \n",
    "   \n",
    "### MultiLabelBinarizer()\n",
    "[0,0,0,0,........1,0,0,1,1,0,...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['machine-learning', 'classification', 'evalua...</td>\n",
       "      <td>How to improve an existing (trained) classifier?</td>\n",
       "      <td>[119, 29, 65]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['machine-learning', 'r', 'logistic-regression...</td>\n",
       "      <td>Random Forest, Type - Regression, Calculation ...</td>\n",
       "      <td>[119, 170, 117, 171]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['visualization']</td>\n",
       "      <td>How to analyze which site has most numbers</td>\n",
       "      <td>[225]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['bigdata']</td>\n",
       "      <td>Privacy through fake data?</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['r', 'data-wrangling']</td>\n",
       "      <td>When to choose character instead of factor in R?</td>\n",
       "      <td>[170, 48]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tag  \\\n",
       "0  ['machine-learning', 'classification', 'evalua...   \n",
       "1  ['machine-learning', 'r', 'logistic-regression...   \n",
       "2                                  ['visualization']   \n",
       "3                                        ['bigdata']   \n",
       "4                            ['r', 'data-wrangling']   \n",
       "\n",
       "                                               title             label_num  \n",
       "0   How to improve an existing (trained) classifier?         [119, 29, 65]  \n",
       "1  Random Forest, Type - Regression, Calculation ...  [119, 170, 117, 171]  \n",
       "2         How to analyze which site has most numbers                 [225]  \n",
       "3                         Privacy through fake data?                  [21]  \n",
       "4   When to choose character instead of factor in R?             [170, 48]  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = preprocessing.LabelEncoder()  \n",
    "le.fit(y_labels) \n",
    "d_set['label_num'] = pd.Series([le.transform(ast.literal_eval(i)) for i in d_set['tag']])\n",
    "d_set.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_y_labels = d_set['label_num'].values.tolist()\n",
    "\n",
    "# print (new_y_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelBinarizer(classes=None, sparse_output=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlb = MultiLabelBinarizer() \n",
    "mlb.fit(new_y_labels)\n",
    "\n",
    "# mlb.transform(new_y_labels).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1223, 231)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tag_dtm = mlb.transform(new_y_labels) \n",
    "\n",
    "# print (type(y_tag_dtm))\n",
    "# y_tag_dtm = pd.Series(y_tag_dtm) \n",
    "\n",
    "y_tag_dtm.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_labels = d_set['title'].values.tolist()\n",
    "\n",
    "# print (X_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1223x339 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 4929 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_title.fit(X_labels)\n",
    "X_title_dtm = vect_title.transform(X_labels)\n",
    "\n",
    "# vect_title.get_feature_names()\n",
    "# vect_title.get_params\n",
    "# print (X_title_dtm.toarray())\n",
    "\n",
    "X_title_dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation\n",
    "## implementation PCA : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.1639</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>-0.1656</td>\n",
       "      <td>-0.0699</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>-0.1085</td>\n",
       "      <td>-0.0659</td>\n",
       "      <td>0.0288</td>\n",
       "      <td>-0.0431</td>\n",
       "      <td>0.2704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0156</td>\n",
       "      <td>-0.0488</td>\n",
       "      <td>0.0473</td>\n",
       "      <td>-0.1021</td>\n",
       "      <td>0.0293</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>-0.0240</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.0615</td>\n",
       "      <td>0.0144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.2537</td>\n",
       "      <td>0.0441</td>\n",
       "      <td>-0.2416</td>\n",
       "      <td>-0.1782</td>\n",
       "      <td>-0.2837</td>\n",
       "      <td>0.5325</td>\n",
       "      <td>0.5208</td>\n",
       "      <td>0.1765</td>\n",
       "      <td>-0.1154</td>\n",
       "      <td>-0.0704</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4197</td>\n",
       "      <td>-0.0383</td>\n",
       "      <td>0.1125</td>\n",
       "      <td>0.0897</td>\n",
       "      <td>-0.2468</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.1474</td>\n",
       "      <td>-0.4358</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.1004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1695</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>-0.1774</td>\n",
       "      <td>-0.0856</td>\n",
       "      <td>-0.0291</td>\n",
       "      <td>-0.1381</td>\n",
       "      <td>-0.0116</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0609</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0456</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0683</td>\n",
       "      <td>0.0442</td>\n",
       "      <td>-0.0050</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0.0719</td>\n",
       "      <td>-0.0203</td>\n",
       "      <td>0.0814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.7518</td>\n",
       "      <td>0.0536</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>-0.0947</td>\n",
       "      <td>-0.0301</td>\n",
       "      <td>-0.0468</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>-0.0295</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0187</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0065</td>\n",
       "      <td>-0.0095</td>\n",
       "      <td>-0.0192</td>\n",
       "      <td>-0.0069</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>-0.0137</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1978</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>-0.1515</td>\n",
       "      <td>-0.1133</td>\n",
       "      <td>-0.0075</td>\n",
       "      <td>-0.1201</td>\n",
       "      <td>-0.0386</td>\n",
       "      <td>-0.0495</td>\n",
       "      <td>-0.0443</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.1913</td>\n",
       "      <td>-0.0783</td>\n",
       "      <td>-0.4500</td>\n",
       "      <td>-0.1523</td>\n",
       "      <td>0.2227</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>0.0730</td>\n",
       "      <td>-0.1094</td>\n",
       "      <td>0.1722</td>\n",
       "      <td>-0.2333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0 -0.1639  0.0399 -0.1656 -0.0699  0.0036 -0.1085 -0.0659  0.0288 -0.0431   \n",
       "1 -0.2537  0.0441 -0.2416 -0.1782 -0.2837  0.5325  0.5208  0.1765 -0.1154   \n",
       "2 -0.1695  0.0244 -0.1774 -0.0856 -0.0291 -0.1381 -0.0116  0.0189  0.0609   \n",
       "3  0.7518  0.0536 -0.0251 -0.0947 -0.0301 -0.0468 -0.0012 -0.0295 -0.0005   \n",
       "4 -0.1978  0.0242 -0.1515 -0.1133 -0.0075 -0.1201 -0.0386 -0.0495 -0.0443   \n",
       "\n",
       "       9    ...        90      91      92      93      94      95      96  \\\n",
       "0  0.2704   ...   -0.0156 -0.0488  0.0473 -0.1021  0.0293  0.0226 -0.0240   \n",
       "1 -0.0704   ...   -0.4197 -0.0383  0.1125  0.0897 -0.2468  0.2057  0.1474   \n",
       "2  0.0381   ...    0.0456  0.0600 -0.0011  0.0683  0.0442 -0.0050  0.0543   \n",
       "3 -0.0187   ...   -0.0065 -0.0095 -0.0192 -0.0069 -0.0045 -0.0257 -0.0137   \n",
       "4  0.0010   ...   -0.1913 -0.0783 -0.4500 -0.1523  0.2227  0.0237  0.0730   \n",
       "\n",
       "       97      98      99  \n",
       "0 -0.0023  0.0615  0.0144  \n",
       "1 -0.4358  0.0248  0.1004  \n",
       "2  0.0719 -0.0203  0.0814  \n",
       "3 -0.0009  0.0067  0.0116  \n",
       "4 -0.1094  0.1722 -0.2333  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=100).fit(X_title_dtm.toarray())\n",
    "pca_samples = pca.transform(X_title_dtm.toarray())\n",
    "\n",
    "pca_df = pd.DataFrame(np.round(pca_samples,4))\n",
    "\n",
    "pca_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1223, 339)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df = pd.DataFrame(X_title_dtm.toarray(),columns=vect_title.get_feature_names())\n",
    "\n",
    "# new_df.head()\n",
    "# new_df.ix[1].to_dict().values()\n",
    "\n",
    "new_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = collections.Counter(vect_title.get_feature_names())\n",
    "# print (d['ai']) \n",
    "\n",
    "new_df['target_list'] = [i for i in y_tag_dtm] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.columns[:100]  \n",
    "# new_df.ix[0]\n",
    "\n",
    "# new_df.head() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1223x339 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4929 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect_title.fit(X_labels)\n",
    "X_title_dtm_tfidf = tfidf_vect_title.transform(X_labels)\n",
    "\n",
    "X_title_dtm_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df_of_tfidf = pd.DataFrame(X_title_dtm_tfidf.toarray(),\n",
    "                               columns=tfidf_vect_title.get_feature_names()) \n",
    "\n",
    "new_df_of_tfidf['target_list'] = [i for i in y_tag_dtm] \n",
    "\n",
    "y = new_df_of_tfidf['target_list'] \n",
    "X = new_df_of_tfidf.drop('target_list',axis=1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = new_df['target_list']\n",
    "# X = new_df.drop('target_list',axis=1)\n",
    "\n",
    "# # X = X.ix[:]                          # it will return each feature row wise to X\n",
    "# # X = X.values\n",
    "\n",
    "# print (type(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8.366e-01 -2.590e-02  4.576e-01 ...  2.100e-02 -4.410e-02  1.900e-03]\n",
      " [ 5.722e-01 -4.540e-01  2.170e-01 ... -3.650e-02 -5.610e-02  9.400e-03]\n",
      " [-3.276e-01 -8.170e-02 -7.700e-02 ...  5.100e-03 -3.100e-03  1.450e-02]\n",
      " ...\n",
      " [-2.637e-01 -1.401e-01 -8.150e-02 ... -3.193e-01 -2.057e-01 -4.500e-03]\n",
      " [-2.342e-01 -4.827e-01  7.541e-01 ... -7.000e-04  3.800e-03  5.900e-03]\n",
      " [-4.055e-01 -4.964e-01  1.827e-01 ... -1.800e-03 -3.100e-03  2.500e-03]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X.values.tolist())       # it will convert list to numpy ndarray\n",
    "y = np.array(y.values.tolist())\n",
    "\n",
    "pca_X = PCA(n_components=200).fit_transform(X)  \n",
    "pca_X = np.round(pca_X,4)\n",
    "\n",
    "pca_y = PCA(n_components=50).fit_transform(y)  \n",
    "pca_y = np.round(pca_y,4)\n",
    "\n",
    "print (pca_y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=1)   \n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(pca_X,\n",
    "#                                                     pca_y,\n",
    "#                                                     test_size=0.2,\n",
    "#                                                     random_state=1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf = Pipeline([('classifier',\n",
    "#                  OneVsRestClassifier(SVC(probability=True,\n",
    "#                                          random_state=0)))])  \n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "# mnb_clf = MultinomialNB()                          # not working for MultiLabelinput\n",
    "\n",
    "# time_pass_y = np.random.randint(2,size=(2838,1))   # produce ndarray of size 2838 X 1\n",
    "\n",
    "knn_clf.fit(X_train, y_train)\n",
    "# mnb_clf.fit(X_train, y_train) \n",
    "\n",
    "knn_pred = knn_clf.predict(X_test)  \n",
    "# mnb_pred = mnb_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 7 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 9 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 25 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 56 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 64 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 94 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 98 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 100 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 126 is present in all training examples.\n",
      "  str(classes[c]))\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/multiclass.py:76: UserWarning: Label not 181 is present in all training examples.\n",
      "  str(classes[c]))\n"
     ]
    }
   ],
   "source": [
    "svc_clf = OneVsRestClassifier(SVC(probability=True,random_state=0))\n",
    "svc_clf.fit(X_train, y_train)\n",
    "svc_pred = svc_clf.predict(X_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024489795918367346"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\" \n",
    "# it will give 0.0, since it's matching with the exact no. of \n",
    "# target labels and not giving credit for partial correct \n",
    "# prediction of labels. \n",
    "# \"\"\"\n",
    "\n",
    "# svc_clf.score(X_test, y_test)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "knn_report = metrics.classification_report(y_test[:], knn_pred[:]) \n",
    "\n",
    "knn_f1_score = metrics.f1_score(y_test[:], knn_pred[:], average='samples') \n",
    "\n",
    "knn_precision_recall_fscore = metrics.precision_recall_fscore_support(y_test, \n",
    "                                                                      knn_pred, \n",
    "                                                                      average='samples')\n",
    "\n",
    "knn_avg_precision_score = metrics.average_precision_score(y_test, \n",
    "                                                          knn_pred, \n",
    "                                                          average='samples')\n",
    "\n",
    "knn_roc_auc_score = metrics.roc_auc_score(y_test, knn_pred, average='samples')\n",
    "\n",
    "# the below line, throws error - mnb_clf can't work on multilabel O/P\n",
    "# mnb_report = metrics.classification_report(y_test[:100], mnb_pred[:100])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "svc_report = metrics.classification_report(y_test[:], svc_pred[:])\n",
    "\n",
    "svc_f1_score = metrics.f1_score(y_test[:], svc_pred[:], average='samples') \n",
    "\n",
    "svc_precision_recall_fscore = metrics.precision_recall_fscore_support(y_test, \n",
    "                                                                      svc_pred, \n",
    "                                                                      average='samples')  \n",
    "\n",
    "svc_avg_precision_score = metrics.average_precision_score(y_test, \n",
    "                                                          svc_pred, \n",
    "                                                          average='samples')\n",
    "\n",
    "svc_roc_auc_score = metrics.roc_auc_score(y_test, svc_pred, average='samples')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"  beacuse it will also give 0.0 for the same reason discussed above. \"\"\"\n",
    "\n",
    "# I think it's (below code) same as calculating hamming_score\n",
    "# metrics.accuracy_score(y_true=y_test[:], y_pred=svc_pred[:])   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg. precision_score :  0.0108843537414966\n",
      "roc_auc_score :  0.5\n"
     ]
    }
   ],
   "source": [
    "# print(\"For svc_clf (LinearSVC) : \")\n",
    "# print(\"precision, recall, fbeta_score, support : \", svc_precision_recall_fscore)\n",
    "# print(\"f1_score : \", svc_f1_score)\n",
    "\n",
    "print(\"avg. precision_score : \", svc_avg_precision_score)\n",
    "print(\"roc_auc_score : \", svc_roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024489795918367346"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think it's same as calculating hamming_score\n",
    "metrics.accuracy_score(y_true=y_test[:], y_pred=knn_pred[:])          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For knn_clf (KNearestNeighbours) : \n",
      "precision, recall, fbeta_score, support :  (0.22176870748299318, 0.12523809523809523, 0.1462779397473275, None)\n",
      "f1_score :  0.1462779397473275\n",
      "avg. precision_score :  0.117744058662426\n",
      "roc_auc_score :  0.5620747887469787\n"
     ]
    }
   ],
   "source": [
    "# print (knn_report)                                   # its type is str\n",
    "\n",
    "print(\"For knn_clf (KNearestNeighbours) : \")\n",
    "print(\"precision, recall, fbeta_score, support : \", knn_precision_recall_fscore)\n",
    "print(\"f1_score : \", knn_f1_score)\n",
    "print(\"avg. precision_score : \", knn_avg_precision_score)\n",
    "print(\"roc_auc_score : \", knn_roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['machine-learning']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test = [\"why is overfitting bad in machine learning ?\"]\n",
    "# test = [\"what is lstm ?\"] \n",
    "\n",
    "# test_dtm = vect_title.transform(test)              # without tfidf\n",
    "test_dtm = tfidf_vect_title.transform(test)          # with tfidf\n",
    "\n",
    "# print (test_dtm.toarray()[0])\n",
    "status = False\n",
    "for i in test_dtm.toarray()[0]:\n",
    "    if (i!=0):\n",
    "        status = True\n",
    "        break\n",
    "\n",
    "ans = knn_clf.predict(test_dtm.toarray())\n",
    "ans = mlb.inverse_transform(ans)\n",
    "\n",
    "if (len(ans[0])==0 or status==False):\n",
    "    print (\"sorry, we can't predict your category!!!\")\n",
    "else:\n",
    "    ans = le.inverse_transform(ans)\n",
    "    print (ans)\n",
    "    \n",
    "    \n",
    "# mlb.transform([[224,0,100]]) \n",
    "# ans \n",
    "# test_dtm.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "           n_jobs=-1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=10, random_state=0)\n",
    "rf_clf = MultiOutputClassifier(forest, n_jobs=-1)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "\n",
    "rf_clf \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04081632653061224"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I think it's same as calculating hamming_score\n",
    "metrics.accuracy_score(y_true=y_test[:100], y_pred=rf_pred[:100])          \n",
    "\n",
    "rf_clf.score(X_test, y_test)\n",
    "\n",
    "# no such function in MultiOutputClassifier(), maybe version issue with scikit-learn\n",
    "# rf_clf.predict_log_proba(X_test)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (knn_clf.predict_proba(X_train))                                         \n",
    "\n",
    "# below code throwing error\n",
    "# print (rf_clf.predict_proba(X_train[:5])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "rf_report = metrics.classification_report(y_test[:], rf_pred[:])\n",
    "\n",
    "rf_f1_score = metrics.f1_score(y_test, rf_pred, average='samples')\n",
    "\n",
    "rf_precision_recall_fscore = metrics.precision_recall_fscore_support(y_test, \n",
    "                                                                     rf_pred, \n",
    "                                                                     average='samples') \n",
    "\n",
    "rf_avg_precision_score = metrics.average_precision_score(y_test, \n",
    "                                                         rf_pred, \n",
    "                                                         average='samples')\n",
    "\n",
    "rf_roc_auc_score = metrics.roc_auc_score(y_test, rf_pred, average='samples') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For rf_clf (RandomForest) : \n",
      "precision, recall, fbeta_score, support :  (0.27925170068027205, 0.1814965986394558, 0.2010689990281827, None)\n",
      "f1_score :  0.2010689990281827\n",
      "avg. precision_score :  0.1536765615337044\n",
      "roc_auc_score :  0.5897131745398898\n"
     ]
    }
   ],
   "source": [
    "# print (rf_report) \n",
    "\n",
    "print(\"For rf_clf (RandomForest) : \")\n",
    "print(\"precision, recall, fbeta_score, support : \", rf_precision_recall_fscore)\n",
    "print(\"f1_score : \", rf_f1_score)\n",
    "print(\"avg. precision_score : \", rf_avg_precision_score)\n",
    "print(\"roc_auc_score : \", rf_roc_auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorry, we can't predict your category!!!\n"
     ]
    }
   ],
   "source": [
    "test = [\"what is lstm ?\"] \n",
    "\n",
    "# test_dtm = vect_title.transform(test)          # without tfidf\n",
    "test_dtm = tfidf_vect_title.transform(test)      # with tfidf\n",
    "\n",
    "status = False\n",
    "for i in test_dtm.toarray()[0]:\n",
    "    if (i!=0):\n",
    "        status = True\n",
    "        break\n",
    "\n",
    "ans = rf_clf.predict(test_dtm.toarray())\n",
    "ans = mlb.inverse_transform(ans)\n",
    "if (len(ans[0])==0 or status==False):\n",
    "    print (\"sorry, we can't predict your category!!!\")\n",
    "else:\n",
    "    ans = le.inverse_transform(ans)\n",
    "    print (ans)\n",
    "    \n",
    "# mlb.transform([[224,0,100]]) \n",
    "# ans \n",
    "# test_dtm.toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## picking (saving the ML model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datascience_classifier.pkl']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(rf_clf, 'datascience_classifier.pkl')\n",
    "# new_clf = joblib.load('classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiOutputClassifier(estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False),\n",
       "           n_jobs=-1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pkl_clf = joblib.load('datascience_classifier.pkl')\n",
    "\n",
    "new_pkl_clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['machine-learning' 'predictive-modeling']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "test = [\"why is overfitting bad in machine learning ?\"] \n",
    "\n",
    "# test_dtm = vect_title.transform(test)              # without tfidf\n",
    "test_dtm = tfidf_vect_title.transform(test)          # with tfidf       \n",
    "\n",
    "status = False\n",
    "for i in test_dtm.toarray()[0]:\n",
    "    if (i!=0):\n",
    "        status = True\n",
    "        break\n",
    "        \n",
    "ans = new_pkl_clf.predict(test_dtm.toarray())\n",
    "ans = mlb.inverse_transform(ans)\n",
    "if (len(ans[0])==0 or status==False):\n",
    "    print ([[\"sorry, we can't predict your category!!!\"]]) \n",
    "else:\n",
    "    ans = le.inverse_transform(ans)\n",
    "    print (ans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
